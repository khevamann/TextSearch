{"version":3,"sources":["assets/sad.png","assets/right.png","assets/search.png","constants.ts","components/SearchBar/SearchBar.tsx","components/ChapterBtn/ChapterBtn.tsx","components/Header/Header.tsx","components/WordBubble/WordBubble.tsx","components/SideBar/SideBar.tsx","components/TextView/TextView.tsx","utils.ts","assets/texts/1.js","assets/texts/2.js","assets/texts/3.js","assets/texts/4.js","assets/texts/5.js","assets/texts/6.js","assets/texts/7.js","assets/texts/8.js","assets/texts/9.js","assets/texts/10.js","assets/texts/11.js","assets/texts/12.js","assets/texts/13.js","assets/texts/14.js","assets/texts/15.js","assets/texts/16.js","components/App/App.tsx","serviceWorker.ts","index.tsx"],"names":["module","exports","colors","SearchBar","state","searchVal","words","timer","callback","time","data","clearTimeout","this","window","setTimeout","value","target","setState","typewatch","split","filter","word","length","Array","from","Set","props","onWordChange","className","type","onChange","handleChange","bind","onFocus","event","select","placeholder","React","Component","ChapterBtn","index","onChapSelected","selected","matchNumber","chapterNum","onClick","chapSelected","undefined","Header","chapMatches","bestChap","onChapterChange","handleSelection","map","key","WordBubble","enabled","wordObj","color","backgroundColor","borderColor","opacity","toggleEnable","style","generateStyle","SideBar","wordObjs","scrollTo","topSentences","Object","keys","src","require","alt","sentence","ReactHtmlParser","text","TextView","sentences","chapter","bestChapter","obj","join","allTexts","getChapterText","getChapterSentences","score","App","currSentence","searchWords","forEach","newWord","toLowerCase","replace","runSearch","fill","chap","findAllChapMatches","arr","max","maxIndex","i","indexOfMax","sentCount","sentIndex","scoreObj","currScore","count","uniqueCount","uniqueWords","valueArr","wordComp","replacement","ending","isValidReplacement","Math","numSent","sort","sent1","sent2","slice","sent","sentNumber","span","document","getElementById","border","parent","getElementsByClassName","parentRect","getBoundingClientRect","parentViewableArea","clientHeight","spanRect","clientWidth","top","bottom","scrollTop","updateWords","getTopSentences","isLocalhost","Boolean","location","hostname","match","registerValidSW","swUrl","config","navigator","serviceWorker","register","then","registration","onupdatefound","installingWorker","installing","onstatechange","controller","console","log","onUpdate","onSuccess","catch","error","ReactDOM","render","StrictMode","URL","process","href","origin","addEventListener","fetch","headers","response","contentType","get","status","indexOf","ready","unregister","reload","checkValidServiceWorker"],"mappings":"4SAAAA,EAAOC,QAAU,87I,iBCAjBD,EAAOC,QAAU,s6B,iBCAjBD,EAAOC,QAAU,0jH,6ICIJC,G,kBAAS,CAClB,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,UACA,YCkCWC,E,4MA7CbC,MAAmB,CACjBC,UAAW,GACXC,MAAO,I,EAETC,MAAgB,E,wDACNC,EAA+BC,EAAcC,GACrDC,aAAaC,KAAKL,OAClBK,KAAKL,MAAQM,OAAOC,YAAW,WAC7BN,EAASE,KACRD,K,sCAG+D,IAAD,OAA1CM,EAA0C,EAApDC,OAAUD,MACvBH,KAAKK,SAAS,CACZZ,UAAWU,IAIbH,KAAKM,WACH,SAACH,GACC,IAAIT,EAAuBS,EAAMI,MAAM,KAAKC,QAAO,SAACC,GAClD,OAAOA,EAAKC,QDlCS,KCoCvBhB,EAAQiB,MAAMC,KAAK,IAAIC,IAAInB,IAC3B,EAAKoB,MAAMC,aAAarB,KAE1B,IACAS,K,+BAGM,IACAV,EAAcO,KAAKR,MAAnBC,UACR,OACE,2BACEuB,UAAU,YACVC,KAAK,OACLd,MAAOV,EACPyB,SAAUlB,KAAKmB,aAAaC,KAAKpB,MACjCqB,QAAS,SAACC,GAAD,OAAWA,EAAMlB,OAAOmB,UACjCC,YAAY,kB,GAxCIC,IAAMC,WCqCfC,G,mLArCG,IAAD,EAC+B3B,KAAKc,MAAzCc,EADK,EACLA,MAAOC,EADF,EACEA,eADF,EACkBC,UAE7BD,EAAeD,K,+BAIT,IAAD,EAC8C5B,KAAKc,MAAlDc,EADD,EACCA,MAAOE,EADR,EACQA,SAAUC,EADlB,EACkBA,YAAaC,EAD/B,EAC+BA,WACtC,OACE,oCACa,IAAVJ,EACC,yBACEZ,UAAS,6BAAwBc,EAAW,kBAAoB,IAChEG,QAASjC,KAAKkC,aAAad,KAAKpB,OAFlC,eAKGgC,GAAcA,GAAc,GAC3B,uBAAGhB,UAAU,kBAAb,WAAuCgB,IAI3C,yBACEhB,UAAS,iBAAYc,EAAW,kBAAoB,IACpDG,QAASjC,KAAKkC,aAAad,KAAKpB,OAFlC,WAIW4B,OACQO,IAAhBJ,GACC,uBAAGf,UAAU,kBAAkBe,EAA/B,kB,GA7BWN,IAAMC,YCwChBU,E,4MArCb5C,MAAQ,CACNsC,SAAU,EACVO,YAAa,GACbC,UAAW,G,8DAEGV,GACd5B,KAAKK,SAAS,CAAEyB,SAAUF,IAC1B5B,KAAKc,MAAMyB,gBAAgBX,K,+BAEnB,IAAD,SACyC5B,KAAKc,MAA7CC,EADD,EACCA,aAAcuB,EADf,EACeA,SAAUD,EADzB,EACyBA,YACxBP,EAAa9B,KAAKR,MAAlBsC,SACR,OACE,yBAAKd,UAAU,oBACb,kBAAC,EAAD,CAAWD,aAAcA,IACzB,yBAAKC,UAAU,kBACb,kBAAC,EAAD,CACEY,MAAO,EACPI,WAAYM,EACZR,SAAuB,IAAbA,EACVD,eAAgB7B,KAAKwC,gBAAgBpB,KAAKpB,QAE3CW,MAAMC,KAAKD,MHnCM,KGmCe8B,KAAI,SAAC3C,EAAM8B,GAAP,OACnC,kBAAC,EAAD,CACEc,IAAKd,EACLA,MAAOA,EAAQ,EACfG,YAAaM,EAAYT,GACzBE,SAAUA,IAAaF,EAAQ,EAC/BC,eAAgB,EAAKW,gBAAgBpB,KAAK,c,GA7BnCK,IAAMC,WC4BZiB,G,wNA5BbnD,MAAQ,CACNoD,QAAS,EAAK9B,MAAM+B,QAAQD,S,8DAEb,IAAD,EAGV5C,KAAKc,MADP+B,QAAWD,EAFC,EAEDA,QAASE,EAFR,EAEQA,MAEtB,OAAIF,EAAgB,CAAEG,gBAAiBD,EAAOE,YAAa,QACpD,CAAEF,MAAO,OAAQE,YAAaF,EAAOG,QAAS,S,+BAG7C,IAAD,EAIHjD,KAAKc,MAFIL,EAFN,EAELoC,QAAWpC,KACXyC,EAHK,EAGLA,aAEF,OACE,yBACElC,UAAU,aACVmC,MAAOnD,KAAKoD,gBACZnB,QAASiB,GAERzC,O,GAvBgBgB,IAAMC,Y,gBC6EhB2B,E,4MAvEb7D,MAAoB,CAClBsC,UAAW,G,uDAGH,IAAD,SACoD9B,KAAKc,MAAxDwC,EADD,EACCA,SAAUJ,EADX,EACWA,aAAcK,EADzB,EACyBA,SAAUC,EADnC,EACmCA,aAClC1B,EAAa9B,KAAKR,MAAlBsC,SACR,OACE,yBAAKd,UAAU,qBACZyC,OAAOC,KAAKJ,GAAU5C,OAAS,EAC9B,oCACG,IACD,uBAAGM,UAAU,yBAAb,uCAGA,yBAAKA,UAAU,0BACZyC,OAAOC,KAAKJ,GAAUb,KAAI,SAAChC,GAAD,OACzB,kBAAC,EAAD,CACEiC,IAAKjC,EACLoC,QAASS,EAAS7C,GAClByC,aAAY,8GAAE,kBAAMA,EAAaI,EAAS7C,aAIvB,IAAxB+C,EAAa9C,QACZ,yBAAKM,UAAU,aACb,yBACEA,UAAU,kBACV2C,IAAKC,EAAQ,IACbC,IAAI,KAEN,0CAGHL,EAAaf,KAAI,SAACqB,GAAD,OAChB,yBACE9C,UACEc,IAAagC,EAASlC,MAClB,sCACA,oBAENc,IAAKoB,EAASlC,MACdK,QAAS,WACP,EAAK5B,SAAS,CAAEyB,SAAUgC,EAASlC,QACnC2B,EAASO,EAASlC,SAGpB,2BAAImC,IAAgBD,EAASE,OAC7B,yBACEH,IAAI,GACJ7C,UAAU,aACV2C,IAAKC,EAAQ,WAMrB,yBAAK5C,UAAU,aACb,yBACEA,UAAU,kBACV2C,IAAKC,EAAQ,IACbC,IAAI,KAEN,uD,GAhEUpC,IAAMC,W,MCSbuC,MAff,YAAmE,IAA/CC,EAA8C,EAA9CA,UAAWC,EAAmC,EAAnCA,QAASC,EAA0B,EAA1BA,YACtC,OACE,yBAAKpD,UAAU,sBACb,uCAAyB,IAAZmD,EAAgBC,EAAcD,GAC3C,2BACGJ,IACCG,EACGzB,KAAI,SAAC4B,GAAD,mCAA+BA,EAAIzC,MAAnC,aAA6CyC,EAAIL,KAAjD,cACJM,KAAK,SCFZC,EAAW,CCjBA,0n1ECAA,gtnHCAA,w+uFCAA,sv6HCAA,0xtHCAA,+5mHCAA,i68ICAA,23tICAA,m2iICAA,u/nICAA,00sICAA,yj2HCAA,m0iKCAA,q13ICAA,2uuICAA,klqJhBoCV,SAASC,EAAeL,GAC7B,OAAIA,EAAU,IAAMA,EAAU,EAAU,GACjCI,EAASJ,EAAU,GAAG5D,MAAM,KAG9B,SAASkE,EAAoBN,GAClC,OAAIA,EAAU,IAAMA,EAAU,EAAU,GACjCK,EAAeL,GAAS1B,KAAI,SAACuB,EAAMpC,GAAP,MAAkB,CACnD8C,MAAO,EACPV,KAAMA,EACNpC,Y,IiBqKW+C,E,4MArLbnF,MAAoB,CAClB0E,UAAW,GACXZ,SAAU,GACVa,QAAS,EACTS,cAAe,EACftC,SAAU,EACVD,YAAa,I,kEAGbrC,KAAKK,SAAS,CAAE6D,UAAWO,EAAoB,O,kCAGrCI,GACV,IAAIvB,EAAgB,GAEpBuB,EAAYC,SAAQ,SAACrE,EAAcmB,GACjC,IAAImD,EAAUtE,EAAKuE,cAAcC,QAAQ,WAAY,IACrD3B,EAASyB,GAAW,CAClBjC,MAAOxD,EAAOsC,EAAQtC,EAAOoB,QAC7BD,KAAMsE,EACNnC,SAAS,MAIb5C,KAAKK,SAAS,CAAEiD,aAEhBtD,KAAKkF,c,gCAGGlD,GAAsB,IAAD,EACDhC,KAAKR,MAA3B8D,EADuB,EACvBA,SAAUa,EADa,EACbA,QAEhB,GAAqC,IAAjCV,OAAOC,KAAKJ,GAAU5C,OAA1B,CAOAyD,OACiBhC,IAAfH,GAA4BA,GAAc,EAAIA,EAAamC,EAE7D,IAAI9B,EjBZD,SAA4BiB,GACjC,IAAIjB,EAAc,IAAI1B,MAAM,IAAIwE,KAAK,GAMrC,OALAZ,EAASO,SAAQ,SAACM,EAAMxD,GACtBwD,EAAK7E,MAAM,KAAKuE,SAAQ,SAACrE,GACnB6C,EAAS7C,IAAO4B,EAAYT,WAG7BS,EiBKagD,CAAmB/B,GACjChB,EjBcD,SAAoBgD,GAIzB,IAHA,IAAIC,EAAMD,EAAI,GACVE,EAAW,EAENC,EAAI,EAAGA,EAAIH,EAAI5E,OAAQ+E,IAC1BH,EAAIG,GAAKF,IACXC,EAAWC,EACXF,EAAMD,EAAIG,IAId,OAAOD,EiBzBUE,CAAWrD,GAAe,EACzB,IAAZ8B,IACFA,EAAU7B,GAGZ,IAAIqD,EAA6BnB,EAAeL,GAAS1B,KACvD,SAACtC,EAAeyF,GACd,IjB9BuBC,EiB8BnBC,EAA2B,CAC7BC,MAAO,EACPC,YAAa,EACbtF,OAAQP,EAAMO,QAEZuF,EAAmB,GAGnBC,EAAW/F,EAAMI,MAAM,KAAKkC,KAAI,SAAChC,GACnC,IAAI0F,EAAW1F,EAAKuE,cAAcC,QAAQ,WAAY,IAClDmB,EjBpBP,SAA4B9C,EAAe6C,GAChD,GAAI7C,EAAS6C,GACX,OAAK7C,EAAS6C,GAAUvD,QACjBuD,EADiC,GAG1C,cAAgB1C,OAAOC,KAAKJ,GAA5B,eAEE,IAFG,IAAIZ,EAAG,KAEV,MADc,CAAC,IAAK,KAAM,IAAK,MAC/B,eAA4B,CAAvB,IAAI2D,EAAM,KACb,GAAI3D,IAAQyD,EAAWE,GAAU3D,EAAM2D,IAAWF,EAChD,OAAK7C,EAASZ,GAAKE,QACZF,EAD4B,GAKzC,MAAO,GiBMmB4D,CAAmBhD,EAAU6C,GAC/C,MAAoB,KAAhBC,GAEGH,EAAYG,KACfN,EAAUE,cACVC,EAAYG,IAAe,GAE7BN,EAAUC,QAGJ,kDAAN,OAAyDzC,EAAS8C,GAAatD,MAA/E,aAAyFrC,EAAzF,cAEKA,KAILiE,GjBxDmBmB,EiBwDIC,GjBvDpBpF,OAAS,IACZ,EAEH6F,KAAKhB,IACO,GAAjBM,EAASE,MAAoC,GAAvBF,EAASG,YAAmBH,EAASnF,OAAS,GACpE,GiBoDI,MAAO,CACLsD,KAAMkC,EAAS5B,KAAK,KACpB1C,MAAOgE,EACPlB,YAKN1E,KAAKK,SAAS,CAAE6D,UAAWyB,EAAWtD,cAAaC,uBArD9BH,IAAfH,GAA4BA,IAAemC,GAC7CnE,KAAKK,SAAS,CAAE6D,UAAWO,EAAoBzC,O,sCAuDrCwE,GAAkB,IACxBtC,EAAclE,KAAKR,MAAnB0E,UAGR,OAFc,YAAOA,GAGlBuC,MAAK,SAACC,EAAiBC,GAAlB,OAAsCA,EAAMjC,MAAQgC,EAAMhC,SAC/DkC,MAAM,EAAGJ,GACThG,QAAO,SAACqG,GAAD,OAAUA,EAAKnC,MAAQ,O,+BAG1BoC,GAAqB,IACtBlC,EAAiB5E,KAAKR,MAAtBoF,aACN,GAAIA,GAAgB,EAAG,CACrB,IAAImC,EAAOC,SAASC,eAAT,mBAAoCrC,IACnC,MAARmC,IACFA,EAAK5D,MAAM+D,OAAS,QAGxB,IAAIH,EAAOC,SAASC,eAAT,mBAAoCH,IAC3CK,EAASH,SAASI,uBAAuB,sBAAsB,GACnE,GAAa,OAATL,EAAe,OAAO,EAE1B,IAAIM,EAAaF,EAAOG,wBACpBC,EACMJ,EAAOK,aAKbC,GAJKN,EAAOO,YAIDX,EAAKO,yBAGlBG,EAASE,KAAON,EAAWM,KAC3BF,EAASG,QAAUP,EAAWM,IAAMJ,IAKpCJ,EAAOU,UACLJ,EAASE,IACTR,EAAOU,UACPR,EAAWM,IAAM,EACjBJ,EAA4B,GAGhCR,EAAK5D,MAAM+D,OAAS,gBAEpBlH,KAAKK,SAAS,CAAEuE,aAAckC,M,sCAGhBlF,GACd5B,KAAKK,SAAS,CAAE8D,QAASvC,IACzB5B,KAAKkF,UAAUtD,K,mCAGJiB,GAAqB,IAC1BS,EAAatD,KAAKR,MAAlB8D,SACNA,EAAST,EAAQpC,MAAMmC,SAAWU,EAAST,EAAQpC,MAAMmC,QAEzD5C,KAAKK,SAAS,CAAEiD,aAEhBtD,KAAKkF,c,+BAGG,IAAD,EACyDlF,KAAKR,MAA7D8D,EADD,EACCA,SAAUY,EADX,EACWA,UAAW5B,EADtB,EACsBA,SAAUD,EADhC,EACgCA,YAAa8B,EAD7C,EAC6CA,QACpD,OACE,yBAAKnD,UAAU,iBACb,kBAAC,EAAD,CACEsB,SAAUA,EACVD,YAAaA,EACbtB,aAAcf,KAAK8H,YAAY1G,KAAKpB,MACpCuC,gBAAiBvC,KAAKuC,gBAAgBnB,KAAKpB,QAE7C,yBAAKgB,UAAU,eACb,kBAAC,EAAD,CACEsC,SAAUA,EACVE,aAAcxD,KAAK+H,gBxBlMF,IwBmMjB7E,aAAclD,KAAKkD,aAAa9B,KAAKpB,MACrCuD,SAAUvD,KAAKuD,SAASnC,KAAKpB,QAE/B,kBAAC,EAAD,CACEkE,UAAWA,EACXC,QAASA,EACTC,YAAa9B,U,GA9KPb,IAAMC,WCjBlBsG,EAAcC,QACW,cAA7BhI,OAAOiI,SAASC,UAEe,UAA7BlI,OAAOiI,SAASC,UAEhBlI,OAAOiI,SAASC,SAASC,MACvB,2DA8CN,SAASC,EAAgBC,EAAeC,GACtCC,UAAUC,cACPC,SAASJ,GACTK,MAAK,SAAAC,GACJA,EAAaC,cAAgB,WAC3B,IAAMC,EAAmBF,EAAaG,WACd,MAApBD,IAGJA,EAAiBE,cAAgB,WACA,cAA3BF,EAAiBtJ,QACfgJ,UAAUC,cAAcQ,YAI1BC,QAAQC,IACN,iHAKEZ,GAAUA,EAAOa,UACnBb,EAAOa,SAASR,KAMlBM,QAAQC,IAAI,sCAGRZ,GAAUA,EAAOc,WACnBd,EAAOc,UAAUT,WAO5BU,OAAM,SAAAC,GACLL,QAAQK,MAAM,4CAA6CA,MClGjEC,IAASC,OACP,kBAAC,IAAMC,WAAP,KACE,kBAAC,EAAD,OAEF1C,SAASC,eAAe,SDiBnB,SAAkBsB,GACvB,GAA6C,kBAAmBC,UAAW,CAMzE,GAJkB,IAAImB,IACpBC,eACA3J,OAAOiI,SAAS2B,MAEJC,SAAW7J,OAAOiI,SAAS4B,OAIvC,OAGF7J,OAAO8J,iBAAiB,QAAQ,WAC9B,IAAMzB,EAAK,UAAMsB,eAAN,sBAEP5B,IAgEV,SAAiCM,EAAeC,GAE9CyB,MAAM1B,EAAO,CACX2B,QAAS,CAAE,iBAAkB,YAE5BtB,MAAK,SAAAuB,GAEJ,IAAMC,EAAcD,EAASD,QAAQG,IAAI,gBAEnB,MAApBF,EAASG,QACO,MAAfF,IAA8D,IAAvCA,EAAYG,QAAQ,cAG5C9B,UAAUC,cAAc8B,MAAM5B,MAAK,SAAAC,GACjCA,EAAa4B,aAAa7B,MAAK,WAC7B1I,OAAOiI,SAASuC,eAKpBpC,EAAgBC,EAAOC,MAG1Be,OAAM,WACLJ,QAAQC,IACN,oEAvFAuB,CAAwBpC,EAAOC,GAI/BC,UAAUC,cAAc8B,MAAM5B,MAAK,WACjCO,QAAQC,IACN,iHAMJd,EAAgBC,EAAOC,OC1C/BE,K","file":"static/js/main.d7f5478a.chunk.js","sourcesContent":["module.exports = \"data:image/png;base64,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\"","module.exports = \"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAQAAAAAYLlVAAAABGdBTUEAALGPC/xhBQAAACBjSFJNAAB6JgAAgIQAAPoAAACA6AAAdTAAAOpgAAA6mAAAF3CculE8AAAAAmJLR0QAAKqNIzIAAAAJcEhZcwAADsQAAA7EAZUrDhsAAAAHdElNRQfkAx4DDjQLFs4qAAABd0lEQVRo3u3Zr2vWURwF4GdzKBNBREEWBobBwLJgMSyYDKYVg2MgQ7C5siSaBq6YbIJaRIPFZDFosFgEwwQFgzAQhkFexhBl23v8E940jqg33/DA98c993wYtR4auDpy1wGugdhzpQe4Zih2LfQIN0T8cqlHuCXihws9wh0RO+Z7hLsiBs61AGPui/jmbIsw7omILbMtwiHPRGw60yIc9kLEZ1MtwqTXIjacbBGOeiPivRMtwnHvRLx1rEU45YOIVyZbhNM+iXjpSIsw7YuI5yZahBlfRTw23iLM2hLxyFiLMOe7iHstAOdti1jrEebtiLjZI1z0U8Rqj7BgVwxd7xGW7It9iz3CsqHYc7lHWOlH+Nv9CL/+P8L/GRH+qYiPozf+lav+CMovYfkzLP+Iyr/i8mFUPo7LgaQcycqhtBzLyxeT8tWsfDktX8/LBUW5oimXVOWarlxUlqvaclldruv/9ahdHlqVx3blwWV9dFsfXj842PH9b+4E4xJoxnNwAAAAJXRFWHRkYXRlOmNyZWF0ZQAyMDIwLTAzLTMwVDAzOjE0OjUyKzAwOjAwVoC35gAAACV0RVh0ZGF0ZTptb2RpZnkAMjAyMC0wMy0zMFQwMzoxNDo1MiswMDowMCfdD1oAAAAZdEVYdFNvZnR3YXJlAHd3dy5pbmtzY2FwZS5vcmeb7jwaAAAAAElFTkSuQmCC\"","module.exports = \"data:image/png;base64,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\"","export const MIN_WORD_LENGTH = 4;\nexport const NUM_CHAPTERS = 16;\nexport const NUM_SENTENCES = 10;\n\nexport const colors = [\n    \"#45777c\",\n    \"#92374D\",\n    \"#542E71\",\n    \"#FF5666\",\n    \"#2B59C3\",\n    \"#C14953\",\n    \"#2E5339\",\n    \"#388697\",\n    \"#CC2936\",\n    \"#314CB6\",\n    \"#B28B84\",\n    \"#392759\",\n    \"#7FB069\",\n    \"#08415C\",\n    \"#91785D\",\n    \"#F08700\",\n    \"#9297C4\",\n    \"#6B7FD7\",\n    \"#EB9486\",\n    \"#00A6A6\",\n];\n\nexport interface WordObj {\n    color: string;\n    word: string;\n    enabled: boolean;\n}\nexport interface Sentence {\n    index: number;\n    score: number;\n    text: string;\n}\n\nexport interface SentenceScore {\n    count: number;\n    uniqueCount: number;\n    length: number;\n}\n","import React, { ChangeEvent } from \"react\";\nimport \"./SearchBar.css\";\nimport { MIN_WORD_LENGTH } from \"../../constants\";\n\ninterface StateType {\n  searchVal: string;\n  words: Array<string>;\n}\ninterface PropType {\n  onWordChange(words: Array<string>): void;\n}\n\nclass SearchBar extends React.Component<PropType> {\n  state: StateType = {\n    searchVal: \"\",\n    words: [],\n  };\n  timer: number = 0;\n  typewatch(callback: (e: string) => void, time: number, data: string) {\n    clearTimeout(this.timer);\n    this.timer = window.setTimeout(() => {\n      callback(data);\n    }, time);\n  }\n\n  handleChange({ target: { value } }: ChangeEvent<HTMLInputElement>) {\n    this.setState({\n      searchVal: value,\n    });\n\n    // typewatch() will wait 500 ms before triggering the onWordChange event to avoid searching every letter press\n    this.typewatch(\n      (value) => {\n        let words: Array<string> = value.split(\" \").filter((word) => {\n          return word.length >= MIN_WORD_LENGTH;\n        });\n        words = Array.from(new Set(words));\n        this.props.onWordChange(words);\n      },\n      500,\n      value\n    );\n  }\n  render() {\n    const { searchVal } = this.state;\n    return (\n      <input\n        className=\"SearchBar\"\n        type=\"text\"\n        value={searchVal}\n        onChange={this.handleChange.bind(this)}\n        onFocus={(event) => event.target.select()}\n        placeholder=\"Search...\"\n      />\n    );\n  }\n}\n\nexport default SearchBar;\n","import React from \"react\";\nimport \"./ChapterBtn.css\";\n\ninterface PropType {\n  index: number;\n  selected: boolean;\n  matchNumber?: number;\n  chapterNum?: number;\n  onChapSelected(index: number): void;\n}\n\nclass ChapterBtn extends React.Component<PropType> {\n  chapSelected() {\n    const { index, onChapSelected, selected } = this.props;\n    if (!selected) {\n      onChapSelected(index);\n    }\n  }\n\n  render() {\n    const { index, selected, matchNumber, chapterNum } = this.props;\n    return (\n      <>\n        {index === 0 ? (\n          <div\n            className={`Button Button_Best ${selected ? \"Button_Selected\" : \"\"}`}\n            onClick={this.chapSelected.bind(this)}\n          >\n            Best Chapter\n            {chapterNum && chapterNum >= 0 && (\n              <p className=\"Button_Matches\">Chapter {chapterNum}</p>\n            )}\n          </div>\n        ) : (\n          <div\n            className={`Button ${selected ? \"Button_Selected\" : \"\"}`}\n            onClick={this.chapSelected.bind(this)}\n          >\n            Chapter {index}\n            {matchNumber !== undefined && (\n              <p className=\"Button_Matches\">{matchNumber} matches</p>\n            )}\n          </div>\n        )}\n      </>\n    );\n  }\n}\n\nexport default ChapterBtn;\n","import React from \"react\";\nimport \"./Header.css\";\nimport SearchBar from \"../SearchBar/SearchBar\";\nimport ChapterBtn from \"../ChapterBtn/ChapterBtn\";\nimport { NUM_CHAPTERS } from \"../../constants\";\n\ninterface PropType {\n  onWordChange(words: Array<string>): void;\n  onChapterChange(index: number): void;\n  bestChap: number;\n  chapMatches: Array<number>;\n}\n\nclass Header extends React.Component<PropType> {\n  state = {\n    selected: 0,\n    chapMatches: [],\n    bestChap: -1,\n  };\n  handleSelection(index: number) {\n    this.setState({ selected: index });\n    this.props.onChapterChange(index);\n  }\n  render() {\n    const { onWordChange, bestChap, chapMatches } = this.props;\n    const { selected } = this.state;\n    return (\n      <div className=\"Header_Container\">\n        <SearchBar onWordChange={onWordChange} />\n        <div className=\"Header_Buttons\">\n          <ChapterBtn\n            index={0}\n            chapterNum={bestChap}\n            selected={selected === 0}\n            onChapSelected={this.handleSelection.bind(this)}\n          />\n          {Array.from(Array(NUM_CHAPTERS)).map((data, index) => (\n            <ChapterBtn\n              key={index}\n              index={index + 1}\n              matchNumber={chapMatches[index]}\n              selected={selected === index + 1}\n              onChapSelected={this.handleSelection.bind(this)}\n            />\n          ))}\n        </div>\n      </div>\n    );\n  }\n}\n\nexport default Header;\n","import React from \"react\";\nimport \"./WordBubble.css\";\n\ninterface Props {\n  wordObj: {\n    color: string;\n    word: string;\n    enabled: boolean;\n  };\n  toggleEnable(): void\n}\n\nclass WordBubble extends React.Component<Props> {\n  state = {\n    enabled: this.props.wordObj.enabled,\n  };\n  generateStyle() {\n    const {\n      wordObj: { enabled, color },\n    } = this.props;\n    if (enabled) return { backgroundColor: color, borderColor: '#ddd', };\n    return { color: '#000', borderColor: color, opacity: '50%'};\n  }\n\n  render() {\n    const {\n      wordObj: { word },\n      toggleEnable,\n    } = this.props;\n    return (\n      <div\n        className=\"WordBubble\"\n        style={this.generateStyle()}\n        onClick={toggleEnable}\n      >\n        {word}\n      </div>\n    );\n  }\n}\n\nexport default WordBubble;\n","import React from \"react\";\nimport \"./SideBar.css\";\nimport { Sentence, WordObj } from \"../../constants\";\nimport WordBubble from \"../WordBubble/WordBubble\";\nimport ReactHtmlParser from \"react-html-parser\";\n\ninterface PropTypes {\n  wordObjs: any;\n  toggleEnable(wordObj: WordObj): void;\n  scrollTo(sentNum: number): void;\n  topSentences: Array<Sentence>;\n}\n\ninterface StateTypes {\n  selected: number;\n}\n\nclass SideBar extends React.Component<PropTypes> {\n  state: StateTypes = {\n    selected: -1,\n  };\n\n  render() {\n    const { wordObjs, toggleEnable, scrollTo, topSentences } = this.props;\n    const { selected } = this.state;\n    return (\n      <div className=\"SideBar_Container\">\n        {Object.keys(wordObjs).length > 0 ? (\n          <>\n            {\" \"}\n            <p className=\"SideBar_ToggleWordMsg\">\n              Select words to remove from search:\n            </p>\n            <div className=\"SideBar_ColorContainer\">\n              {Object.keys(wordObjs).map((word: string) => (\n                <WordBubble\n                  key={word}\n                  wordObj={wordObjs[word]}\n                  toggleEnable={() => toggleEnable(wordObjs[word])}\n                />\n              ))}\n            </div>\n            {topSentences.length === 0 && (\n              <div className=\"NoResults\">\n                <img\n                  className=\"NoResults_Image\"\n                  src={require(\"../../assets/sad.png\")}\n                  alt=\"\"\n                />\n                <p>No Results</p>\n              </div>\n            )}\n            {topSentences.map((sentence: Sentence) => (\n              <div\n                className={\n                  selected === sentence.index\n                    ? \"SentenceContainer Sentence_Selected\"\n                    : \"SentenceContainer\"\n                }\n                key={sentence.index}\n                onClick={() => {\n                  this.setState({ selected: sentence.index });\n                  scrollTo(sentence.index);\n                }}\n              >\n                <p>{ReactHtmlParser(sentence.text)}</p>\n                <img\n                  alt=\"\"\n                  className=\"RightArrow\"\n                  src={require(\"../../assets/right.png\")}\n                />\n              </div>\n            ))}\n          </>\n        ) : (\n          <div className=\"NoResults\">\n            <img\n              className=\"NoResults_Image\"\n              src={require(\"../../assets/search.png\")}\n              alt=\"\"\n            />\n            <p>Start Searching...</p>\n          </div>\n        )}\n      </div>\n    );\n  }\n}\n\nexport default SideBar;\n","import React from \"react\";\nimport ReactHtmlParser from \"react-html-parser\";\nimport \"./TextView.css\";\nimport { Sentence } from \"../../constants\";\n\ninterface PropTypes {\n  sentences: Array<Sentence>;\n  chapter: number;\n  bestChapter: number;\n}\n\nfunction TextView({ sentences, chapter, bestChapter }: PropTypes) {\n  return (\n    <div className=\"TextView_Container\">\n      <h2>Chapter {chapter === 0 ? bestChapter : chapter}</h2>\n      <p>\n        {ReactHtmlParser(\n          sentences\n            .map((obj) => `<span id=\"sentence_${obj.index}\">${obj.text}</span>`)\n            .join(\".\")\n        )}\n      </p>\n    </div>\n  );\n}\n\nexport default TextView;\n","import { text as text1 } from \"./assets/texts/1\";\nimport { text as text2 } from \"./assets/texts/2\";\nimport { text as text3 } from \"./assets/texts/3\";\nimport { text as text4 } from \"./assets/texts/4\";\nimport { text as text5 } from \"./assets/texts/5\";\nimport { text as text6 } from \"./assets/texts/6\";\nimport { text as text7 } from \"./assets/texts/7\";\nimport { text as text8 } from \"./assets/texts/8\";\nimport { text as text9 } from \"./assets/texts/9\";\nimport { text as text10 } from \"./assets/texts/10\";\nimport { text as text11 } from \"./assets/texts/11\";\nimport { text as text12 } from \"./assets/texts/12\";\nimport { text as text13 } from \"./assets/texts/13\";\nimport { text as text14 } from \"./assets/texts/14\";\nimport { text as text15 } from \"./assets/texts/15\";\nimport { text as text16 } from \"./assets/texts/16\";\nimport { Sentence, SentenceScore } from \"./constants\";\nconst allTexts = [\n  text1,\n  text2,\n  text3,\n  text4,\n  text5,\n  text6,\n  text7,\n  text8,\n  text9,\n  text10,\n  text11,\n  text12,\n  text13,\n  text14,\n  text15,\n  text16,\n];\n\nexport function getChapterText(chapter: number): Array<string> {\n  if (chapter > 16 || chapter < 1) return [];\n  return allTexts[chapter - 1].split(\".\");\n}\n\nexport function getChapterSentences(chapter: number): Array<Sentence> {\n  if (chapter > 16 || chapter < 1) return [];\n  return getChapterText(chapter).map((text, index) => ({\n    score: 0,\n    text: text,\n    index,\n  }));\n}\n\nexport function calculateScore(scoreObj: SentenceScore): number {\n  if (scoreObj.length < 30) {\n    return -1;\n  }\n  return Math.max(\n    scoreObj.count * 10 + scoreObj.uniqueCount * 50 - scoreObj.length / 10,\n    0\n  );\n}\n\nexport function findAllChapMatches(wordObjs: any): Array<number> {\n  let chapMatches = new Array(16).fill(0);\n  allTexts.forEach((chap, index) => {\n    chap.split(\" \").forEach((word) => {\n      if (wordObjs[word]) chapMatches[index]++;\n    });\n  });\n  return chapMatches;\n}\n\nexport function isValidReplacement(wordObjs: any, wordComp: string): string {\n  if (wordObjs[wordComp]) {\n    if (!wordObjs[wordComp].enabled) return \"\";\n    return wordComp;\n  }\n  for (let key of Object.keys(wordObjs)) {\n    let endings = [\"s\", \"ed\", \"d\", \"es\"];\n    for (let ending of endings) {\n      if (key === wordComp + ending || key + ending === wordComp) {\n        if (!wordObjs[key].enabled) return \"\";\n        return key;\n      }\n    }\n  }\n  return \"\";\n}\n\nexport function indexOfMax(arr: Array<number>): number {\n  let max = arr[0];\n  let maxIndex = 0;\n\n  for (let i = 1; i < arr.length; i++) {\n    if (arr[i] > max) {\n      maxIndex = i;\n      max = arr[i];\n    }\n  }\n\n  return maxIndex;\n}\n","export const text = `Taste buds contained in the papillae of the tongue are far more responsive to bitter tastes than to sweet tastes. Studying the Science of Psychology can lead you to see yourself and other people in completely new ways. Hundreds of years ago, people believed that the world was flat and the Sun and stars circled the Earth. Careful scientific research slowly dispelled these inaccurate notions. Nonetheless, we hold tightly to many equally false commonsense beliefs about the human mind and behavior. We all know that opposites attract, but we also know that birds of a feather flock together—so why do we need psychology to tell us what we already know? The problem is that both statements cannot be true at the same time, so the real state of affairs is neither obvious nor simple. Just as careful science was required to understand our planet’s place in the universe, the same scientific techniques are providing us with a more accurate, complete view of the human mind. Let’s begin with a seemingly simple and familiar example: our ability to taste. We know a lot about taste—what we like or dislike, the different qualities of taste, and so on. Most of us can taste sweetness in a solution made of 1 part sugar and 200 parts water. As remarkable as this sensitivity appears to be, however, people can detect 1 part bitter substance (like quinine or the chemicals in broccoli) in 2 million parts water. This contrast in taste sensitivity between sweet and bitter does not reflect the actual difference between sweet and bitter substances—that is, bitter tastes are not 10,000 times stronger than sweet tastes—but rather how we experience them. Why would we have such a vast difference in sensitivity between these types of tastes? Our personal experience of taste does not help us much in answering this question, but psychological science can. As it turns out, our greater sensitivity to bitter tastes is highly adaptive: Most poisons or toxins taste bitter, and if you want to stay alive, it is more important to avoid swallowing poison than to enjoy something sweet. Being far more sensitive to tastes that are bitter is a trait that has served our species well because it helps us avoid eating things that could kill us. Psychology helps us understand why we do the things we do by providing a context for understanding the mind and behavior. Introspection is the personal observation of our own thoughts, feelings, and behaviors. Because we are not perfect observers of the operations of our own minds, psychologists developed other methods that provide scientific insight into the mind. In this functional magnetic resonance imaging (fMRI) scan, areas of the brain that were more active when participants were hungry than when they were full are highlighted. Through technology, researchers can better understand how the brain regulates hunger. To gain that understanding, psychology addresses questions from multiple scientific perspectives. One can think of this like the zoom feature in Google Earth. In some parts of this textbook, we will zoom in on human behaviors, like looking at the highly magnified image of the papillae on the tongue (pictured on page 2), which allow us to taste, and trace the messages about taste sent from the tongue to the brain. At other times, we’ll zoom out, to take in the larger picture and better understand why the boy on the previous page is giving his bitter-tasting broccoli a skeptical look. Psychologists approach the study of mind using various in-depth perspectives, which will be described in this chapter. For example, we can look at the little boy’s reaction to his broccoli from a developmental perspective, which tells us that taste sensitivity decreases over the life span. Using a biological perspective, we can determine the neural mechanisms responsible for the difference in taste sensitivity. Or, using the social perspective, we can think about social influences like culture on food preferences. Cottage cheese, enjoyed by many Americans, is viewed with disgust in some other parts of the world. Meanwhile, fruit bat pie, a delicacy in Palau, might not be a popular item in the United States. Although single perspectives can tell us a lot about a phenomenon like our sensitivity to bitter tastes, no one perspective can give us a complete answer. The best view comes from putting multiple perspectives together. You can learn a lot about your house by zooming in on it in Google Earth, but when you see how your home fits into the larger context of city, state, country, and planet, that viewpoint adds something special to your understanding. We’ll start by learning more about psychology’s main perspectives, along with a little background about their origins. Our approach to these perspectives is consistent with recent recommendations for teaching introductory psychology made by the American Psychological Association’s Board of Educational Affairs (Gurung et al., 2014). Once we understand these perspectives, we’ll be in a better position to understand how they come together to give us the big picture. The study of the mind The brain and its activities, including thought, emotion, and behavior.        mind        The brain and its activities, including thought, emotion, and behavior. is as fascinating as it is complex. Psychological scientists view the mind as a way of talking about the activities of the brain, including thought, emotion, and behavior. A quick look at this textbook’s table of contents will show you the variety of approaches to mind that you will encounter, such as the thinking mind (cognitive psychology) and the troubled mind (abnormal psychology). The word psychology is a combination of two Greek words: psyche (or psuche), or soul, and logos, meaning the objective study of. For the ancient Greeks, a soul was close to our modern view of a spirit or mind. Logos is the source of all our ologies, such as biology and anthropology. Literally translated, therefore, psychologypsychologyThe scientific study of behavior, mental processes, and brain functions.        psychology        The scientific study of behavior, mental processes, and brain functions. means the objective study of the mind. Contemporary definitions of psychology refine and update this basic meaning. Today’s psychologists define their field as the scientific study of behavior, mental processes, and brain functions—that is, the scientific study of the mind. Increased recognition that the brain is the organ of the mind has led many psychology departments to expand their names to the Department of Psychological and Brain Sciences (or the equivalent). The phrase behavior, mental processes, and brain functions has undergone several changes over the history of psychology. Behavior refers to any action that we can observe. As we will see in Chapter 2, observation has been an important tool for psychologists from the early days of the discipline. Our definition does not specify whose behavior is to be examined. Although the bulk of psychology focuses on human behavior, animal behavior has been an essential part of the discipline, both for understanding animals better and for comparing and contrasting animal and human behavior. The study of both mental processes and brain functions has been highly dependent on the methods available to psychologists. Early efforts to study mental processes were generally unsatisfactory because they relied on the use of introspectionintrospectionA personal observation of your own thoughts, feelings, and behavior.        introspection        A personal observation of your own thoughts, feelings, and behavior., or the personal observation of your own thoughts, feelings, and behaviors. Because it is difficult for others to confirm your introspections, this subjective approach does not lend itself well to the scientific method. If you say that you are feeling hungry, how can anyone else know whether your observation is accurate? In addition, your mind and behavior are governed by a host of structures, factors and processes, most of which are not available through introspection. Innovations in the methods and mathematics used to investigate brain activity and behavior have allowed psychologists to revisit the question of mental processes and brain functions with greater objectivity and success. Most readers of this book are not pursuing careers in psychology, so how will this material help you in your chosen career? Psychology is all about people, and nearly all occupations require an understanding of people and their behavior. An architect cannot design a functional space without considering how people respond to being crowded. An attorney cannot cross-examine a witness without an understanding of memory, motivation, emotion, and stress. A teacher cannot encourage students to reach their potential without an understanding of child development and learning. Business leaders and economists cannot predict the movements of markets without understanding the minds making the relevant decisions. The study of psychology, then, provides you with better insight into and understanding of many occupations and fields of study. You probably have seen applications that allow you to map your friendship networks on social media, with shorter links indicating greater connectivity and larger bubbles indicating more overlapping friendships with another person. Kevin Boyack and his colleagues generated a similar map of the sciences (see Figure 1.1) but used reference lists in journal articles instead of friendship networks (Boyack, Klavans, & Börner, 2005). The resulting map shows the extent to which each of the sciences are influential and what other sciences they most influence. Boyack and colleagues referred to the most influential sciences as hub sciences. Their analysis shows that psychology is one of the seven major hub sciences, with strong connections to the medical sciences, the social sciences, and education. In the upcoming chapters of this book, we will highlight these connections with examples that are relevant to each particular chapter. This map of science was generated by comparing citations from more than 1 million papers published in more than 7,000 journals since 2000. Psychology appears among the seven major areas of science, indicated in the map by a different font. The other six major areas are social sciences, mathematics, physics, chemistry, earth sciences, and medicine. The empiricist philosophers had a profound influence on the foundations of American political thought—that all of us are created equal. For generations, Europe had been ruled by people who were born into positions of power instead of earning the privilege of leading through hard work and education. If knowledge is not innate or inborn, any of us can learn enough to grow up to be president. Psychology is a relatively young discipline, dating back only to the 1870s. However, topics that interest modern psychologists go back farther in the history of human thought. People living as long ago as 6000 to 5000 bce in Assyria described their dreams (Restak, 1988). Among these accounts are descriptions of being chased, which are still among the most common dreams that people experience (Nielsen et al., 2003). See Figure 1.2 for common dream themes. Although we don’t understand why we dream about certain things, many people report similar themes in their dreams. The psychology family tree includes two major roots: philosophyphilosophyThe discipline that systematically examines basic concepts, including the source of knowledge.        philosophy        The discipline that systematically examines basic concepts, including the source of knowledge. and the natural sciences Sciences that study the physical and biological events that occur in nature.        natural sciences        Sciences that study the physical and biological events that occur in nature.. Psychologists answer questions traditionally posed by philosophers by borrowing the methods of the natural sciences. We examine scientific methods in detail in Chapter 2. Philosophers and psychologists share an interest in questions regarding the nature of the self, the effects of early experience, the existence of free will, and the origin of knowledge. Both disciplines consider the relative balance of biological factors (nature) and environmental factors (nurture) in the resulting human behavior. Both attempt to determine the relationships between self-interest and community welfare, between body and mind, and between humans and other species with which we share the planet. Although we typically consider questions of the unconscious mind and abnormal behavior to be the realm of the psychologist, philosophers investigated these issues thousands of years before the first psychologist was born. One of the most significant questions shared by philosophy and psychology asks whether the mind is inborn or is formed through experience. (a) Philosophers beginning with Aristotle (384–322 bce) believed that all knowledge is gained through sensory experience. (b) Beginning in the 17th century, this idea flourished in the British philosophical school of empiricism. Empiricists, like John Locke, viewed the mind as a blank slate at birth, which then was filled with ideas gained by observing the world. (c) Contemporary psychologists believe that experience interacts with inborn characteristics to shape the mind. Intelligence, for example, is influenced by both genetics and experience. During the 1970s, Romanian orphans adopted at young ages recovered from the effects of their seriously deprived social circumstances, but those who endured years of deprivation had more severe cognitive deficits (Ames, 1997). Running along a parallel track to the early philosophers, ancient physicians were laying the foundation of our biological knowledge of the brain and nervous system, discussed in greater detail in Chapter 4. During this pursuit, physicians helped develop the scientific methods that would become central to contemporary psychology and previewed the application of the knowledge that they gained to the improvement of individual well-being. Ancient people might have attempted to cure headaches, seizures, or psychological disorders by drilling holes in the skull. Bone growth around the hole indicates that some patients survived the procedure. Until fairly recently, the whole of medicine remained a primitive business. Beginning in the 17th and 18th centuries, scientists armed with new technologies, including the light microscope (see Figure 1.3), began to make a series of important discoveries about the human body and mind. For example, they demonstrated that a single sensory nerve carried one type of information instead of multiple types. You might have already duplicated this research yourself while rubbing your sleepy eyes—you see a flash of light. The nerves serving the retina of the eye do not know how to process information about touch or pressure. When stimulated, they are capable of only one type of message—light. Hermann von Helmholtz (1821–1894) asked his participants to push a button when they felt a touch. When a thigh was touched, participants reacted faster than when a toe was touched. Because the toe is farther from the brain than the thigh, signals from the toe required more time to reach the brain. These types of discoveries about the physical aspects of mind convinced scientists that the mind was not supernatural and could be studied scientifically. This light microscope was used by Anton von Leeuwenhoek to discover red blood cells in 1676. Microscopes opened a new world to scientists interested in living things. A = Screw for adjusting the height of the object being examinedB = Metal plate serving as the bodyC = Skewer to impale the object and rotate itD = Lens, which was sphericalPhilosophers began to incorporate physiological and psychological concepts into their work, and natural scientists began to explore the questions asked by philosophers. The gradual merger of these approaches resulted in a series of experiments that looked increasingly like contemporary psychology. Scientists began to ask questions about the relationships between physical stimulation and its resulting sensations. For example, Gustav Fechner (1801–1889) was able to identify the softest sound that a person could hear by randomly presenting sounds of different intensities to which a participant would respond yes or no. When the yes responses reached 50%, Fechner concluded that the sound was within the range that the human ear could detect (see Chapter 5). Although Fechner’s research seems very similar to Helmholtz’s, note the importance of mental processes in Fechner’s work, as opposed to the simple measurement of physiology in Helmholtz’s experiment. The stage was set for a modern science of psychology. The work of Hermann von Helmholtz (1821–1894) on reaction time helped establish the mind as something that could be studied scientifically. Person or groupThings to rememberAncient Greek philosophersObservations can be accounted for by natural, not supernatural, explanations. British empiricistsKnowledge is the result of experience. Ancient physiciansThe brain is the source of the mind.17th- and 18th-century natural scientistsDiscoveries about sensation and movement showed that the mind was physical. Hermann von HelmholtzStudies of reaction time reinforced the idea of the mind as physical. As psychology developed from the gradual merger of philosophical questions and scientific reasoning, the young discipline struggled to determine which questions and methods were best suited to its goals. Lively debates arose among psychologists who helped to shape the field. The credit for being the first psychologist goes to Wilhelm Wundt (1832–1920), a former research assistant to von Helmholtz, who conducted the first documented psychological experiment in his laboratory at the University of Leipzig in 1879. This landmark experiment was a simple test of reaction time: How quickly after hearing a ball drop onto a platform could a person respond by striking a telegraph key? Wilhelm Wundt (1832–1920), seated in this photo, is considered the first experimental psychologist. We have given credit to Wilhelm Wundt for conducting the first experiments in psychology. What did those crucial first experiments look like? Wundt’s experiments reflected both his interests in consciousness and his training as a medical doctor. He was aware of methods that were used by researchers in physiology, such as the reaction-time measures pioneered by F. C. Donders in the Netherlands, and he sought to apply these methods to measure psychological processes such as attention and decision making (Danziger & Ballantyne, 1997). The Questions: Is it possible to time mental processes? Are simple reaction times different from reaction times involving choices? Wundt’s methods involved two sets of apparatus: one that would deliver a stimulus precisely to a participant, and a second that would measure and record the participant’s responses. His imposing-looking brass instruments used to carry out these tasks were displayed to an admiring public at the 1893 Chicago World’s Fair. The first experiments carried out by Wundt involved the presentation of stimuli, such as the sound of a ball dropped onto a platform, and measurements of reaction time, as indicated by the participant pressing a telegraph key. In addition to these simple reaction-time experiments, Wundt asked participants to make decisions: When you see this light, press the button on the left, but if you see that light, press the button on the right. As you continue reading your textbook, you will review a number of experiments like this one. Many will highlight important ethical considerations regarding the treatment of participants. These ethical concerns will be reviewed in more detail in Chapter 2, but in the meantime, Wundt’s experiment appears to have posed little risk to his participants. After you consider the criteria for conducting ethical research outlined in Chapter 2, however, you might want to return to this description and see if you agree with our assessment or not. Wundt viewed reaction time as mental chronometry (Hergenhahn & Henley, 2013, p. 255). In other words, he believed that reaction time provided a measure of the amount of mental processing required to carry out a task. As his tasks became more complex, reaction time increased accordingly. As mentioned earlier in this chapter, Wundt’s mentor, Hermann von Helmholtz, had performed a number of experiments similar to those performed by Wundt. Von Helmholtz touched the participant on the thigh and toe and discovered that the participant pushed a button faster in response to the thigh touch than the toe touch. What makes von Helmholtz’s demonstration a physiological experiment and Wundt’s a psychology experiment? Part of the answer is the interpretation that each scientist made of his results. For von Helmholtz, differences in reaction time in these two instances represented the effects of the speed of conduction of neural signaling. Because the toe is farther from the brain than the thigh is, messages from the toe take more time to reach the brain. Wundt’s simple reaction-time experiments were not that different, but his experiments on choice were more clearly psychological. As decisions became more complex, reaction time increased. Max Wertheimer (1880–1943) was one of the founders of Gestalt psychology. Wundt saw mental experience as a hierarchy. The mind constructs an overall perception (the food I’m eating tastes good) out of building blocks made up of separate sensations, such as taste and vision, and emotional responses. One of Wundt’s students, Edward Titchener (1867–1923), expanded on Wundt’s views to establish a theory of structuralismstructuralismAn approach in which the mind is broken into the smallest elements of mental experience        structuralism        An approach in which the mind is broken into the smallest elements of mental experience, in which the mind could be broken down into the smallest elements of mental experience. Titchener’s approach to psychology paralleled the general trends in the physical sciences of his day, such as efforts in chemistry to break molecules into elements and attempts by physicists to describe matter at the level of the atom. The structuralists’ effort to break behavior into its essential elements was rejected by a group of early 20th-century German psychologists, including Kurt Koffka, Max Wertheimer, and Wolfgang Köhler, who founded Gestalt psychologyGestalt psychologyAn approach that saw experience as being different from the sum of its elements.        Gestalt psychology        An approach that saw experience as being different from the sum of its elements.. Gestalt, although lacking a clear translation into English, basically means form or whole. The Gestalt psychologists believed that breaking a whole perception into its building blocks, as advocated by the structuralists, would result in the loss of some important psychological information. For example, look at the middle image in Figure 1.4. It is the same in both the top and the bottom rows, yet in the context of the first row, most people interpret the image as the letter B, and in the context of the bottom row, the image looks like the number 13. The structuralists would have a difficult time explaining why the same visual building blocks could lead to such different conclusions. Participants usually see the middle figure as a B when instructed to look at the first row, but see a 13 when instructed to read the second row, even though the images are exactly the same. Structuralists, who believed that experiences could be reduced to small building blocks, would have difficulty explaining these results. In contrast, Gestalt psychologists, who emphasized the role of context or the whole in perception, would have no problem. While the structuralists and Gestalt psychologists continued their debate, a new type of psychology emerged, partly in response to the publication of Charles Darwin’s The Origin of Species in 1859 and The Descent of Man in 1871. FunctionalismFunctionalismAn approach that saw behavior as purposeful and contributing to survival        Functionalism        An approach that saw behavior as purposeful and contributing to survival viewed behavior as purposeful because it led to survival. Instead of restricting themselves to exploring the structure of the mind, functionalists were more interested in why behavior and mental processes worked in a particular way. William James (1842–1910) proposed functionalism, an approach to the mind that viewed behavior as purposeful. Functionalism’s chief proponent was William James (1842–1910), whose textbook, Principles of Psychology (1890), dominated the field of psychology for 50 years. There are few topics in psychology that James did not address in his book, and many of his ideas sound modern. For example, he coined the term stream of consciousness to describe the flow of ideas that people experience while awake. Throughout his discussions of mental processes and behavior, James emphasized the role of evolution. For the functionalist, the value of an activity depended on its consequences. If we enjoy ice cream, it must be because eating sweet, high-fat foods enhances survival—at least it did for our ancestors, for whom famine was more of a problem than obesity. Mary Whiton Calkins (1863–1930) was a student of William James at Harvard University, although she could not officially register because of her gender. She studied memory and the self and served as president of the American Psychological Association in 1905. It is difficult to overestimate the impact of James on psychology. Structuralism came and went, but all contemporary psychologists are functionalists at heart. As described by two psychology historians, As a systematic point of view, functionalism was an overwhelming success, but largely because of this success it is no longer a distinct school of psychology. It was absorbed into the mainstream psychology. No happier fate could await any psychological point of view (Chaplin & Krawiec, 1979, p. 53). When a case of capital punishment is discussed, we often hear about the prisoner’s terrible childhood from one side of the argument and the need to protect society from further misdeeds by this person from the other side. Where would the Freudians and humanistic psychologists line up in this debate? With the exception of occasional bursts of insight from the ancient Egyptians and Greeks, the most common view of psychological disorders over the course of history has been the supernatural approach. According to this view, psychological disorders resulted from the actions of evil spirits or other external, magical forces. Between the 17th and the 19th centuries, supernatural explanations for psychological disorders began to give way to two scientific approaches: a medical model and a psychological model. The medical model of psychological disorder emphasized physical causes of abnormal behavior and medical treatments, such as medication. The psychological model suggested that abnormal behavior can result from life experiences, leading to fear, anxiety, and other counterproductive emotional responses. Psychological treatments take many forms, from offering support to applying cognitive and behavioral methods to help people think and problem-solve in new ways. As Chapters 14 and 15 will explain, contemporary psychologists typically combine these approaches to understand disorders and develop effective treatments. For example, we know that feeling depressed has both physical components (changes in the activity of chemical messengers in the brain) and experiential components (exposure to stressful situations). Treatment for depression often combines medication with efforts to change the way a person thinks about situations. You have read about a number of reaction-time experiments in this chapter, including those conducted by Hermann von Helmholtz and Wilhelm Wundt. It is possible to conduct similar experiments without the brass equipment used by these early researchers. This exercise, developed by Dr. Erik Chudler of the University of Washington, is designed to measure your reaction time to a visual stimulus. All you need is a partner and a simple footlong ruler. Hold the ruler vertically, with the highest numbers at the top, and ask your partner to place a hand at the bottom of the ruler without touching it. Tell your partner that you will drop the ruler sometime in the next 5 seconds and to grab the ruler as quickly as possible and hold it. Note the number at the top of the person’s hand after the ruler is caught, and use this chart to convert your results to reaction time. Number on ruler at top of person’s handReaction time2 in. (about 5 cm)0.10 s (100 ms)4 in. (about 10 cm)0.14 s (140 ms)6 in. (about 15 cm)0.17 s (170 ms)8 in. (about 20 cm)0.20 s (200 ms)10 in. (about 25.5 cm)0.23 s (230 ms)12 in. (about 30.5 cm)0.25 s (250 ms)17 in. (about 43 cm)0.30 s (300 ms)24 in. (about 61 cm)0.35 s (350 ms)31 in. (about 79 cm)0.40 s (400 ms)39 in. (about 99 cm)0.45 s (450 ms)48 in. (about 123 cm)0.50 s (500 ms)69 in. (about 175 cm)0.60 s (600 ms) Test your participant five times and average the response times. Wundt eventually discarded reaction time as a measure because he became so frustrated with the variability that he observed among participants and across tasks (Hergenhahn & Henley, 2013), but you can have some fun exploring these same sources of variability. Are you faster than your friends? Try testing people who are older than you. Are they faster or slower? Do your participants improve with practice? What happens to your reaction time if you dim the lights? Measuring the spot where a person catches a falling ruler gives you a rough estimate of that individual’s reaction time to a visual stimulus. Sigmund Freud (1856–1939) built a bridge from his medical training as a physician to his belief in the impact of life experiences on behavior. His psychodynamic theory and its applications to the treatment of psychological disorders dominated much of psychological thinking for the first half of the 20th century. Freud’s ideas about the existence of the unconscious mind, the development of sexuality, dream analysis, and psychological roots of abnormal behavior influenced not just psychology, but also culture. He nearly single-handedly founded the study of personality in psychology, a topic explored more fully in Chapter 12. He developed the techniques of psychoanalysis for treating mental disorders, which are discussed in Chapter 15. He popularized the use of psychological principles for explaining everyday behavior, and his theories are as likely to be covered in an English literature course as in a psychology course. The work of Sigmund Freud (1856–1939) on consciousness, sexuality, abnormal behavior, and psychotherapy played a dominant role in psychology during the first half of the 20th century. Our enthusiasm for Freud is tempered by a number of valid concerns. As you read further about Freud, keep in mind that his methods were not scientific. His theories are based on observations of his patients, primarily upper-class Viennese housewives who were not typical of the human population. Freud’s theories do not lend themselves to experimentation, an essential requirement for any scientific theory, as discussed further in Chapter 2. For example, how could you design an experiment to demonstrate that dreaming about water indicates you have unconscious concerns about sex? Finally, although psychoanalysis is still used on occasion as a therapy technique, it is rarely conducted in the strict Freudian manner. Other techniques, discussed in Chapter 15, exceed psychoanalysis in effectiveness and popularity among contemporary therapists. Abraham Maslow (1908–1970) contributed a theory of motivation and ideas about exceptional people to the growing humanistic psychology movement. By the 1960s, American psychology was dominated by behaviorism (discussed in a later section of this chapter) on one side and Freud’s theories on the other. Structuralism had fallen into disfavor, and functionalism and Gestalt psychology were no longer distinct schools of thought. Just as other aspects of American culture at this time began to feature rebelliousness against current ways of thinking, some psychologists began to push against the restrictions of psychodynamic theory. Many of these disenchanted psychologists had been trained in psychoanalysis but were not seeing the results they desired. This dissatisfaction with prevailing views led these psychologists to propose new ways of thinking about the human mind through an approach known as humanistic psychologyhumanistic psychologyAn approach that saw people as inherently good and motivated to learn and improve.        humanistic psychology        An approach that saw people as inherently good and motivated to learn and improve.. Freud, James, and the behaviorists all believed that human behavior was on a continuum with animal behavior, which led to their assumption that humans naturally shared the aggressive impulses of animals. For Freud in particular, society had a civilizing function on the otherwise selfish and aggressive human. In contrast, the humanistic psychologists extended the philosophy of Jean-Jacques Rousseau and other 18th-century Romantic philosophers into a belief that people are innately good, are motivated to improve themselves, and behave badly only when corrupted by society. Instead of focusing on what went wrong in people’s lives, humanistic psychologist Abraham Maslow (1908–1970) asked interesting questions about what made a person good. Maslow introduced a major theory of motivation, which is described in more detail in Chapter 7. As Chapter 16 will show, Maslow’s emphasis on what is good about people, as opposed to Freud’s focus on what goes wrong with people, reemerged in the form of contemporary positive psychology. Prior to advances in psychological science, people with psychological disorders were subjected to bizarre treatments, such as this 18th-century spinning device intended to calm patients. Humanistic therapists rebelled against Freudian approaches to treatment. As described in more detail in Chapter 15, one humanistic therapist, Carl Rogers (1902–1987), developed a new approach to therapy called client-centered therapy. In this type of therapy, the people receiving treatment are called clients rather than patients, reflecting their equal standing with the therapist and their active role in the therapy process. Humanistic approaches to therapy have also influenced communication, group process, parenting, and politics. The emphasis on active listening and the use of I hear what you’re saying reflections have become nearly cliché in courses of leadership training and interpersonal communication. Advice to parents to provide unconditional love to their children is a direct application of humanistic beliefs, which are discussed in more detail in Chapter 11. Finally, humanistic psychology continues to flavor our political and social domains. When issues such as capital punishment arise, the humanistic contention that there are no bad people, just bad societies that fail people, typically appears as part of the debate. Beginning at the dawn of the 20th century, the concepts of mental processes and brain function in our definition of psychology took a back seat to observable behavior for the better part of the next 50 years because psychologists following the approach of behaviorismbehaviorismAn approach that features the study and careful measurement of observable behaviors        behaviorism        An approach that features the study and careful measurement of observable behaviors concentrated on observable, measurable behaviors. As part of their effort to measure behavior carefully, many behaviorists restricted their research to studies using animals. Armed with Darwin’s evidence linking humans to animals, the behaviorists comfortably drew parallels between their observations of animals and their assumptions about human behavior. In particular, behaviorists were fascinated by learning, which is examined in depth in Chapter 8. Humanistic therapists, like Carl Rogers (1902–1987), often rebelled against Freudian approaches to therapy. For example, Rogers (in the white shirt leading a group therapy session) referred to people as clients rather than patients, the term that Freud used. Ivan Petrovich Pavlov (1849–1936) had a particularly significant impact on behaviorism and psychology. While studying digestion in dogs, he realized that the dogs’ salivation in response to the arrival of the handler or to being harnessed for an experiment, rather than just to the food itself, indicated that the dogs had associated, or linked, these signals with the arrival of food. The dogs’ ability to use this learned association to anticipate important future events was a remarkable advantage in terms of survival. This type of learning is now called classical or Pavlovian conditioning, which will be covered in detail in Chapter 8. In 1920, Francis Cecil Sumner (1895–1954) became the first African American to receive a doctorate in psychology for his work on psychoanalysis. Sumner’s later work focused on religion and racism. Psychology textbooks would spend little time on Pavlov if his research applied only to salivating dogs. Although classical conditioning occurs in rather primitive organisms, including fruit flies, snails, and slugs, it also occurs quite frequently in humans. Many of our emotional responses associated with environmental cues are the result of this type of learning. If you feel especially anxious prior to taking an exam, you can thank classical conditioning. If you are repulsed by the idea of eating a food that you once consumed just before becoming ill, this is again a likely result of classical conditioning. John B. Watson (1878–1958) began experimenting with learning in rats and independently came to many of the same conclusions as Pavlov. Watson echoed the blank-slate approach of the British empiricist philosophers in his emphasis on the role of experience in forming human behavior. Later in his career, Watson applied his understanding of behavior to the budding American advertising industry. By 1930, he was earning $70,000 per year as an advertising executive—an astronomical salary for the time, much higher than the $3,000 per year he earned as a professor. After discovering that blindfolded participants couldn’t tell the difference between brands of cigarettes, Watson concluded that to be successful, a product must be associated with an appealing image. The advertising industry was never the same, and today’s advertisers continue to apply Watson’s principles. The Freudians and humanistic psychologists had conflicting views on human nature, with the Freudians believing that we are naturally selfish and aggressive and the humanistic psychologists believing that we are naturally good. These philosophical differences continue to color our discussions of topics: Is a criminal a bad person who was never properly socialized or a good person who was corrupted? While studying digestion, Ivan Petrovich Pavlov (1849–1936) realized that his dogs could learn that certain signals meant food was on the way. Watson’s legacy in psychology was enormous. He restricted psychology to the study of observable behavior. As will be established in Chapter 2 and throughout this text, even psychologists who are interested in internal events, like the visual recognition of an object, seek related observable behaviors, such as brain images or reaction time. Like Pavlov, Watson approached psychology with a focus on the relationships between environmental cues and behavior. Other behaviorists were more interested in the effects of consequences on behavior, an idea that was derived from basic functionalism. Edward Thorndike (1874–1949) proposed the law of effect, which suggested that behaviors followed by pleasant or helpful outcomes would be more likely to occur in the future, whereas behaviors followed by unpleasant or harmful outcomes would be less likely to occur. He based his law on observations of cats’ behavior in a puzzle box he had constructed (see Figure 1.5). To escape the box, a cat was required to complete a sequence of behaviors. Through trial-and-error learning, the cat would escape faster and faster on successive trials. In other words, the cat repeated effective behaviors and abandoned ineffective ones. If you own a cat, you probably know that cats don’t like to be enclosed in boxes. Edward Thorndike (1874–1949) studied the escape strategies of a cat to build his law of effect. Like Thorndike, B. F. Skinner (1904–1990) was interested in the effects of consequences on how frequently behaviors were performed. Skinner shared Watson’s belief that psychology did not benefit from consideration of consciousness or internal mental states (see Figure 1.6). He believed that inner, private states such as thinking and feeling existed, but he viewed them as behaviors that followed the same rules as public behaviors, like driving a car (Jensen & Burgess, 1997). He not only reduced his study of behavior to the actions of rats and pigeons in adapted cages that came to be known as Skinner boxes, but he also was comfortable generalizing from the behavior of rats and pigeons to complex human behaviors. Despite its strong focus on a limited set of animals and situations, Skinner’s behaviorism has provided a wealth of beneficial applications. Smokers attempting to quit, doctors and nurses engaging in self-paced continuing education courses, and children receiving treatment for autism spectrum disorder are all likely to be benefiting from Skinner’s efforts. Strict behaviorists often referred to a black box model, in which stimuli enter and responses exit, but you don’t need to know much about what the box is doing to the data. When psychologists began substituting what they learned about the brain for the inner workings of the black box, this led to the development of the biological psychology perspective, also known as behavioral neuroscience. Classical conditioning helps us understand the links that we make between environmental cues and our emotions. If a soldier associated the smell of diesel fuel with traumatic experiences, smelling diesel fuel at a gas station back home could trigger distress. By the 1950s, the behaviorists’ lack of interest in mental states and activity was challenged by scientists from diverse fields, including linguistics and computer science, leading to a cognitive revolution. Cognition covers the private and internal mental processes that the behaviorists avoided studying—information processing, thinking, reasoning, and problem solving. Ulric Neisser (1928–2012) gave the new field its name in his 1967 book, Cognitive Psychology. John B. Watson (left, 1878–1958) was a strong believer in the blank-slate approach of the earlier empiricist philosophers. After working as a psychology professor, he applied his knowledge of human behavior to advertising with great success. Watson believed that a product would sell better if it were paired with an appealing image. His ideas are still used by advertisers today. For instance, an image of Chinese basketball star Yao Ming is being used to sell health insurance in Suzhou, eastern China. Breakthroughs in computer technology allowed these new cognitive psychologists to use mathematical and computer models to illuminate the mental processes leading to observable behaviors. Alan Newell (1927–1992) and Herbert Simon (1916–2001) wrote groundbreaking artificial intelligence programs using human information processing as their model. By the 1980s, most university psychology departments were offering courses in cognition. By the 1990s, collaborations between cognitive and biological psychologists led to the new field of cognitive neuroscience, which seeks to identify brain structures and functions involved in processing information. Chapter 10 will explore the contributions of cognitive psychologists in more detail. B. F. Skinner (1904–1990), shown here with the apparatus that bears his name—the Skinner box, was interested in the effects of reward and punishment on future behavior. Computers were named after the job title of the women who did most computation tasks before the machines were invented, and who continued to operate them. Although these early computers were less powerful than your cell phone (not to mention more expensive), their operation gave psychologists new ideas about how the mind might process information. Ulric Neisser (1928–2012) contributed the term cognition to the emerging field that studied information processing, thinking, reasoning, and problem solving. Foundation of psychologyThings to rememberWilhelm Wundt (1832–1920) StructuralismBehavior can be broken down into its components. Max Wertheimer (1880–1943) Gestalt psychologyBreaking behavior into components loses meaning. William James (1842–1910) FunctionalismBehavior is purposeful and contributes to survival. Sigmund Freud (1856–1939) Psychodynamic theoryIdeas about the unconscious mind, the role of experience in abnormal behavior, and new approaches to therapy laid a foundation for later study in personality and therapy. Abraham Maslow (1908–1970) Humanistic psychologyPeople are naturally good and are motivated to improve. Ivan Petrovich Pavlov (1849–1936) BehaviorismExperience is the primary source of behavior. Ulric Neisser (1928–2012) Cognitive revolutionPrivate mental processing can be studied scientifically. William James, the Freudians, and the behaviorists all tried to answer psychological questions with a comprehensive big theory approach. However, it is difficult to build a big theory without a large body of experimental data, and psychology was still a young science. To fill this gap, psychological scientists began to build a database by specializing in more specific points of view, or perspectives. By focusing on one part of the discipline, as opposed to trying to answer everything at once, psychologists began to gain an in-depth understanding of at least one aspect of mind at a time. By the second half of the 20th century, most psychologists were examining psychological phenomena from one of a handful of perspectives. The use of different perspectives does not imply disagreement or conflict. In most cases, the use of each perspective depended on specialized expertise and methods, so different fields of psychology became characterized by their distinct theories and methods. For example, understanding how a child learns a new vocabulary word would be investigated using different theories and methods by the biological, developmental, cognitive, social, or behavioral psychologist. Reflecting the traditional divisions of the field, it is common today for psychologists to refer to themselves as social psychologists, developmental psychologists, and so on, indicating their area of specialization and interest. Psychology departments of universities often continue this organization, and students applying to graduate school in psychology might specialize in one particular area, like choosing an undergraduate major. Biological psychologists explore the relationships among mind, behavior, and their underlying biological processes. They often use technology such as functional magnetic resonance imaging (fMRI). Scott Grafton of the University of California, Santa Barbara, is pointing out the features of the brain of one of the authors of this book. The need to consider major perspectives in psychology was reinforced in a report titled Strengthening the Common Core of the Introductory Psychology Course, published in 2014 by the American Psychological Association (Gurung et al., 2014). We have already seen how the various perspectives might address the question of why some children don’t like broccoli. To further illustrate the distinctions among some of the main perspectives, we will consider how each might approach the question of human memory, discussed in detail in Chapter 9. Biological psychologyBiological psychologyThe psychological perspective that focuses on the relationships between mind and behavior, as well as their underlying biological processes, including genetics, biochemistry, anatomy, and physiology; also known as behavioral neuroscience        Biological psychology        The psychological perspective that focuses on the relationships between mind and behavior, as well as their underlying biological processes, including genetics, biochemistry, anatomy, and physiology; also known as behavioral neuroscience, also called behavioral neuroscience, focuses on the relationships between mind and behavior and their underlying biological processes, including genetics, biochemistry, anatomy, and physiology. In other words, biological psychologists are interested in the physical mechanisms associated with behavior. In addition to the basic behavioral genetics presented in Chapter 3 and the biological psychology presented in Chapter 4, this perspective is emphasized in Chapter 5 (sensation and perception), Chapter 6 (consciousness), and Chapter 7 (motivation and emotion). As Chapter 4 will show, technological advances beginning in the 1970s, especially new methods for observing brain activity, initiated an explosion of knowledge about the connections between brain and behavior. Using these new technologies, biological psychologists have approached the question of storage and retrieval of memories in many ways, ranging from observing changes in communication between nerve cells in slugs to investigating the effects of stress hormones on the ability to form memories. A branch of the biological perspective, evolutionary psychologyevolutionary psychologyA psychological perspective that investigates how physical structure and behavior have been shaped by their contributions to survival and reproduction        evolutionary psychology        A psychological perspective that investigates how physical structure and behavior have been shaped by their contributions to survival and reproduction, attempts to answer the question of how our physical structure and behavior have been shaped by their contributions to our species’ survival. This perspective should sound familiar—it is a modern extension of James’s functionalism, discussed previously in this chapter. Earlier, we also saw evolutionary psychology at work in the shaping of our sensitivity to bitter tastes. The basic principle of evolutionary psychology is that our current behavior exists in its present form because it provided some advantage in survival and reproduction to our ancestors. An evolutionary psychologist might be interested in our good memory for faces, and particularly for faces of people who have cheated us in the past (Barclay & Lalumière, 2006). In the world of the hunter–gatherer, being cheated out of a fair share of the hunt was likely to lead to starvation for a family, and people who could not keep track of the cheaters were unlikely to survive and reproduce. Evolutionary psychologists are interested in how our modern behaviors are shaped by our species’ history. Cognitive psychologyCognitive psychologyA psychological perspective that investigates information processing, thinking, reasoning, and problem solving        Cognitive psychology        A psychological perspective that investigates information processing, thinking, reasoning, and problem solving focuses on the process of thinking, or the processing of information. Because our ability to remember plays an integral part in the processing of information, a cognitive psychologist is likely to have a lot to say about the storage and retrieval of memories. A cognitive psychologist might ask why processing seems different when we are trying to remember names and dates while taking a history test compared to remembering how to ride a bicycle. What processes lead to the frustrating experience of having something on the tip of your tongue, in which you remember the first letter or a part of a word you’re trying to retrieve, but not the whole thing? What strategies can we use to make our memories more efficient? These and similar issues are addressed in Chapters 9 and 10. Developmental psychologyDevelopmental psychologyA psychological perspective that examines the normal changes in behavior that occur across the life span.        Developmental psychology        A psychological perspective that examines the normal changes in behavior that occur across the life span. explores the normal changes in behavior that occur across the life span. Using the developmental perspective, a psychologist might look at how memory functions in people of different ages. Without further practice, 3-month-old babies can retain for about a month the memory that kicking moves a mobile suspended above their crib (Rovee-Collier, 1997). However, most adults have difficulty recalling events that occurred before the age of 3 or 4 years. Teens and young adults are able to remember names faster than are older adults (Bashore, Ridderinkhof, & van der Molen, 1997). These and other age-related changes are explored in Chapter 11. Cognitive psychologists investigate the ways that the human mind processes information. This cognitive psychologist is studying the use of mirrored images to help individuals overcome phantom pain due to the loss of a limb. Seeing images of what appears to be a healthy limb in place of the missing limb changes the way the mind thinks about the missing limb, leading to a reduction in perceived pain. Social and personality psychology describes the effects of the social environment, including social and cultural diversitycultural diversityVariations in the practices, values, and goals shared by groups of people        cultural diversity        Variations in the practices, values, and goals shared by groups of people, and individual differences on the behavior of individuals (see Figure 1.8). Social and personality psychologists recognize that we construct our own realities and that the social environment influences our thoughts, feelings, and behavior. Early psychologists were limited in their understanding of mind by their exclusive focus on their own sociocultural contexts. More recently, social psychologists have emphasized the need to explore the influences of sociocultural context and biology on our behavior. Returning to our memory example, the social psychologist might ask how being in the presence of others influences the storage and retrieval of data. When we are sitting comfortably in our own homes, the answers to Jeopardy! questions come relatively easily. In front of millions of viewers, however, we might be lucky to remember our own names. The emergence of social media has provided social and personality psychologists with new methods for examining the social mind and individual differences. What do your updates say about your personality? When the updates of more than 70,000 people who completed a Facebook personality test were examined, clear differences in their choices of words were observed. (Data from  Park et al., 2015.) Developmental psychologists look at the behavior that is typical for people of certain ages, from infancy to old age. The amount of time this infant spends looking at moving stick figures helps us understand at what point in life we perceive biological motion. Although much of psychology explores how the average person thinks, feels, or acts, some people are not average. Behavior can vary dramatically from one individual to another as a function of personality factors and many aspects of diversity, including age, gender and gender identity, sexual orientation, race, ethnicity, disability status, and socioeconomic status. Using our example of memory, we can see how individual differences in need for cognition can predict memory for verbal material (Cacioppo, Petty, Feinstein, & Jarvis, 1996). People who have a high need for cognition enjoy mental challenges, like solving difficult puzzles. As Chapter 13 will explore, individuals who are high in need for cognition also remember more of the messages to which they are exposed and respond differently to persuasive messages. Finally, the clinical psychologyclinical psychologyA psychological perspective that seeks to explain, define, and treat abnormal behaviors        clinical psychology        A psychological perspective that seeks to explain, define, and treat abnormal behaviors perspective seeks to explain, define, and treat psychological disorders, as explained in detail in Chapters 14 and 15. More recently, the clinical perspective has expanded to include the promotion of general well-being and health, which is described in Chapter 16. Many types of psychological disorders affect memory. Freud believed that traumatizing experiences were more difficult to remember, a process that he labeled repression (which will be discussed later, in Chapters 9 and 14). In other cases, war veterans and others who have experienced trauma might be troubled by memories that are too good, producing intrusive flashback memories of disturbing events. Social psychologists explore the effects of the social environment on our individual behavior. In this example, the man in the middle is deciding whether to conform with the other two men in a simple judgment of line length. Although the 20th-century perspective approach to psychology generated detailed understanding of aspects of behavior and mental processes, it has become apparent that single perspectives are insufficient for fully describing and explaining psychological phenomena. Armed with in-depth research results compiled from these various perspectives, many psychological scientists in the 21st century have returned to the more comprehensive view of the mind envisioned more than 100 years ago by James (Cacioppo, 2013). Their questions and methods are more likely to blur the lines of the perspectives outlined earlier, often with remarkable results. For example, a full understanding of romantic relationships is more likely to emerge from combinations of perspectives than through the use of single perspectives. Zooming out to combine an understanding of cultural and social contexts, biological factors (such as the bonding hormone oxytocin), personality (individual traits), social experience (such as self-fulfilling prophesies), cognitions (such as automatic thought), and the effects of psychological disorders (as in borderline personality disorder) give us a more comprehensive view of the phenomenon. (see Figure 1.9). What accounts for our romantic attraction to that special someone? Single perspectives provide considerable insight, but combining perspectives gives us a richer understanding of human behavior. In addition to the five psychological perspectives reviewed in this chapter, a more general perspective provided by culture and diversity is also essential to our understanding of behavior. The guidelines proposed for the introductory psychology course by Gurung et al. (2014) emphasize the need to use culture and diversity as a cross-cutting theme. In accordance with this view, each of the editions of this textbook has been crafted following the guidance provided by Trimble, Stevenson, and Worell (2003) about integrating issues of diversity into all relevant discussions in an organic way. In addition to incorporating diversity seamlessly throughout our topics, we also believe that it is useful to provide opportunities to highlight diversity issues in greater depth, which is the purpose of having this feature in each chapter. As suggested by Betancourt and López (1993, p. 636), psychology as a discipline will benefit both from efforts to infuse culture in mainstream research and theory and from efforts to study culture and develop theory in cross-cultural and ethnic psychology. This first chapter has reviewed psychology’s historical timeline and explored some of the career paths that psychologists can follow. From a diversity perspective, we see that the history of psychology features a dramatic underrepresentation of ethnic and racial minorities and a very short timeline. Stanley  Sue (2009) noted that it wasn’t until 1975 that the first African-American woman was licensed as a psychologist (Gail Wyatt) in the state of California. Sue himself was told that in 1971, he was the only Chinese-American with a regular faculty appointment in an American Psychological Association–accredited clinical psychology program in the entire nation. As Sue and others have pointed out, underrepresentation has negative consequences both for psychologists themselves and for the people they serve as teachers, researchers, or therapists. Recognition of the problem has led to remedial efforts, but there is still a long way to go. Kenneth and Mamie Clark conducted some of the most famous experiments in psychology in 1939–1940. Children in segregated schools were shown two dolls that were identical except for hair, eye, and skin color, and asked questions such as whether one doll was nicer, or if the child would prefer to play with one doll over the other. The majority of children, regardless of their own race, preferred the white doll. The Clarks not only made Contributions to our understanding of the social context of child development, but their research was applied as evidence in hearings by the U. S. Supreme Court on the detrimental effects of segregated schools. In 2005, an 18-year-old high school student, Kiri Davis, repeated the Clark’s doll study for her film A Girl Like Me: The Gwen Araujo Story. She found that 15 out of 21 African-American girls still preferred the white doll. Psychology, of course, is not alone in facing this problem, but as students of the mind, we must include diverse viewpoints to ensure accuracy. In Chapter 5, you will read about how white Americans and Asians respond differently to objects (a tiger) and backgrounds (a jungle) in a photograph. If we studied only one of these groups, we would end up with faulty conclusions. Clinical psychologists seek to understand and treat psychological disorders. We don’t have a crystal ball that will allow us to foresee psychology’s future. However, we strongly believe that this future will involve combining and integrating new and existing perspectives. Many of these new ways of looking at the mind will take advantage of the revolution in techniques for studying the brain that began in the 1970s and continues. Already, today’s cognitive neuroscientists investigate the brain as an information-processing system and search for the biological basis of topics such as attention, decision making, and memory. Social neuroscientists investigate the biological factors that vary with people’s feelings and experiences of social inclusion, rejection, or loneliness. Behavioral neuroscientists pick up previous lines of research on learning, memory, motivation, and sleep, and search for connections between these processes and our biology. Clinical and counseling psychologists are likely to consider biological processes in their theories about the causes of psychological disorders. By merging the five perspectives of mind, we stand a better chance of tackling the remarkable problem of understanding the human mind (see Figure 1.10). Viewing the mind by zooming in and using specialized perspectives has led to significant increases in our understanding, but 21st-century psychology is characterized by efforts to zoom out and integrate multiple, cross-cutting perspectives. We have argued that restricting our thinking about an aspect of mind to the information provided by one perspective can result in an incomplete picture, but can this single-perspective approach actually lead us in the wrong direction? The answer to that question is a resounding yes. Consider the following example. For many years, researchers were puzzled in their efforts to understand the relationships between child maltreatment and later antisocial behavior. Although maltreatment often seems to be linked to later criminal behavior, the majority of maltreated children do not become delinquents or adult criminals (Caspi et al., 2002). To solve this dilemma, a clinical psychologist might focus on environmental factors, such as the presence of a trusted adult or delinquent peer group, or personal factors, like resilience. Working in parallel, biological psychologists know something about certain candidate genes and their relationships with aggressive behavior in animals. In particular, animals with a low-activity version of the MAOA gene seemed to be more aggressive than animals with a higher-activity version. However, links between variations in MAOA and human aggression are not clear. Experiencing childhood maltreatment does not reliably predict aggressiveness in youth, but combining genetics and exposure to maltreatment provides a clearer picture. Separately, neither group of psychologists is likely to do a very good job of explaining why some children exposed to maltreatment engage in antisocial behavior while others do not. The solution, however, becomes apparent when we combine the clinical psychologists’ observations of environmental factors with the genetic information provided by the biological psychologists. It appears that a gene-environment interaction takes place, in which children with the low-activity version of the MAOA gene responded to maltreatment by becoming antisocial, while children with the higher-activity version do not (Caspi et al., 2002;  Fergusson, Boden, Horwood, Miller, & Kennedy, 2011). Restricting ourselves to one perspective might cloud our understanding, but combining perspectives leads us to an accurate conclusion. In 2013–2014, 117,000 students in the United States received a bachelor’s degree in psychology (U. S. Department of Education, 2016). This amounts to approximately 6% of the 1.9 million bachelor’s degrees awarded that year, making psychology the fourth-most-frequent bachelor’s degree program (following business, the health professions, and combined social sciences and history). What are these psychology graduates likely to be doing in the workforce? Forensic psychologists attempt to understand the criminal mind and to develop effective treatments for criminal behavior. Psychology is unlike some other disciplines in which people with a bachelor’s degree can refer to themselves as a practicing member of the relevant profession, such as a chemist or biologist. Calling oneself a psychologist is restricted to holders of graduate (usually doctorate) degrees. This does not mean that students who completed a bachelor’s degree in psychology cannot be employed in relevant fields. Some people with undergraduate degrees in psychology prefer employment in fields that are directly related to psychology, such as working in research facilities or rehabilitation centers for drug abuse or brain damage. Others are quite successful in a variety of people-oriented jobs, such as those found in management, sales, customer service, public affairs, education, human resources, probation, and journalism. This diversity of career pathways reflects the hub nature of psychology to other fields, as described earlier in this chapter. More than half of people holding graduate degrees in psychology work in health care, counseling, financial services, or legal services professions (Stamm, Lin, & Christidis, 2016). Graduates with a master’s degree in psychology, usually requiring 1 to 2 years of additional study past the bachelor’s degree, can teach at the community college (2-year) level and obtain licensing as therapists in most states, as discussed further in Chapter 15. School psychologists with a master’s degree typically work on elementary, middle school, or high school campuses. These psychologists participate in academic and career counseling, as well as the identification and remediation of problems that interfere with student success. Sports psychologist Julie Elion gives Phil Mickelson some final tips during his preparation for an important PGA championship. Many people working in psychology have earned doctoral degrees, which usually take 2 to 5 years of study beyond the master’s level. As shown in Figure 1.11, about 34% of new doctoral-level psychologists do what your professor and the authors of this textbook do: teach, conduct research at colleges and universities, or both. About 45% of new doctoral-level psychologists work as therapists. Smaller numbers of new doctoral-level psychologists find employment in business and government settings, elementary and secondary schools, and other related fields. Most psychologists with graduate degrees are employed in clinical and higher-education settings, but opportunities for students of psychology also exist in schools, businesses, government, and other places in which an understanding of human behavior is helpful. Psychologists entering doctoral programs traditionally identify with one of the major perspectives discussed earlier, such as social, cognitive, or biological. Choosing a graduate perspective is similar to choosing an undergraduate major. Although all psychology graduate students might take core courses in research methods and statistics, they typically pursue coursework and research in their particular area of specialization. However, training of psychologists in the 21st century is beginning to reflect the connections occurring in the field. Increasingly, students are being trained in combined specialties (e.g., cognitive neuroscience) as psychology becomes an increasingly integrated field of study. The most rigid distinction occurs between graduate students who plan to specialize in clinical or counseling psychology and those who do not. The clinical or counseling track includes extensive internships and supervised training prior to government-regulated licensure that usually add at least 1 year to students’ graduate studies. Do not assume that your psychology professors are all therapists; it is most likely they are not. It is important to distinguish between therapists with doctoral degrees in psychology (PhDs or PsyDs) and psychiatrists, who are medical doctors (MDs). The biggest difference between the two professions is that psychiatrists can prescribe medication, but psychologists usually cannot. In New Mexico, Louisiana, Illinois, Iowa, and Idaho however, specially trained psychologists can legally prescribe medications, and this trend might spread to other states. In Chapter 15, we will provide more detail about the types of therapists who treat adjustment problems and psychological disorders. We gave quite a lot of thought to how we might help you, the students reading our book, to see psychology more as an interconnected discipline than as a collection of separate perspectives. The introductory psychology course provides a unique opportunity to see all perspectives at one time, in contrast to upper-division courses, which typically focus on one specialty at a time (biological, developmental, social, cognitive, clinical, and so on). At the beginning of each chapter, we will take a question relevant to the chapter, such as Why are we sensitive to bitter tastes? and explore it using multiple perspectives. At the end of each chapter, we’ll mirror this process by taking a single problem and seeing how the material in the chapter can be applied. This feature of the textbook will have more of a perspective flavor than the beginnings of each chapter, but by the end of the textbook, you will have seen how each of the major perspectives in psychological science contributes to the whole picture. Because it is most helpful to know something about the perspectives before you see how they can be applied, we save this feature for the end of each chapter. Ideally, seeing the chapter material in action will make it more memorable for you. The single problem we have chosen to consider this way is one that we know to be on the minds of many of our students: their relationships with others. We are a social species, and the quality of our relationships has a huge impact on our physical and psychological well-being (Cacioppo & Hawkley, 2009). We will see how each of the major perspectives views the question of relationship quality and how the perspectives work together to give us the best possible understanding of this important aspect of life. As you have seen in this chapter, psychological science is the study of behavior, mental processes, and brain functions. As such, psychology has much to say about the causes and solutions of contemporary human problems. To illustrate psychology’s power to contribute to the understanding and solution of these problems, each chapter will focus on a big problem from the point of view of the chapter content. While each chapter addresses only a part of the problem, by the time you reach the end of the textbook, you will have been exposed to many ways that psychology can contribute to solving some of the biggest challenges that human beings face today, such as pollution, climate change, education, poverty, terrorism, pandemics, food insecurity, crime, and social injustice. Solving such big problems might seem overwhelming, but by breaking each problem into little steps, real progress can be made. We’d like to model this process for you by tackling a single problem, but we encourage you to expand on our model by applying what you’ve learned to other issues that you find important. The problem we’ll address is cyberbullying, or the use of electronic technology to bully another person. Bullying, in turn, is defined as aggressive, repeated behavior that involves a real or perceived power imbalance. The Centers for Disease Control and Prevention (CDC) reported that in 2015, nearly one out of every six high school students experienced cyberbullying in the previous 12 months (Kann et al., 2016). Outcomes for the individuals involved included depression, anxiety, health problems, academic problems, and even suicide. In the upcoming chapters, we want you to think about how the different perspectives might view both the causes and solutions to our problem. What research results and other information do you need to understand the problem better? Do ethical concerns and issues of diversity influence the causes and solutions of the problem? How would our lives be affected if the problem continues to worsen or if solutions actually can be found? PerspectiveThings to rememberBiological and evolutionary psychologyInvestigates the connections among mind, behavior, and biological processes, and asks how our evolutionary past continues to shape our behaviorCognitive psychologyInvestigates mental processes, including thinking, problem solving, and information processingSocial and personality psychologyAsks how our behavior is affected by the presence of others; Recognizes that behavior varies around averages and that individual differences often interact with environmentsDevelopmental psychologyInvestigates the normal changes in behavior that occur across the life spanClinical psychologyExplains, defines, and treats psychological disorders and promotes general well-being`;\n","export const text = `Scientific methods, including brain imaging, have allowed researchers to pinpoint structural and functional differences between the brains of fluent speakers and those of people who stutter. Among the many celebrities who stutter is action actor Samuel L. Jackson. At a gala for the American Institute for Stuttering, Jackson shared one of his strategies for avoiding stuttering—uttering a particular swear word under his breath. Stuttering involves disruptions in normal speech production, such as repeating the starting letter of a word (b-b-b-bird), holding a vowel sound for a long time (ah ah ah), or having difficulty initiating speech. About 5% of the population experience stuttering (Månsson, 2000), with males two to five times more likely to stutter than females (Craig & Tran, 2005). Stuttering has been described since the days of the ancient Greeks and occurs across all cultures and ethnicities. Many interesting myths exist regarding its causes and remedies (Kuster, 2005). South African traditions suggest that stuttering results from leaving a baby out in the rain or tickling the infant too much. A Chinese folk remedy for stuttering was to hit the person in the face when the weather was cloudy. In Iceland, people believed that a pregnant woman who drank from a cracked cup was likely to produce a child who stuttered. Many of us might ask, How could anybody believe these things? But how do we know these are merely myths and not facts? Instead of dismissing these efforts to explain and predict, think about what might have happened to lead people to these particular conclusions. What is missing from these conclusions is a system for reaching logical, objective results. Science provides us with this system. What does science have to say about stuttering? Based on the careful evaluation of stuttering using the methods outlined in this chapter, scientists have concluded that there are multiple causes for stuttering (Kang et al., 2010). Many cases seem to have a basis in genetics, which is discussed in Chapter 3 (Raza et al., 2015). Scientists have used brain-imaging technologies to zoom in on brain structures and functions that appear to differ between stutterers and fluent speakers, with stutterers showing more activation of the right hemisphere during speech (Gordon, 2002). As mentioned in the previous chapter, some of the most thorough explanations combine multiple psychological perspectives (Ward, 2013). A complete explanation of stuttering zooms back out to combine a predisposition for the problem resulting from genetics and biology with developmental, emotional, and social factors, like feeling embarrassed or anxious about speaking in front of peers. Although there is no cure for stuttering, carefully tested scientific explanations combining input from various perspectives are leading to more effective treatments. In this chapter, you will learn how science provides a system that allows us to construct increasingly realistic models of the world around us. Science has provided explanations for many natural phenomena, like lightning, that were probably quite frightening for our ancestors. Throughout human history, we have been motivated to understand, predict, and control the world around us. To meet these goals, we need methods for gaining knowledge. We often take contemporary scientific knowledge for granted, but our ancestors did not enjoy the benefits of science while trying to explain and predict their world. Early in history, people attempted to understand natural phenomena by applying human characteristics to nature (Cornford, 1957). Skies could look angry or a lake could be calm. Other explanations involved spirits inhabiting humans and all other objects. Earthquakes and illness were viewed as the actions of spirits, and people attempted to influence these spirits through magical rituals. Later on, people looked to authorities, such as religious leaders and philosophers, for explanations of natural phenomena. People often form strong beliefs about their world based on faith, which literally means trust. Faith is belief that does not depend on logical proof or evidence. We might accept friends’ excuses for being late based on our faith in their honesty, without knowing for certain whether they are telling the truth. In contrast to faith, science requires proof and evidence. The word sciencescienceA method for learning about reality through systematic observation and experimentation            science        A method for learning about reality through systematic observation and experimentation             comes from the Latin scientia, which means knowledge. Science doesn’t refer to just any type of knowledge, but rather to a special way of learning about reality through systematic observation and experimentation. The methods described in this chapter are designed to supply that evidence. Throughout history, people have often turned to authorities, such as religious leaders, instead of to science. The astronomer Galileo Galilei was interrogated as part of the Roman Inquisition for believing that the Earth was not the center of the universe. Not all observations are scientific. How does science differ from everyday observations, like the belief that opposites attract? As you will learn in Chapter 13, opposites do not, in fact, find each other very attractive. First, science relies on objectivityobjectivityThe practice of basing conclusions on facts, without the influence of personal emotion and bias.            objectivity        The practice of basing conclusions on facts, without the influence of personal emotion and bias.            , rather than subjectivity. ObjectivityObjectivityThe practice of basing conclusions on facts, without the influence of personal emotion and bias.            Objectivity        The practice of basing conclusions on facts, without the influence of personal emotion and bias.             means that conclusions are based on facts, without influence from personal emotions or biases. In contrast, subjectivity means that conclusions reflect personal points of view. In a study by Allport and Postman (1945), research participants described what they had heard about photos from memory. Some participants switched the race of a man threatening another person from White (which was the objective fact in the photo) to Black, possibly to fit a biased, subjective worldview. Scientists strive to be objective, but any observation by a human is, by definition, subjective. Recognizing when we are being subjective can be difficult, so scientists cannot rely on their introspections to maintain objectivity. Most of us have had the experience of witnessing an accident in the presence of other people. It can be astonishing to hear the different accounts of what happened. Didn’t we all see the same thing? Individuals like to believe that their own view of the events is the accurate one. As discussed in Chapters 9 and 10, objective facts can be altered easily when processed subjectively by individuals. The scientific methods described in this chapter promote objectivity and help prevent biased, subjective observations from distorting a scientist’s work. The second important difference between science and everyday observations is the use of systematic as opposed to hit-or-miss observation. By hit or miss, we mean making conclusions based only on whatever is happening around us. If we want to make conclusions about the human mind, we cannot restrict our observations to our immediate circle of acquaintances, friends, and loved ones. Our observations of the people we see frequently are probably quite valid. It’s just that the people we know represent a small slice of the greater population. For example, we might be surprised to learn that our favorite candidate lost an election because everyone we know voted for that candidate. Science strives to be objective, making judgments that are free from personal emotion or bias. In contrast, people’s subjectivity emerged in a classic experiment by Allport and Postman (1945). Some participants remembered hearing that this picture illustrated a Black man threatening another person rather than the objective fact—a White person is doing the threatening. Based on observations of their surroundings, college students often believe that drinking alcohol, and even binge drinking (five drinks on one occasion for men, four drinks for women), is a nearly universal behavior for anyone over the age of 18 or so. We can see why a college student might believe this after making hit-or-miss observations. Among 18- to 22-year-olds enrolled in college, nearly two-thirds reported binge drinking in the previous 30 days (NIAAA, 2015). In contrast, the overall prevalence of binge drinking at least once in the past year among U. S. adults was 24.9%, and the rate among all young adults between 18 and 24 years was 33.4% (see Figure 2.1), well below the frequency observed among those in college (CDC, 2016). Possibly even more surprising to college students, 43% of the adult population did not drink alcohol at all in the last month, and 29% did not drink alcohol during the past year (NIAAA, 2016). Science provides ways to make systematic observations. Judgments that we make based on the people we know might not apply to larger groups of people. Based only on personal experience, college students living in Nebraska and California might disagree about the prevalence of binge drinking. Finally, science relies on observable, repeatable evidence, whereas everyday observation often ignores evidence, especially when it runs counter to strongly held beliefs and expectations. Many people are convinced that women talk more than men. If you hold this belief, you are likely to notice and remember instances that support your belief more than instances that contradict it. This difference in attention and memory is termed confirmation biasconfirmation biasThe tendency to notice and remember instances that support your beliefs more than instances that contradict them.            confirmation bias        The tendency to notice and remember instances that support your beliefs more than instances that contradict them.            , and it represents one reason why objective and systematic observation are so important in scientific inquiries. Careful scientific studies have called into question the belief that women talk more than men. One group of researchers recorded students’ talking throughout the day and concluded that the widespread and highly publicized stereotype about female talkativeness is unfounded (Mehl, Vazire, Ramírez-Esparza, Slatcher, & Pennebaker, 2007, p. 82). Other studies make the argument that in most circumstances, men may actually talk more than women (James & Drakich, 1993;  Leaper & Ayres, 2007). We have all had the experience of witnessing an event with other people, only to discover that everyone seems to have a different memory of what happened. Scientific knowledge is both stable and changing. It is a work in progress, not a finished product. The fact that we may learn something new tomorrow should not make you assume that today’s knowledge is flawed. Most change occurs slowly on the cutting edges of science, not quickly or at the main core of its knowledge base. Unlike with many other fields, we expect science to improve over time. An important feature of scientific literacy is to learn to be comfortable with the idea that scientific knowledge is always open to improvement and will never be considered certain (AAAS, 2009). Critical thinkingCritical thinkingThe ability to think clearly, rationally, and independently.            Critical thinking        The ability to think clearly, rationally, and independently.            , or the ability to think clearly, rationally, and independently, is one of the foundations of scientific reasoning. The skilled critical thinker can follow logical arguments, identify mistakes in reasoning, prioritize ideas according to their importance, and apply logic to personal attitudes, beliefs, and values. Critical thinking is not built in; rather, it is a skill that people need to learn. You can begin by using five critical thinking questions to evaluate new information you come across in your everyday life, starting with what you read in this textbook (Bernstein, 2011): What am I being asked to believe or accept? What evidence supports this position? Are there other ways that this evidence could be interpreted? What other evidence would I need to evaluate these alternatives? What are the most reasonable conclusions? It is also helpful to recognize the signs that you are not thinking critically (excerpt taken from Lau, 2016): I prefer being given the correct answers rather than figuring them out myself. I don’t like to think a lot about my decisions, as I rely only on gut feelings. I don’t usually review the mistakes I have made. I don’t like to be criticized. Popular press reports often state, Experts agree that some fact or another is true. It is very important to use your very best critical thinking in evaluating these statements. The humor website Cracked.com often publishes lists from their contributors. One such list was called 28 Underrated Ways Life Is Different for Men and Women (Cracked Readers, 2016). This list was the result of a contest in which the participants needed to submit at least one scientific article to back up their claim. This sounds like a good opportunity to apply the critical thinking skills that you’ve learned in this chapter. We’d like you to walk through this process, and then you’ll have a chance to try one of the other facts from this site or another of your choice. A popular press website, Cracked.com, suggested that eye-tracking data was proof that men stare at men’s crotches a lot more than women do. Evaluating claims like this requires our best critical thinking skills. Coming in at #4 on the Cracked.com list is the statement, accompanied by an image of eye-tracking results, that Men stare at men’s crotches a lot more than women do!What Am I Being Asked to Believe or Accept? Our answer: We are being asked to accept the fact that men stare at men’s crotches a lot more than women do. Note that a lot more isn’t very specific. This could mean that more men than women stare at men’s crotches, or that men spend more time staring, or something completely different. The best scientific statements are very specific. What Evidence Supports This Position? Our answer: Here, things become very complicated. Cracked.com does not provide the source that their contributor used, so finding the original piece required some fancy Googling. The result traces back to an in-house eye-tracking study conducted in 2005 by the Nielsen/Norman Group, experts in user experience research. So far as we can tell, the study was never published in a peer-reviewed scientific journal (a huge weakness), but it was written up by a number of blogs. The best report was a description in the USC Annenberg’s Online Journalism Review (Ruel, 2007). Ruel’s report noted that the 255 participants between the ages of 18 and 64, 58% female and 42% male, looked for different lengths of time at parts of a photo of baseball player George Brett while undergoing eye-tracking analysis. In addition, when asked to browse the American Kennel Club website, the Nielsen/Norman researchers found that men fixated longer on the dogs’ genitalia than the women did. Are There Other Ways That This Evidence Could Be Interpreted? Our answer: Eye-tracking data, handled well, can be an excellent research tool. However, we don’t know much about how fixation length was calculated by Nielsen/Norman. We don’t know if the heat map image in Cracked is from a single participant or represents an average of viewer responses. Although the image seems clear (women or a single female participant fixated only on the head and shoulders area), psychologists would usually want a quantitative measure of a supposed difference between two groups. Inferential statistics are needed before we can make conclusions about the behavior of populations of men and women. What Other Evidence Do We Need? Our answer: One of the key points discussed in this chapter is the concept of generalization. When we say Men do something more than women, we are making a very general statement about gender than crosses other variables like age, race or ethnicity, nationality, education, and so on. We would want to ensure that the Nielsen/Norman sample was truly representative before making such a claim. One of the biggest weaknesses in the interpretation of these data is the use of two types of stimuli—a photo of George Brett and photos of dogs. A much wider selection of stimuli would help us understand if a general principle were involved or whether people were just reacting to these particular stimuli. What Are the Most Reasonable Conclusions? Our answer: While the result presented by Cracked.com is entertaining, which is the purpose of the website, it does not represent good science. We would want to see quite a bit more detail about the methods and analyses, along with publication in peer-reviewed sources, before we accept the conclusions as valid. Using our model critical thinking questions, explore one of the other Cracked.com list items or a popular press headline about psychology of your choice, and evaluate the item. Do you think you might have evaluated the claim differently before reading this chapter? Why or why not? Critical thinking not only is essential to good science, but also provides the underpinning of a free society. Our ability to think clearly, rationally, and independently gives us the confidence to question the actions of people in authority instead of engaging in blind obedience. We hope you will continue to practice good critical thinking skills long after you finish reading this textbook. Scientific research results do not support the common stereotype that women talk more than men. Learning scientific facts is not the same as understanding how science works. Science, including psychological science, is more than a collection of facts—it is a process (see Figure 2.2). From top to bottom, an hourglass shape starts out wide, narrows, and then expands again. The steps of scientific reasoning follow the same pattern. Beginning broadly with an examination of a phenomenon or research question, scientists then narrow their thinking to generate specific hypotheses and methods. After obtaining the results, they finally consider the broader implications of a study. Science seeks to develop theoriestheoriesA set of facts and relationships between facts that can explain and predict related phenomena.            theories        A set of facts and relationships between facts that can explain and predict related phenomena.            , which are sets of facts and relationships between facts that can be used to explain and predict phenomena (Cacioppo, Semin, & Berntson, 2004). In other words, scientists construct the best possible models of reality based on the facts known to date. Unfortunately, the English language can be the source of considerable confusion regarding the nature of scientific theories. In addition to its use in science, the word theory can be used in nonscientific ways to describe a guess, such as I have a theory about why my professor seems unusually cheerful this morning, or a hypothetical situation, as in That’s the theory, but it may not work in practice. Confusion over the multiple meanings of the word theory have led people mistakenly to view truly scientific theories, like the theory of evolution, as nothing more than casual guesses or hunches, rather than the thoroughly investigated and massively supported principles that they are. The best scientific theories not only explain and organize known facts, but also generate new predictions (see Figure 2.3). The word prediction comes from the Latin words for saying before. A scientific prediction is more than a guess or hunch. It is usually stated in a rigorous, mathematical form that allows the scientist to say that under a certain set of circumstances, a certain set of outcomes are likely to occur (if A, then B). In some cases, a theory’s predictions can be surprising. For example, you might believe that it’s impossible to be happy and sad at the same time. However, one model of emotion, discussed in Chapter 7, predicted that it is quite possible to feel happy and sad at the same time (Cacioppo, Berntson, Norris, & Gollan, 2012). This prediction was confirmed by research showing that first-year college students reported feeling either happy or sad, but not both, on a normal day of school, but experiencing both emotions simultaneously on the day they moved out of campus housing to go home for the summer. Theory building begins with generating hypotheses that are then systematically tested. Hypotheses that are not rejected contribute to the theory and help generate new hypotheses. Before attempting to generate your own scientific questions, it pays to become familiar with relevant theories and previous discoveries. As Sir Isaac Newton noted, scholars stand on the shoulders of giants (Turnbull, 1959)—we build on the work of those who came before us. New lines of research can also originate in observation. Scientists are observers not just in the laboratory, but also in everyday life. Scientific progress often takes a giant leap forward when a gifted observer recognizes a deeper meaningfulness in an everyday occurrence, such as when Newton observed a falling apple and considered its implications for a law of gravity. As we discovered in Chapter 1, Ivan Petrovich Pavlov realized that when his dogs learned to salivate to signals predicting the arrival of food, something more significant than slobbering dogs was happening. The learning that he observed explains why we get butterflies in our stomach before a performance and avoid foods that we think made us ill. Once you understand the theoretical foundations of your area of interest, you are ready to generate a hypothesis. A hypothesishypothesisA proposed explanation for a situation, usually taking the form If A happens, then B will be the result.            hypothesis        A proposed explanation for a situation, usually taking the form If A happens, then B will be the result.             is a type of inference, or an educated guess, based on prior evidence and logical possibilities (see Figure 2.4). A good hypothesis links concrete variables based on your theory and makes specific predictions. For example, researchers predicted that participants who viewed a video featuring a Stress is good for you message would show improved psychological symptoms and work performance relative to a group exposed to Stress is bad for you messages (Crum, Salovey, & Achor, 2013). The concrete variables in this study are exposure to the two different videos (Stress is good for you versus Stress is bad for you) and the measures of psychological symptoms and work performance. The researchers also must consider the possibility that there would be no difference in the effects of exposure to the video messages. Crum, Salovey and Achor (2013) tested a hypothesis that viewing a Stress is good for you message or a Stress is bad for you message would produce different outcomes. Participants viewing the Stress is good for you messages experienced increased soft work outcomes (maintain focus and communicate), and hard work outcomes (quality, quantity, accuracy, and efficiency) relative to the Stress is bad for you and control groups. Scientists can never prove that a hypothesis is true because some future experiment, possibly using new technology not currently available, might show the hypothesis to be false. All they can do is show when a hypothesis is false. A false hypothesis must always be modified or discarded. Once you have a hypothesis, you are ready to collect the data necessary to evaluate it. The existing scientific literature in your area of interest provides considerable guidance regarding your choice of methods, materials, types of data to collect, and ways to interpret your data. Science is a vastly collaborative enterprise. Not only do we stand on the shoulders of giants because we benefit from the work that has been done previously, but we depend on many others in the scientific community to help us improve our work and avoid mistakes. Normally, this evaluation is done by submitting research to conferences or for publication. During this process, research undergoes peer reviewpeer reviewThe process of having other experts examine research prior to its publication            peer review        The process of having other experts examine research prior to its publication            , in which it is scrutinized by other scientists who are experts in the same area. Only if other experts conclude that new research is important, accurate, and explained thoroughly will it be added to the existing body of scientific knowledge. To demonstrate the importance of this peer review, contrast this process to what happens when a person simply decides to transmit a tweet or launch a personal website. The author is solely responsible for the content, and there are no checks on the accuracy of that content. A contemporary theory of emotion correctly predicted the circumstances for when we might experience mixed emotions of happiness and sadness. Graduation from college is an important accomplishment, but we might feel sad about leaving our friends. The methods of psychological science can help us evaluate questions like the effects, if any, that playing violent video games has on physical aggression. One of the checks on science is the practice of replicating, or attempting to reproduce, scientific data. A scientist producing the original data might want to double-check his or her results, or other scientists might want to see if they can produce the same results (Simons, 2014). What happens if a study fails to replicate? When a group of nearly 300 psychologists led by Brian Nosek, known as the Open Science Collaboration (OSC), set out to replicate 100 studies from three well-respected psychology journals, the results were not encouraging (Nosek, et al., 2015). Only 36% of the replications produced significant results, meaning that the researchers were unable to duplicate the original findings of the studies the majority of the time. The average effect size, or the strength of a phenomenon, in the replications was only about half of that reported in the original studies. In other words, if a study found that 50% of the difference in physical aggressiveness was due to hot weather, the replication might show that only about 25% of the difference in aggressiveness was accounted for by the weather. Does this mean that we have to throw out these results? While failure to replicate should give any scientist a reason to reflect on his or her results, other psychologists are not particularly alarmed by the findings of Nosek and his colleagues (Gilbert, King, Pettigrew, & Wilson, 2016). The methods used in the replication efforts were often different from the original study. For example, a sample of Italians substituted for Americans in a study of attitudes toward African Americans and a study of college students being called on by a professor was replicated with people who had not attended college. These differences in sampling, rather than the validity of the original study, might have been the key reason for finding different outcomes. Scientific debates usually move us in the right direction. Regardless of how big a replication problem psychology might or might not have, psychologists are looking for new methods to make their results even more reliable. For example, submitting a public registered report of the methods and statistical analyses that a researcher plans to run prior to conducting a study discourages any after-the-fact efforts to tweak data and results to find something interesting when the main purpose of the study fails. Sharing data, detailed methods, and results in public places allows others to evaluate a researcher’s findings. Addressing unrealistic pressure on academics to publish or perish and putting quality of research ahead of quantity should also contribute to a more robust scientific environment. One of the studies cited frequently in psychology’s replication crisis is a 2008 paper by Schnall, Benton, and Harvey that reported that handwashing reduced the severity of participants’ moral judgments. Two efforts conducted by Johnson, Cheung, and Donnellan (2014) to replicate these findings failed, leading to a heated debate about what replication means. During peer review, research that fits with existing knowledge is typically accepted more rapidly than work that is less consistent with previous reports. Results often undergo replicationreplicationRepeating an experiment and producing the same results.            replication        Repeating an experiment and producing the same results.            , which means that other scientists independently attempt to reproduce the results of the study in question (Klein, et al., 2014). If the data are replicated, they will be accepted quickly. If other scientists are unable to replicate the data, their extra effort will have prevented inaccurate results from cluttering the scientific literature. Although this process might slow the publication of some innovative research, the result—more accuracy—is worth the effort. Questions for Detecting Good Critical ThinkingQuestions for Detecting Poor Critical Thinking(All answers should be no)What am I being asked to believe or accept? Do I prefer being given the correct answers rather than figuring them out myself? What evidence supports this position? Do I rely on gut feelings instead of thinking a lot about my decisions? Are there other ways that this evidence could be interpreted? Am I forgetting to review my conclusions to check for mistakes? What other evidence would I need to evaluate these alternatives? Am I oversensitive to criticism about my conclusions? What are the most reasonable conclusions? Psychological scientists use a variety of research methods, including descriptive, correlational, and experimental methods, depending on the type of question being asked. Descriptive methods, including surveys, case studies, and observations, provide a good starting place for a new research question. Correlational methods help psychologists see how two variables of interest, like the number of hours spent playing video games and level of physical aggression, relate to each other. Psychologists use experiments to test their hypotheses and to determine the causes of behavior. In the next sections, we will describe the common research methods used in psychological science and then compare how they might be used to approach a particular question—whether exposure to video game violence increases aggression. Each method—descriptive, correlational, and experimental—provides a different view of the phenomenon in question, and each has a particular profile of strengths and weaknesses. Each requires different types of statistical analyses, which are described in more depth later in the chapter. Many psychological studies combine several of these methods. When similar outcomes are observed using multiple methods, we have even more confidence in our results. Descriptive methodsDescriptive methodsResearch methods designed for making careful, systematic observations.            Descriptive methods        Research methods designed for making careful, systematic observations.             include case studies, naturalistic observations, and surveys. As we have seen, personal observations and common-sense ideas are especially vulnerable to bias, but descriptive methods allow a researcher to make careful, systematic, real-world observations. Descriptive methods can illuminate associations between variables and establish prevalence rates. Armed with these scientific observations, the researcher will be in a strong position to generate hypotheses. A case studycase studyAn in-depth analysis of the behavior of one person or a small number of people.            case study        An in-depth analysis of the behavior of one person or a small number of people.             provides an in-depth analysis of the behavior of one person or a small number of people. Many fields, including medicine, law, and business, use the case study method. Psychologists often use case studies when large numbers of participants are not available or when a particular participant possesses unique characteristics, as in the case described in this section. Interviews, background records, observation, personality tests, cognitive tests, and brain imaging provide information necessary to evaluate the case. Case studies not only are a useful source of hypotheses, but also can be used to test hypotheses. If you did a case study on a planet outside our solar system and discovered life there, you would disprove a hypothesis that no life exists outside our solar system. One of the most productive case studies in psychology chronicled more than 50 years of examinations of Henry Molaison (1926–2008), known in the scientific literature as the amnesiac patient H. M. In 1953, Molaison underwent brain surgery to control his frequent, severe seizures. Although the surgery may have saved his life, he was left with profound memory deficits, which are described in Chapter 9. Through painstaking testing and evaluation of Molaison, psychologists learned a great deal about the brain structures and processes that support the formation of memories (Corkin, 2002). Even after his death, Molaison continues to contribute to our knowledge. Researchers from the Massachusetts Institute of Technology (MIT), the Massachusetts General Hospital, and the University of California, San Diego, are analyzing Molaison’s brain today. How could you use the case study method to learn about exposure to video game violence and aggression? You could conduct a case study of Michael Carneal, who was sentenced to life in prison after he began shooting students at his high school in West Paducah, Kentucky, killing three students and seriously wounding five others. He had never shot a real gun before, but he was fond of playing first-person-shooter games like Doom. To conduct your case study, you gather background facts about Carneal’s case, possibly by interviewing others associated with the case, viewing legal and medical documents, and observing media accounts. You interview Carneal and possibly administer established personality and clinical tests like those discussed in Chapters 12 and 15. One of the most famous case studies in psychology is that of Henry Molaison (left), who was known in the literature as the amnesic patient H. M. until his death in 2008. For more than 50 years, Molaison allowed psychologists to evaluate his memory deficits resulting from brain surgery. After his death, scientists like Jacopo Annese (below) of the University of California, San Diego, began a careful examination of Molaison’s brain that continues today. What are the advantages of using the case study method to learn about the effects of playing violent video games on aggression? Mass shooters are thankfully quite rare, and the case study method is well suited to learning about unusual situations. A case study can contribute to science by testing hypotheses. If a hypothesis made a strong prediction that all mass shooters play violent video games, finding and documenting a case of a mass shooter who did not play violent video games would require rejection of the hypothesis. Based on the detailed data that you obtain from Carneal’s case, you will be better prepared to generate and test additional hypotheses. If you are interested in learning about larger groups of people than are possible with the case study method, you might pursue naturalistic observationnaturalistic observationAn in-depth study of a phenomenon in its natural setting.            naturalistic observation        An in-depth study of a phenomenon in its natural setting.            , or in-depth study of a phenomenon in its natural setting. Compared to the case study method, we are looking at a larger group of people, which will strengthen our ability to apply our results to the general population. We also have the advantage of observing individuals in their natural, everyday circumstances. Michael Carneal is serving a life sentence in prison for shooting and killing fellow students at his high school in 1997, when he was 14. A classic example of the method of naturalistic observation is the careful, long-term study of chimpanzees conducted in their habitat by Jane Goodall. In the summer of 1960, Goodall, then 26 years old, began her painstaking observations of chimpanzees living in Gombe National Park in Tanzania. Among her discoveries was that chimpanzees were not vegetarians, as previously assumed (Goodall, 1971, p. 34):I saw that one of them was holding a pink-looking object from which he was from time to time pulling pieces with his teeth. There was a female and a youngster and they were both reaching out toward the male, their hands actually touching his mouth. Presently the female picked up a piece of the pink thing and put it to her mouth: it was at this moment that I realized the chimps were eating meat. As a result of Goodall’s years spent following the chimpanzees, scientists have a rich, accurate knowledge of the behavior of these animals in the wild. Impressed by Goodall’s results, you plan to pursue further knowledge about violent video games and aggression by attending local area network (LAN) parties, where attendees bring their computers to a gathering place to play multiplayer, networked video games. You hope that by observing people playing violent video games and watching their subsequent behavior for signs of aggression, you might reach some conclusions about the relationships between video game violence and aggression. As in Goodall’s case, this approach has the advantages of providing insight into natural, real-world behaviors with large numbers of participants. Some naturalistic observations are conducted when people know that they are being observed, while in other cases, people are unaware of being observed. Both situations raise challenges. If we know we are being observed, we might act differently. Your LAN party participants know that aggression is not viewed positively in our culture, so they might act less aggressively when they know that they’re being watched. Watching people who do not know that they’re being watched raises ethical issues, which will be explored later in this chapter. How would you feel if you discovered that you had been an unwitting participant in a study? Jane Goodall used naturalistic observation to illuminate the world of the chimpanzee. The use of naturalistic observation illustrates the importance of choosing a method that is well suited to the research goals. Like the case study method, naturalistic observation can be helpful for developing hypotheses, but other methods must be used to test them. Most hypotheses in psychology look at the relationships between two or more concepts, like the exposure to violent video games and aggression in this example. Testing a hypothesis would allow you to say whether a relationship between exposure to violent video games and aggression exists, how strong the relationship is, what direction it goes in, and so on. It might appear to you that the people you observe are more aggressive following their LAN parties, but you have no way to demonstrate your point. People engage in lots of behaviors during LAN parties. Perhaps eating pizza or staying up all night at LAN parties enhances aggressive tendencies. With only your naturalistic observations to go on, you can’t say for sure. SurveysSurveysA descriptive method in which participants are asked the same questions            Surveys        A descriptive method in which participants are asked the same questions            , or questionnaires, allow us to ask large numbers of people questions about attitudes and behavior. Surveys provide a great deal of useful information quickly, at relatively little expense. Commercial online survey services make conducting surveys easier than in the past. One of the primary requirements for a good survey is the use of an appropriate samplesampleA subset of a population being studied.            sample        A subset of a population being studied.            , or subset of a population being studied. The populationpopulationThe entire group from which a sample is taken.            population        The entire group from which a sample is taken.             consists of the entire group from which a sample is taken. Good results require large samples that are typical, or representative, of the population that we wish to describe. Major pollsters, like the Pew Research Center and Gallup, take great pains to recruit survey participants who mirror the characteristics of the public across factors such as gender, age, race or ethnicity, education, occupation, income, and geographical location. Surveys use self-reporting, so results can be influenced by people’s natural tendency to want to appear socially appropriate (Corbett, 1991). As we will discover in Chapter 13, people have strong tendencies to conform to the expectations of others. In some surveys, this factor is not a problem. If you ask people whether they prefer lattes or mochas, you will probably get a fairly honest answer. However, when people believe that their true attitudes and behaviors will not be viewed favorably by others, they are more likely to lie, even when their answers are confidential and anonymous. To pursue the question of the effects of violent video games on aggression, you could do a naturalistic observation at a LAN party. Let’s see how scientists have used the survey method to explore video game violence. One survey involved 1,254 middle school students attending public schools in South Carolina and Pennsylvania (Olson, 2010). Although the sample did not include children from other geographic locations or children who are homeschooled or attending private schools, it appeared to be relatively representative of this age group. Children were asked to respond to 17 motives for playing video games on a 4-point scale, from strongly agree to strongly disagree. Like all surveys, this one depended on self-reporting, which raises the possibility that children would give the right answer instead of responding honestly. What did this scientific survey discover about violent video games? As shown in Figure 2.5, a surprising outcome of the survey was the more than 20% of boys who reported a violent video game helps me relax and helps me get my anger out. Cheryl  Olson (2010) asked middle school students why they liked to play violent video games. More than 20% of the boys agreed that a game helps me relax and helps me get my anger out. If we want to generalize our conclusions to people, it is very important for us to sample from the population of people rather than depending on handy convenience samples of undergraduate students enrolled in psychology courses. Given limited resources (time and money), how do psychological scientists reach a diverse sample of participants? You might think that using online recruitment, such as MTurk and Survey Monkey, could solve this problem, but that does not appear to be the case (Maner, 2016). These participants are probably not typical of the adult population in terms of their education and understanding of technology, and they have been exposed to the research methods used by behavioral scientists. Jon  Maner (2016) makes a strong case for conducting studies in the field as a way of recruiting more diverse and representative samples. He describes a study of diet and exercise effects on diabetes conducted in 27 geographically diverse clinical centers. Not only were the researchers able to find a large sample (over 3,000 people participated), but 45% of the sample identified themselves as members of underrepresented minority groups. An additional advantage of these field studies, according to Maner, is their stronger likelihood of being replicable. In another section of this chapter, we explored the controversy over the replicability of psychological research. Psychological scientists continue their search for methods that will provide more accurate results. Ensuring that a research sample is diverse, mirroring the population, is an important step in this direction. CorrelationsCorrelationsA measure of the direction and strength of the relationship between two variables.            Correlations        A measure of the direction and strength of the relationship between two variables.             measure the direction and strength of the relationship between two variablesvariablesA factor that has a range of values.            variables        A factor that has a range of values.            , or factors that have values that can change, like a person’s height and weight. Correlations allow psychologists to explore whether hours of sleep are related to student grade point averages (Trockel, Barnes, & Egget, 2000) or whether the age of parents is related to the rate of autism spectrum disorder among their children (Reichenberg, Gross, Kolevzon, & Susser, 2011). If you’re curious about the results of these studies, the first showed that sleep and GPA were not related, and the second showed that the age of parents is related to the rate of autism spectrum disorder among their children. We begin our analysis of correlations by measuring our variables. A measuremeasureA method for describing a variable’s quantity.            measure        A method for describing a variable’s quantity.             answers the simple question of how much of a variable we have observed. After we obtain measures of each variable, we compare the values of one variable to those of the other and conduct a statistical analysis of the results. Three possible outcomes from the comparison between our two variables can occur: positive, negative, or zero correlation. In a positive correlation, high levels of one variable are associated with high levels of the other. Height and weight usually show this type of relationship. In most cases, people who are taller weigh more than people who are shorter. Two variables also can show a negative correlation, in which high values of one variable are associated with low values of another. For example, high levels of alcohol consumption among college students are usually associated with low GPAs. The third possible outcome is a zero correlation, in which the two variables have no systematic relationship with each other. When variables have a zero correlation, knowing the value of one variable does not tell us anything about the value of the other (see Figure 2.6). For example, emergency room and law enforcement personnel are often convinced that they are busier with emergencies and crime on nights with a full moon. In contrast, numerous scientific studies of lunar cycles show zero correlation with emergency room admissions, traffic accidents, or other types of trauma (Stomp, ten Duis, & Nijsten, 2011). People taking surveys might be more interested in pleasing others or appearing normal than in answering honestly.(a) In positive correlations, high levels of one variable are associated with high levels of the other variable. (b) In negative correlations, high values of one variable are associated with low levels of the other variable. (c) In zero correlations, the two variables do not have any relationship with each other. Correlational research results are frequently misunderstood. Correlations permit us to discuss the relationships between two variables but tell us nothing about whether one variable causes changes in the other. Let us say that we discover a positive correlation between violent video games and aggression: Youth who play the most hours of violent video games have the most reports of physical aggression at school. However, we still cannot say that playing violent video games causes physical aggression at school. This conclusion may seem reasonable and possibly true, so why must we abandon it? First, the two variables in a correlation can influence each other simultaneously. Although it may be true that playing violent video games leads to physical aggression at school, youth who experience physical aggression at school may be more attracted to violent video games as an outlet for their frustration. Second, we might be observing a situation in which a third variablethird variableA variable that is responsible for a correlation observed between two other variables of interest.            third variable        A variable that is responsible for a correlation observed between two other variables of interest.             is responsible for the correlation between our two variables of interest. Consider the observation that many school shootings have been perpetrated by people who had been bullied relentlessly by others. Perhaps the experience of having been bullied (the third variable in this case) predisposes both a choice of violent recreation and a tendency to engage in aggressive behavior at school (see Figure 2.7). Third variables can be responsible for the correlation that we observe in two other variables. In our example of video game violence and aggression, being bullied could be a third variable that predicts both a choice of violent games and a tendency to be aggressive at school. The possibility of third variables is one reason that we must be careful when we reach conclusions based on correlational data. When you’re reading or listening to the news, watch for the use of words like link, association, or relationship used to describe two variables, such as a headline that states Lack of sleep linked to depression or Drinking red wine associated with lower rates of heart disease. These key words usually mean that the data are correlational but are often mistaken for causal. Now you know how you should—and should not—interpret these reports. If we cannot make conclusions about causality using correlations, why would we use them? In a number of circumstances, correlations are more appropriate than other research methods. For example, it would be unethical to ask pregnant women to consume different amounts of alcohol to assess the effects of prenatal alcohol on their infants. Instead, we can ask pregnant women to identify their alcohol intake in diaries, which can then be correlated with various measures of infant functioning. Although this method will not allow us to conclude that drinking during pregnancy causes damage to the fetus, we can correctly identify the strength and direction (positive, negative, or zero) of any correlation between drinks consumed and infants’ outcome measures (see Figure 2.8). Using this approach, researchers have shown that heavy maternal alcohol consumption is correlated with abnormal amounts of thickening of the outer layer of the brain (the cortex, which is discussed in Chapter 4; also see  Gautam, Warner, Kan, & Sowell, 2015). Even though we cannot make conclusions about causes based on correlations, we can obtain useful information. It would be unethical to assign pregnant women to drinking and nondrinking groups, but the negative correlation that we find between drinks consumed by pregnant women and measures of infant mental development can tell us that drinking during pregnancy is not a good idea. The more alcohol the women consumed, the lower their children scored on tests of infant mental development. The scientist’s most powerful tool for drawing conclusions about research questions is the formal experimentexperimentA research method that tests hypotheses and allows researchers to make conclusions about causality.            experiment        A research method that tests hypotheses and allows researchers to make conclusions about causality.            . Unlike cases in which descriptive methods are used, the researcher conducting an experiment has a great deal of control over the situation. Unlike correlational methods, the use of the formal experiment allows us to talk about cause (see Figure 2.9). A good experimental design features random assignment of participants to groups, appropriate control groups, control of situational variables, and carefully selected independent and dependent variables. A researcher begins designing an experiment with a hypothesis, which can be viewed as a highly educated guess based on systematic observations, a review of previous research, or a scientific theory. An experimental hypothesis takes this form: If I do this, that will happen. To test the hypothesis, the researcher manipulates or modifies the value of one or more variables and observes changes in the values of others. The variable controlled and manipulated by an experimenter (If I do this) is known as the independent variableindependent variableAn experimental variable controlled and manipulated by the experimenter; the if A happens part of a hypothesis            independent variable        An experimental variable controlled and manipulated by the experimenter; the if A happens part of a hypothesis            . We need some way to evaluate the effects of this manipulation. We use a dependent variabledependent variableA measure that demonstrates the effects of an independent variable; the result part of a hypothesis            dependent variable        A measure that demonstrates the effects of an independent variable; the result part of a hypothesis            , defined as a measure used to assess the effects of the manipulation of the independent variable, to tell us what will happen as a result of the independent variable. Like the independent variable, our choice of dependent variable is based on our original hypothesis. After determining our independent and dependent variables, we still have quite a bit of work to do. In most experiments, we want to know how simply going through the procedures of being in an experiment influences our dependent variable. Perhaps the hassle of going to a laboratory and filling out paperwork changes our behavior. To evaluate these irrelevant effects and establish a baseline of behavior under the experimental conditions, we assign some of our participants to a control groupcontrol groupA group that experiences all experimental procedures, with the exception of exposure to the independent variable.            control group        A group that experiences all experimental procedures, with the exception of exposure to the independent variable.            . In many experiments, the control group will experience all experimental procedures except exposure to the independent variable. When a new treatment is being tested, the control group might experience the standard treatment for a condition. The experience of the control group should be as similar as possible to that of the experimental groupsexperimental groupsA group of participants who are exposed to the independent variable.            experimental groups        A group of participants who are exposed to the independent variable.            , who experience different values of the independent variable. We want to ensure that our dependent variables reflect the outcomes of our independent variables instead of individual differences among the participants’ personalities, abilities, motivations, and similar factors. To prevent these individual differences from masking or distorting the effects of our independent variable, we randomly assign participants to experimental or control groups. Random assignmentRandom assignmentThe procedure in which each participant has an equal chance of being placed in any group in an experiment            Random assignment        The procedure in which each participant has an equal chance of being placed in any group in an experiment             means that each participant has an equal chance of being assigned to any group in an experiment. With random assignment, differences that we see between the behavior of one group and that of another are unlikely to be the result of the individual differences among the participants, which tend to cancel each other out. Individual differences among participants are an example of confounding variablesconfounding variablesVariables that are irrelevant to the hypothesis being tested but can alter a researcher’s conclusions.            confounding variables        Variables that are irrelevant to the hypothesis being tested but can alter a researcher’s conclusions.            , or variables that are irrelevant to the hypothesis being tested and can alter or distort our conclusions. For example, a researcher might want to test the effects of aerobic exercise on blood pressure. If some participants competed in triathlons without the researcher’s knowledge, their athletic experience would confound the interpretation of the results. Random assignment to groups typically controls for confounds because of these types of individual differences, but other sources of confounds exist. Situational confounds, such as time of day or noise levels in a laboratory, also could affect the interpretation of an experiment. Scientists attempt to run their experiments under the most constant circumstances possible to rule out situational confounding variables. Let’s return to our question about violent video games and aggression and see how one real experiment addressed the issue. The researchers tested a hypothesis stating that playing video games varying in violent content (the independent variable) would influence subsequent levels of aggression (the dependent variable; see  Anderson & Dill, 2000). The experimental group played a violent game (Wolfenstein 3D), while the control group played a nonviolent game (Myst). Participants were randomly assigned to groups. You can imagine how the results might be distorted if the experimental group consisted of football players and the control group consisted of members of the chess club. Following their game-playing, participants had an opportunity to choose the intensity and duration for a blast of noise to punish a remotely located loser (who didn’t exist) in a competitive reaction time activity, and their choices served as the dependent variable. We’re assuming that laboratory conditions were held constant to avoid situational confounds. For example, we know that people feel more aggressive in hot temperatures (Carlsmith & Anderson, 1979). What would happen to our experiment if the air conditioner in the laboratory broke while some participants played violent video games, but was fixed right before the next participants played nonviolent games? Results of this carefully controlled experiment confirmed the researchers’ hypothesis: Playing the violent game led to the stated intent to administer longer noise blasts to another person (see Figure 2.10). Anderson and Dill (2000) randomly assigned participants to groups playing violent and nonviolent video games. After playing, each participant was asked to choose the strength and duration of a blast of noise to be administered to another person. Keeping conditions in the laboratory as constant as possible would help to reduce the impact of situational variables. As powerful as it is, the experimental method, like the other methods discussed previously, has some limitations. Experiments can be somewhat artificial. Participants know that they are in a research study, and they may vary their behavior as a result. However, making a laboratory experiment more realistic can raise ethical challenges. In a study conducted in 1962, before current ethical guidelines for research had been adopted, military personnel were led to believe that their lives were in danger so that experimenters could realistically assess the effects of panic on performance (Berkun, Bialek, Kern, & Yagi, 1962). Although the responses of these participants were probably quite representative of real life, few of us would want to be put in their position. This type of research could not be conducted under today’s ethical standards, as explained later in this chapter. Artificiality is also a problem with the Anderson and Dill (2000) video game study. Our real-world question relates to the likelihood that playing violent video games may elicit real physical violence in real situations. Is a person’s willingness to inflict a loud sound on another person in a professor’s laboratory truly representative of the type of violence involved in mass shootings like Carneal committed? It may or may not be; we need additional research to find out. Researchers attempt to reduce the impact of confounding variables on their results. In a test of the effects of aerobic exercise on blood pressure, situational confounding variables, such as (1) traffic outside the building, (2) a noisy treadmill, and (3) a neighbor breathing heavily, can be controlled by holding the environment as constant as possible for all participants. Individual differences, such as (4) an early morning after little sleep or (5) superior fitness, can be controlled by randomly assigning participants to groups. To conduct an experiment, we must carefully operationalize, or define our variables in practical terms. One way to operationalize physical aggression is to measure how often a preschooler has a physical fight with others. Another issue with the experimental method arises from differences in the choices of independent and dependent variables. Independent (controlled) and dependent (measured) variables have to be defined and implemented in some concrete fashion. The process of translating abstract independent and dependent variables into measurable forms is called operationalizationoperationalizationDefining variables in ways that allow them to be measured.            operationalization        Defining variables in ways that allow them to be measured.            . The first step in operationalization is to identify the concept to be measured clearly, such as the violence in a video game. Next, the quantitative measures of the concept must be determined. What measures do you need to tell a violent video game from a nonviolent one? You might develop a rating scale based on the number of times violent images, such as depictions of physical injury or blood, are presented in a particular game. Finally, a method for obtaining this measure must be developed. The Entertainment Software Rating Board (ESRB) uses a panel of trained raters to evaluate a DVD of representative game-play submitted by a manufacturer of a new game to establish an appropriate age group rating (ESRB, 2014). There are many ways to operationalize variables in practical terms. Anderson and Dill (2000) operationalized aggression in terms of how lengthy and loud a sound blast a person was willing to inflict on another person. One of the odder dependent variables used in video game aggression research is the hot sauce paradigm, in which the amounts of hot sauce that participants choose to be administered to another participant are used to measure aggression (Lieberman, Solomon, Greenberg, & McGregor, 1999). Other researchers might choose different ways to operationalize aggression, such as frequency of physical fights among preschoolers. As a result, even though there is a large body of work regarding the impact of violent video games on aggression, the variables used are so different that few direct comparisons can be made among the many studies. The point of this discussion is not to convince you that scientists don’t know what they’re talking about, but rather to impress upon you the importance of reviewing research results using your best critical thinking skills. In seeking to understand something as complicated as the science of mind, it is unlikely that any single study could provide complete information about a phenomenon. Instead, progress in our understanding results from the work of many scientists using diverse methods to answer the same question. Conducting a meta-analysismeta-analysisA statistical analysis of many previous experiments on a single topic.            meta-analysis        A statistical analysis of many previous experiments on a single topic.            , or a statistical analysis of many previous experiments on the same topic, often provides a clearer picture than do single experiments observed in isolation. Nearly all studies of video games and aggression use participants playing alone. If we add the social perspective by studying people playing together, we find that aggressive behavior actually decreases following play, regardless of whether the game was violent. Meta-analyses have their own share of challenges, however. A meta-analysis is only as good as the studies on which it is based. Published studies available to researchers conducting a meta-analysis might be subject to publication biaspublication biasThe possibility that published studies are not representative of all work done on a particular phenomenon.            publication bias        The possibility that published studies are not representative of all work done on a particular phenomenon.            , or the possibility that they are not representative of all the work done on a particular problem. A file drawer problem also exists, in which journals are more likely to publish studies that demonstrate significant effects of an independent variable, such as video game violence, on a dependent variable, such as aggression, than studies that show no significant effects. If publication bias is present, the results of any meta-analysis might be misleading. As we will see in so many examples in this textbook, even the best research designs might mislead us if they fail to take multiple perspectives into account. Most of the research discussed so far regarding video games and aggression has involved single participants playing alone. What happens when we zoom out, using the social perspective to look at the effects of video games on groups of people playing together? Playing video games cooperatively is associated with less subsequent aggressive behavior, regardless of whether the game played was violent or not (Jerabeck & Ferguson, 2013). The research discussed throughout the remainder of this textbook has been subjected to considerable skeptical review by other experts in the field. Most has stood the dual tests of peer review and replication by others. Converging evidence from descriptive, correlational, and experimental research provides us with confidence in our conclusions. Psychology, like any science, has followed its share of wrong turns and dead ends, but most knowledge presented here has been carefully crafted to present the most accurate view possible of behavior and mental processes. The methods described previously can be used by psychologists to provide guidance across many disciplines. For example, how many times have you heard a parent complain about a child’s out-of-control behavior and blame it on too much sugar? How would a scientist know whether sugar or any other food ingredient affected children’s behavior? Scientific discoveries in this area could benefit the fields of nutrition, health, medicine, and child development. The gold standard for demonstrating the objective effects of any substance, whether a food additive, medication, or recreational drug, is the double-blind proceduredouble-blind procedureA research design that controls for placebo effects in which neither the participant nor the experimenter observing the participant knows whether the participant was given an active substance or treatment or a placebo.            double-blind procedure        A research design that controls for placebo effects in which neither the participant nor the experimenter observing the participant knows whether the participant was given an active substance or treatment or a placebo.            . This procedure requires a placeboplaceboAn inactive substance or treatment that cannot be distinguished from a real, active substance or treatment.            placebo        An inactive substance or treatment that cannot be distinguished from a real, active substance or treatment.            , an inactive substance that cannot be distinguished from a real, active substance. The first blind aspect of this procedure is the inability of participants to know whether they have taken a real substance or a placebo. This feature controls for effects of the participants’ expectations. When we drink coffee, for example, we expect to feel more alert, or when we take an aspirin, we expect our headache to disappear; therefore, we may feel better long before the substance has had time to produce effects. Not letting subjects know whether they received the real substance or the placebo helps offset these misleading effects. The second blind is achieved when the researchers also do not know whether a participant has been given a real substance or a placebo until the experiment is over. This aspect ensures that the researchers’ expectations do not tilt or bias their observations. If scientists expect participants to act more alert after drinking coffee, for example, this bias could be reflected in their observations and conclusions. Double-blind, placebo-controlled studies have provided insight into the relationship between common food additives in fruit drinks and hyperactivity in children. Returning to the question of food additives, what do double-blind, placebo-controlled studies have to say about their effects on child behavior? In one careful study, young children were given drinks that either had no additives (placebo) or a combination of colorings and preservatives used frequently in packaged foods (McCann, et al., 2007). Because it was a double-blind study, the children did not know which drink they had received, and the researchers responsible for observing the children did not know which drink had been served to each child. The outcome of this study showed that general measures of hyperactivity, or unusually large amounts of movement, were higher in the groups that had consumed the additives than in the group that had consumed the placebo. This finding has implications for attention deficit hyperactivity disorder (ADHD), which is discussed in Chapter 14. The ability of these common food additives to make normal children more hyperactive is cause for concern and worthy of further study (see Figure 2.11). The results obtained by  McCann et al. (2007) showed that hyperactivity was higher in children who consumed one of the drinks containing common food additives (left and middle bars) than in children who consumed the placebo drink containing no additives (right bar). Modifications of the methods discussed previously might be necessary for answering specific questions. Psychological scientists frequently ask questions about normal behaviors related to age. As Chapter 11 will show, aggression in preschoolers means something different from aggression in a 16-year-old. Considering the impact of video game violence on aggression within the context of age-related change adds a new and useful dimension to our hypothesis, but it requires additional attention to the research methods to be used. Psychologists have three specific techniques for assessing the normal behaviors associated with age: cross-sectional, longitudinal, and mixed longitudinal designs. To do a cross-sectional studycross-sectional studyAn experimental design for assessing age-related changes in which data are obtained simultaneously from people of differing ages.            cross-sectional study        An experimental design for assessing age-related changes in which data are obtained simultaneously from people of differing ages.            , we might gather groups of people of varying ages and assess both their exposure to violent video games and their levels of physical aggression. We might be able to plot a developmental course for age-related differences in both video game exposure and aggressive behavior. However, the cross-sectional method introduces what we refer to as cohort effects, or the generational effects of having been born at a particular point in history. Being 20 years old in 1959 was different from being 20 years old in 1989 or 2019 because of a variety of cultural influences. Today’s 10-year-olds, who do not know of a time without the Internet, might respond differently to violent video games than today’s 50-year-olds, for reasons that have nothing to do with age. Any such cohort effects could mask or distort our cross-sectional results. Cross-sectional studies usually show that intelligence scores decrease with age. These results are most likely a cohort effect. Performance on IQ tests has risen approximately 3 points per decade over the past 100 years, for reasons that are not fully understood (Flynn, 1984). A method that lessens this dilemma is the longitudinal studylongitudinal studyAn experimental design for assessing age-related changes in which data are obtained from the same individuals at intervals over a long period of time.            longitudinal study        An experimental design for assessing age-related changes in which data are obtained from the same individuals at intervals over a long period of time.            , in which a group of individuals is observed for a long period (see Figure 2.12). For example, the Fels Longitudinal Study began in 1929 to observe the effects of the Great Depression on children, and it now has enrolled great-grandchildren of the original participants. To use the longitudinal method to answer our question, we could start with a group of infants and carefully plot their exposure to violent video games and their levels of physical aggression into adulthood. The longitudinal approach has few logical drawbacks but is expensive and time consuming. Participants drop out of the study because of moves or lack of incentive. Researchers then must worry about whether those who remain in the study still comprise a representative sample. Longitudinal designs control for the cohort effects that are often seen in cross-sectional designs. This longitudinal study shows that verbal ability and verbal memory are fairly stable over the lifetime, but that perceptual speed gradually worsens with age. The third approach, the mixed longitudinal designmixed longitudinal designA method for assessing age-related changes that combines the cross-sectional and longitudinal approaches by observing a cross-section of participants over a shorter period than is used typically in longitudinal studies.            mixed longitudinal design        A method for assessing age-related changes that combines the cross-sectional and longitudinal approaches by observing a cross-section of participants over a shorter period than is used typically in longitudinal studies.            , combines the cross-sectional and longitudinal methods. Participants from a range of ages are observed for a limited time (usually about 5 years). This approach is faster and less expensive than the longitudinal method and avoids some of the cohort effects of the pure cross-sectional method. Research MethodStrengthsWeaknessesDescriptive methodsCase studyCan explore new and unusual phenomena; can falsify a hypothesisHas limited generalizationNaturalistic observationProvides insight into natural, real-world behaviorsParticipants act differently when watched; the process has ethical issuesSurveyProvides large amounts of data quickly and inexpensivelyRequires a large representative sample; people wish to appear socially appropriate and may lieCorrelationsAllow us to predict behavior; stimulate development of hypotheses; address some difficult ethical situationsCannot be used to discuss causalityExperimentsAllow control of situations, strong hypothesis testing, and judgment of causalityRaises artificiality and ethical concerns; are time consumingAssessing the effects of time:• Cross-sectional studyIs quick and relatively inexpensiveIs subject to cohort effects• Longitudinal studyReduces the impact of cohort effectsIs expensive and time consuming; people drop out• Mixed longitudinal studyIs less expensive and time consuming than longitudinal, with some control of cohort effectsIs still relatively expensive and time consuming; dropout problem remainsAsking the right questions and collecting good information are only the beginning of good science. Once we have collected our results, or data, we need to figure out what those data mean for our hypotheses and theories. The interpretation of data is not an arbitrary act—scientists follow specific rules when drawing their conclusions. A valid measure actually measures what it is supposed to measure. In this case, your bathroom scale is supposed to tell you how much you weigh. Data are only as good as the measures used to obtain them. How would we know whether a measure is good or bad? Two standards that any measure must meet are reliability and validity. ReliabilityReliabilityThe consistency of a measure, including test–retest, interrater, intermethod, and internal consistency.            Reliability        The consistency of a measure, including test–retest, interrater, intermethod, and internal consistency.             refers to the consistency of a measure. There are several meanings of reliability in science, including test–retest, interrater, inter-method, and internal consistency. For example, test–retest reliability of the Scholastic Aptitude Test (SAT) is quite good, as you might have noticed if you took the test more than once. If you were ill for the first test, or you invested in a preparation course before taking it the second time, your score might change; otherwise, your scores are likely to be about the same. Good measures also show high interrater reliability, or consistency in the interpretation of a measure across different observers. You can imagine how distressing it would be if, for example, one lab identified you as having a fatal disease on the basis of a blood test and another did not. Inter-method reliability describes the positive correlation of several approaches to measure a feature in an individual. Returning to the SAT example, it is likely that your SAT and high school grades are positively correlated, which supports the reliability of these measures. Finally, internal consistency results from measures within a single test that positively correlate with one another. ValidityValidityA quality of a measure that leads to correct conclusions (i.e., the measure evaluates the concept that it was designed to do).            Validity        A quality of a measure that leads to correct conclusions (i.e., the measure evaluates the concept that it was designed to do).             means that a measure leads to correct conclusions or evaluates the concept that it is designed to do. For example, your bathroom scale is supposed to measure how much you weigh. The data obtained from your bathroom scale can lead you to a valid conclusion (this is how much I weigh) or an invalid conclusion (wow, I’m much lighter than the doctor’s scale said I am). How would we determine whether a measure leads to valid conclusions? One approach is to see whether a measure correlates with other existing, established measures of the same concept. Many universities use the SAT to help select the best candidates for admission. This test is supposed to measure a student’s aptitude for success in college, but some universities have abandoned it in favor of other measures. We could ask which of the following measures—the SAT, SAT Subject Tests, or high school GPA—shows the highest positive correlation with first-year college grades. The results of this comparison indicate that the SAT Subject Tests show the highest positive correlation with first-year college grades compared to both the SAT and the high school GPA (Geiser & Studley, 2001). Thus, we can conclude that SAT Subject Tests are the most valid of the three measures for predicting first-year college grades. Reliability and validity are not the same. You can obtain a consistent result (reliability) that lacks meaning (validity), but a measure cannot be valid without also being reliable. For example, if you weigh 200 pounds and your bathroom scale consistently reports that you weigh 150 pounds, whether you’re looking at the number or your roommate reads it for you, the scale has reliability but not validity. If you get a wildly different number each time you step on the scale, you have neither reliability nor validity. The measure is not consistent (no reliability) and also fails to measure the construct—weight in this case—that it is designed to measure (no validity). Just as we might explore a new research topic using the descriptive research methods outlined earlier, we can use descriptive statistics to explore the characteristics of the data obtained from our research. Descriptive statisticsDescriptive statisticsStatistical methods that organize data into meaningful patterns and summaries, such as finding the average value.            Descriptive statistics        Statistical methods that organize data into meaningful patterns and summaries, such as finding the average value.             help us organize individual bits of data into meaningful patterns and summaries. For example, research investigating the ability of the SAT, SAT Subject Tests, and high school grades to predict grades in colleges and universities across the country would be overwhelming without some way of summarizing the individual data points for tens of thousands of students in meaningful ways. Descriptive data tell us only about the sample we have studied. To determine whether the results from our sample apply to larger populations requires additional methods, described in a subsequent section of this chapter. We might approach this mass of data first by asking how the scores and grades are distributed. We could arrange SAT scores from high to low and note how many students obtained each score. The result of our work would be a frequency distribution. We often illustrate frequency distributions with a bar chart, or histogram, like the one shown in Figure 2.13. Descriptive statistics, such as these frequency distributions of SAT mathematics scores, allow us to see meaningful patterns and summaries in large sets of data. Frequency distributions are a useful starting place, but we might also be interested in identifying the average score on our measures, or the central tendency of our data set. There are three types of measures for central tendency: the mean, median, and mode for each group of scores. The meanmeanThe numerical average of a set of scores.            mean        The numerical average of a set of scores.             is the numerical average of a set of scores, computed by adding all the scores together and dividing by the number of scores. For all students taking the SAT in 2009, the mean on the critical reading section was 501, the mean on the mathematics section was 515, and the mean on the writing portion was 493. Many colleges and universities use the SAT or other standardized tests to predict success in college courses. Evaluating the resulting data is easier if we use descriptive statistics to summarize the performances of the thousands of individual students who take the tests. The medianmedianThe halfway mark in a set of data, with half of the scores above it and half below.            median        The halfway mark in a set of data, with half of the scores above it and half below.             represents a halfway mark in the data set, with half of the scores above it and half below. The median is far less affected by extreme scores, or outliers, than the mean. In our SAT data, the median scores—500 for critical reading, 510 for mathematics, and 490 for writing—are quite close to the mean scores. Why, then, would you ever need to consider a median? In some sets of results, you might find some extreme scores that could affect the mean. For example, if you asked employees at a small business to report their current annual salaries, you might get the following results: $40,000, $45,000, $47,000, $52,000, and $350,000 (we’re assuming this is the boss). The mean in this example would be $106,800, but that figure doesn’t provide a good summary of these numbers. The median, $47,000 in this case, is more representative of your overall results. Look at Figure 2.14 for another example. In many cases, like the SAT data, means and medians are similar. In other cases, such as annual income, these two measures of central tendency provide different pictures. This graph shows mean and median annual incomes in the United States. The ratio of the median to the mean has decreased from 72% in 1990 to 65% in 2012. This suggests that the poorer citizens’ income has not increased as fast as the wealthier citizens’ income. The modemodeThe most frequently occurring score in a set of data            mode        The most frequently occurring score in a set of data             refers to the score that occurs most frequently, and it is easy to determine by looking at a histogram. We do not have exact numbers for modal scores on the SAT example, but the most frequent range of scores is between 400 and 490 for all three tests (critical reading, mathematics, and writing). The usefulness of the mode depends on the research question that you are asking. In the case of the SAT, the mode provides little interesting information, so let’s return to our drinking data mentioned earlier in the chapter. It might surprise you to learn that although the modal pattern of drinking among U. S. adults overall is current regular, among Asian adults living in the United States, the modal pattern is lifetime abstainer (Center for Behavioral Health Statistics and Quality, 2015). The mode is also an advantage over a mean or median when there is more than one mode in a set of data. For example, the mean age of onset for anorexia nervosa, which is discussed in Chapter 7, is 17 years of age, but the distribution is bimodal, which means that it has two substantial modes. One peak occurs around the age of 14, when many teens struggle with their changing shapes, and another occurs around the age of 18, when many teens leave home and make food choices without the watchful eyes of their parents (Halmi, Casper, Eckert, Goldberg, & Davis, 1979). An intervention program designed to coincide with the most likely ages of onset would probably be more effective than one timed to coincide with the mean age of onset (see Figure 2.15). The average age of onset for the anorexia nervosa is 17 years, but this measure masks the important fact that age of onset shows two modes—one at 14 years and a second at 18 years. For public health officials wishing to target vulnerable groups for preventive education, the modes provide better information than the mean. In addition to being curious about central tendency, we might want to know how clustered our scores are. The traditional way to look at the variance of scores is to use a measure known as the standard deviationstandard deviationA measure of how tightly clustered around the mean a group of scores is            standard deviation        A measure of how tightly clustered around the mean a group of scores is            , which tells us how tightly clustered around the mean a group of scores is. A smaller standard deviation suggests that most scores might be found near the mean, whereas a larger standard deviation means that the scores are spread out from the mean. Returning to our salary example, we had five salaries with a mean of $106,800. To obtain the standard deviation, which is easy to do with a calculator, you subtract each salary from 106,800, square each difference (to eliminate minus signs), add the squares, find the mean of those differences by dividing the total by five (the number of salaries), and take the square root of the result. In this case, we end up with a standard deviation of 136,021, which means that the average difference between a salary and the mean of the salaries is 136,021. If we discard the extreme salary ($350,000) and find the standard deviation of the remaining four salaries, it turns out to be smaller: 4,966.56. These results suggest that the distribution of the first four salaries is tightly clustered, whereas the distribution of all five salaries is more spread out. Many measures of interest to psychologists, such as scores on intelligence tests, which are discussed in Chapter 10, appear to form a normal distributionnormal distributionA symmetrical probability function            normal distribution        A symmetrical probability function            , illustrated in Figure 2.16. The ideal normal curve in this illustration has several important features. One, it is symmetrical. Equal numbers of scores should occur above and below the mean. Second, its shape indicates that most scores occur near the mean, which is where our measure of variability plays a role. In the standard normal curve, shown in Figure 2.16(a), 68% of the population falls within one standard deviation of the mean, 95% falls within two standard deviations, and 99% of the population falls within three standard deviations. Instruments for assessing intelligence, discussed in Chapter 10, frequently convert raw scores earned by many participants to fit a normal distribution with a mean of 100 and a standard deviation of 15. As a result, we would expect 68% of test takers to receive an IQ score between 85 and 115. Another 95% would most likely score between 70 and 130, leaving only 2.5% to score above 130 and another 2.5% to score below 70. Many measures of interest to psychologists take the approximate form of a normal distribution. These graphs compare a standard normal curve, shown in (a), to the distribution of scores on a standardized test of intelligence, the Wechsler Adult Intelligence Scale, shown in (b). In our discussion so far, we have been describing single variables, such as number of drinks consumed, salaries, and SAT scores. In psychological research, we often want to describe the relationships among multiple variables. We can illustrate the relationship between two variables in a scatterplot like the one shown in Figure 2.17. Each dot represents the intersection between scores on two variables of interest. For example, we can compare the distributions of math SAT scores as a function of GPA. From our scatterplot, it looks like math SAT scores and GPAs are systematically related to each other. As one increases, the other does as well, which you should recognize from the earlier discussion as a positive correlation. A scatterplot allows us to visualize the relationship between two variables, such as the math SAT score and the high school GPA for one small sample of college students. This relationship should look familiar to you—it is an example of a positive correlation. ESP stands for extrasensory perception, with extra in this case meaning outside the boundaries of the normal information that we obtain from our various senses, such as vision, hearing, and touch. The study of ESP is part of the larger field of parapsychology, or the study of psychic phenomena lying outside the typical boundaries of the field of psychology described in Chapter 1. Among the abilities grouped into the category of ESP are telepathy (the ability to communicate with other minds without using the usual methods of speaking or writing), clairvoyance (the ability to perceive objects that do not affect the known senses), precognition (knowledge of future events), and premonition (emotional anticipation of future events). In 2005, Gallup pollsters found that 41% of Americans said that they believed in ESP, with 25% not sure and 32% not believing (Gallup Poll News Service, 2005). Of the scientists who are members of the National Academy of Sciences, 96% said that they did not believe in ESP (McConnell & Clark, 1991). A study of ESP (Bem, 2011) generated considerable discussion in the scientific community about everything from the statistics that we use to the effects of investigator bias. Evaluating this study provides a good opportunity to practice your critical thinking skills and to apply what you have learned about validity and reliability. A total of 100 undergraduates (50 men and 50 women) participated. Stimuli (both erotic and nonerotic photographs) were selected from a standard set known as the International Affective Picture System. During each trial, participants saw two curtains on a computer screen and were asked to predict which curtain hid a picture. The sequencing of the erotic and nonerotic pictures and the left–right positions was determined by a random number generator after the participant made a selection. This timing was designed to test the precognition of future events (the participants selected a side of the screen before the random number generator selected a location for the picture). Here is where Bem’s methods get a bit murky. The first 40 participants saw 12 erotic pictures, 12 negative pictures (unpleasant images), and 12 neutral pictures. Then, for reasons not well explained in the paper, the method was changed for the remaining 60 participants, who were shown 18 erotic and 18 nonerotic photos, 8 of which were described as romantic but not erotic (couple at a wedding, etc.). It is quite unusual for researchers to change their methods in the middle of an experiment, and more troubling when there doesn’t seem to be a good reason to do so. The popularity of television shows like Ghost Hunters might be related to the large number of Americans who report a belief in ESP (41%). In contrast, 96% of the members of the National Academy of Sciences say they do not believe in ESP. The Question: Nine experiments involving more than 1,000 participants tested different types of precognition and premonition. We will focus our attention on the first experiment, which tested the following hypothesis: Participants should be able to anticipate the position (right side or left side of a computer screen) of an erotic photograph (see Figure 2.18). Is the following study of this hypothesis valid and reliable? The only potential ethical challenge in this study is the presentation of erotic photos. Potential participants should be warned of this aspect before they agree to continue with the study. If we have two choices, we have a 50% chance of guessing correctly on each trial. Bem reported that the future position of the erotic images was chosen correctly on 53.1% of the trials, and the future position of the nonerotic images was chosen correctly on 49.8% of the trials. Bem reported that his results were statistically significant, or were unlikely to happen because of chance. Participants in Bem’s (2011) study were supposed to predict behind which of two curtains a picture would appear. Bem concluded that the choices made by his participants were better than chance, supporting his hypothesis that precognition could be demonstrated. What do others think of Bem’s results? James Alcock, writing for the Skeptical Inquirer, concluded that just about everything that could be doNe wrong in an experiment occurred here (2011, p. 31). Among Alcock’s concerns were Bem’s changing of his method midway through the experiment and his questionable use of statistical analyses. As discussed earlier, replication provides an important check on possible researcher bias, and failure to replicate indicates serious flaws in an experiment. So far, the three known replications of Bem’s experiments have failed to produce significant results. Despite the flaws, however, Bem’s experiments have contributed to science by stimulating a lively discussion of scientific and statistical methods. Although this scatterplot gives a sense that GPAs and math SAT scores have a systematic relationship, we can compute that relationship exactly using a correlation coefficient. Correlation coefficients can range from −1.00 to +1.00. A correlation of −1.00 and a correlation of +1.00 are equally strong but differ in the direction of the effect. A zero correlation indicates that the two variables have no systematic relationship. The closer a correlation coefficient is to −1.00 or to +1.00, the stronger is the relationship between the two variables. When the score is −1.00 or +1.00, the correlation is perfect—all data points follow the pattern. A value between 0 and 1.00 or 0 and −1.00 indicates a correlation direction, but the relationship is not perfect—not every data point conforms to the pattern. Although we can learn a great deal from descriptive statistics, most research described in this textbook features the use of inferential statisticsinferential statisticsStatistical methods that allow experimenters to extend conclusions from samples to larger populations            inferential statistics        Statistical methods that allow experimenters to extend conclusions from samples to larger populations            , so called because they permit us to draw inferences or conclusions from data. Descriptive statistics allow us to talk about our sample data but do not allow us to extend our results to larger groups. To reach conclusions about how our observations of a sample might fit the bigger picture of groups of people, we use inferential statistics. Although inferential statistics can be powerful, we must be cautious about making generalizationsgeneralizationsThe tendency to respond to stimuli that are similar to an original conditioned stimulus (CS)            generalizations        The tendency to respond to stimuli that are similar to an original conditioned stimulus (CS)             from our results to larger populations. To generalize means to extend your conclusions to people outside your research sample. Psychology over the years has been justifiably criticized as being the psychology of the college sophomore. This criticism arises because researchers are usually college professors, and the students in their courses are a convenient source of willing study participants, especially when extra credit is available. Today’s psychological scientists recognize that college students do not comprise a representative sample of people, and they go to great lengths to recruit samples of participants that are more diverse in age, race, ethnicity, socioeconomic background, and other demographic variables. Having a diverse sample supports more generalization than using a sample of college students. To illustrate the use of inferential statistics, let’s consider the 2009 SAT mathematics test. Men scored an average of 534 and women scored an average of 499 on this test. Does this mean that men perform better than women on this test? Or do men and women perform similarly, and this group is just an unusual sample of test takers? The default position, stating that there is no real difference between two measures (math scores produced by men and women in this example), is known as the null hypothesisnull hypothesisA hypothesis stating the default position that there is no real difference between two measures            null hypothesis        A hypothesis stating the default position that there is no real difference between two measures            . Recall that we cannot prove a hypothesis to be correct, but we can demonstrate that a hypothesis is false. Rejecting the null hypothesis suggests that alternative hypotheses (there might be a relationship between gender and math scores) should be explored and tested. How do we know when a hypothesis should be rejected? Like most sciences, psychology has accepted odds of 5 out of 100 that an observed result is due to chance as an acceptable standard for statistical significancestatistical significanceA standard for deciding whether an observed result is because of chance.            statistical significance        A standard for deciding whether an observed result is because of chance.            . We can assess the likelihood of observing a result due to chance by repeating a study, like throwing dice multiple times. We could give the mathematics portion of the SAT to 100 randomly selected samples of male and female college-bound students. If men and women score about the same or women score higher than men in 5 or more of these 100 samples, we would reject our Men score higher than women hypothesis as false. This type of careful analysis of the SAT data has confirmed that the differences on the mathematics portion of the test between male and female test takers are statistically significant (Halpern, et al., 2007; see Figure 2.19). Does this result mean that we should consider this difference when deciding on a major or career? Probably not. Keep in mind that the goal of the SAT is to predict college grades. When the math SAT scores of men and women receiving the same letter grade in a mathematics course at the same university were compared, the women scored 33 points less than the men (Wainer & Steinberg, 1992). This is about the same difference that we observed between the mean scores of men and women on the SAT math test. It appears that for some unknown reason, the SAT might be underestimating women’s ability to achieve in college mathematics courses. Inferential statistics allow us to decide whether the observed differences between the performance of males and that of females on the math SAT represents a real gender difference or whether they just occur by chance. Although psychology and most other sciences have relied heavily on testing the significance of the null hypothesis using the 5 times out of 100 (p < .05) odds as a criterion, this approach has its share of weaknesses. Testing and rejecting a null hypothesis tend to encourage us to view variables, like exposure to violent video games, as producing discrete either/or outcomes (aggression or no aggression) when that is unlikely to be the case in reality. An alternative approach is the use of estimation, which includes a report of the 95% confidence interval in addition to the mean (Cumming, 2012; see Figure 2.20). Applying estimation to the video game question might look like this. You could report that the average score on a measure of aggression was 37 ± 2 following exposure to a violent game and 32 ± 2 following exposure to a nonviolent game. This means that 95% of participants are likely to score between 35 and 39 following exposure to an aggressive game, while 95% of participants are likely to score between 30 and 34 following exposure to a nonviolent game. The additional information provided by the confidence interval helps us evaluate the magnitude of the difference better than a simple reporting of the statistical significance of the difference. Earlier in this chapter, we reviewed a study comparing hyperactivity scores among children drinking two juices containing common food additives (A and B) and plain juice (placebo). Computing 95% confidence intervals, noted by the red lines, gives us an idea of how big the differences are (these are actually quite small, but significant). We shouldn’t assume that overlapping confidence intervals indicate a lack of statistical significance. Mixture A was different from placebo at the p < .05 level, while Mixture B was different from placebo at the p < .01 level. We mentioned earlier that deceiving participants into thinking they were truly in danger raised ethical questions. On what basis do researchers decide what they can and cannot do to their research participants? Many populations that are interesting to psychologists are unable to sign informed consent forms legally and require additional ethical protection. In the case of research with infants, parents are required to sign informed consent forms on their child’s behalf. Although most studies published in psychological journals involve the use of human participants, psychology also has a rich heritage of animal research. Separate guidelines have been developed for each type of subject. Researchers working in universities and other agencies receiving federal funding seek the approval of institutional review boards (IRBs) for human participant research and institutional animal care and use committees (IACUCs) for animal research before conducting their studies. The IRBs and IACUCs are guided by federal regulations and research ethics endorsed by professional societies such as the American Psychological Association (APA), the Association for Psychological Science (APS), and the Society for Neuroscience (SfN). IRBs and IACUCs must include at least one member of the community outside the university or agency, avoiding the possibility that inappropriate research might be conducted in secret. These procedures do not apply to institutions that do not have federal funding, such as private genetics research corporations or consumer product corporations (cosmetics, etc.), although efforts are being made to bring these organizations into compliance with federal standards. As you review the ethical guidelines for both human and animal research participants, keep in mind that the guidelines look simpler when you read about their provisions than when you try to implement them in the context of real research. This is why the final approval decision lies with a committee, as opposed to an individual. At the core of ethical standards for human research is the idea that participation is voluntary. No participant should be coerced into participating. Although psychologists are well aware that people who volunteer to participate in research are probably quite different in important ways from those who don’t volunteer, we have chosen to give research ethics a higher priority than our ability to generalize research results. The Tuskegee syphilis experiment, conducted by the U. S. Public Health Service from 1932 to 1972, involved 400 African-American men who had contracted syphilis. The study’s failure to treat or inform these participants about their health led to new regulations to prevent such unethical research from being repeated. To ensure that a participant is a willing volunteer, researchers must make provisions for reasonable incentives. Incentives, such as pay or extra credit for participation, must not be so extreme that they become the primary motivation for prospective volunteers. To decide whether to volunteer for research, a person must have some knowledge of what the research will entail. Researchers must provide prospective participants with an informed consentinformed consentPermission obtained from a research participant after the risks and benefits of an experimental procedure have been thoroughly explained informed consent Permission obtained from a research participant after the risks and benefits of an experimental procedure have been thoroughly explained form, which provides details about the purpose of the study and what types of procedures will occur. In psychological research, we have the added burden of occasionally dealing with participants who are limited in their abilities to provide informed consent because of the conditions that make them interesting to study. Developmental psychologists have an obvious interest in children, but a person cannot sign a legal informed consent form until age 18. Can you obtain informed consent from a patient with schizophrenia, who suffers from hallucinations and irrational, delusional beliefs, or from a person in the later stages of Alzheimer’s disease, whose memory and reasoning have deteriorated because of the condition? In these cases, legal permission must be obtained from a qualified guardian. The university IRBs play an essential role in evaluating these ethical dilemmas case by case. Research also should be conducted in a manner that does no irreversible harm to participants. In some cases, to avoid participants’ desire to appear normal and their tendency to try to outguess the research, researchers might say that they are investigating one factor when they are interested in another. Most cases of deception are quite mild, such as when participants are told that a study is about memory when it is actually a study of some social behavior. When researchers must deceive their participants, extra care must be taken to debrief participants and answer all their questions following the experiment. While examining the papers of Dr. John Cutler, who led the Tuskegee syphilis study, Wellesley College historian Susan Reverby discovered that during the 1940s, U. S. and Guatemalan health officials had deliberately exposed prisoners, soldiers, and mental patients to syphilis and gonorrhea to test the effectiveness of penicillin. Research using human participants should be rigorously private and confidential. Privacy refers to the participants’ control over the sharing of their personal information with others, and methods for ensuring privacy are usually stated in the informed consent paperwork. For example, some studies involve the use of medical records, which participants agree to share with the researchers for the purpose of the experiment. Confidentiality refers to the participants’ right to have their data revealed to others only with their permission. Confidentiality is usually maintained by such practices as substituting codes for names and storing data in locked cabinets. Collecting data anonymously, so that even the researchers do not know the identity of participants, is the surest way to protect privacy and confidentiality. Science learns from its past ethical lapses. One of the most egregious examples of unethical research was the Tuskegee syphilis experiment, which lasted from 1932 until 1972. Researchers from the U. S. Public Health Service recruited about 400 impoverished African-American men who had contracted syphilis to study the progression of the disease. None of the men were told they had syphilis, and none were treated, even after penicillin became the standard treatment for the disease in 1947. Many current federal regulations related to research ethics were developed in response to this experiment. The topic of using animals in research is guaranteed to stimulate lively, and possibly heated, discussion. Some people are adamantly opposed to any animal research, whereas others accept the concept of using animals, so long as certain conditions are met. About 7% to 8% of published research in psychology journals involves the use of animals as subjects (American Psychological Association [APA], 2012). A total of 90% of the animals used are rodents and birds, with 5% or fewer studies involving monkeys and other primates. According to the APA, the use of dogs and cats in psychological research is rare. Ethical guidelines for animal research require setting a clear purpose for the experiment, providing excellent care for the animals, and minimizing pain and suffering. Research using animals must demonstrate a clear purpose, such as benefiting the health of humans or other animals. In addition to serving a clear purpose, animal research requires excellent housing, food, and veterinary care. The most controversial ethical standards relate to minimizing the pain and suffering experienced by animal research subjects. Federal regulations provide guidelines for the use of pain, surgery, stress, and deprivation with animal subjects, as well as the termination of an animal’s life. The standards approximate the community standards that we would expect from local humane societies tasked with euthanizing animals that are not adopted. In Chapter 1, we suggested using the problem of cyberbullying as our real-world problem. In this chapter, we’ll try to examine the different ways that we could approach this question using our best research methods and ethics. Bullying is not new, but cyberbullying is. To study a relatively new phenomenon, we might begin with descriptive methods. In addition, correlational methods might help us identify factors that predispose individuals to bullying or being bullied, as well as likely outcomes of cyberbullying (see Figure 2.21). A quick search of Google Scholar from 2015 to 2016 shows that survey and correlational approaches dominate this area of research. Survey data on a sensitive topic like cyberbullying must be interpreted with caution. The frequency of behaviors known to be undesirable is likely to be under-reported by perpetrators, and victims might underplay the severity of their experience of abuse as a coping strategy. A search of Google Scholar for articles published between 2015 and 2016 shows that survey and correlational studies dominate this area of research. Typical of this approach is a study by Kowalski & Limber (2013). You can see that youth engaging in traditional or cyberbullying share many outcomes, but appear to differ in their levels of anxiety and suicidal ideation. The most likely related question to be approached using experiments is the efficacy of intervention programs. For example, one research team evaluated whether the Cyber Friendly Schools Program used in Australian schools was effective in reducing cyberbullying (Cross et al., 2016). Schools were randomly assigned to intervention or control conditions. The study did show that perpetrating and being victimized became less likely as a result of the program. Cyberbullying is a sensitive issue, and that raises significant ethical concerns. First, most cyberbullying research focuses on schoolchildren and young adults. Using minors as research participants requires additional steps, such as obtaining both parental consent and the assent of the participant. We do not want your parents to have the ability to sign you up for research against your will. Second, a participant who has been a cyberbullying victim might react negatively to reminders of the experience. We would need to make this risk very clear in the informed consent materials and take special care to remind participants they can quit the study at any time. The informed consent form should include referrals to counseling services available to participants in the event that they feel a need to discuss their experiences. The end of chapter 1 noted that each chapter would feature a discussion of how the chapter’s material could be used to talk about relationships. For this chapter on research methods, we ask whether psychological research methods can be used to differentiate between liking and loving another person. Using the methods described in this chapter, you can probably think of many ways to approach this question. You could observe friends and compare their behavior to that of romantic partners. You could conduct a survey, asking people to rate certain characteristics of people they love versus those of people they like. Perhaps you could correlate physiological arousal measures, like pupil diameter or heart rate, with ratings of how much a participant likes or loves another person. Conducting an actual experiment might be a bit tricky, though, in terms of both methods and ethics. How could people be randomly assigned to love and like groups? A fun example of research on relationships was contributed by Bartels and Zeki (2000). These researchers chose to use correlational methods in the form of brain imaging, which we discuss further in Chapter 4. Brain imaging studies are typically correlational because we assume that brain activity in particular areas has a relationship with some ongoing behavior. It is important to recall that we cannot make conclusions about causality based on correlational data. Bartels and Zeki’s participants were instructed to supply photographs of people they like and people they love. The photographs were then presented to the participants while their brain activity was observed using functional magnetic resonance imaging (fMRI; see Chapter 4). Several differences in brain activity occurred when the participants viewed a lover as opposed to a friend. Not too surprisingly, areas of the brain associated with reward became more active when viewing lovers than when viewing friends. Areas of the brain associated with social judgment, negative emotions, and assessing the intentions and emotions of other people were less active when viewing lovers than when viewing friends. Love, according to these results, appears to be a combination of reward and less social judgment (see Figure 2.22). When people view a photo of a person they say they love, brain imaging shows increased activation in reward areas of the brain and decreased activation in areas involved with social judgment. The results of this scientific research might help us understand some of our everyday experiences with relationships. We feel rewarded in the presence of a loved one, and we have to admit that we may miss a flaw or two in someone we love. Friends and family members, whose social judgment has not been silenced, do not miss these flaws and often try in vain to alert us to them. Human ParticipantsAnimal SubjectsNo coercionNecessityInformed consentExcellent food, housing, and veterinary careNo harmMinimal pain and sufferingConfidentiality and privacyBe sure that you can define these terms and use them correctly.`;\n","export const text = `Environmental factors such as stress, diet, smoking, and exercise can influence whether a gene is turned on or off. We all know that identical twins, like the little girls appearing in the photo, are, well, identical. Or are they? For a long time, scientists were puzzled not by the similarities between identical twins, but by their differences. If two people share the same deoxyribonucleic acid (DNA), as identical twins do, why does one get cancer while the other stays healthy? Why does one become obese while the other remains fit? When science began to unravel some mysteries about genes and how they work, answers to these questions emerged. You have the same DNA in each cell of your body, yet some cells develop into heart cells, others into brain cells, and so on. How does the same set of DNA know how to make these different types of cells? Genes, or segments of DNA that produce specific proteins, can be turned on and off. The genes that are not turned off are free to produce the proteins needed to build a particular kind of cell, whether that is a skin cell or a liver cell. Only about 10% to 20% of the genes in a particular type of cell, such as a skin cell or brain cell, are active. Genes do not just turn on and off as they build a body during development. Your ongoing interactions with the environment can also turn genes on or off. What you eat, whether you smoke or drink, your stress levels, and other environmental factors can influence how your DNA works. Our understanding of these ongoing interactions between genes and the environment is the reason psychologists no longer argue about the separate contributions of nature and nurture. In this chapter, we will explore these interactions in more detail. Let’s zoom in to see what’s happening when the environment interacts with DNA. In mice, a gene called Agouti produces yellow fur and obesity when it is turned on, but brown fur and normal weight when it is turned off (Dolinoy, Huang, & Jirtle, 2007). Certain environmental factors can turn the gene on or off. When pregnant mother mice ate food containing bisphenol A (BPA), a chemical found in food and beverage containers, baby bottles, dental sealants, and food cans, their babies had yellow fur and were obese. The BPA turned on the Agouti gene. When pregnant mice are fed a diet containing BPA, found in plastics and other consumer products, their offspring are more likely to have yellow fur and to be obese. How does studying the fur color of mice help us understand the differences between identical human twins? The young twins on the previous page have a great deal in common, but as they get older, they are more likely to eat different foods and have different experiences. These environmental influences can change the way their genes are turned on or off, just as the BPA affected the Agouti gene in the mice. These changes accumulate over time, so identical twins become less similar as they age. In this chapter, we will explore how nature and nurture interact to build the mind across the life span, and how the interactions between nature and nurture have been shaping the human mind over millennia. Along with an understanding of the structures and processes of the brain, covered in Chapter 4, knowing how our biological history shapes our behavior is an important part of understanding the mind. Contemporary psychologists view the contributions of naturenatureThe contributions of heredity to our physical structure and behaviors.            nature        The contributions of heredity to our physical structure and behaviors.             (our heredity or innate predispositions) and nurturenurtureThe contributions of environmental factors and experience to our physical structure and behaviors.            nurture        The contributions of environmental factors and experience to our physical structure and behaviors.             (the results of our experience with the environment) as being closely intertwined, as opposed to somehow competing for control over structure and behavior. These identical twins probably do not look as similar as they did as small children. The twin on the left is a nonsmoker, while her sister on the right smoked for 29 years. Scholars have not always thought about nature and nurture the way we do today. Instead of viewing the actions of nature and nurture as inseparable, earlier scholars talked in terms of nature versus nurture and debated the relative contributions of nature or nurture to a particular type of behavior. Credit for describing the contrast between heredity and environment as  nature versus nurture usually goes to Francis Galton (1869), who was Charles Darwin’s cousin. Galton believed that intelligence was largely the result of inheritance, a topic tackled in Chapter 10. Over the next 150 years or so, many thinkers engaged in a highly spirited debate on this question. As Chapter 10 will demonstrate, contemporary psychologists view intelligence as another example of an outcome shaped by both genetic inheritance and environment. Francis Galton was the first to use the phrase  nature versus nurture. We can say with some certainty that the either/or approach to human behavior has produced some of the most contentious discussions in the history of psychology. Our motive for arguing in favor of the intertwined approach to nature and nurture is intended not to sidestep difficult questions, but rather to support good science. By zooming out to integrate a number of perspectives, both biological and experiential, we can achieve a more accurate understanding of these questions. Before we explore the interactions between nature and nurture that contribute to psychological phenomena, let’s look at the genetic mechanisms that help shape the mind. Every nucleus in the approximately 37 trillion cells of your body, with the exception of your red blood cells and sperm or eggs, contains two complete copies of the human genome, a set of instructions for building a human. Your personal set of instructions is known as a genotypegenotypeAn individual’s profile of alleles            genotype        An individual’s profile of alleles            , which interacts with the environment to produce observable characteristics, known as a phenotypephenotypeAn observable characteristic            phenotype        An observable characteristic            . Some genes have a large number of alleles. Of the more than 500 alleles for the BRCA1 (Breast Cancer 1) gene, a small number are associated with a higher risk for breast and other cancers. Actress Angelina Jolie, who possesses the high-risk alleles, chose to undergo a preventive mastectomy to reduce her chances of developing breast cancer. One half of your genotype was provided by your mother’s egg, and the other half was provided by your father’s sperm. Each parent contributes a set of 23 chromosomes, which in turn are composed of many molecules of DNA. A smaller segment of DNA located in a particular place on a chromosome is known as a genegeneA small segment of DNA located in a particular place on a chromosome            gene        A small segment of DNA located in a particular place on a chromosome            . Each gene contains instructions for making a particular type of protein. Gene expressionGene expressionThe process in which genetic instructions are converted into a feature of a living cell            Gene expression        The process in which genetic instructions are converted into a feature of a living cell             occurs when these genetic instructions are used to produce a particular protein. Each cell contains the instructions for an entire human organism, but only a subset of instructions is expressed at any given time and location. Gene expression in a nerve cell is different from gene expression in a muscle cell or a skin cell. Twenty-three pairs of chromosomes make up the human genome. Different versions of a gene, or allelesallelesOne of several versions of a gene, as in having an A, B, or O blood type allele            alleles        One of several versions of a gene, as in having an A, B, or O blood type allele            , can give rise to different phenotypical traits. Many alleles can occur for a given gene, but an individual receives only two—one from each parent. For example, alleles for blood type include A, B, and O, but typically, nobody has all three. As shown in Figure 3.1, combinations of your two alleles make your blood Type A (AA or AO), Type B (BB or BO), Type AB (AB), or Type O (OO). The three possible blood type alleles—A, B, and O—can be combined to produce Type A, Type B, Type O, or Type AB blood. If both parents contribute the same type of allele, such as a version of the MC1R gene related to having freckles, the child would be considered homozygoushomozygousHaving two of the same alleles for a gene.            homozygous        Having two of the same alleles for a gene.             for that gene (homos means  same in Greek). If the parents contribute different alleles, such as one for freckles from one parent and one related to not having freckles from the other, the child is heterozygousheterozygousHaving two different alleles for a gene.            heterozygous        Having two different alleles for a gene.             for that gene (hetero means  different in Greek). RecessiveRecessiveA feature of an allele that produces only a phenotype in the homozygous condition            Recessive        A feature of an allele that produces only a phenotype in the homozygous condition             alleles determine a phenotype only when an individual is homozygous for a particular gene, whereas dominantdominantA feature of an allele that determines a phenotype in either the homozygous or the heterozygous condition            dominant        A feature of an allele that determines a phenotype in either the homozygous or the heterozygous condition             alleles determine a phenotype in either the homozygous or the heterozygous condition. Because alleles for no freckles are recessive and alleles for freckles are dominant, the typical way that an individual has no freckles is if that person receives two copies of the no-freckle allele, one from each parent. Any individual receiving either two freckle alleles or one freckle allele and one no-freckle allele will have freckles (see Figure 3.2). Having freckles (F) is dominant, whereas having no freckles (f) is recessive. The only way that a child can have no freckles is to inherit two of the recessive  no freckle alleles, one from each parent. In this example, both parents are heterozygous (Ff) with freckles. DNA belonging to different individuals can be compared using autoradiographs formed by labeling DNA fragments with a radioactive marker and then exposing the DNA to x-ray film. In this case, we are looking at a simple paternity test (Public Broadcasting Service, 1998). DNA samples have been taken from three people: a mother, her child, and the possible father of the child. The two letters for each segment represent the individual’s genotype. For example, in the first autoradiograph, the mother is an AD, the child is an AC, and the possible father is a BC. Use the autoradiographs to complete Table 3.1.1 (A, B, C, D)2 (E, F, G)3 (H, I, J, K)4 (L, M, N, O)MotherGGChildACMOPossible fatherHKTo evaluate the likelihood of the possible father’s paternity, identify which letters the child inherited from the mother and which from the father. The child will receive one allele from each parent, so the child’s combinations of alleles must be possible based on the mother’s and father’s genotypes. Overall, is it possible for this man to have fathered this child? As with any hypothesis, genetic tests never prove a relationship: They can only disprove one. Based on the evidence in this case, we cannot prove the hypothesis that this man is the child’s biological parent. All we could do is show that he is not the father. If the child had an AD genotype instead of an AC genotype in the first box, this man could not possibly be the father because he could not supply either the A or the D allele. Such an exclusion result is considered absolute evidence of nonpaternity. In contrast, the results of this test suggest that it is very likely that this man is the biological parent of the child. Whether you have freckles or not is a simple example of how dominant and recessive genes interact, but one allele does not always dominate another. In a study of gene–environment interactions involving the serotonin transporter gene (SERT) and a child’s response to bullying, the authors note that with two types of alleles (S for short or L for long), individuals could have one of three genotypes: SS, SL, or LL (Sugden et al., 2010). Neither the S nor the L allele dominates the other. As you can see in Figure 3.3, The SL group had levels of emotional disturbance in response to frequent bullying that fell between the extremes of the SS and the LL groups (Sugden et al., 2010). If either the S or the L alleles were dominant, the SL group would behave like that dominant group instead. Neither the S nor the L allele of the serotonin transporter gene dominates the other. Among children who have been bullied frequently, those with the SL genotype (shown in green) experience a level of emotional problems midway between the levels of those with the SS and LL genotypes. If either the S or the L allele was dominant, we would expect the SL group to behave the same way as the homozygous dominant group. Given that each parent can pass along more than 8 million combinations of chromosomes, it might be surprising that family resemblance can be so strong, as it is in actors and brothers Liam (left) and Chris (right) Hemsworth. If you have siblings, you are aware that having the same biological parents does not guarantee similar appearance, personality, and behavior. The development of an egg or sperm cell is like shuffling a deck of cards. In both cases, a large number of possible outcomes may occur. When a parent’s cell divides to make an egg or a sperm cell, each resulting cell contains 23 chromosomes, one chromosome from each of the parent’s original 23 chromosome pairs. As a result, a single human can produce eggs or sperm with (8,388,608) combinations of chromosomes. Add this variability to the different possibilities provided by the other parent, and it may seem surprising that we resemble our relatives as much as we do. Despite this potential variability, we remain similar to our genetic relatives. This important point will be revisited later in the chapter, when the evolution of social behavior is discussed. RelatednessRelatednessThe probability that two people share the same allele from a common ancestor            Relatedness        The probability that two people share the same allele from a common ancestor             is defined as the probability that two people share copies of the same allele from a common ancestor. If we go back in history far enough, we all share common ancestors. Relatedness, however, is usually computed within a limited number of generations. The chance that you share an allele with one of your parents is one-half, as is the chance that you share an allele with a sibling. First cousins have a one-eighth likelihood of sharing an allele (see Figure 3.4). These types of calculations led geneticist J. B. S. Haldane to allegedly proclaim,  I would lay down my life for two brothers or eight cousins! (Bynum & Porter, 2005, p. 261). Haldane was computing the likelihood that his genes would be passed down to future generations. As discussed later in the chapter, evolutionary psychologists suggest that sacrificing yourself for others is more likely when the others are genetically related relatives. Relatedness refers to the probability that two people share a particular allele from a common ancestor. The chance that you share an allele with one of your parents or a brother or sister is 0.50, or one-half. The chance that you share an allele with a niece or nephew is 0.25, or one-quarter. Of the 23 pairs of human chromosomes from each parent, 22 pairs are perfectly matched. In other words, a gene appearing on one of a pair of chromosomes (perhaps a gene for blood type) has a corresponding gene on its partner. In contrast, the X and Y chromosomes do not carry the same genes. The much-larger X chromosome contains about 2,000 active genes, while the Y has fewer than 100. Females carry two copies of the X chromosome, whereas males carry one X and one Y chromosome. X chromosomeY chromosomeThe allele responsible for hemophilia, a disease characterized by the failure of blood to clot, is found only on the X chromosome. This allele is recessive, leading to different outcomes based on the sex of the child receiving the alleles. If a female receives a healthy allele on the X chromosome from one parent and a disease-causing allele on the X chromosome from her other parent, she will be a carrier for the condition but not experience it. In contrast, a male receiving a disease-causing allele on the X chromosome from his mother will have the condition. Because there is no equivalent allele on the Y chromosome to offset the disease-causing recessive allele, the disease-causing allele will be expressed. As a result, conditions such as hemophilia are more frequent among males and are called sex-linked characteristics (see Figure 3.5). As illustrated in Figure 3.6, the family of Queen Victoria of Great Britain (1819–1901) spread hemophilia to a number of European royal houses. If a daughter inherits her mother’s X chromosome containing the allele for hemophilia, she will be a carrier but will not have the disease. If a son inherits this chromosome, he will have the disease. Unlike his sister, he does not have a healthy X chromosome to offset the disease allele. Queen Victoria of Great Britain (1819–1901) had nine children. One son (Prince Leopold, Duke of Albany) had hemophilia, and two daughters (Princess Alice and Princess Beatrice) were carriers for the condition. As a result of their marriages to other European royalty, the three children spread the hemophilia gene to the royal families of Germany, Russia, and Spain. Therefore, hemophilia was once popularly called  the royal disease. Even when genes are duplicated on the X and Y chromosomes, they can perform very differently depending on their location. Forensic experts determine the sex of the source of a genetic sample by observing the amelogenin gene, which contributes to the development of tooth enamel (Akane, 1998). The size of the amelogenin gene on the X chromosome is different than on the Y chromosome. Differences between genes for immune system function located on the sex chromosomes might explain the higher risks associated with organ transplants in which the gender of the donor and recipient do not match (Ge, Huang, Yuan, Zhou, & Gong, 2012). Our species shares quite a few genes with chimpanzees, mice, fruit flies, yeast, and a weed known as thale cress (see Figure 3.7). At the same time, humans have genes that definitely set them apart from other animals (and plants). For example, research points to differences between humans and chimpanzees in a single gene, FoxP2, which appears to have had a significant effect on distinctly human behaviors, including spoken language (Konopka et al., 2009). Humans share quite a few genes with other species, such as the 18% of genes that we share with a weed known as thale cress. However, geneticists are most interested in the genes that differ from those of other species, such as the FoxP2 gene, which appears to be responsible for spoken language. Mutations of this gene cause severe speech and language disorders. Table 3.2 compares several subfields of genetics that are relevant to our understanding of behavior and mental processes. Behavioral geneticists attempt to discover the strength of genetic influences on a particular behavior. Molecular geneticists look for candidate genes, or genes that have a greater impact on a trait of interest than other genes. Functional geneticists study the entire genome, looking for whole patterns of genetic differences linked to a given trait. Finally, geneticists studying gene–environment interactions look for situations in which candidate genes appear to have different effects. Branch of GeneticsTopicExampleBehavioral geneticsAmount of heritabilityVariations in loneliness across the population appear to be strongly influenced by genetics. Molecular geneticsCandidate genesCertain genes seem to impact loneliness more than others. Functional genomicsLinks between the global genome and particular traitsGenes might be expressed differently in lonely and nonlonely individuals. Gene–environment InteractionsCandidate genes have different effects in different situationsCandidate genes seem to have more impact on loneliness in some environments compared to others. Behavioral geneticsBehavioral geneticsThe scientific field that attempts to identify and understand links between genetics and behavior.            Behavioral genetics        The scientific field that attempts to identify and understand links between genetics and behavior.             investigates the strength of genetic influences on a particular behavior. HeritabilityHeritabilityThe statistical likelihood that variations observed in a population are because of genetics.            Heritability        The statistical likelihood that variations observed in a population are because of genetics.             is the statistical likelihood that variations observed across individuals in a population are due to genetics. If genes play no part in producing phenotypical differences among individuals, heritability is zero. For example, genes are responsible for human hearts, but there is no individual variation in the population in terms of the presence of a heart—each of us has one. Consequently, the heritability of having a heart is zero. If genes are totally responsible for all phenotypical differences among individuals, heritability is 1.0. For example, all variation in the population in terms of having or not having a fatal neurological condition known as Huntington’s disease is entirely because of genetics. If you inherit a Huntington’s gene from one parent, you will develop the condition. Heritability of most human traits is typically in the range of 0.30 to 0.60. Heritability is a concept that is frequently misunderstood. It always refers to populations, never to individuals. Saying that a trait such as shyness is 40% heritable does not say that 40% of one individual’s shyness is produced by genes and the other 60% by his or her environment. Instead, this suggests that the variations in shyness that we see across the population (some people are shy and others are not) are the result of both genetic and environmental factors (see Figure 3.8). Heritability rates tell us how much of the variability seen in a population can be because of genetics. According to these data, we can say that genes have a greater influence on autism spectrum disorder than on reading disabilities. Heritability cannot be assessed without taking the environment into account, which is another source of potential confusion. If the environment is held constant (i.e., everybody is treated the same way), the heritability of a trait will appear to be high. For example, if you plant seeds in trays with identical nutrients, water, and sunlight, the height of the resulting plants will reflect their genetic differences. In variable environments, heritability is lower. If you plant seeds in trays receiving different amounts of nutrients, water, and sunlight, the height of the resulting plants will appear to be less influenced by genetics. If you studied the heritability of human intelligence in participants living in extremely wealthy or extremely poor circumstances, genetic influences would be exaggerated, just as they are when you hold the nutrients, water, and sunlight constant for your plants. Researchers assessing the heritability of human traits attempt to do so within a typical range of environments. Genetic Research often pushes the boundaries of research ethics, discussed in Chapter 2. Conclusions from genetic research can apply to entire groups of people, far beyond the individuals who agreed to participate. Even when individual participants are protected, the possibility that data could stigmatize an entire group should be considered and avoided. For example, most anthropologists argue that the political concept of  race is not clearly defined at the genetic level. Individuals of European and Asian backgrounds, but not individuals from sub-Saharan backgrounds, usually trace 1% to 4% of their DNA back to non–Homo sapiens ancestors, such as Neanderthals or Denisovans. This type of finding could be misinterpreted by the public as representing an important racial divide, an idea that has been soundly rejected by geneticists (Yudell, Roberts, DeSalle, & Tishkoff, 2016). Many indigenous groups have been reluctant to participate in genetics research. The possibility of promoting social prejudices due to stigmatization is a common source of concern, along with cultural values that are inconsistent with genetics methods. Many studies require storage of biological samples in biorepositories. This practice might be inconsistent with cultural values. For example, the Maori people of New Zealand view tissue samples, DNA, and any associated data as taonga ( precious) and tapu ( sacred). To engage the Maori in genetics research, researchers developed a culturally sensitive plan that tailored informed consent, control over the procedures and data, and community engagement to existing Maori values (Beaton et al., 2017). Although the model was developed for use with Maori participants, it provides guidance for researchers setting up biorepositories with many types of communities. Using culturally sensitive procedures for handling biosamples collected for genetics research built trust between researchers and Maori communities in New Zealand. Because of the influence of environment on heritability, some researchers question the use of adoption studies for assessing the relative influences of genetics and environment on development. These studies compare adopted children to their biological and adoptive parents in an effort to assess the relative impact of heritability. Like plants with constant amounts of nutrients, water, and sunlight, adoptive families share many common features as a result of the screening process required before adopting. Consequently, adoptive parents rarely represent as much diversity as the group of biological parents whose children they adopt. If all adoptive families provide a consistent environment, this factor would inflate the apparent heritability of characteristics examined in their adopted children. The environments provided by adoptive parents are more similar to one another than to the environments provided by biological parents. This environmental similarity can exaggerate genetic influences—a result that must be taken into account when investigating heritability using adopted children. Comparisons of twins are often used in behavioral genetics to evaluate relative contributions of genetics and environment. The differences between identical twins, who share identical DNA, and fraternal twins, whose DNA shows the same approximately 50% of overlap found in any two siblings, are particularly useful because both types of twins share similar environments. The Minnesota Study of Twins Reared Apart (Bouchard, Lykken, McGue, Segal, & Tellegen, 1990) not only compares identical to fraternal twins, but includes pairs of identical twins and fraternal twins adopted at birth and raised in separate homes (see Figure 3.9). Comparisons of identical twins raised together or in separate adoptive families show that some traits are more similar in twin pairs (fingerprint ridge count) than others (nonreligious social attitudes). However, regardless of the degree of similarity for a trait, living apart or together seems to have had relatively little impact on the degree of similarity within twin pairs. In other words, twins raised apart are just about as similar on each trait as twins raised together. Twin studies are also useful in establishing concordance ratesconcordance ratesThe statistical probability that a trait in one person will be shared by another; usually discussed in relation to identical and fraternal twins.            concordance rates        The statistical probability that a trait in one person will be shared by another; usually discussed in relation to identical and fraternal twins.            , which are statistical probabilities that a trait observed in one person will be seen in another. Concordance rates are especially useful to psychologists interested in psychological disorders because they provide estimates of the heritability of a condition. For example, concordance rates for autism spectrum disorder (see Chapter 14) are high in identical twins (95.2%) relative to fraternal twins (4.3%;  Nordenbæk, Jørgensen, Kyvik, & Bilenberg, 2014). Because both types of twins share a uterine environment and are exposed to similar parenting, this discrepancy makes a strong argument for the importance of genetic influences on autism spectrum disorder. Our species shares genes for eye color, but combinations of alleles produce many possible shades. Although we can say that all humans share 100% of their genes, we do not share 100% of our alleles, giving each of us a unique version of the genome. In other words, we all share genes that produce eye color of some sort, but our different combinations of alleles result in a variety of shades. There are at least 3 million DNA variations in the human genome—enough for us to differ from one another in nearly every gene (Plomin & Spinath, 2004). Analyzing variations of DNA in individuals who do or do not have a particular trait of interest, such as an illness or psychological disorder, can help molecular geneticists pinpoint the causes of problems and suggest preventive strategies. A common misunderstanding is the belief that we can identify  a gene for a particular behavior. For example, a recent headline in Time magazine trumpets,  The Genes for Pot Addiction Have Been Identified (Szalavitz, 2016). It is important to remember that genes encode for proteins, not behaviors. Genes build proteins that construct brains, and brains might or might not become addicted to cannabis. Rather than viewing a gene as causing a complex behavior, it is more accurate to view genes as contributing to the development and functioning of the nervous system, which in turn generates observable behavior. Prior to the development of databases made possible by the Human Genome Project and the International HapMap Project, investigating more than a few genes at a time was not feasible. Instead, candidate genecandidate geneA gene that has a greater impact on a trait of interest than other genes.            candidate gene        A gene that has a greater impact on a trait of interest than other genes.             research studies were conducted, in which one gene or a small number of genes were compared between groups of people with and without a condition of interest. This search for candidate genes for a particular phenotype, such as schizophrenia, did not result in accurate or complete findings (Farrell et al., 2015). Rather than testing single genes, contemporary functional geneticists often use genomewide association studies (GWAS)genomewide association studies (GWAS)A scan of complete sets of DNA from many participants, which is performed to look for variations associated with a particular phenotype, condition, or disease.            genomewide association studies (GWAS)        A scan of complete sets of DNA from many participants, which is performed to look for variations associated with a particular phenotype, condition, or disease.             or whole-genome sequencing (WGS). Emerging technologies now allow researchers to scan complete sets of DNA from many participants, looking for variations associated with a particular phenotype, condition, or disease. When 25 historical candidate genes for schizophrenia were reevaluated using GWAS, effects for 24 of the 25 genes were not confirmed, and 4 genes that had been missed by the candidate gene approach appear to be quite important (Farrell et al., 2015). As our methods for conducting genetic research improve, our answers become more complete and accurate. Having identical genotypes, as is the case with identical twins, does not guarantee identical phenotypes, or observed characteristics. As we explained in the example of the Agouti gene’s effects on the fur color and weight of baby mice, different phenotypes can result from the same genotype due to interactions between the organism and its environment. When factors other than the genotype produce changes in a phenotype, we say that an epigeneticepigeneticThe study of gene–environment interactions in the production of phenotypes.            epigenetic        The study of gene–environment interactions in the production of phenotypes.             change has occurred. Epi is Greek for  over or  above, so epigenetics refers to the reversible development of traits by factors that determine how genes perform. The field of epigenetics explores these gene–environment interactions. Epigenetic change influences gene expression, the process by which DNA builds proteins that contribute to features of living cells. Genes can be turned on or off by internal signals (hormones or neurochemicals) or by signals from external sources (diet or toxins). There is an obvious need for epigenetics in development, as the differences between a skin cell and a muscle cell result from turning on the right set of genes and turning off others. Thus, the magnitude of epigenetic change depends on an organism’s age. The fetus experiences the highest rate of epigenetic change, followed by the child and finally, the adult. While epigenetic changes are reversible, many last entire lifetimes. For example, individuals who experienced traumatic life events during childhood were found to have long-term epigenetic changes in the hippocampus, a structure associated with memory and responses to stress (Abdolmaleky, Zhou, & Thiagalingam, 2015). Although we have cautioned you against  a gene for reasoning for complex human behaviors, the examination of candidate genes continues to be a viable approach to understanding the  nature part of the nature–nurture interaction (see Figure 3.10). In some cases, identification of candidate genes can have profound influences on public policy. One such candidate gene is the MAOA (monoamine oxidase A) gene, which has been implicated in antisocial behavior. The protein produced by the MAOA gene is an enzyme that affects several important neurochemicals, including dopamine and serotonin, which are discussed further in Chapter 4. Variations in MAOA are classified as low or high activity. The low-activity version has been linked to impulsive antisocial behavior, leading to its popular reputation as  the warrior gene. How valid is this point of view? Low and high activity versions of the MAOA gene interact with the experience of child maltreatment to predict antisocial behavior (Caspi et al., 2002). Here, we see that youth with either allele who are not exposed to child maltreatment have a low risk of being convicted for a violent crime. Youth with histories of probable or known severe maltreatment have a higher risk of being convicted of a violent crime if they also possess the low-activity version rather than the high-activity version of the MAOA gene. An early case study (see Chapter 2) of a Dutch family characterized by an unusual zero-activity version of MAOA and a history of extreme aggression encouraged further investigations of the  warrior gene (Brunner, Nelen, Breakefield, Ropers, & van Oost, 1993). Studies of more typical MAOA alleles, however, demonstrate very small effect sizes. This means that only a small part of the variation in antisocial behavior across the population can be linked to people’s MAOA alleles. Many other factors must play important roles in determining levels of aggressiveness. This has not, however, stopped the legal system from considering MAOA status in criminal cases. Not only has MAOA status been admitted as evidence in the European Union and in the United States, but this information has been used to support the idea that defendants had reduced responsibility for aggressive actions because they  couldn’t help being violent. However, it is also possible that courts of law might administer more stringent punishments, such as longer prison terms, under the belief that the person is relatively untreatable (González-Tapia & Obsuth, 2015). Geneticists and psychologists have a responsibility to communicate the correct interpretation of findings to policymakers in order to avoid improper applications of research results. Among the factors known to produce epigenetic change are nutrition, disease-causing organisms, drugs, stress, and environmental toxins. In particular, malnutrition and stress experienced by pregnant women have the potential to influence the epigenetics of the fetus, leading to lifelong effects on physical and psychological well-being. In the discussion of psychological disorders in Chapter 14, you will see that many disorders trace their roots to a combination of genetic vulnerability and disruptions experienced by the pregnant woman, such as illness or malnourishment. Fraga et al. (2005) studied 160 identical twin pairs between the ages of 3 and 74. The two chromosomes on the left belong to a pair of 3-year-old twins, and those on the right belong to a pair of 50-year-old twins. Areas of red indicate differences between the two chromosomes in each pair related to differences in gene expression. As twins age, their gene expression becomes more different, as indicated by the greater amount of red in the chromosomes from the older twins. Twins who had spent the most time apart showed the greatest epigenetic differences. Geneticists have identified four processes that produce lasting but reversible changes in gene expression: ribonucleic acid (RNA) interference, RNA editing, histone modification, and DNA methylation (see Figure 3.11). For the purposes of this overview, we will focus on histone modification and DNA methylation. Histones are protein structures around which your DNA is wound. If the DNA in a single cell were not wound up in this fashion, it would be over 6 feet long (Annunziato, 2008). When either the core or the tail of a histone interacts with regulatory proteins, the expression of nearby segments of DNA can become more or less likely. DNA methylation occurs when a methyl group (one carbon atom bonded to three hydrogen atoms) is added to the DNA molecule. This has the result of turning genes off. You can think about DNA methylation as being similar to stapling some pages in a book together. Because of the staples, you can’t read the pages. Two processes for producing epigenetic change are histone modification and DNA methylation. Histone modification occurs when certain chemicals interact with the tail or core of a histone. DNA methylation occurs when a methyl group (one carbon atom bonded to three hydrogen atoms) attaches to the DNA molecule. These modifications affect the likelihood that particular genes will be expressed or silenced. We typically think of Nutritionists as providing good advice about what to eat, especially for pregnant women, children buying school lunches, and people facing health challenges. But we also understand that the food we eat is one of the key environmental factors that produce epigenetic change. A new field known as nutrigenomics has emerged to explore the epigenetic influences of diet. In the not-too-distant future, you might be given an individualized diet plan by a nutritionist based on your personal methylation pattern. Although this might initially help us with certain cancers and other disease states, we know that some psychological disorders are also influenced by epigenetics. It is possible that in addition to more conventional treatments, individuals with psychological disorders might benefit from diets tailored to their epigenetic histories. Many common foods have known epigenetic effects. Intake of garlic, broccoli, and dietary fiber can turn on anticancer and other protective genes. We saw earlier in this chapter how mother mice exposed to BPA gave birth to offspring that had yellow fur and were obese due to reduced methylation of the Agouti gene. However, if the mother exposed to BPA was also given a diet rich in the nutrients choline, betaine, and vitamin B12, all of which contribute to increased methylation, the offspring had normal weight and brown fur. We might not be able to control the environmental toxins to which we are exposed, but a greater understanding of epigenetics might help balance our exposure with appropriate nutrition. Although many epigenetic studies examine physical features like fur color, more complex features of interest to psychologists are also subject to epigenetic influences. For example, rats that were licked frequently during infancy by their mothers (the rat equivalent of getting a hug from mom) were calmer when faced with stress later in life than rats licked infrequently (Champagne, Francis, Mar, & Meaney, 2003). By licking their pups, these mothers influenced the expression of genes that determined responses to a stress hormone. The nurture provided by the mother had a lifelong impact on the offspring’s ability to cope with stress. Children exposed to child abuse have been found to have similar long-lasting changes in the expression of genes related to stress hormones (Neigh, Gillespie, & Nemeroff, 2009), as well as a number of genes associated with later medical problems and psychological disorders (Yang et al., 2013). Happily, these changes appear to be reversible in children who experience consistent and responsive caregiving by foster parents (Fisher, Van Ryzin, & Gunnar, 2011). A human being has about 100 trillion meters of DNA. To put this in perspective, this means that our DNA could go to the Sun and back more than 300 times. Epigenetics has the potential to illuminate the causes of many psychological disorders, as discussed in Chapter 14. Hundreds of separate genes appear to be linked to disorders like schizophrenia, autism spectrum disorder, and bipolar disorder, yet no single gene produces more than a tiny effect on a person’s risk of developing these disorders. In contrast, patterns of DNA methylation and unusual histone modifications are strongly associated with risk for these conditions. For example, epigenetic differences help to distinguish between identical twin pairs in which one twin has schizophrenia or bipolar disorder while the other remains healthy (Abdolmaleky et al., 2015). We have seen in this chapter that individual experience can influence gene expression, but scientists also are considering the possibility that epigenetic changes can be passed along to subsequent generations. How might this happen? Brian Dias and Kerry  Ressler (2014) investigated transgenerational epigenetics using fear conditioning in mice. As explained in Chapter 8, animals (including humans) can learn to associate a signal with an important event through the process of classical conditioning. If a mouse learned to fear an odor associated with shock prior to becoming a parent, could this experience affect the offsprings’ reaction to the odor? The Question: Can a learned fear response to an odor be passed from one generation to the next? Dias and Ressler began by training mice to fear the smell of a chemical called acetophenone, which to us smells like almonds and cherries (see Figure 3.12). After the odor was infused into the mouse’s cage, the mouse experienced electric shock through the grid floor. After sufficient training, the mice showed fear in the presence of the odor, even when they didn’t get shocked. The researchers conditioned fear to another odor in one control group and included another control group that had no fear conditioning at all. According to Dias and Ressler (2013), a changed response to a distinctive odor can be passed to a mouse’s children and grandchildren. A male mouse  is exposed to an odor paired with foot shock. The male offspring of this mouse is tested with a combination of odor and a blast of noise (a standard way of measuring startle). The grandchild of the original mouse is tested the same way. Compared to mice whose fathers and grandfathers did not receive a pairing of odor and foot shock, these mice demonstrated an enhanced startle effect to the odor plus noise, suggesting that a greater sensitivity to the odor might have been passed along due to epigenetic change. All of the mice were mated following conditioning, and their offspring were tested for their reaction to acetophenone. As you learned in Chapter 2, all animal research in psychology is regulated by a set of strict ethical guidelines. The use of electric shock in this experiment must conform to federal regulations regarding pain. Offspring of the father mice receiving the acetophenone–shock pairings showed fear to the acetophenone, even though they had never experienced it. Even more surprisingly, the grandchildren of the original conditioned mice also showed fear of acetophenone, as did mice conceived using in vitro fertilization with sperm from the original trained mice. What could possibly produce this response to an odor that a parent learned to fear? The original trained mice responded to their fear training by increasing the production of a protein related to detecting acetophenone. The researchers suggested that DNA methylation had occurred on the genes responsible for producing the receptor protein. Because the next two generations showed similar changes, the researchers argued that the epigenetic changes were passed along through the sperm by mechanisms that are not fully understood. While some scientists support the conclusions put forward by Dias and Ressler, others are more skeptical. Successful replications and further research on mechanisms of transgenerational epigenetics might clarify the processes responsible for these observations. ConceptDefinitionExampleGenotypeAn individual’s genetic makeup. A person might have an allele for freckles and an allele for no freckles. PhenotypeAn individual’s observable trait. A person has green eyes. GeneA sequence of DNA in a specific location on a chromosome that contains the instructions for making a protein. The serotonin transporter gene is located on chromosome 17. Gene expressionInformation from a gene is used to produce a protein. Maternal diet in mice can affect the expression of the Agouti gene. AlleleOne of two or more possible variations of a gene. A small number of BRCA1 alleles are associated with cancer risk. HomozygousHeterozygousHaving two of the same alleles. Having two different alleles. A person has the SS or the LL form of the serotonin transporter gene. A person has the SL form of the serotonin transporter gene. DominantRecessiveAn allele that is expressed regardless of whether it is homozygous or heterozygous. An allele that is expressed only when it is homozygous. The allele for freckles is dominant. The allele for no freckles is recessive. Charles Darwin’s theory of evolution described how species change in an orderly manner. The human genome is the product of millions of years of evolutionevolutionDescent with modification from a common ancestor.            evolution        Descent with modification from a common ancestor.            , defined by biologists as descent with modification from a common ancestor. The study of evolution allows us to trace the family tree of living things. In his book The Origin of Species, Charles Darwin proposed that species evolve or change from one form to the next in an orderly manner (Darwin, 1859). Darwin was well aware of the procedures used by farmers to develop animals and plants with desirable traits by mating particular individuals. A farmer’s goal to raise the strongest oxen for pulling a plow might be accomplished by breeding the strongest available oxen to each other. In these cases, the farmer is using artificial selection to determine which individuals have the opportunity to produce offspring. Darwin suggested that instead of a farmer making these decisions, the pressures of survival and reproduction in the wild would make the choice, a process that he called natural selectionnatural selectionThe process by which survival and reproduction pressures act to change the frequency of alleles in subsequent generations.            natural selection        The process by which survival and reproduction pressures act to change the frequency of alleles in subsequent generations.            . Organisms that survive long enough to reproduce would pass their traits to the next generation. Organisms that did not reproduce would not have the opportunity to pass their traits to future generations. As geneticists often remind us, we have no infertile ancestors. In the more than 150 years since The Origin of Species was first published, our understanding of genetics and the fossil record has expanded exponentially, lending substantial further support for Darwin’s views. Surprisingly, Darwin was able to derive his theory without the benefit of a basic understanding of genetics. He was unable to account for the variations he observed in a particular trait. That understanding was provided by Gregor Mendel (1822–1884), who discovered ways to predict the inheritance of particular traits, like the color of flowers, in his research on pea plants (Mendel, 1866). Mendel, in turn, was working without our modern understanding of genes. Combining current understanding of genetics with the natural selection processes proposed by Darwin provides scientists with powerful hypotheses about the progression of species over time. In addition to the process of natural selection described by Darwin, evolution can result from mutation, migration, and genetic drift. A mutationmutationAn error that occurs when DNA is replicated            mutation        An error that occurs when DNA is replicated             is an error that occurs when DNA is replicated. The average human baby is born with about 130 new mutations, but most have no effect (Zimmer, 2009). Mutant alleles providing some advantage spread through the population, but most mutant alleles that result in a disadvantage disappear from future generations. MigrationMigrationMovement to a new location            Migration        Movement to a new location             occurs when organisms move from one geographical location to the next. Moving to a new location can affect the survival of individuals and the frequency of certain alleles in the population. Phenotypical traits that are advantageous in one environment might be less so in another. Genetic driftGenetic driftChange in a population’s genes from one generation to the next because of chance or accident            Genetic drift        Change in a population’s genes from one generation to the next because of chance or accident             produces change from one generation to the next through chance or accident. Type B blood is virtually absent in contemporary populations of Native Americans, most likely due to chance (Halverson & Bolnick, 2008). The group of ancestors who first made their way to the Western hemisphere did not include anyone with the Type B allele. If the ancestors’ blood type alleles had been more representative of the entire human population, more of their descendants would have Type B blood. Darwin understood that breeders could influence the traits of offspring by mating particular individuals. He believed that natural selection operated according to the same principles. The pressures of survival and reproduction in the wild would take the role of the breeder—determining which traits are passed to the next generation. We can explore the effects of evolutionary processes—mutation, migration, genetic drift, and natural selection—on the history of one allele: the recessive allele for blond hair. The original appearance of the allele for blond hair was probably the result of a random mutation occurring in northern Europe some 10,000 years ago (Frost, 2006). Migration, or rather the lack of it, might account for the relatively restricted area in northern Europe populated by blondes until fairly recently. Genetic drift undoubtedly reduced the global frequency of the blond allele between 1300 and 1700, as waves of bubonic plague decimated the European population. If by chance every person carrying the blond allele had died from the plague before reproducing, the allele would have disappeared from the human genome (see Figure 3.13). After the first appearance of the blond hair allele about 10,000 years ago as a result of a chance mutation, its frequency might have been affected by migration, genetic drift, and natural selection. Has natural selection influenced the blond gene? Some scientists believe it has. When people have a choice of mates of equal value, they will select the one that stands out from the crowd (Frost, 2006;  Bem, 2001). Therefore, individuals with relatively rare blond hair might have enjoyed more reproductive success than those with more common, darker hair colors (Field et al., 2016). In Germany, with its high percentage of blondes, the trait of blondness is viewed differently than in countries where blondes are relatively rare. German men report that they view blondes as good homemakers, but they find women with dark hair more sexually attractive. Gregor Mendel (1822–1884) made important discoveries about inheritance at about the same time that Darwin was working on his theory of evolution, but neither scientist knew about genes and chromosomes. Modern geneticists combine this knowledge to form powerful hypotheses about the nature of living things. Any consideration of evolution must include the question of what natural selection selects. Natural selection favors the organism with the highest degree of fitnessfitnessThe ability of one genotype to reproduce more successfully relative to other genotypes.            fitness        The ability of one genotype to reproduce more successfully relative to other genotypes.            , defined as the ability of one genotype to reproduce relative to other genotypes. The concept of fitness includes survival to adulthood, ability to find a mate, and reproduction. Fitness is not some static characteristic, such as being strongest or fastest. Instead, fitness describes the interaction between characteristics and the environment in which they exist. A genotype that succeeds during the Ice Age may be at a significant disadvantage during periods of warmer temperatures. Animals in cold climates have short legs and stocky bodies, which help retain heat, whereas animals in warm climates have long legs and slim bodies, which release heat. Again, we see the need to consider nature within the context of nurture. Marilyn Monroe and Jane Russell starred in the 1953 film Gentlemen Prefer Blondes. In some parts of the world, the sentiment in this title appears to be true. AdaptationAdaptationA change because of natural selection            Adaptation        A change because of natural selection             refers to either the process or the result of change because of natural selection. In other words, a species can respond to an environmental change by adapting, and features of the new phenotype may be called adaptations. Adaptations can take many forms. They can be behaviors, such as jumping higher to better avoid a predator, or anatomical features, such as eyes that can see color. Adaptations do not necessarily produce perfection. Any adaptation that is good enough to contribute to the fitness of an organism will carry forward into future generations. A classic example of rapid adaptation is the case of the English peppered moth (Biston betularia). Before the Industrial Revolution, most peppered moths found in Britain were light gray, which allowed them to hide against the similar colors of tree bark. Darker moths occasionally appeared beginning around 1848, but because they were less capable of hiding from predators, they made up only about 1% of the population. With increasing industrialization, tree bark became frequently coated in soot. The once-camouflaged light gray moths became an easy target for predators against the darker background of the sooty trees. The darker moths rapidly became the norm, reaching frequencies of about 98%. As pollution came under better control, tree bark returned to its original light gray, and the lighter moths again became the norm. The peppered moth population successfully adapted to changing environmental circumstances, with color playing the role of an adaptation. The moths did not  decide to change color. Natural selection, in the form of greater rates of reproductive success on the part of moths with a particular color, changed the frequencies of color alleles within the population. Adaptations often appear to be compromises between costs and benefits. Adult human males have about 10 times as much testosterone as adult human females. Testosterone conveys a reproductive advantage because men with higher testosterone report having more sex partners and earlier age at intercourse (Lassek & Gaulin, 2009). On the negative side, however, high testosterone levels lead to lower immune system functioning, making the high testosterone males more vulnerable to disease (Muehlenbein & Bribiescas, 2005). Adaptation is only one source of evolutionary change. Random events, such as the collision of meteors with the Earth and resulting climate changes, are believed to have destroyed some types of life and provided broad opportunities for others. We are also limited in our ability to use adaptation to predict the future. The study of evolution is similar to the study of history. Although we gain insight into wars by studying the causes of World War II, we cannot use our knowledge to predict future wars with precision. We can be fairly certain that antibiotic-resistant bacteria, climate change, pollution, and reproductive technologies are probably changing the face of the human population as this text goes to press, but where all these changes will lead us remains unknown. Fitness varies across environments. Characteristics like long ears and long legs work in hot, desert climates, but short ears and legs conserve heat in colder climates. The dark and light coloring of the peppered moth population in Great Britain changed in response to pollution from soot that collected on trees and changed again when pollution controls reduced the soot. Our interest as psychologists is in the mind, and the mind and the behavior that it produces originate in the structures and processes of the brain and nervous system. Nervous systems are fairly recent innovations that separate animals from plants (see Figure 3.14). We can place the origin of the Earth at 4.5 billion years ago and the first single-celled life forms 1 billion years later, but the first neural nets appeared only 700 million years ago. In these primitive animals, the nerves in the abdomen were as likely to be important to behavior as the ones in the head. True brains residing in heads did not appear until animals formed skeletons, around 500 million years ago. The first decidedly human brain made its appearance only 7 million years ago, a small blip in the timeline of evolution (Calvin, 2004). The current model of the human brain has only been available for the last 100,000–200,000 years. If the 4.5-billion-year history of the Earth were expressed as a 24-hour day: The first single-celled organisms would have emerged 18 hours ago. The first nervous systems would have emerged about 3 hours and 45 minutes ago. The first true brain would have emerged about 2 hours and 40 minutes ago. The first hominin brain would have emerged less than 2.5 minutes ago. The current version of the human brain would have emerged less than 3 seconds ago. Anthropologists use the term hominin to describe species that walked on two feet, had large brains, and are assumed to be related to modern humans. Over the 7 million years of hominin evolution, brains grew rapidly, suggesting that improved intelligence was quickly translated into substantial advantages in survival. Early tool-using hominins, the australopithecines, had brains that were about the same size as those of modern chimpanzees, or about. Homo erectus, a hominin living about 1.5 million years ago, had a brain of about nd the brains of modern humans, or Homo sapiens, are about. Males with high testosterone report more chances to reproduce, but testosterone also might lower the response of the immune system. Extremely high levels of testosterone, because of the use of performance-enhancing drugs, might be contributing to the early deaths of some professional wrestlers, like Curt Hennig (1958–2003). Hominins were not the only creatures who evolved large brains and considerable intelligence. The other primates, elephants, and whales are not lacking in these areas. Although the challenges of finding food, avoiding predators, and navigating through territories require considerable intelligence, these ecological challenges are no match for the complexity of social life faced by the hominins. The major factor distinguishing human intelligence from intelligence of other species is the richness and complexity of the social behavior supported by the human brain. Managing the abilities to distinguish friend and foe, imitate the behavior of others, use language to communicate, recognize and anticipate the emotions, thoughts, and behavior of others, maintain relationships, and cooperate required the evolution of a special brain (Cacioppo et al., 2002;  Hrdy, 2005;  Roth & Dicke, 2005). Comparisons of the challenges faced by different species support a stronger role for social complexity than for ecological complexity in building bigger brains (Cacioppo & Decety, 2011a;  Dunbar & Schultz, 2007). Over the course of hominin evolution, the size of the brain increased dramatically. It is possible that the ability of bigger brains to manage social relationships could have driven this rapid change. You surf the Internet, complete your calculus homework, and read this textbook with a brain that is essentially the same size as that of your early Homo sapiens ancestors. Although we can understand the advantages to survival of big, intelligent brains, we do not know why advances such as agriculture, literacy, and urbanization have not been accompanied by additional increases in brain size. It is possible that we have reached equilibrium between our needs for intelligence and the costs of a big brain. Brains are expensive to run in terms of nutrients. Although the brain comprises only about 2% of the body’s weight, it requires at least 15% of the body’s resources. In addition, brain size may be limited by the dimensions of the birth canal. Further change may not occur unless we experience a drop in the costs of big brains, a change in nutrients, or additional pressures for greater intelligence. Although brain size has changed little during Homo sapiens’s time on the Earth, the evolution of the human brain has not ended, and average intelligence has not remained the same. Modern genetic techniques allow researchers to date changes in a particular gene. Genes involved with brain development appear to have changed as recently as 6,000 years ago (Evans et al., 2004,  2005). As discussed in Chapter 10, IQ test scores have increased dramatically worldwide over the last 100 years (Flynn, 1999). If brain size has not changed for 100,000 years, let alone during the last 100 years, how can we account for the observed increase in intellectual performance? It is likely that environmental factors, including nutrition and education, might explain the improvement. It is somewhat surprising that humans figured out how to go to the Moon and back with brains that were the same size as those of Homo sapiens living 100,000 years ago. ConceptDefinitionExampleNatural selectionA trait’s frequency in a population is determined by the survival and reproductive success of the organisms with the trait. Faster rabbits are more likely to survive and reproduce than slow rabbits, leading to more fast rabbits in subsequent generations. MutationGenetic changes that occur spontaneously or because of external factors like radiationA mutation about 10,000 years ago led to the appearance of blond hair. MigrationPopulations move from one area to another. Moving from a cold to a warm climate might change the natural selection of traits in a species. Genetic driftChance events influence the frequency of alleles. Huntington’s disease is more common than usual among the Afrikaner population of South Africa because a carrier of the disease was among the small number of Dutch immigrants who survived to reproduce. FitnessThe ability of one genotype to reproduce relative to other genotypes in a particular environment. The recessive sickle-cell trait has low fitness in general because it can produce disease, but because the trait protects the individual from malaria, it has higher fitness in populations living where malaria is common. AdaptationEvolutionary changes whereby a population becomes better suited to its environment. The moth population changed from primarily light to dark colors when the trees they inhabited became darker because of pollution. We mentioned in an earlier section that behavior can be adaptive. If an animal that is good at hiding in the bushes is more successful than others of its species in escaping predators, it is likely that the ability to hide will spread through subsequent generations of the population. Physical features, like the coloring of an animal that allows it to hide, are well-understood types of adaptations. Evolutionary psychologists attempt to explain how behaviors can be adaptive too. Behavior like hiding, however, is unlike the other adaptations discussed so far, such as the color of a moth or blond hair. Color is a physical characteristic, and it is a fairly simple matter to identify the genes associated with these phenotypes. Behavior as a phenotype is considerably more complex. It is not an anatomical structure like a wing or an eye. Can we assume that behavior is shaped by the same evolutionary forces that affect physical traits? Darwin thought so. In The Descent of Man and Selection in Relation to Sex, he writes:The difference in mind between man and the higher animals, great as it is, certainly is one of degree and not of kind. We have seen that the senses and intuitions, the various emotions and faculties, such as love, memory, attention, curiosity, imitation, reason, etc., of which man boasts, may be found in an incipient, or even sometimes in a well-developed condition in lower animals. (Darwin, 1871, p. 126)Among the psychological specialties discussed in Chapter 1, evolutionary psychology, a subspecialty within biological psychology, is the most relevant to our current discussion of the evolution of behavior. This approach to the mind assumes that our current behavior exists in its present form because it provided some advantage in survival and reproduction to our ancestors (Cosmides & Tooby, 1997). The evolutionary psychology approach not only owes an obvious debt to Darwin, but also is a direct descendant of the functionalism supported by William James. As the term functionalism implies, behavior is seen as promoting survival, as opposed to being random and pointless. The goal of evolutionary psychology is to explain how the patterns of behavior that we share with other humans have been shaped by evolution. Predator fish typically hunt on the perimeter of a fish ball, where it is easier to isolate and capture prey. Humans who are on the social perimeter also may be at risk from predation, but usually from their own kind. Reconstructing the evolution of the nervous system is difficult, and tracing the origins of individual behavior is even more challenging, but identifying the roots of social behavior might be the most difficult task of all. Occasionally, physical evidence has allowed scientists to determine whether dinosaur parents stayed around to look after their young, but such accounts leave much detail unexplored and unexplained. Individuals belonging to social species congregate in a number of ways, from pairs to families to whole societies. Belonging to a social group provides the benefits of mutual protection and assistance. Predatory fish are most likely to hunt in the perimeter of a school of fish because it’s easier to isolate prey there than in the middle of the school (Ioannou, Guttal, & Couzin, 2012). Being on the social perimeter is risky for our species as well. This simple fact of survival might be one of the reasons that we react so emotionally when we believe that we are being socially excluded. Being social carries costs as well as benefits. Social animals face injury in competition for food and mates and are exposed to contagious illnesses (Alexander, 1974), but the benefits of being social have clearly outweighed the costs for many species. In typical environments, individual animals experience a variety of possible interactions when they come into contact with others of their kind. In each case shown in Table 3.3, individuals either benefit or do not benefit from the interaction, ultimately affecting their survival and reproductive success. Both parties benefit equally if they cooperate. For example, two hunters can work together to bring down an animal that neither could successfully hunt alone. Sharing the resulting meat with the families of both hunters would contribute to their survival and reproductive success. Much social behavior probably originated in these types of situations, in which the benefits of cooperation for an individual’s survival and reproduction outweighed the disadvantages of cooperating. And the second organism:The first organism:WinsLosesWinsCooperationSelfishnessLosesAltruismSpiteAlthough no cultures today exist exactly like the hunters–gatherers of the past, the Ache of Paraguay are often used as a model of how that life might have been. Here, the Ache cooperate with one another to fish. Social behaviors like cooperation might have allowed humans, who are not particularly strong individuals, to survive. Cooperation, however, is not the only way that two individuals can interact. In selfish interactions, one person could steal food from another, allowing the thief’s family to survive while the victim’s family starves to death. In spiteful interactions, both participants lose. In some divorce proceedings, the partners are so determined to keep each other from maintaining resources that everything goes to the attorneys. Finally, in altruismaltruismBehavior on behalf of another that fails to benefit or harms the individual performing it            altruism        Behavior on behalf of another that fails to benefit or harms the individual performing it            , one individual’s self-sacrifice is designed to benefit another individual. Altruism, or the sacrifice of yourself for others, is more common among related individuals, but it also occurs when we are in close social contact with others. Honeybees sting to defend their hive, but in doing so, they end their own lives. Altruism is widespread in the animal kingdom. Most of us have experienced a honeybee sting, which is suicidal behavior on the part of the bee in an effort to protect its hive (Wilson, 1975). Altruism can extend to entire social organizations, regardless of the degree of relatedness (Chicago Social Brain Network, 2011). Among emperor penguins (Aptenodytes forsteri), survival of the chicks in the hostile Antarctic cold depends not only on an individual parent, but on the larger huddle formed by other parents. As discussed in greater detail in Chapter 13, altruism is one of the most challenging social behaviors to explain in evolutionary terms. Darwin himself was puzzled by the apparent sacrifice of some individuals that led to the survival of the group. If altruism results in the destruction of the individual with altruistic genes, why doesn’t this behavior disappear? To explain this phenomenon, we return to the concept of relatedness presented earlier in the chapter. Sacrificing your life to save a close blood relative might increase the likelihood that your alleles would be passed along to subsequent generations. Self-sacrifice in this case does not need to be a conscious decision. Any behavior that results in a greater frequency of the relevant genes in subsequent generations will become more common. Cooperation allows humans to carry out complex behaviors that would be impossible for a single individual to perform successfully. You might be thinking that you often behave altruistically toward people who are not related to you. You might have comforted a friend who experienced a death in the family instead of studying for an upcoming midterm. This type of behavior, known as reciprocal altruismreciprocal altruismHelp that you provide for another person when you expect the person to return the favor in the future            reciprocal altruism        Help that you provide for another person when you expect the person to return the favor in the future            , occurs when we help another individual who is likely to return the favor at some future date (Trivers & Burt, 1999). Sexual selectionSexual selectionThe development of traits that help an individual compete for mates.            Sexual selection        The development of traits that help an individual compete for mates.             was Darwin’s term for the development of traits that help an individual compete for mates (Darwin, 1871). To what extent is human behavior influenced by sexual selection? Survival of emperor penguin chicks in the hostile cold of the Antarctic depends not just on the individual parent, but also on the larger huddle formed by the other parents. Sexual selection is influenced by the different investments in parenting made by males and females (Emlen & Oring, 1977). In many species, including our own, the female bears most costs of reproduction, from the carrying of the developing organism until birth to the nurturance of the young until adulthood. As a result, human females face sharper limitations than human males on the number of children that they can produce in a lifetime. If the goal is to pass your genes to subsequent generations and you are going to produce only one or two children, each child had better be as healthy and well nurtured as possible. The average number of children per woman worldwide dropped dramatically from 4.98 in 1960 to 2.45 in 2014 (The World Bank, 2016; see Figure 3.15). The average number of children per woman worldwide dropped dramatically between 1960 and 2014. Because males have a lower investment of time and resources in reproduction compared to females, it might seem that the best reproductive strategy for males would be promiscuity, but this is not usually the case. In species such as our own, with lengthy and complex development leading to adulthood, a male who abandons his offspring puts their survival at risk (Gibson, 2008). Even if a man fathers many children, his genes are less likely to make it into the next generation if most or all perish from lack of care or protection. The mother can maximize her children’s chances of survival by choosing a father who will not only pass along healthy genes, but also participate in the raising of children. Women have the ability to make accurate predictions of a man’s interest in children simply by looking at a photograph of his face (Roney, Hanson, Durante, & Maestripieri, 2006). Men with facial features correlated with high testosterone (strong brow ridge, square chin) are viewed as less likely to participate in childrearing than are men with facial features correlated with lower testosterone. These results suggest that women would be able to determine a man’s potential as a father before reproductive investment occurs. Earlier in this chapter, we discussed how blond hair might have provided a reproductive advantage because its relative novelty made it an attractive feature. What other types of human traits appear to fit Darwin’s ideas about sexual selection? Genghis Khan may have been the most prolific human male in history. His distinctive Y chromosome has been identified in 16 million living men, or 0.5% of the world’s current total (Zerjal et al., 2003). Sexual selection might occur in two ways. In intrasexual selection (intra means  within), members of one sex compete with one another for access to the other sex. In some species, such as deer, males engage in fights that determine which males are allowed to mate and which are not. Features like large antlers, which assist in winning a fight, could become sexually selected. In intersexual selection (inter means  between), characteristics of one sex that attract the other might become sexually selected. The male peacock’s luxurious tail appears to have developed for the sole purpose of attracting mates. Evolutionary psychologists have argued that a number of human traits might have been subjected to sexual selection, including humor and vocabulary. According to this argument, human males use humor (Cherkas, Hochberg, MacGregor, Snieder, & Spector, 2000) and their vocabularies (Rosenberg & Tunney, 2008) to impress females with their intelligence because of intersexual selection. In romantic situations, males use more uncommon words than they do in other situations. We can also tell you about one behavior that does not successfully attract females—taking unnecessary risks (Wilke, Hutchinson, Todd, & Kruger, 2006). However, risky activities might have indirect positive outcomes for males. Dominant, successful males are likely to attract more females, and dominance among males is often decided on the basis of intrasexual competition in risky endeavors. Women show the ability to predict a man’s score on the Infant Interest Questionnaire, which might indicate how involved a father he would be, by detecting the influence of testosterone on his facial features. The face at the top indicates higher testosterone, while the one at the bottom indicates lower testosterone. Not only are human societies directly influenced by our biological history, but they also bear the stamp of the inventions of the biological brain in the form of culture. Cultures, which arise from socially transmitted knowledge, provide practices, values, and goals that can be shared by groups of people. Languages, morality, arts, laws, and customs make up a diverse and vibrant part of human social interactions. Experiences shaped by culture, like other types of experiences, interact with survival and reproductive pressures. How might cultural differences have affected our ancestors’ survival? We can gain insight into our species’ cultural history by observing contemporary preagricultural societies, such as the Waorani and Yanomamö of the Amazon Basin. These groups are remarkably combative. Fights between villages account for 30% of the deaths among Yanomamö males (Chagnon, 1988) and 54% of deaths among Waorani males (Beckerman et al., 2009). Cultural traditions in the two groups have led to different patterns of interactions between aggression and reproductive success. Aggressive Yanomamö men produce more children than less aggressive Yanomamö, but less aggressive Waorani men have more surviving offspring than aggressive Waorani (Beckerman et al., 2009). A simple cultural distinction—the Yanomamö practice of standing down for a period of time between raids, which is not a practice shared by the Waorani—appears to account for the differences observed in the impact of aggression on reproductive success. Our social minds were shaped by the cultures of hunter–gatherer groups until the development of agriculture approximately 10,000 years ago. With improved control of the food supply, less geographical mobility, and larger communities, humans entered a new era of social interaction. Although we believe that many features found in modern human behavior, such as reciprocal altruism, originated in the hunter–gatherer society, further social adjustments were required as groups became larger and more complex. Agriculture, with its emphasis on land ownership, might have been the origin of patriarchal systems, in which men maintain control of resources, and inheritances follow the male line. Unlike hunter–gatherer societies, which are relatively egalitarian so far as the rights of men and women go, agricultural societies tilted the control of food and important resources in favor of men. Early industrialization merely built upon agricultural systems and, if possible, accentuated the power differential between males and females. Contemporary trends are again moving in a more egalitarian direction, with women in developed countries enjoying considerable financial independence and reproductive choice. These changes will no doubt have further effects on our social environment. The Waorani (left) and Yanomamö (right) of the Amazon Basin share high rates of aggression and yet experience different reproductive outcomes. Reproductive success is higher among the most aggressive Yanomamö and the least aggressive Waorani. This outcome is probably the result of the Yanomamö practice, not shared by the Waorani, of standing down between raids. This gives aggressive Yanomamö men chances to rest, heal, and reproduce that are not available to aggressive Waorani men. As societies became larger, humans took advantage of their large brains to devise new cultural systems to maintain group cohesion. Emerging societies shared many of the same types of internal conflict, so we typically find similar moral, religious, and legal systems across diverse cultures that attempt to control marriage,  character issues such as honesty, and the transfer of precious resources. With all the flaws of our groups, whether we’re looking at families, communities, or nations, we retain a strong need to belong. As we will see on many occasions in this textbook, humans do not thrive in isolation. Our dependence on kinship, friendship, and group membership, honed over the course of 100,000 years or more of social living, continues to influence our behavior today. Humans have developed a strong need to belong and often adopt traditions that enhance a sense of group membership. The development of agriculture 10,000 years ago changed the shape of human cultures. A Cluster of genes known as the major histocompatibility complex (MHC) appears to be subject to sexual selection (Wedekind & Füri, 1997). As described in this chapter, this type of sexual selection operates on traits in one sex that influence the choices by the other. A child with a heterozygous set of MHC genes is better prepared to battle infections than is a child with MHC genes that are similar to one another. As a result, our children are more likely to survive if we select a mate that has a different set of MHC genes than we do. There is some evidence that too much MHC similarity affects not just our children, but also our relationships. Garver-Apgar et al. (2006) reported that among romantic couples, increases in the number of MHC alleles shared by a couple reduced the woman’s sexual responsivity to her partner and increased her number of extrapair sexual partners and her attraction to men other than her partner. In a world before genetic testing, it appears that we were able to select partners with compatible MHC profiles on the basis of odor. Different configurations of the MHC genes produce distinctive body odors. When participants were asked to rate the pleasantness of the odors of T-shirts that had been worn by other people, they preferred smells associated with MHC genotypes that were different from their own (Wedekind & Füri, 1997). Instead of relying on smell, some people are taking advantage of relatively affordable genetic testing to assess their relationships. Instant Chemistry provides clients and their partners with information about genes related to immune function, serotonin transport, oxytocin receptors, and dopamine function. It is too soon to tell whether such endeavors will improve relationships. We hope that you will use the critical thinking skills discussed in Chapter 2 to evaluate Instant Chemistry’s claim that they  can help give you insights into how your relationship ticks and where strengths and weaknesses may lie ( Instant Chemistry Uses Biological and Psychological Factors, 2017). Participants in a study preferred the smell of T-shirts worn by people who have MHC genotypes different from their own. Historically, these smell preferences might have helped people select partners with whom they are most likely to have healthy children. What can we learn about our cyberbullying problem by incorporating research about genetics? Several researchers have identified interactions between genes and the experience of bullying that influence a victim’s outcomes (see Figure 3.16). Variations in the serotonin transporter gene (SERT) interacted with the frequency of being bullied to predict the extent of emotional disturbances in youth (Sugden et al., 2010). Having a  risk allele of the FKBP5 gene, which influences the development of stress-related glucocorticoid receptors, interacted with the experience of bullying to predict negative mood, adverse reactions to social stress, and problems with perceiving reality in youth (Cristóbal-Narváez et al., 2016). The risk alleles appear to increase a person’s sensitivity to repeated social defeat, such as being victimized by bullies. Childhood emotional problems become more common when a child experiences frequent, not occasional, bullying, especially when the child has the SS genotype for the serotonin transporter gene. Understanding that some people are more vulnerable to the adverse effects of being bullied might help us identify youth at risk and provide appropriate interventions to help them cope. But what can genetics tell us about the people doing the bullying? Bullies in general, and cyberbullies in particular, share what are known as the Dark Triad of personality traits: Machiavellianism (manipulates others), narcissism (feelings of superiority and entitlement), and psychopathy (lacking empathy) (Goodboy & Martin, 2015). The Dark Triad traits, like all personality traits, are somewhat heritable (Vernon, Villani, Vickers, & Harris, 2008). Do we see the same evidence for gene–environment interactions for these traits as we do in the case of victims of bullying? It is likely that many environmental influences interact with basic genetic predispositions to these traits, including the quality of parenting (Jonason, Lyons, & Bethell, 2014). Others see Dark Triad traits as shaped by evolution to cope with harsh and unpredictable environments (Jonason, Icho, & Ireland, 2016). Because Dark Triad traits resulted in greater survival in certain environmental niches, the traits remain in our contemporary population. In other words, bullies happen. Understanding the nature of bullies and their victims does not lead directly to a solution to the cyberbullying problem, but it might serve as a starting point. We can recognize, at least, that individuals do not have an equal likelihood of becoming bullies and that not all of those who are bullied will react the same way. ConceptDefinitionExampleCooperationWorking together to benefit all parties involved. Hunters work together to kill an animal that an individual could not kill alone. AltruismSacrificing your personal interests to benefit another. A young mother threw herself over her infant during a parking lot shootout. She died, but the infant survived. Reciprocal altruismSacrificing when you expect the recipient might do the same for you at some future time. You share food with a hungry person, thinking that someone might share with you later. Sexual selectionThe evolutionary pressure on traits that help individuals find mates. A woman may choose a man whose facial appearance is correlated with being a good father. Be sure that you can define these terms and use them correctly.`;\n","export const text = `Perceived social isolation or connectedness can produce changes in the cells of our immune system. Throughout history, human survival has been threatened by the various bacteria and viruses that try to make us their home. The bacteria-driven Black Death decimated Europe between 1346 and 1400, killing an estimated 30 to 60% of the population (Austin Alchon, 2003). Smallpox, measles, and influenza carried by Europeans to the Western Hemisphere killed as many as 90% of native populations (Public Broadcasting Service [PBS], 2005). The Spanish flu of 1918, which is related to contemporary bird flu strains, killed between 50 million and 100 million people worldwide in about a year (Patterson & Pyle, 1991). Although you might think surviving a pandemic is more a question of biology and medicine, behavior and mental processes have a considerable amount of influence on our abilities to fight bacteria and viruses (Cacioppo & Berntson, 2011). Again, zooming out to the human social environment and then zooming back in to a smaller scale gives us a complete and interesting picture. Humans, who lack impressive teeth or claws, formed groups to enhance the odds of their survival. Anyone who was socially excluded from these groups experienced a more hostile environment. Social exclusion not only separated a person from the help of others in life-threatening situations, perhaps in fending off a predator, but worse, could lead to outright conflict with others, including combat. Under such hostile circumstances, socially excluded people faced a greater risk from bacterial infections than from viruses. Bacteria enter the body through cuts and scratches, whereas viruses are transmitted through body fluids (e.g., sneezing), so you are most likely to be exposed to them when you are in close contact with other people. Human brains such as this one, carefully held by one of your authors, weigh about 3 pounds and contain approximately 86 billion neurons. That’s about the same number as the stars in our galaxy, the Milky Way. With that background in mind, look at the group of people in the image on the previous page. Do you think the woman on the left is feeling included or excluded? Surprisingly, whether we typically feel socially isolated or socially connected can have serious implications for our health (Cole et al., 2015). If this woman normally feels isolated and often left to fend for herself, she will, like her excluded ancestors, face a greater threat from bacteria than from viruses. Her brain will respond to her feelings of isolation by generating hormonal signals that will tell her immune system (shown in the larger image at the beginning of the chapter) to gear up to protect her against bacteria. In contrast, if she usually feels socially connected to others, her brain will initiate a cascade of hormonal signals that tell her immune cells to prepare to protect her against viruses. This is just one example of how the mind’s perceptions of the social environment—whether it is friendly or not, for instance—can affect biological processes that are important to health and survival. In Chapter 3, we learned how the challenges of surviving and reproducing in particular physical and social environments could shape a species’ biology and behavior. In turn, the resulting biological structures and processes of the mind exert profound influences on our physical and social environments. In this chapter, we will provide a foundation for understanding the biological bases of behavior and mental processes by exploring the structures of the nervous system and the ways that they function. Many of us find the concept that our minds are somehow a result of the activity of nerve cells a bit unsettling. How could our feelings, thoughts, and memories be caused by a bunch of cells? Shouldn’t there be more to who we are than something so physical? Such ideas led thinkers like Renaissance philosopher René Descartes to propose a philosophy of dualism, which suggests that our mind is somehow different and separate from our physical being. If you are more comfortable with thinking about mind this way, go ahead, as long as you recognize that the field of biological psychology, and the neurosciences in general, embrace the competing philosophy of monism. According to the monistic approach, the mind is what the brain does. Biological psychology, also known as behavioral neuroscience, is the scientific study of the reciprocal connections between the structure and activity of the nervous system and behavior and mental processes. Biological changes often influence behavior and cognition. For example, when your stomach is empty, a gut hormone called ghrelin is released. When ghrelin reaches the brain, you respond by feeling hungry. After you eat, ghrelin release is suppressed and you feel satisfied, so biology (amount of ghrelin released) initiates behavior and cognitions (feeling hungry and beginning to eat, or feeling full and stopping eating). However, your behavior and cognitions can also have substantial effects on your biology. When participants were told that the 380-calorie milkshake that they consumed was a sensible 140-calorie shake, their ghrelin levels barely changed, whereas consuming the same 380-calorie milkshake was followed by a steep decrease in ghrelin when they were told it was a 620-calorie indulgent shake (Crum, Corbin, Brownell, & Salovey, 2011). In other words, the way people thought about the shake (cognitions) had remarkable effects on their biology (amount of ghrelin released). We can’t guarantee that praising your healthy salad for its indulgent qualities will make it easier to stick to your diet, but the milkshake experiment reinforces the power of thought to influence biology. Consistent with our argument that multiple perspectives considered together produce a richer and more accurate analysis of the mind, we would like to introduce you to a combination of two seemingly distant fields—culture and neuroscience—to form cultural neuroscience. Cultural neuroscience has been defined as an interdisciplinary field that examines how cultural and biological mechanisms mutually shape human behavior across phylogenetic, developmental, and situational timescales (Chiao, 2015, p. 283). In other words, cultural neuroscientists explore how genetics, brain structures, and cultures interact to shape behavior (see Figure 4.1). Scientists working in the new discipline of cultural neuroscience argue that brain activity can be different when people of different cultures complete the same task. Here participants undergoing brain imaging were asked to think about their own traits (self), their mother’s traits (mother) and a famous person’s traits (other). Both American and Chinese participants showed more activation in relevant areas of the brain in response to thinking about themselves versus thinking about the other famous person, but their responses to their mothers differed. The Chinese did not show different patterns of activity when thinking about themselves and their mothers, but the Americans did. Cultural neuroscience asks two main questions. First, how do cultural phenomena such as beliefs and values influence genetics and brain structures? We saw an example of this question in our discussion of the Waorani and Yanomamö tribes in Chapter 3. The Yanomamö cultural value of standing down between raids, not shared by the Waorani, led to changed reproductive success on the part of the most aggressive Yanomamö warriors. Among the Yanomamö, the most aggressive men had the most children, while among the Waorani, the least aggressive men had the most children. In other words, a difference in cultural values influenced the genetic make-up of subsequent generations of these groups. The second question asks how genetics and brain structure shape cultural phenomena. At a minimum, we might argue that forming cultures is something the human brain always seems to do. You will read about more specific examples of this type of process later in the textbook. In Chapter 12, we explore evidence suggesting that cultures with fewer individuals carrying the SS SERT genotype (also discussed in Chapter 3) are more likely to be individualistic than cultures where the SS genotype is more typical. Not only can our biology affect our behavior, but our behavior and cognitions can have significant effects on our biology as well. When Alia Crum and her colleagues made participants think they were drinking an indulgent milkshake instead of a sensible milkshake, their levels of the hormone ghrelin were more consistent with feelings of satisfaction. Advances in the methods we use to observe the structure and function of the nervous system have driven the history of biological psychology. The development of contemporary methods, such as the recording and imaging of brain activity, opened new areas of inquiry to biological psychologists. Before these methods were available, however, most of our knowledge of the nervous system resulted from clinical observations of injured or mentally ill individuals or from autopsy, the examination of bodies after death. When used with other contemporary methods, clinical observation and autopsy are quite accurate, but early thinkers lacking contemporary methods often struggled in their attempts to understand the physical basis of mind. They understood many things correctly while making some notable errors. Aristotle (384–322 BCE) mistakenly believed that the heart, not the brain, was the source of mental activity. An interesting historical mistake was phrenology. Toward the end of the 18th century, phrenologists proposed that the pattern of bumps on an individual’s skull correlated with that person’s personality traits and abilities (Simpson, 2005). The brain supposedly worked like a muscle, getting larger through use, which led frequently used areas of the brain to grow so much that the skull above these areas would bulge. Phrenologists read a person’s character by locating the bumps on a person’s head and identifying the personality traits below each bump according to a map. None of these ideas were close to being accurate. Although the phrenologists were wrong about the significance of bumps on the skull and the effects of activity on the structure of the brain, they did reach one correct conclusion: Their notion that some behavioral functions are localized to certain areas of the brain is one we share today. More modern perspectives of the nervous system emerged from the work of scientists such as Nobel Prize–winning anatomist Santiago Ramón y Cajal (1852–1934) and neurologist John Hughlings Jackson (1835–1911). Ramón y Cajal’s work helped us understand the microscopic level of the nervous system, while Jackson’s conclusions illuminate the relationships among the larger structures of the brain. Surprisingly, it took scientists a long time to accept the idea that the nervous system was made up of separate cells just like other tissues in the body. Even in the late 19th century, scientists such as Camillo Golgi still argued that the nervous system was a single continuous network. Ironically, Ramón y Cajal used a microscopic stain invented by Golgi to prove him wrong. Using Golgi’s stain, Ramón y Cajal demonstrated conclusively that the nervous system was made up of separate cells, an idea that became known as the Neuron Doctrine. Both men shared the 1906 Nobel Prize in Physiology or Medicine. Phrenologists believed that reading the bumps on a person’s head, using a bust like this as a reference, could tell them about a person’s character. Based on observations of his patients with seizure disorders, Jackson proposed that the nervous system is organized as a hierarchy, with progressively more complicated behaviors being managed by more recently evolved and complex structures (Jackson, 1884). We can see Jackson’s hierarchy at work when we observe people drinking alcohol. Alcohol specifically decreases the activity of parts of the brain involved with decision making. When a person has had too much to drink, the more complex social controls (e.g., knowing how close you should stand to a stranger) normally provided by higher level areas of the brain are diminished. Without the influence of these controls, people start doing things that they would not do while sober. This change in behavior reflects the now-unrestrained influence of the more primitive parts of the brain involved with behaviors such as aggression and sexuality. You might, for example, pick a fight with someone when you normally think fighting is wrong. The aggression and sexuality were there all along, but the activity of the higher levels of the nervous system usually restricted their expression to more appropriate circumstances (Siever, 2008). Over the last 100 years, our understanding of the correlations between brain and behavior leaped forward with continuing improvements in research methods, including many of those found in Table 4.1. In particular, methods that allow scientists to observe the activity of the living brain, including positron emission tomography (PET) and functional magnetic resonance imaging (fMRI), began to answer questions that were impossible to study previously. While these imaging technologies are still far away from mind reading, researchers have been able to use imaging data to identify the visual content of a participant’s dreams (Horikawa, Tamaki, Miyawaki, & Kamitani, 2013) and distinguish between images remembered from one’s own experience and similar but novel images (Rissman, Chow, Reggente, & Wagner, 2016). Research methodDescriptionWhat questions can we answer? HistologyMicroscopic examination of the nervous system. How does the structure of nervous system cells correlate with behavior? Skin conductance response (formerly galvanic skin response)Measurement of electricity passed between two surface electrodes placed on the skin of the hand or finger. What is a person’s state of arousal? Electroencephalogram (EEG)Measurement of the brain’s electrical activity using electrodes placed on the scalp. What is a person’s state of arousal? Event-related potential (ERP)Measurement formed by averaging EEG responses to a stimulus, such as a light or tone. Did the person perceive the stimulus? Single cell recordingMeasurement of a single neuron’s activity obtained through a surgically implanted electrode. What types of stimulation make this neuron respond? MagnetoencephalographyRecording of the tiny amounts of magnetic output of the brain. What parts of the brain react to this stimulus? Positron emission tomography (PET)Measurement that uses the accumulation of radioactively tagged glucose or oxygen to identify activity levels in parts of the brain. What parts of the brain are active during a particular task? Functional magnetic resonance imaging (fMRI)Identification of active parts of the brain using magnetism to track the flow of oxygen. What parts of the brain are active during a particular task? Electrical stimulationApplication of small amounts of electricity through a surgically implanted electrode. What behaviors occur if we stimulate this part of the brain? OptogeneticsGenetically inserted light-sensitive proteins allow cells in the brain to be turned on with light. Which types of cells are active during particular behaviors? Transcranial magnetic stimulationApplication of magnetic fields to the brain through an instrument held near the scalp. What behavioral changes occur when magnetism is applied to the brain? LesionsNaturally occurring or deliberate damage to the brain. What behavioral changes are correlated with brain damage? This functional magnetic resonance image (fMRI) was taken while one of your authors engaged in a finger-tap exercise, touching each digit of her right hand one by one with her thumb for 20 seconds followed by holding very still for 20 seconds. The red and yellow areas indicate parts of her brain that were selectively more active during the finger-tapping task than when she tried to stay very still. Moving into the 21st century, the ranks of neuroscientists continue to grow, from 500 members of the Society for Neuroscience in 1969 to more than 40,000 members today (Society for Neuroscience, 2016). Many departments of psychology have added phrases such as and neuroscience, and behavioral neuroscience, or and brain sciences to their names. In 1986, only seven universities in the United States offered undergraduate degrees specifically in neuroscience (Ramos et al., 2011). That number tripled by 1996 and more than tripled again by 2006. It is very likely that your college or university psychology department offers a number of courses in neuroscience, and you might have a separate major or minor as well. As more scientists are trained in the neurosciences, we can look forward to continued innovations in both technology and our knowledge about the biological basis of mind. Reductionism in science is defined as the explanation of complex things as sums of simpler things. Taking a rather extreme reductionist approach, science fiction often features scenes in which an android reminds a human that they’re not so different after all—the brain is just a computer made up of chemicals, nothing more, nothing less. In some ways, all modern science is reductionist. Scientists assume that whether you are studying particle physics or human behavior, a single set of fundamental laws explains much of what we observe. We do not need new sets of rules for the features of table salt (sodium chloride) in each context in which it appears. Regardless of whether the chemical is participating in neural signaling, flavoring our food, contributing to high blood pressure, or making us float more easily when we swim in the ocean, the fundamental principle is the same: Salt is salt. The scientific search for fundamental principles has been fruitful, but it does have limitations. Although we can learn a lot by breaking apart complex things to study simple things, we saw some risks to this approach in the debates between structuralists and Gestalt psychologists, described in Chapter 1. Fish swim in schools, geese fly in a V formation, ants and bees swarm, cattle form herds, and humans form societies. We could never understand these complex phenomena by studying the behavior of an individual member of the group. Nobel laureate physicist P. W. Anderson (1972, p. 393) reminded scientists that large collections of simple things do not always behave in the same way that simple things behave in isolation. He wrote that at each stage (of complexity), entirely new laws, concepts, and generalizations are necessary, requiring inspiration and creativity to just as great a degree as in the previous one. Psychology is not applied biology, nor is biology applied chemistry. This chapter on the biological foundations of behavior and mental processes relies extensively on reductionist thinking. As you work through the chapter, however, it is important to keep Anderson’s cautions in mind. Some aspects of behavior will continue to be governed by rules that explain the actions of simple things, while others will require the introduction of rules better suited to more complex combinations and interactions of simple things. Viewing a complex concept as a sum of its simpler parts is not always the best way to understand its full meaning. To examine the relationships between nervous system and behavior, we will first zoom out to examine the larger view of the structures making up the nervous system, and then we’ll zoom in later in the chapter to look at the microscopic world of the nerve cells, or neurons. Talking about the connections between structures of the nervous system and behavior requires a quick word of caution. As mentioned in Chapter 3, saying that we have a gene for a behavior is overly simplistic. Saying we have a center for a behavior in the brain is equally misleading. Although we can identify structures that participate in certain behaviors, the biology of mind involves intricate and overlapping patterns of activity involving networks made up of richly connected structures. The nervous system can be divided into two major components: the central and the peripheral nervous systems (PNS) (see Figure 4.2). The central nervous system (CNS)The central nervous system (CNS)The brain and spinal cord.            The central nervous system (CNS)        The brain and spinal cord.             consists of the brain and the spinal cordspinal cordA long cylinder of neural tissue extending from the medulla of the brain down to the middle of the back; part of the central nervous system            spinal cord        A long cylinder of neural tissue extending from the medulla of the brain down to the middle of the back; part of the central nervous system            , which extends from the brain down the back of the body. Although we often see the brain and the spinal cord referred to as separate structures, they form one continuous unit of tissue. You might think of the brain as having a tail known as the spinal cord. Nerves branch outward from the CNS to all areas of the body—the lungs, heart, and other organs; the eyes and ears; and the arms, legs, fingers, and toes. As soon as a nerve branches outward from the CNS, it is considered part of the peripheral nervous system (PNS)peripheral nervous system (PNS)The nerves exiting the central nervous system that carry sensory and motor information to and from the rest of the body            peripheral nervous system (PNS)        The nerves exiting the central nervous system that carry sensory and motor information to and from the rest of the body            . Another way to know you have left the CNS for the PNS is to look for the protection of bone. Nerves of the CNS are encased in bone, but those of the PNS are not. The nervous system has two major divisions: the CNS, containing the brain and the spinal cord, and the PNS, containing all nerves that exit the brain and the spinal cord. The CNS is protected by bone, but the PNS is not. The appearance of the human brain is not particularly impressive. It is covered in wrinkles, measures about 5-1/2 inches (140 millimeters) wide, 6-1/2 inches (167 millimeters) long, and 3-2/3 inches (93 millimeters) high, and weighs about 3 pounds (1.5 kilograms). The brain contains about 86 billion nerve cells, which make trillions of connections. The spinal cord contains about 1 billion nerve cells, reaches a length of 18 inches (45 millimeters) in men and 17 inches (43 millimeters) in women, and weighs about 1.2 ounces (35 grams). Its diameter ranges from about 0.4 inches (1 centimeter) to 0.6 inches (1.5 centimeters). The spinal cord is shorter than your spine. Your bony spinal column continues to grow between birth and adulthood, but the spinal cord itself does not. The brain and the spinal cord are among the best protected parts of your body, which is not surprising given their importance for your survival. Surrounding the brain and the spinal cord are the heavy bones of the skull and spinal vertebrae. Under these bones, membranes known as meninges provide further protection. Infections of these membranes result in potentially life-threatening cases of meningitis, for which you were possibly vaccinated before beginning your college studies. Meningitis is the inflammation of the membranes (meninges) covering the brain and the spinal cord. This condition can result from infection with bacteria, viruses, or fungi, with the bacterial infections being the most dangerous. Fortunately, most cases of bacterial meningitis can be prevented by vaccination. This image shows the distortion of the membranes caused by infection. The brain and the spinal cord are further protected by a clear, plasmalike fluid known as cerebrospinal fluid (CSF). CSF seeps out of the lining of hollow spaces in the brain known as the ventricles (see Figure 4.3). Near the base of the skull, openings enable CSF to flow from the ventricles into a space within the meninges, allowing the fluid to flow around the outer surfaces of the brain and spinal cord. CSF is constantly produced, so blockages in its circulation cause the fluid to build up. The result is hydrocephalus, which means water on the brain. The ventricles of the brain are filled with circulating cerebrospinal fluid (CSF), which floats and cushions the brain. The cushioning provided by the CSF limits the damage produced by a blow to the head. As a result, single minor concussions are unlikely to produce long-term problems, but medical experts are becoming increasingly concerned about the effects of multiple concussions (Mannix, Meehan III, & Pascual-Leone, 2016). The CSF also floats the brain within the skull, preventing false signals that might result from the weight of some neurons pressing down on others. To diagnose some medical conditions, it is helpful to obtain a sample of CSF. This is done through a spinal tap, in which a physician removes some of the CSF circulating through the meninges surrounding the spinal cord. French neurologists were shocked when they saw brain images taken of a middle-age man who sought treatment for some minor weakness in one leg. His lateral ventricles (LV) take up most of the volume in his skull, and the brain appears as a thin layer on the outside. The patient reported that he had been treated for hydrocephalus from infancy until the age of 14. The patient was a married father of two who worked as a civil servant with a low but still normal IQ of 75. The spinal cord extends from the lowest part of the brain down into the middle of your back (see Figure 4.4). If you feel the back of your skull where it curves to meet your spine, your fingers will be just below the junction of the spinal cord and the lowest structure of the brain. Although the spinal cord is only 2% of the weight of the CNS, its functions are vital, as evidenced by the challenges faced by people with spinal damage. The spinal cord serves as a major conduit for information flowing to and from the brain along large bundles of nerve fibers, carrying sensory information from the body and delivering commands to muscles. A total of 31 pairs of spinal nerves exit the spinal cord between segments of the bony vertebrae in your spine to serve the body. Thirty-one pairs of spinal nerves exit between the bones of the vertebrae to bring sensory information back to the CNS and carry motor commands to muscles. Many important reflexes are initiated by the spinal cord without any assistance from the brain. One type of spinal reflex makes you pull your body away from a source of pain. It doesn’t take long for your hand to fly up when you’ve touched something hot on the stove. When tapping your knee with a hammer during a routine physical, your doctor is checking another type of spinal reflex, the knee-jerk reflex (see Figure 4.5). This reflex is interesting to your doctor because certain medical conditions, such as diabetes, affect the strength of the reflex. Still other spinal reflexes help us stand and walk. When your physician taps on your knee, your thigh muscle stretches. Information about the stretch is carried to the spinal cord by a sensory nerve. The spinal cord sends a command to the muscle to contract to counteract the stretch, and your foot kicks out. The spinal cord manages this reflex alone. No higher level of processing in the nervous system is necessary for this reflex to occur. Spinal reflexes give us an opportunity to look at the functions of three types of nerve cells, or neurons. Sensory neurons carry information from the external environment or from the body back to the CNS. In the knee-jerk reflex, sensory neurons tell the spinal cord that a muscle has been stretched by the tap of the hammer. Motor neurons carry commands from the CNS back to the muscles and glands of the body. In response to information about the stretched muscle, the spinal cord sends a message through motor neurons back to your leg, telling the muscle to contract to counteract the stretch. You know what happens next—your foot kicks as the muscles contract. Neurons that have neither sensory nor motor functions are called interneurons. Inter in this case means between, because many interneurons form bridges between sensory and motor neurons. The knee-jerk reflex forms a simple arc between a sensory neuron and a motor neuron and does not require interneurons. However, interneurons play important roles in other reflexes and throughout the nervous system. Moving up from the spinal cord brings us to the brainstem. Early in prenatal development, the emerging brain forms three bulges. The most forward of these bulges develops into the two large cerebral hemispheres, which we discuss in a later section. The remaining two bulges form the brainstembrainstemThe part of the brain containing the midbrain, pons, and medulla.            brainstem        The part of the brain containing the midbrain, pons, and medulla.            . If you examine Figure 4.6, you can see that the brainstem looks like the stem of a flower, supporting the larger blossom of the cerebral hemispheres. Directly branching from the brainstem are the cranial nerves, which perform the same functions for the head and neck areas that the spinal nerves manage for the remainder of the body. We will discuss the cranial nerves in more depth in a later section on the PNS. The brainstem contains structures responsible for reflexive behaviors, heart rate, breathing, arousal, sleep, preliminary sensory analysis, balance, and movement. The spinal cord merges with our first brainstem structure, the medullamedullaThe brainstem structure that lies just above the spinal cord.            medulla        The brainstem structure that lies just above the spinal cord.            . Like the spinal cord, the medulla contains large bundles of nerve fibers traveling to and from higher levels of the brain. The medulla manages many functions essential to life, such as heart rate, breathing, and blood pressure. Just above the medulla is the ponsponsA part of the brainstem located between the medulla and the midbrain.            pons        A part of the brainstem located between the medulla and the midbrain.            , which contains structures involved with the management of sleep, arousal, and facial expressions. Pons means bridge in Latin. The pons not only serves as a bridge between the higher and lower portions of the brain, but it also connects the cerebellumcerebellumA structure attached to the brainstem that participates in skilled movement and, in humans, complex cognitive processing            cerebellum        A structure attached to the brainstem that participates in skilled movement and, in humans, complex cognitive processing             to the rest of the brain. Essential for maintaining balance and motor coordination, the cerebellum is one of the first structures in the brain to be affected by alcohol. As a result, alcohol consumption impairs balance (walking a straight line) and motor coordination (touching your finger to the tip of your nose with your eyes closed). Most sobriety tests are the same tests a neurologist would use to assess the function of the cerebellum. Most sobriety tests assess the function of the cerebellum, which helps us maintain balance and muscle coordination. Opiate painkillers such as morphine and OxyContin produce some of their analgesic effects by interacting with opioid receptors in the periaqueductal gray. Surprisingly, the cerebellum contains more nerve cells than the rest of the brain combined. Not only does the cerebellum contain huge numbers of neurons, but it is also richly connected with the rest of the CNS. Because of the cerebellum’s position on the brainstem, which is relatively ancient in terms of evolution compared to the cerebral hemispheres, neuroscientists initially underestimated its importance to human behavior. They believed that the cerebellum’s activities were restricted to managing the timing and strength of movements. While we still do not know exactly what the cerebellum does, today’s neuroscientists believe that it has a broader role in making mental and motor skills more automatic. Damage to the human cerebellum produces subtle deficits in language, cognition, and perception. In autism spectrum disorder, a condition that affects language, sensory, and social behaviors, abnormalities in the cerebellum are common (Courchesne, 1997;  Fatemi et al., 2012). The midbrainmidbrainThe part of the brainstem that lies between the pons and the cerebral hemispheres            midbrain        The part of the brainstem that lies between the pons and the cerebral hemispheres             sits above the pons and contains a number of structures involved in sensory reflexes, movement, and pain. For example, the periaqueductal gray of the midbrain plays an important role in the body’s management of pain because it contains receptors for endorphins, our natural opioids, discussed later in this chapter and in Chapter 6. When endorphins are present in the periaqueductal gray, they reduce the perception of pain by decreasing the strength of pain messages traveling to higher levels of the brain. Nearby are other cell clusters that serve as the major sources of two important chemical messengers in the brain, serotonin and norepinephrine. These structures participate in states of arousal, mood, appetite, and aggression. Running the length of the brainstem’s core from the upper medulla into the midbrain is the reticular formationreticular formationA collection of structures located along the midline of the brainstem that participate in mood, arousal, and sleep.            reticular formation        A collection of structures located along the midline of the brainstem that participate in mood, arousal, and sleep.            , which participates in the management of levels of arousal, discussed further in Chapter 6. The cells in the reticular formation have two settings—fast and slow. When the cells are firing quickly, we usually show other signs of being awake. When the cells are firing slowly (or are damaged due to a stroke or other injury), an individual will enter either deep sleep or unconsciousness. Embedded within the vast tracts of nerve fibers or white matter that make up the bulk of the cerebral hemispheres are a number of subcortical structures that participate in self-awareness, learning, emotion, movement, communication, the inhibition of impulses, and the regulation of body states. We call them subcortical because they lie sub, which means below, the cerebral cortex, which comprises the wrinkled outermost covering of the cerebral hemispheres. Early anatomists collected some of these subcortical structures into a limbic system (limbic means border, and these structures form a gentle curve below the cerebral cortex), but this term is losing popularity with contemporary anatomists (Rolls, 2015). You might also have heard the limbic system called your emotional brain. As you will see in the next sections, some of these structures do participate in our emotional life, but they perform many other functions as well. We usually discuss subcortical structures in the singular, as in thalamus or hippocampus, but they actually are paired sets of structures, one on either side of the brain. Almost at the center of the brain lies the thalamusthalamusA subcortical structure involved with the processing of sensory information, states of arousal, and learning and memory            thalamus        A subcortical structure involved with the processing of sensory information, states of arousal, and learning and memory            . The thalamus is often called the gateway to the cortex, because input from most of our sensory systems (vision, hearing, touch, and taste) travels first to the thalamus, which then forwards the information to the cerebral cortex. The cortex, in turn, forms large numbers of connections with the thalamus. In addition to its role in sensation, the thalamus is involved with memory and states of consciousness. Lesions in the thalamus are associated with profound memory loss (Cipolotti et al., 2008). As you will learn in Chapter 6, during our deepest stages of sleep, the thalamus coordinates the activity of cortical neurons, tuning out the outside world, making it difficult to awaken. Disturbances in the circuits linking the thalamus and the cortex accompany some seizures. The basal gangliabasal gangliaA collection of subcortical structures that participate in the control of movement            basal ganglia        A collection of subcortical structures that participate in the control of movement             consist of several large structures involved with voluntary movement that curve around to cup the thalamus (see Figure 4.6 and Figure 4.7). The basal ganglia form complex circuits with motor structures located in the brainstem, the thalamus, and the cerebral cortex. Near the center of the brain, the thalamus receives input from most of our sensory systems and relays the information to the cerebral cortex. Curving around the thalamus are the basal ganglia, which form an important part of our voluntary movement systems. Degeneration of the basal ganglia occurs in Parkinson’s disease, a condition that makes the initiation of voluntary movement extremely difficult. The basal ganglia also contribute to several psychological disorders described in Chapter 14, including obsessive-compulsive disorder (OCD) and attention deficit hyperactivity disorder (ADHD). These disorders are characterized by inadequate control of voluntary movement. In the case of OCD, patients may endlessly repeat a behavior, such as hand washing, while in ADHD, voluntary movements can be unusually frequent, rapid, and impulsive. The hypothalamushypothalamus4F A subcortical structure that participates in the regulation of thirst, temperature, hunger, sexual behavior, and aggression.            hypothalamus        4F A subcortical structure that participates in the regulation of thirst, temperature, hunger, sexual behavior, and aggression.             is involved with motivation and homeostasis (see Chapter 7), or the regulation of body functions such as temperature, thirst, hunger, biological rhythms, and sexual activities (see Figure 4.8). The hypothalamus is often described as contributing to the 4F behaviors: feeding, fleeing, fighting, and, well, sex (fornication). The hypothalamus carries out its motivational and homeostatic tasks by directing the autonomic nervous system and the endocrine system and its hormones, which we discuss in detail later in this chapter. Subcortical structures located under the cerebral cortex participate in attention, decision making, learning, memory, and emotion. The hippocampushippocampusA subcortical structure that participates in memory.            hippocampus        A subcortical structure that participates in memory.            , named for its shape after the Greek word for seahorse, is essential to the formation of long-term memories, which we discuss in more detail in Chapter 9. Memories are not stored permanently in the hippocampus, but it is likely that the hippocampus is involved in the storage and retrieval of memories located elsewhere in the brain. Damage to the hippocampus results in profound impairments in the ability to form new memories, but intelligence, personality, and most memories of events that occurred before hippocampal damage remain intact. The cingulate cortexcingulate cortexA subcortical structure above the corpus callosum. Its anterior (forward) segment participates in decision making and emotion, and its posterior (rear) segment participates in memory and visual processing.            cingulate cortex        A subcortical structure above the corpus callosum. Its anterior (forward) segment participates in decision making and emotion, and its posterior (rear) segment participates in memory and visual processing.             forms a fold of tissue on the inner surface of each cerebral hemisphere. The forward two thirds of this structure, known as the anterior cingulate cortex (ACC), participate, along with the hypothalamus, in the control of the autonomic nervous system, which we discuss later in this chapter. The ACC also plays significant roles in decision making, emotion, anticipation of reward, and empathy. The rear third, or posterior cingulate cortex (PCC), participates in memory and visual processing. The amygdalaamygdalaA subcortical structure located in the temporal lobe believed to participate in emotional processing.            amygdala        A subcortical structure located in the temporal lobe believed to participate in emotional processing.             gets its name from the Greek word for almond because of its shape. One amygdala is deeply embedded in the temporal lobe, the wing of cortex that curves around the side of the brain, in each hemisphere. The amygdala receives sensory information (vision, hearing, and smell) and produces emotional and motivational output that is sent to the cerebral cortex. Although the amygdala responds to both positive and negative stimuli, it is best known for its role in identifying, remembering, and responding to fear and aggression. Research studies have found that the amygdala becomes more active when people are looking at pictures of fearful facial expressions. The more intense the expression of fear, the more activation is observed in the amygdala (Vuilleumier, Armony, Driver, & Dolan, 2001). Monkeys with damaged amygdalae approached unfamiliar monkeys boldly and fearlessly, which is uncharacteristic of these animals (Emery et al., 2001). They also failed to show their species’ typical fear of rubber snakes and unfamiliar humans (Mason, Capitanio, Machado, Mendoza, & Amaral, 2006). This image is taken from a man who is blind because of damage caused by a stroke to his visual areas, indicated by the red and green arrows. The orange area indicates activation of his amygdala when he is shown a photo of an angry face. This man couldn’t tell you whether he is looking at a tree, a building, or a face, let alone a happy or angry one, but his amygdala knows and reacts appropriately. Several behavioral deficits have been identified in a patient, known as Patient S. M., who experienced a rare medical condition that damaged her amygdalae in both hemispheres (Adolphs, Tranel, & Damasio, 1998) (see Figure 4.9). Although the patient can identify facial expressions of happiness, sadness, and disgust in photographs, she has a specific difficulty identifying expressions of fear. When researchers exposed Patient S. M. to snakes, spiders, and scary movies, she showed no signs of fear (Feinstein, Adolphs, Damasio, & Tranel, 2011). Although Patient S. M. shows no signs of antisocial behavior, other research indicates that people who harm others without feeling guilt are also impaired in their abilities to perceive fear in facial expressions or voices (Blair et al., 2002;  Marsh & Blair, 2008). The condition affecting Patient S. M. does not usually begin until after the age of 10, so having functional amygdalae in childhood might have helped Patient S. M. learn to act in prosocial ways. In addition to her difficulties in responding to fear stimuli, Patient S. M. shows other behavioral deficits related to her damaged amygdalae. Personal space, although variable across cultures, is consistent within cultures. Healthy control participants in the United States stand about 0.64 meter (about 2 feet) or arm’s length away from people they don’t know. In contrast, Patient S. M. prefers to stand nearly twice as close to others, or 0.34 meter (about 1 foot). Standing closer to strangers than is normal for your culture is likely to send inappropriate social messages of either threat or intimacy. The nucleus accumbensnucleus accumbensA subcortical structure that participates in reward and addiction.            nucleus accumbens        A subcortical structure that participates in reward and addiction.             is an important part of the brain’s reward and pleasure circuitry. Whether you are eating, having sex, using addictive drugs, gambling, or simply enjoying a beautiful sunset, this circuit comes into play (Comings & Blum, 2000). The circuit originates in cell bodies located in the midbrain that form connections with many subcortical structures and with the nucleus accumbens in particular. The activity of the nucleus accumbens is related to a person’s sense of social inclusion. When people who have strong connections to friends and family view a happy social scene, their nucleus accumbens becomes active. In contrast, when people with weaker social connections view the same happy scenes, their nucleus accumbens shows less activity than those of the socially connected people (Cacioppo, Norris, Decety, Monteleone, & Nusbaum, 2009). Above the brainstem, we find the two large cerebral hemispheres, which are connected by a large bundle of nerve fibers known as the corpus callosumcorpus callosumA wide band of nerve fibers connecting the right and left cerebral hemispheres.            corpus callosum        A wide band of nerve fibers connecting the right and left cerebral hemispheres.            . The thin layer of cells covering the outer surface of the cerebral hemispheres is the cerebral cortexcerebral cortexThe thin layer of neurons covering the outer surface of the cerebral hemispheres.            cerebral cortex        The thin layer of neurons covering the outer surface of the cerebral hemispheres.             (see Figure 4.10). The cortex, which means bark in Latin, covers the cerebral hemispheres like the bark of a tree. Most of the remaining bulk of the hemispheres is made up of white matter, or nerve fiber pathways, that connects the cortex with other parts of the nervous system. The average 20-year-old human brain has around 100,000 miles (162,500 kilometers) of white matter (Marner, Nyengaard, Tang, & Pakkenberg, 2003). The subcortical structures discussed earlier are distributed within this white matter. The cerebral cortex (cortex means bark) is a thin layer of cells on the outer surface of the brain. The closeup shows different views of the cortex, including the distribution of complete single cells, cell bodies, and myelin, the insulating material that covers most nerve fibers. If stretched out flat, the human cerebral cortex would cover an area of about 2.5 square feet (about 0.23 square meters). To fit within the confines of the skull, the cortex is convoluted or wrinkled. The degree of cortical convolution positively correlates with the general intellectual capacities of a species. For instance, human brains are more convoluted than sheep brains, which in turn are more convoluted than rat brains (see Figure 4.11). As species’ behavior becomes more complex, we see a corresponding increase in the degree of convolution (wrinkling) of the cerebral cortex. This wrinkling of the brain permits more brain tissue to fit within the skull. As a result, cortical size has increased more quickly over the course of evolution than skull size—an important adaptation given that large skulls are difficult to get through the birth canal. Each hemisphere of the cerebral cortex may be divided into four lobes, named after the bones of the skull that cover them (see Figure 4.12). Toward the front of the brain, we find the frontal lobefrontal lobeThe most forward of the four lobes of the cerebral cortex; location of the primary motor cortex and areas responsible for some of the most complex cognitive processes.            frontal lobe        The most forward of the four lobes of the cerebral cortex; location of the primary motor cortex and areas responsible for some of the most complex cognitive processes.            , and directly behind the frontal lobe lies the parietal lobeparietal lobeThe lobe of the cerebral cortex that lies at the top of the head between the frontal and the occipital lobes; location of the primary somatosensory cortex.            parietal lobe        The lobe of the cerebral cortex that lies at the top of the head between the frontal and the occipital lobes; location of the primary somatosensory cortex.            . At the back of the brain is the occipital lobeoccipital lobeThe lobe of the cerebral cortex located at the back of the brain; location of the primary visual cortex.            occipital lobe        The lobe of the cerebral cortex located at the back of the brain; location of the primary visual cortex.            . Curving around the side of each hemisphere, we find the temporal lobetemporal lobeThe lobe of the cerebral cortex that curves around the side of each hemisphere; location of the primary auditory cortex.            temporal lobe        The lobe of the cerebral cortex that curves around the side of each hemisphere; location of the primary auditory cortex.            . Because we have two hemispheres, it follows that we have pairs of each type of lobe, usually denoted right or left (e.g., right frontal lobe and left frontal lobe). The cerebral cortex is traditionally divided into four lobes: frontal, parietal, occipital, and temporal. Diffusion tensor imaging highlights the rich arrays formed by the white matter in the brain. As we mentioned earlier in this chapter, the phrenologists were wrong in assuming that behavioral characteristics were reflected in bumps of the skull, but they were correct in suggesting that some functions were localized in particular parts of the brain. The functions performed by different areas of the cerebral cortex within the lobes fall into three categories: sensory, motor, and association. The sensory cortex processes incoming information from the sensory systems, such as vision or hearing, which we describe in Chapter 5. The primary visual cortex is in the occipital lobe, and the primary auditory cortex is in the temporal lobe. The primary somatosensory cortex (soma refers to body) is in the parietal lobe and processes information about touch, pain, body position, and skin temperature. The primary motor cortex is in the rearmost portion of the frontal lobe and provides the highest level of voluntary control over movement. Areas of the cortex that do not have specific sensory or motor functions are known as association cortex. Association means connection in this case, and association cortex helps us form bridges between sensation and action, language, and abstract thought. Association areas are distributed throughout the cortex. In addition to being the home of the primary motor cortex, the frontal lobe has a number of important, sophisticated cognitive functions. Adjacent to the primary motor cortex is Broca’s area, named after Paul Broca, who helped identify its functions in the 1860s. Broca’s area participates in the production of speech. Consequently, damage to Broca’s area caused by a stroke or a tumor produces considerable difficulty in speaking, although comprehension of speech remains good. The most forward portion of each frontal lobe, known as the prefrontal cortexprefrontal cortexThe most forward part of the frontal lobe of the cerebral cortex.            prefrontal cortex        The most forward part of the frontal lobe of the cerebral cortex.             (pre means before), is involved with the planning of behavior, attention, and judgment. Abnormalities of frontal lobe activity may account for characteristics of some psychological disorders, including schizophrenia and ADHD, which we will discuss in detail in Chapter 14. The role of the frontal lobes in the planning of behavior is illustrated by a bizarre condition known as alien hand syndrome, which occurs when connections between the prefrontal cortex and the lower areas of the brain involved in movement are damaged (Kikkert, Ribbers, & Koudstaal, 2006). This condition has no effect on sensory feedback from the limb, such as touch and position. However, patients with this condition do not seem to have control over their affected limbs and often remain unaware of the limbs’ activities until they are pointed out by another person. For instance, a hand might undo a button or remove clothing without the patient’s awareness of the activity. Patients do not recognize the rogue limb as their own and may use the other hand to wrestle with it in an attempt to control it forcibly or punish it for its activities. The importance of the frontal lobes is also illustrated by the results of a terrible accident that befell a young railroad worker named Phineas Gage in 1848. While Gage was preparing to blast through some granite, a freak accident sent an iron tamping rod through his head before it landed about 30 feet (about 9 meters) away. The rod entered his head under his left cheekbone, passed behind his left eye, and exited out the middle of his forehead. Remarkably, Gage survived, but he was not the same person as before his accident. Although outwardly normal in his intelligence, speech, and movement, Gage became prone to angry outbursts and unreliability, which made it difficult for him to find and keep employment. As his doctor noted, His contractors, who regarded him as the most efficient and capable foreman in their employ previous to his injury, considered the change in his mind so marked that they could not give him his place again (Fleischman, 2002, p. 20). Early scientists discovered that nerves carry only one type of information. When you take a blow to the back of the head, where your primary visual cortex is found, your occipital lobe does not know how to say, ouch. Instead, it responds to the blow as if you saw a flash of light (not necessarily the tweeting birds or stars indicated in cartoons). Gage suffered his accident when modern imaging technologies were nowhere to be seen on the scientific horizon. Today, we know more about why Gage behaved the way he did because of his accident. Scientists initially recreated the pathway that Gage’s tamping rod must have traveled (Damasio, Grabowski, Frank, Galaburda, & Damasio, 1994). More recent analyses show that the rod did more damage to the fiber pathways in Gage’s brain than to the cells of his cerebral cortex (Van Horn et al., 2012). The rod disrupted 11% of the connections in Gage’s brain, compared to 4% of the cells in his cortex. The rod missed areas involved in speech, voluntary movement, and the senses of touch, body position, and pain. Thus, Gage had no difficulty speaking or performing complex movements, such as driving a stagecoach, following his accident. Modern patients with damage in this area exhibit many of the same social deficits that Gage demonstrated (Damasio & Anderson, 1993;  Damasio, Tranel, & Damasio, 1991;  Eslinger & Damasio, 1985). They are impulsive, emotionally unstable, unpredictable, and unable to make reasonable decisions. Phineas Gage suffered a terrible injury to his frontal lobes. Modern imaging techniques allowed scientists to recreate the pathway of Gage’s tamping iron through his brain. Remarkably, Gage survived, although his friends described him as a changed man. Knowing the outcome of the Gage case, you might be astonished to learn that physicians in the 1940s and 1950s deliberately damaged the frontal lobes of nearly 50,000 American patients in a procedure known as a frontal lobotomy. The intent of the procedure was to reduce fear and anxiety in patients with serious psychological disorders, which it accomplished in many cases but at great expense to the patient. As you might suspect from reading about Gage, many patients who underwent the procedure were unable to work or live normal lives because of their impulsive, antisocial behavior. The orbitofrontal cortexorbitofrontal cortexA part of the prefrontal cortex located right behind the eyes that participates in impulse control.            orbitofrontal cortex        A part of the prefrontal cortex located right behind the eyes that participates in impulse control.            , a part of the prefrontal cortex located just behind the bony orbits protecting the eyes, plays an important role in our emotional lives (see Figure 4.13). People with damage to the orbitofrontal cortex demonstrate dramatic deficits in their social behavior and experience of emotion, despite retaining their intelligence, language skills, and abilities to pay attention, learn, and remember (Bechara, Damasio, & Damasio, 2000;  Damasio, 1994;  Damasio & Anderson, 1993). People with damage to their orbitofrontal cortex have difficulty controlling impulses and anticipating the negative outcomes of poor decisions. This part of the brain is one of the last areas to mature. Patient E. V. R. experienced orbitofrontal damage during surgery for a tumor (Eslinger & Damasio, 1985). Before his surgery, E. V. R. was considered a role model and a respected member of his community. Following his surgery, E. V. R. lost his job, went bankrupt, and divorced his wife to marry a prostitute, whom he divorced 2 years later. Although he had no difficulties talking about moral dilemmas, he experienced enormous problems when trying to make everyday decisions, such as buying toothpaste or choosing a restaurant. Researchers have made other connections between abnormalities in the orbitofrontal cortex and antisocial behavior. In a sample of 21 individuals diagnosed with antisocial personality disorder, a condition characterized by disregard for others that we discuss in Chapter 14, the volume of the prefrontal cortex, which includes the orbitofrontal cortex, was about 11% less than in control participants who did not have the condition (Raine, Lencz, Bihrle, LaCasse, & Colletti, 2000). Individuals with antisocial personality disorder or orbitofrontal damage not only fail to anticipate the emotional consequences of situations but also are unable to delay gratification. They typically choose immediate rewards over long-term benefits, such as stealing something now despite knowing the long-term benefits of staying out of jail. The occipital lobe, located at the back of the brain, is home to the primary visual cortex. The primary visual cortex begins the process of interpreting input from the eyes by responding to basic information about an image, such as its borders, shading, color, and movement. This amount of processing by itself does not allow you to read this page or recognize your professor in the library. Two important pathways link the occipital lobe with the rest of the brain. A pathway connecting the occipital lobe with the temporal lobe allows you to recognize objects you see. A second pathway connects the occipital lobe with the parietal lobe and allows you to process the movement of objects. We discuss these processes further in the next sections. The temporal lobe has several areas that are specialized for particular functions. The temporal lobe is home to the primary auditory cortex, which allows us to process incoming sounds. As mentioned earlier, the temporal lobe processes some higher visual system tasks, including the recognition of objects and the faces of familiar people. Patients with damage to the temporal lobe are often unable to recognize their loved ones by sight. They must wait until the person speaks. We discuss this processing of vision and hearing by the temporal lobe in more detail in Chapter 5. We saw earlier how damage to Broca’s area in the frontal lobe produced difficulty in speaking. Damage to another language area located in the temporal lobe, Wernicke’s area, produces different results (see Figure 4.12). As we discuss in Chapter 10, patients with damage to their Wernicke’s area speak fluently but make no sense. They cannot comprehend speech, but they seem blissfully unaware of their deficits. The parietal lobe is home to the primary somatosensory cortex, which helps us localize touch, pain, skin temperature, and body position. Damage to the parietal lobe can produce the odd symptoms of neglect syndrome. Patients with this condition have difficulty perceiving part of their body or part of the visual field (see Figure 4.14). Patients with certain types of brain damage in the parietal lobe experience hemispatial neglect, or the inability to pay attention to stimuli located in space on the opposite side relative to their damage. They seem unaware that there is anything wrong with their perceptions. The patient whose drawings are featured in this figure experienced damage to the right hemisphere, resulting in neglect for anything to the left in space. Neglect does not affect vision alone but can also affect the sense of body location. One patient was unable to recognize his own leg and suspected the hospital staff of putting a cadaver leg into his bed. The parietal lobe processes input about taste and, like the temporal lobe, it engages in some complex processing of vision. Whereas the temporal lobe participates in visual recognition, the parietal lobe tells us how quickly something is moving toward us. This can be an essential bit of information when deciding whether it is safe to make a left turn in front of oncoming traffic. We discuss these functions of the parietal lobe in depth in Chapter 5. In the early 1990s, Giacomo Rizzolatti and a team of Italian scientists were busy studying the brain correlates of movement when they noticed something odd (Di Pellegrino, Fadiga, Fogassi, Gallese, & Rizzolatti, 1992). They had observed that certain neurons in a part of a monkey’s brain became especially active when the monkey performed certain actions, like reaching for a piece of banana or a peanut. When an experimenter picked up a piece of food to place it within the monkey’s reach, some of the same neurons began to fire. Suspecting something important was behind these observations, the researchers began to study these mirror neurons more carefully. The scientists believed that mirror neurons provided a mechanism for understanding the actions of others (Caggiano et al., 2011). Psychology, and biological psychology in particular, have a great deal to say to the criminal justice system regarding a person’s capacity for responsible behavior. In 1843, Daniel M’Naghten planned to assassinate British prime minister Sir Robert Peel but shot his secretary, Edward Drummond, instead. M’Naghten was acquitted by reason of insanity and spent the remainder of his life in hospitals for the insane rather than prison. M’Naghten’s legacy in English and U. S. law is the insanity defense, which means that a mental condition that prevents understanding of the act and/or the wrongfulness of the act removes responsibility for the act (Mobbs, Lau, Jones, & Frith, 2007). The U. S. criminal justice system continues to struggle with issues of maturity, mental illness, and responsibility for one’s actions. In 2005, 13-year-old Roberto Holguin murdered 87-year-old Army veteran Gerald O’Malley by hitting him in the head with a skateboard. In 2014, while still incarcerated, Holguin sent California Governor Jerry Brown an email that said, I will shoot you with a real gun. Brown subsequently overturned a parole board decision for early release for Holguin, but by law, Holguin could not be held longer than his 25th birthday in 2017. These issues came close to the home of one of the authors. In 2005, a local 13-year-old boy used a skateboard to bludgeon 87-year-old Gerald O’Malley to death. The boy told police, I knew it was wrong; I just did it. Joseph Wu, director of the Brain Imaging Center at the University of California, Irvine, told the court that the young murderer had abnormally reduced activity in parts of his brain that govern a person’s judgment (Sneed, 2006, p. B1). Does the insanity defense apply when there is evidence of abnormalities in parts of the brain that govern a person’s judgment? Further understanding of the brain’s structure and function in both law-abiding people and criminals will contribute to the legal system’s ability to handle such cases fairly. If you’re curious about the young murderer’s outcome, the court rejected suggestions that he needed treatment instead of incarceration. He was sentenced to a state juvenile corrections facility, from which he was automatically freed at age 25 (Fountain, 2016). Athletic success often depends upon the athlete’s ability to predict an opponent’s intentions quickly enough to react appropriately. In tennis, the reading of an opponent’s serve is especially important, as the time required to plan and implement a response is longer than the flight time of the ball (Cacioppo et al., 2014). The Question: How do expert tennis players predict their opponents’ serve quickly enough to respond well? Twenty-five male tennis players were observed using fMRI as they viewed films of players serving the ball in one of two conditions: the player serving the ball knew where he wanted to place the serve (initially intended) orthe player was told by an experimenter where he should place the serve after he had already tossed the ball (noninitially intended). The players viewing the films were asked to indicate where they thought the ball would go. Because fMRI is a safe procedure, this study raises few ethical concerns. Nonetheless, the participants must freely volunteer and provide informed consent. Their confidentiality should be strictly protected. The more expert the tennis player being scanned, the more successful he was at predicting where the serve would land, but only when the server in the film knew where the ball was supposed to go before he tossed the ball in the air. When the server didn’t know until the last minute where the ball was supposed to go, all viewers performed no better than chance. Accurate predictions of the location of the serve were correlated with activation of mirror neuron systems and subcortical structures involved with motor memory. The scientists demonstrated that reading the intentions of others is a remarkably complex process involving the mirror neuron system. This system is more likely to be activated when we observe a person whose actions are relevant to our goals, such as our opponent in a tennis match, than if a person’s actions are irrelevant, such as an usher in the grandstand. Correctly reading your opponent’s intentions, but not the usher’s, is critical to your success. Do human beings, as well as monkeys, possess mirror neurons? Research on humans typically involves brain imaging, such as functional magnetic resonance imaging (fMRI), so we see the activity of larger areas of the brain (mirror systems) rather than single neurons. It does appear, though, that human beings possess mirror systems that help us understand not just the actions and emotions of others, but their intentions as well (Iacoboni et al., 2005). Initially, researchers looked for mirror systems in the vicinity of the motor cortex in the frontal lobe, but other areas of the brain seem to have similar mirror capacities, allowing us to relate to feelings of disgust and of being touched expressed by others (Keysers et al., 2004;  Wicker et al., 2003). A special type of localization of function in the cerebral cortex is known as lateralization, or the localization of a function in either the right or the left cerebral hemisphere. A basic type of lateralization occurs in the somatosensory and voluntary motor systems in the brain. Movement and sensation on the right side of the body are processed by the left hemisphere, and movement and sensation on the left side of the body by the right hemisphere. If you observe a person who is paralyzed on the right side of the body, you can be fairly certain that this paralysis is a result of motor cortex damage in the left hemisphere. In a similar manner, the visual cortex of the left hemisphere processes all data from the right half of the visual field, while the right hemisphere processes all data from the left half of the visual field. In other words, when you look straight ahead, holding your eyes and head still, everything to the right or left of center is processed by the opposite hemisphere. Much of our knowledge of lateralization of the human brain resulted from the careful analysis of a surgical procedure known as a split-brain operation (see Figure 4.15). To treat rare cases of life-threatening seizures, surgeons in the 1960s cut the patients’ corpus callosum and other pathways connecting the right and left cerebral hemispheres (Bogen, Schultz, & Vogel, 1988). The procedure not only succeeded in reducing or eliminating seizures, but it also produced no changes in personality, intelligence, or speech. Only when patients were studied in the laboratory were the consequences of their surgery evident. A typical experiment demonstrating the differences in processing by the right and left hemispheres (Gazzaniga, 1967) is illustrated in Figure 4.16. To save patients from life-threatening seizures, physicians cut the corpus callosum, a large band of nerve fibers connecting the right and left hemispheres. If participants fixate on a dot in the middle of the screen, information to the left of the dot is processed by the right hemisphere, and information to the right of the dot is processed by the left hemisphere. When asked verbally what word was seen, participants answered art, which was seen by the verbal left hemisphere. When asked to point with the left hand, which is controlled by the right hemisphere, to the word that was seen, the participants pointed to he, which is the word seen by the right hemisphere. Subsequent research indicated that language, for most people, is lateralized to the left hemisphere, although a minority of individuals process language either in the right hemisphere or in both hemispheres (Rasmussen & Milner, 1977). The lateralization of language is correlated, although not completely, with a person’s handedness. As shown in a later section, nearly all right-handed people lateralize language to the left hemisphere, as do about 70% of people who are left-handed. The remaining individuals process language either in the right hemisphere or in both hemispheres. For most people, activity in the left hemisphere is correlated with positive emotions while activity in the right hemisphere is correlated with more negative emotions. Language is not the only cognitive process to show evidence of lateralization. Other suspected lateralized processes include mathematical computation and logical reasoning (left hemisphere) and some music functions, spatial information, intuition, and the visual arts (right hemisphere). Emotional behavior also appears to be lateralized. In most people, activity of the left hemisphere is correlated with positive emotions, whereas activity in the right hemisphere is correlated with negative emotions, providing the cortex with a rough distinction between approaching positive things (left hemisphere activity) and avoiding negative things (right hemisphere activity) (Davidson & Irwin, 1999). Anesthetizing the left hemisphere results in temporary feelings of depression, while anesthetizing the right hemisphere produces happiness (Lee et al., 2004). A word of caution is in order here. As noted by Roger Sperry, who won the 1981 Nobel Prize for his investigations of lateralization in the human brain, The left–right dichotomy in cognitive mode is an idea with which it is very easy to run wild (Sperry, 1982, p. 1225). Most of us have not undergone a split-brain procedure. Our intact corpus callosum and other connections between the two cerebral hemispheres allow information to pass rapidly from one hemisphere to the other. Suggestions that you can improve your artistic or athletic talent or reduce inattention by learning to access your right brain have gained considerable attention in the popular press, but they do not hold up to scrutiny in the laboratory. One of the most popular myths about lateralization is the idea that individual differences in artistic talent or logical thinking correlate with a person’s dominant hemisphere. Hemisphere dominance, as measured by the relative size of the hemispheres and the localization of language and handedness, does not predict occupational choice or artistic talent (Springer & Deutsch, 1998). Humans are not the only animals to have a preferred hand. However, other primates, such as this chimpanzee using a stone to open nuts, are equally likely to be right- or left-handed. In contrast, more than 90% of humans are right-handed. What are the advantages of lateralization? Most species of animals show a preference for one hand or the other, such as when a cat reaches for its prey (Cole, 1955;  Holder, 1999). Lateralization might provide organisms with the ability to multitask (Rogers, 2000). Chicks raised in the dark fail to lateralize visually guided responses normally and are at a disadvantage compared to normal chicks when feeding and watching for predators simultaneously. Success in this type of multitasking has obvious survival advantages. Human lateralization of brain structures might have made language possible (Berlim, Mattevi, Belmonte-de-Abreu, & Crow, 2003). This development could have a big price tag, however, because lateralization might account for our species’ vulnerabilities for schizophrenia, as discussed in Chapter 14. People with schizophrenia show abnormal hemisphere lateralization and are more likely to be left-handed or to have ambiguous handedness (Berlim et al., 2003). As shown in Table 4.2, lateralization of language is correlated with handedness (Milner, 1974). Handedness represents a continuum, with some people being nearly ambidextrous and others having strong preferences for using one hand or the other. Although most of us would have no trouble answering a question asking whether we are right- or left-handed, researchers like Milner must apply systems for determining a person’s handedness. One of the frequently used instruments follows. HandednessLanguage leftLanguage rightMixed dominanceRight-handed (90%)96%4%0%Left-handed (10%)70%15%15%Simply read each of the questions in Table 4.3. Decide which hand you use for each activity, and then circle the answer that describes you the best. If you are unsure of any answer, try to act out the action. To find your score, count the number of circled right answers and subtract the number of circled left answers. Ignore either answers. Ambidextrous people will score around 0, very right-handed people will score near +12, and very left-handed people will score near −12.1. With which hand do you draw? LeftRightEither2. Which hand would you use to throw a ball to hit a target? LeftRightEither3. In which hand would you use an eraser on paper? LeftRightEither4. Which hand removes the top card when you are dealing from a deck? LeftRightEither5. With which hand do you normally write? LeftRightEither6. In which hand do you use your racquet for tennis, squash, etc.? LeftRightEither7. With which hand do you use your toothbrush? LeftRightEither8. Which hand holds a knife when you are cutting things? LeftRightEither9. Which hand holds the hammer when you are driving a nail? LeftRightEither10. In which hand would you hold a match to strike it? LeftRightEither11. Which hand holds the thread when you are threading a needle? LeftRightEither12. In which hand would you use a fly swatter? Although this test can’t tell you which hemisphere you use for language, your odds of using your left hemisphere for language if you’re very right-handed are quite high. Although efforts to connect brain lateralization, which is highly correlated with handedness, with occupational choice or artistic talent have not been successful, being in the left-handed minority might provide advantages in sports and in combat. Right-handed pitchers, such as Tim Lincecum, are often challenged by left-handed hitters such as Adrian Gonzalez. StructureWhat To RememberSpinal CordContinuous with brainstemLarge white matter pathwaysReflexesBrainstem and CerebellumLarge white matter pathways, arousal, reflexes, and body functions (heart rate, etc.)MidbrainPonsCerebellumMedullaReticular formationSubcortical StructuresEmbedded in the white matter of the cerebral hemispheresThalamusBasal gangliaHypothalamusHippocampusCingulate cortexAmygdalaNucleus AccumbensCerebral CortexThin layer of gray matter enveloping the hemispheresDivided into four lobesAreas with sensory, motor, or association functionsSome localized and lateralized functionsThe brain and spinal cord are spectacular processing units, but without input or ability to implement commands, they would be no different from your computer’s central processing unit (CPU) without its mouse, keyboard, monitor, printer, and Internet connection. The lights may be on, but not much is going to happen. In this section, we will explore the structures of the PNS that provide these essential input and output functions in the body. The PNS can be separated into two divisions: the somatic nervous system and the autonomic nervous system. The somatic nervous system includes the peripheral portions of the sensory and voluntary movement systems. The autonomic nervous system is responsible for the actions of many glands and organs. Additional output is provided by the endocrine system, through which the CNS can communicate with the body by releasing chemical messengers into the bloodstream. These systems coordinate their efforts to produce consistent patterns of movement, hormone release, and arousal. The somatic nervous systemsomatic nervous systemThe part of the peripheral nervous system that brings sensory information to the central nervous system and transmits commands to the muscles.            somatic nervous system        The part of the peripheral nervous system that brings sensory information to the central nervous system and transmits commands to the muscles.             is the part of the PNS that transmits commands for voluntary movement from the CNS to the muscles and brings sensory input back to the CNS for further processing. These functions are carried out by the 31 pairs of spinal nerves serving the torso and limbs and the 12 pairs of cranial nerves serving the head, neck, and some internal organs (see Figure 4.17). Twelve pairs of cranial nerves carry sensory and motor information from the brain to the head, the neck, and some internal organs. The function of the autonomic nervous systemautonomic nervous systemThe division of the peripheral nervous system that directs the activity of glands, organs, and smooth muscles.            autonomic nervous system        The division of the peripheral nervous system that directs the activity of glands, organs, and smooth muscles.             is the control of tissues other than the skeletal muscle (Langley, 1921)—in other words, our glands and organs. The term autonomic has the same root as the word autonomy, or independence. You might think of this system as the cruise control of the body because it ensures that your heart keeps beating and your lungs continue to inhale and exhale without your conscious direction. The autonomic nervous system contains three subdivisions: the sympathetic, the parasympathetic, and the enteric. Biofeedback training helps people gain conscious control over some autonomic processes that normally run in the background. People who suffer from migraine headaches can be trained to reduce blood flow to the brain. The sympathetic and parasympathetic divisions are active under different circumstances. The sympathetic nervous systemsympathetic nervous systemThe division of the autonomic nervous system that coordinates arousal            sympathetic nervous system        The division of the autonomic nervous system that coordinates arousal             prepares the body for situations requiring the expenditure of energy, while the parasympathetic nervous systemparasympathetic nervous systemThe part of the autonomic nervous system associated with rest, repair, and energy storage.            parasympathetic nervous system        The part of the autonomic nervous system associated with rest, repair, and energy storage.             directs the storage of energy. You have probably experienced intense sympathetic arousal, perhaps because of a close call on the highway. In the aroused state produced by the sympathetic nervous system, our hearts race, we breathe rapidly, our faces become pale, and our palms sweat. All these activities are designed to provide the muscles with the resources they need for a fight-or-flight reaction. The sympathetic nervous system is important to our understanding of stress, described further in Chapter 16. In contrast, the parasympathetic nervous system controls the glands and organs at times of relative calm. Instead of using up energy like the sympathetic nervous system does, the parasympathetic nervous system allows you to store nutrients, repair your body, and return the activities of internal organs to baseline levels. Stress activates the sympathetic nervous system, preparing the body for fight or flight or, in this case, fast paddling. The responses of the internal organs to environmental stimuli reflect a sophisticated combination of inputs from both the sympathetic and the parasympathetic nervous systems (Berntson, Cacioppo, & Quigley, 1991). These systems usually have antagonistic effects on the organs they serve and are designed to alternate their activities (see Figure 4.18). We cannot be simultaneously relaxed and aroused. The sympathetic nervous system dilates the pupils of the eye, whereas the parasympathetic nervous system constricts the pupils. The heart responds to sympathetic commands by beating faster but responds to parasympathetic commands by slowing down. The two divisions do manage, however, to cooperate during sexual activity. The sympathetic nervous system (left) usually has the opposite effect on an organ compared with the parasympathetic nervous system (right). For example, sympathetic input tells the heart to beat faster, while parasympathetic input tells the heart to slow down. However, both systems cooperate during sex. The enteric nervous systementeric nervous systemA division of the autonomic nervous system consisting of nerve cells embedded in the lining of the gastrointestinal system            enteric nervous system        A division of the autonomic nervous system consisting of nerve cells embedded in the lining of the gastrointestinal system            , shown in Figure 4.19, consists of nerve cells embedded in the lining of the gastrointestinal system. This system is often called a second brain because it contains as many nerve cells as are found in the spinal cord. The enteric nervous system communicates with the endocrine system, described later in this chapter, to ensure the release of chemicals essential to digestion. Some functions of the enteric nervous system result in conscious perceptions, such as gastrointestinal pain, hunger, and satiety (fullness), while others operate below the threshold of conscious awareness. The latter give rise to our references to having a gut feeling. Disturbances of the enteric environment might contribute to the development of autism spectrum disorder (Slattery, MacFabe, Kahler, & Frye, 2016). The enteric nervous system is the source of 95% of the body’s serotonin, a neurochemical discussed later in this chapter. Individuals with autism spectrum disorder (see Chapter 14) show higher than normal levels of serotonin in their blood (Janusonis, 2008) and often experience gastric distress (Kazek et al., 2013). The enteric nervous system is often called a second brain. It has about the same number of neurons as the spinal cord, or about as many found in the entire brain of an adult cat. The nervous system communicates by passing messages along nerves. In contrast, the endocrine systemendocrine systemA system responsible for the release of hormones into the bloodstream.            endocrine system        A system responsible for the release of hormones into the bloodstream.             is made up of glands that release chemical messengers known as hormones into the blood (see Figure 4.20). These chemicals are often identical to those used by one nerve cell to communicate with another, but their actions affect more distant cells in a coordinated fashion. Ultimately, the endocrine system responds to input from the nervous system and from the hypothalamus in particular. The endocrine system is especially involved with arousal, metabolism, growth, and sex. Among the important glands of the endocrine system are the pineal gland, the pituitary gland, the thyroid gland, the adrenal glands, the islets of Langerhans, and the ovaries in females and testes in males. Human growth hormone, released normally by the pituitary gland, has become a popular substance among actors and athletes. Sylvester Stallone (1946- ), star of Rocky and Rambo, is one of the few Hollywood stars to discuss his use of the hormone publicly. The endocrine system communicates with other body tissues by releasing hormones from glands into the bloodstream. The pineal gland, and its release of the chemical messenger melatonin, is important in the maintenance of our sleep–wake cycles, which we discuss in Chapter 6. Although not an officially approved and tested medication, melatonin is used by some travelers to offset the unpleasant effects of jet lag. Melatonin is normally released in the early evening, and it breaks down in the presence of light. Thus, exposure to artificial light at night can have negative implications for our health. For example, higher rates of cancer among people working night shifts in hospitals have been attributed to the breakdown of melatonin by light (Dopfel, Schulmeister, & Schernhammer, 2007). The pituitary gland, located just above the roof of your mouth, is often called the body’s master gland, because many of the hormones it releases activate other glands. The pituitary in turn is regulated by the hypothalamus, which lies directly above it. The pituitary hormones form two groups. One group, including oxytocin, vasopressin, and human growth hormone, is released directly from the pituitary. The second group consists of hormones that influence the release of hormones by other glands. Oxytocin and vasopressin participate in several important physical functions, such as breastfeeding and maintenance of fluid levels, respectively. However, these hormones are also implicated in cooperation and trust, memory for social information, recognition of emotions, and resilience during stress (Meyer-Lindenberg, Domes, Kirsch, & Heinrichs, 2011). Growth hormone stimulates growth and regeneration, making it a popular performance-enhancing substance used illegally by some elite athletes. Oxytocin released by the pituitary gland is correlated with human bonding between parent and child and between romantic partners. Other pituitary hormones control the production and release of sex hormones by the ovaries and the testes, initiating puberty and maintaining fertility. In response to pituitary hormones, the thyroid gland—located just below your larynx, or voice box, in your throat—raises or lowers your rate of metabolism, or the chemical processes your body needs to sustain life. Low levels of thyroid can mimic the symptoms of depression, described further in Chapter 14. At times of stress, pituitary hormones activate the adrenal glands, which are located just above the kidneys in the lower back. In response, the adrenal glands release other hormones, including cortisol, that travel throughout the body and the brain to provide a general wake-up message. The islets of Langerhans, located in the pancreas, produce hormones essential to digestion, including insulin. StructureWhat to rememberSomatic nervous systemSensation and movement12 pairs of cranial nerves31 pairs of spinal nervesAutonomic nervous systemSympathetic nervous system: arousal and fight or flightParasympathetic nervous system: rest and repairEnteric nervous system: control of the gastrointestinal systemEndocrine systemMetabolism, arousal, growth, sexGlandsHormonesWe now zoom in to explore the microscopic building blocks of the nervous system, which are the nerve cells, or neuronsneuronsA cell of the nervous system that is specialized to send and receive neural messages.            neurons        A cell of the nervous system that is specialized to send and receive neural messages.            . Human brains have about 100 billion neurons. To put this number into perspective, consider the following: If each neuron represented a second, ticking off the neurons in your body alone would take more than 3,170 years. With each neuron forming an average of several thousand connections with other neurons, the connections in the human brain number in the hundreds of trillions. In addition to these large numbers of neurons, the nervous system contains many supporting cells, known as glia. Although it sounds like a script from a science fiction film, researchers are capable of growing neurons, such as these from the retina of the eye, on silicon chips. The chip electrically stimulates the growing neurons. Future uses of this type of technology might include brainlike computer networks and better prostheses for people who have lost a limb. Once you are familiar with the structure of neurons and glia, we will explore the ways neurons communicate with one another. Neural communication is a two-step process. The first step takes place within a single neuron and involves the generation of an electrical signal. The second step takes place between two neurons and involves the release of a chemical messenger from one neuron that affects the activity of the second. Neurons share many characteristics with other cells found in the body. Like other cells, a neuron has a large central mass or cell bodycell bodyThe large, central mass of a neuron, containing the nucleus.            cell body        The large, central mass of a neuron, containing the nucleus.            , and within the cell body, it has a nucleus (see Figure 4.21). Most housekeeping tasks of the cell, such as the translation of genetic codes into the manufacture of proteins, take place in the cell body. Like other cells, neurons feature an outer membrane, which surrounds the neuron and forms a barrier between the fluid outside the cell (the extracellular fluid) and the fluid inside the cell (the intracellular fluid). The neural membrane is composed of fatty materials that do not dissolve in water, so even though it is only two molecules thick, it is able to hold apart the water-based fluids on either side. Pores within the membrane act as channels that allow chemicals to move into or out of the cell. Neurons share many features with other living cells but are specialized for the processing of information. (a) Parts of the neuron. Like other types of animal cells, the neuron features a nucleus in its cell body and a fatty membrane that separates intracellular and extracellular fluids. Unlike most other cells, the neuron has specialized branches, the axon and the dendrites, that pass information to and receive information from other cells. (b) A closeup view of the axon membrane. A thin, oily membrane separates the intracellular fluid inside the neuron from the extracellular fluid outside the neuron. Pores spanning the membrane act as channels that allow ions to move into and out of the neuron. (c) A closeup view of the axon terminal. Within the axon terminal are synaptic vesicles, which contain chemical messengers called neurotransmitters that transmit signals between neurons. Later in the chapter, we’ll see how these neurotransmitters communicate with receptors on the dendrites of other neurons. Unlike other types of body cells, neurons have two types of branches that extend from the cell body to allow the neuron to perform its information-processing and communication functions. The branches known as axonsaxonsThe branch of a neuron that is usually responsible for transmitting information to other neurons.            axons        The branch of a neuron that is usually responsible for transmitting information to other neurons.             are responsible for carrying information to other neurons, while the branches known as dendritesdendritesA branch from the neural cell body that usually receives input from other neurons.            dendrites        A branch from the neural cell body that usually receives input from other neurons.             receive input from other neurons. Although neurons may have many dendrites, each neuron typically has only one axon. Many axons communicate with immediately adjacent cells and are, therefore, only small fractions of a millimeter in length, but other axons are much longer. When you stub your big toe on a rock, the neurons that process this information have cell bodies in your lower back and axons that extend all the way down to your sore toe, a distance of around 3 feet (about 0.9 meters), depending on your height. At its farthest point from the cell body, an axon bulges to form a terminal. If you look inside an axon terminal with an electron microscope, you can see round, hollow spheres known as synaptic vesicles, which contain molecules of chemical messengers. We have been using the term white matter to describe pathways formed by nerve fibers or axons. You have probably heard the term gray matter as well. Now that you understand the structure of neurons, these terms will make more sense. When we prepare neural tissue for study using microscopes, the chemicals used to preserve the tissue are absorbed by cell bodies. This gives cell bodies a pink–gray coloring. In contrast, these chemicals are repelled by the insulating material covering most axons because the insulation has a fatty composition that doesn’t mix well with the watery preservatives (we discuss the nature of this insulation shortly). As a result, axons look white, like the fat in a steak. When we examine images of the brain, areas that look gray have a high density of cell bodies, whereas areas that look white consist of large bundles of axons. The blood–brain barrier might offer too much protection to the brain in some cases. Many chemotherapy agents used to treat cancer in other parts of the body cannot penetrate the blood–brain barrier, which complicates the treatment of tumors in the brain. If neurons are the stars of the nervous system team, gliagliaNervous system cells that perform a variety of support functions, including formation of the blood-brain barrier and myelin.            glia        Nervous system cells that perform a variety of support functions, including formation of the blood-brain barrier and myelin.             are the trainers, coaches, and scorekeepers. They make it possible for neurons to do their job effectively. Some glia (from the Greek word for glue) provide a structural matrix for neurons, ensuring that the neurons stay in place (see Figure 4.22). Other glia are mobile, allowing them to move to a location where neurons have been damaged to clean up debris. Glia form tight connections with the blood vessels serving the nervous system. This forms a blood–brain barrier that prevents many toxins circulating in the blood from exiting into brain tissue where neurons could be harmed. Psychoactive drugs, by definition, are substances capable of penetrating the blood–brain barrier with ease. We discuss psychoactive drugs and the ways in which they act on the nervous system in Chapter 6. Glia form tight connections with the blood vessels in the nervous system, preventing many toxins from entering the brain. Glia also help hold neurons in place and form the myelin on some axons. In vertebrates such as humans, glia wrap around some axons like sausages on strings at a delicatessen, forming an important layer of insulation called myelinmyelinThe insulating material covering some axons.            myelin        The insulating material covering some axons.            . Myelin makes neural signaling fast and energy efficient. We will discuss how myelin accomplishes this in a later section on neural signaling. By speeding up the transmission of neural signals and contributing to quicker recovery between signals, myelin increases the amount of information a neuron can transmit per second by a factor of 3,000 times (Giedd et al., 2015). Not all axons in the human nervous system are myelinated. When you hurt yourself, the fast, sharp ouch message is carried to the brain by myelinated axons, but the dull, achy message that lasts a lot longer is carried by unmyelinated axons. One type of glia forms the myelin in the brain and the spinal cord, and a second type forms the myelin in the remainder of the nervous system (see Figure 4.23). These two types of glia behave quite differently from each other when they are damaged. Glia in the brain and the spinal cord form scar tissue, inhibiting repair to the damaged nerves. Because of this feature, we consider damage in the CNS to be permanent. Considerable research is under way to figure out how to repair such damage, including work using stem cells to grow bridges across the damaged areas. In contrast, damaged glia in the PNS do not form scar tissue and instead help the damaged axons regrow. As a result, nerve damage in these areas can heal. If this were not so, operations to reattach limbs would be doomed to failure. Today, not only are digits and even limbs that were lost in accidents routinely reattached to their rightful owners, but a number of patients whose own hands or faces were damaged beyond repair have undergone successful transplants from cadavers (Clarke & Butler, 2009;  Dubernard, Owen, Lanzetta, & Hakim, 2001). One type of glia forms myelin in the CNS, and a second type forms myelin in the PNS. These types of glia respond differently to nerve damage, making nerve damage outside the brain and spinal cord easier to repair. While on duty as a volunteer fireman in 2001, Patrick Hardison experienced catastrophic burn damage to his face. His original appearance before the accident is shown on the left. In 2015, a team of over 100 medical professionals transplanted a donor face onto Hardison. His current appearance is shown on the right. This type of operation would be useless unless the nerves in the face were able to establish new connections. Hardison expressed joy in an interview about his ability to feel his children’s touch on his face again. As we explore further in Chapter 11, myelin growth in the human nervous system begins before birth, but it is not completed until early adulthood, possibly as late as age 25. The last area of the nervous system to be myelinated is the prefrontal cortex, which is involved with judgment and morality (Hayak et al., 2001). Until myelin in this area is mature, these neurons do not work as efficiently, which is one of the possible reasons teenagers and adults sometimes make different decisions (Baird et al., 1999). You may recall some experiences from your early teens that appear shocking and overly risky to your adult brain. Worse yet, as you move through your 20s, you might find yourself agreeing more frequently with your parents. Now that we have a working knowledge of the structure of neurons, we are ready to talk about how they function. A neuron is a sophisticated communication and information-processing system that receives input, evaluates it, and decides whether to transmit information to neurons downstream. Its actions are similar to your own when you receive a juicy bit of gossip from a friend and then decide whether to tell somebody else. Myelination of the human nervous system takes more than 20 years to complete. As we mentioned earlier, neural communication is a two-step process. In the first step, which takes place in the signaling neuron’s axon, the neuron generates an electrical signal known as an action potentialaction potentialThe electrical signal arising in a neuron’s axon            action potential        The electrical signal arising in a neuron’s axon            . This signal travels the length of the axon from its junction with the cell body to its terminal. In the second step, which takes place between two neurons, the arrival of an action potential at the axon terminal of the first neuron signals the release of chemical messengers, which float across the extracellular fluid separating the two neurons. These chemicals influence the likelihood that the second neuron will respond with its own action potential, sending the message along. The production of action potentials can be demonstrated using axons dissected from a squid and placed in a tub of seawater, which has a chemical composition similar to the fluid surrounding our body cells (Hodgkin & Huxley, 1952). Of all the possible sources of axons on the Earth, why choose squid? Certain axons from a squid can be as much as 1 millimeter in diameter, large enough to see with the naked eye. The squid axon is also large enough that you can insert a recording electrode into its interior without disrupting its function. The readings from inside the axon can then be compared with readings from a recording electrode placed in the seawater. Some squid axons are large enough to be seen with the naked eye and remain active in a bath of seawater for hours. These features make studying neural activity in a squid axon relatively simple. When a neuron is not processing information, we say that it is at rest. When a cell is at rest, the difference between the readings from the interior of the axon and the external fluid is known as the resting potentialresting potentialThe measure of the electrical charge across a neural membrane when the neuron is not processing information.            resting potential        The measure of the electrical charge across a neural membrane when the neuron is not processing information.            . Our recording will show that the interior of the neuron is negatively charged relative to its exterior due to the different chemical composition of the intracellular and extracellular fluids. Let’s assume that our resting neuron now begins to receive chemical messages from another neuron, a process we discuss in more detail shortly. Neurons can respond to incoming chemical signals by becoming either depolarized or hyperpolarized. The word polarized means far apart, such as when political factions disagree. Being depolarized means we have moved closer together, and being hyperpolarized means we have moved even farther apart than before. In the case of neurons, depolarization means that the difference between the electrical charges of the extracellular and the intracellular recordings is decreasing. Hyperpolarization means that the difference is increasing. When a neuron is depolarized by sufficient input, it reaches a threshold for producing an action potential. A threshold is the point at which an effect, the action potential in this case, is initiated. Once this threshold is reached, the generation of an action potential is inevitable. Approaching the threshold for initiating an action potential is similar to pulling the trigger of a gun. As you squeeze the trigger, nothing happens until you reach a critical point. Once that critical point is reached, the gun fires, and there is nothing you can do to stop it. Reaching threshold initiates a sequence of events that reliably produces an action potential (see Figure 4.24). These events involve the opening and the closing of pores or channels in the neural membrane, which in turn allow certain chemicals to move into and out of the cell. These chemicals are in the form of ions or electrically charged particles dissolved in water. When threshold is reached, channels open, allowing one type of ion, sodium, to rush into the neuron. Because sodium ions carry a positive electrical charge, we can see their movement reflected in a steep rise in our recording of the difference between the internal and the external electrodes. At the peak of the action potential, our recording has reversed itself from the resting state. Now the interior of the cell is more positively charged than the outside. Once threshold is reached, an action potential is triggered. The movement of sodium and potassium ions across the axon membrane is reflected in the rise and fall of our recording, respectively. A refractory period follows each action potential, and triggering another action potential during this time is more difficult. Near the peak of the action potential, channels open that allow another type of ion, positively charged potassium, to move across the membrane. Potassium begins to leave the cell. As the interior loses these positively charged potassium ions, our recording heads in the negative direction again. Following the production of the action potential, the neuron requires a time-out or refractory period, during which it returns to its resting state. During this refractory period, the cell is unable or unlikely to respond to further input by producing another action potential. The size and shape of action potentials are always the same, whether we’re recording them in a squid or in a human. You won’t see recordings of short, fat action potentials or tall, skinny ones. Either an action potential occurs, or the cell remains at rest—there is no middle ground. Because of this consistency, we say that action potentials are all or none. Action potentials do not affect the entire axon all at once. The process we just described takes place first in a small segment of the axon where the axon connects to the cell body. The next step is propagation, or the duplication of the electrical signal down the length of the axon to the axon terminal, where it initiates the release of chemical messengers. We mentioned earlier that myelinated neurons enjoyed some advantages in efficiency and speed, and we are now ready to discuss why that is the case. Propagation takes place differently in myelinated and unmyelinated axons. In an unmyelinated axon, action potentials occur step by step, from one small section of the axon to the next adjacent section, down the entire length of the axon. In contrast, action potentials in myelinated axons are formed only at the sections of the axon membrane between adjacent segments of myelin, known as nodes of Ranvier. In other words, propagation in myelinated axons can skip the sections covered by myelin. You might think about propagation in unmyelinated versus myelinated axons as being similar to shuffling your feet versus taking long strides. Which covers the most ground faster and more efficiently? Sushi made from puffer fish, known as fugu, is a delicacy, but when prepared poorly, it can result in sickness or death. Chefs who prepare fugu undergo extensive training and licensing in Japan. The puffer fish toxin blocks the movement of sodium into cells, making electrical signaling impossible. As a result, diners who eat poorly prepared fugu can become paralyzed and suffocate to death. Propagation in unmyelinated axons works well, as evidenced by the wealth of invertebrate life on the Earth, from the snails in your garden to the giant squid of the oceans. These animals survive with no myelin, but their neural communication is not fast or energy efficient compared to ours. Forming action potentials at each section down the length of the axon is time consuming, like taking the local bus that stops at every block. In addition, cleaning up after all these action potentials uses a lot of energy (Swaminathan, Burrows, & McMurray, 1982). The more action potentials it takes to move a signal down the length of the axon, the more energy is expended returning the cell to its resting state. A node of Ranvier, located between two adjacent segments of myelin, is rich in sodium channels, which makes the formation of action potentials at the node possible. Propagation in myelinated axons is fast and efficient (see Figure 4.25). After an initial action potential is generated near the cell body, the current flows beneath a segment of myelin until it reaches a node of Ranvier, where another action potential occurs. Like the express bus, the action potentials skip the myelinated sections of the axon, reaching their destination, the axon terminal, about 20 times faster than if the axon were unmyelinated. By covering the same distance with fewer action potentials, the myelinated axon uses less energy returning to the resting potential than an unmyelinated axon would need. Action potentials move down the length of the myelinated axon more quickly than they move down an unmyelinated axon. Once the action potential reaches the axon terminal, the neural communication system switches from an electrical signaling system to a chemical signaling one. The point of communication between two neurons is known as a synapse. At the synapse, neurons do not touch one another physically. Instead, they are separated by tiny gaps filled with extracellular fluid. Because electrical signals are unable to jump this gap, neurons send chemical messengers instead. These chemical messengers are called neurotransmittersneurotransmittersA chemical messenger that communicates across a synapse.            neurotransmitters        A chemical messenger that communicates across a synapse.             (see Table 4.4). NeurotransmitterBehaviors influenced by the neurotransmitterAcetylcholine (ACh)MovementMemoryAutonomic nervous system functionEpinephrine (adrenalin)ArousalNorepinephrine (noradrenalin)ArousalVigilanceDopamineMovementPlanningRewardSerotoninMoodAppetiteSleepGlutamateExcitation of brain activityGABAInhibition of brain activityEndorphinsPainFigure 4.26 illustrates the sequence of events triggered by the arrival of an action potential at an axon terminal. The neurotransmitters in the axon terminal are contained in synaptic vesicles. The arrival of an action potential releases the vesicles from their protein anchors, much like boats leaving a dock, and the vesicles migrate rapidly to the cell membrane. Because the vesicles are made of the same thin, oily material as the membrane, they easily fuse with the membrane and spill their contents into the synaptic gap, similar to popping soap bubbles in a bathtub. Following release, the vesicles are pinched off the membrane and refilled. Because most neurons are separated from one another by extracellular fluid, the action potential cannot jump from one neuron to the next. To cross the gap between neurons, chemical signals are used instead. (a) Neurotransmitter release. The arrival of an action potential at the axon terminal triggers a sequence of events that results in the release of neurotransmitter molecules, which float across the synaptic gap to interact with receptors on the receiving neuron. (b) Reuptake. After interacting with receptors, neurotransmitter molecules are often recaptured by the neuron that released them to be recycled and used again later. Two axons (in purple) are forming synapses or junctions where communication will occur with a neuron’s cell body (in yellow). The neurotransmitters released across the synaptic gap come into contact with special channels on the receiving neuron, known as receptors. ReceptorsReceptorsA special channel in the membrane of a neuron that interacts with neurotransmitters released by other neurons.            Receptors        A special channel in the membrane of a neuron that interacts with neurotransmitters released by other neurons.             work with the neurotransmitters like locks and keys. Only a neurotransmitter with the right shape (the key) can attach itself or bind to a particular receptor (the lock). Neurotransmitters do not stay bound to receptors long. Once they pop out of the receptor binding site, neurotransmitter molecules drift away from the gap, are broken down by enzymes, or return to the axon terminal from which they were released in a process called reuptake. In reuptakereuptakeA process in which molecules of neurotransmitter in the synaptic gap are returned to the axon terminal from which they were released            reuptake        A process in which molecules of neurotransmitter in the synaptic gap are returned to the axon terminal from which they were released            , special channels in the axon terminal membrane known as transporters allow the neurotransmitters to move back into the releasing neuron where they are repackaged for later use. Many important drugs, including the antidepressant drug fluoxetine (Prozac), interfere with or inhibit this reuptake process. The interaction between neurotransmitters and their receptors can have one of two effects on the receiving neuron: excitation or inhibition. When a neurotransmitter has an excitatory effect, it slightly depolarizes the receiving neuron, increasing the likelihood that the neuron will reach threshold and initiate an action potential. Recall that depolarization reduces the difference between the electrical environments inside and outside the neuron. When a neurotransmitter has an inhibitory effect, it slightly hyperpolarizes the receiving neuron, moving the cell farther from threshold and reducing the likelihood that it will initiate an action potential. Recall that hyperpolarization increases the difference between the electrical environments inside and outside the neuron. Excitatory messages seem logical. One neuron is telling another to pass the message along. Inhibitory messages, however, seem somewhat strange at first glance. Why would our nervous systems need a message that says, Don’t pass the message along? Tetanus, for which you probably have been vaccinated, provides a dramatic example of what can happen when inhibition doesn’t work properly. The toxin produced by the bacteria responsible for tetanus selectively damages inhibitory neurons in the parts of the nervous system that control muscle contraction. Normally, excitatory inputs that contract muscles coordinate their activity with inhibitory inputs that tell muscles to relax, allowing the steady hands we need to put in a contact lens, for example. Without the input of the inhibitory neurons, the system is left with excitation only, and the result is the extreme muscle contraction that gives tetanus its other name—lockjaw (see Figure 4.27). The severe muscle contraction that gives tetanus its nickname of lockjaw results from the toxin’s blocking the release of inhibitory neurotransmitters in the motor systems of the brain and spinal cord. (a) Normally, inhibitory input balances contraction to maintain smooth movements. (b) Without the counterbalance of inhibition, too much muscle contraction occurs. The SSRI label for some antidepressant medications, including Prozac, stands for selective serotonin reuptake inhibitor. People who are depressed often have lower-than-normal serotonin activity at the synapse. If you inhibit reuptake of serotonin, more molecules remain in the synaptic gap longer where they can continue to interact with receptors. Serotonin activity increases, relieving depression, because we get more bang for the buck each time serotonin is released. Synapses usually occur in many locations on the dendrites or cell body of the receiving neuron, and the depolarizing or hyperpolarizing current that results from neurotransmitter activity at these synapses drifts to the junction of the cell body and axon. If there is sufficient depolarization to reach threshold at this junction, the neuron generates an action potential. If not, it remains at rest. The neuron’s decision to generate an action potential or not is called summation; the neuron is adding up all incoming messages and making a decision based on that information. The neuron’s task is not unlike the situation we face when we ask friends and family for advice. We will receive some excitatory advice (Go for it!), along with some inhibitory advice (Don’t even think about it!). Our job, like the neuron’s, is to sum our input and make a decision. Unlike us, however, the neuron cannot disregard the advice it receives. Researchers have identified more than 50 chemicals that serve as neurotransmitters. Table 4.4 lists some neurotransmitters that are particularly interesting to psychologists, and we will highlight a few of these in this section. In Chapter 6, we explore examples of psychoactive drugs, both therapeutic and recreational, that interact with the normal biochemistry of the nervous system. A Yagua hunter in Peru loads a dart tipped with curare into the mouthpiece of his blowgun. Curare is derived from native plants and causes paralysis by blocking receptors for acetylcholine (ACh) at synapses between the nervous system and the muscle fibers. Acetylcholine (ACh) is a neurotransmitter found in many systems important to behavior. ACh is found at the neuromuscular junction, the synapse at which the nervous system commands muscles. Interference with the action of ACh at the muscles can result in paralysis and death, making drugs that act on ACh popular for use as pesticides and as bioweapons. ACh also serves as a key neurotransmitter in the autonomic nervous system, discussed previously, which carries commands from the brain to the glands and organs. ACh is also intimately involved in the brain circuits related to learning and memory. These brain circuits are among the first to deteriorate in Alzheimer’s disease. Not surprisingly, memory deficits are among the earliest symptoms of Alzheimer’s disease to appear. Among the many drugs that act on ACh systems is the nicotine found in tobacco. Norepinephrine activity in the brain leads to arousal and vigilance. Consistent with this role in arousal, norepinephrine is also released by the sympathetic nervous system. As we observed previously, the sympathetic nervous system prepares us to react to emergencies by providing necessary resources, such as the extra oxygen that is needed to run or throw a punch. Abnormalities in norepinephrine activity have been implicated in several psychological conditions that feature disturbances in arousal and vigilance, including bipolar disorder and posttraumatic stress disorder (PTSD), discussed in Chapter 14. Dopamine is involved with systems that govern movement, planning, and reward. Parkinson’s disease, which makes normal movement difficult, results when dopamine-releasing neurons in the brain’s movement circuits begin to die. In addition, dopamine participates in the brain’s reward and pleasure circuits by becoming active whenever we engage in behaviors that promote survival and successful reproduction, such as eating a great meal or having sex. Most drugs that produce addiction, including cocaine and methamphetamine, stimulate increased activity in dopamine circuits. In Chapter 14, we will see how disruptions to dopamine circuits have been implicated in schizophrenia and ADHD. Serotonin is involved with systems regulating sleep, appetite, mood, and aggression. Consequently, these behaviors are tightly linked. As we will see in Chapter 14, people who experience depressed mood also show abnormalities in appetite and sleep. Sleep deprivation can result in changes in mood and appetite, even leading to significant overeating (Spiegel, Tasali, Penev, & Van Cauter, 2004). Botox, which is used to treat muscle spasms or reduce wrinkles, is made from an inactive form of the toxin responsible for botulism, which is produced by bacteria that spoil food. Botox interacts with ACh by preventing its release from the axon terminal. Without the activity of ACh telling a muscle to contract, the muscle remains paralyzed. Endorphins, short for endogenous morphine or morphine produced by the body, modify our natural response to pain. In evolutionary terms, it makes sense to have a system that reduces your chances of being disabled by pain during an emergency. All too frequently, however, we underestimate the extent of our injuries until we wake up the next morning feeling sore. Runner’s high, in which people who regularly engage in endurance sports experience a sense of well-being and reduced sensation of pain, results from the release of endorphins initiated by high levels of activity. Opioid drugs such as morphine, heroin, and oxycodone (OxyContin) produce their pain-relieving effects by mimicking the action of endorphins at the synapse. In other words, the opioid drugs are so similar in chemical structure to our natural endorphins that the receptors cannot tell them apart and treat the opioids as if they were natural endorphins. This image highlights areas of the human brain that are rich in receptors for endorphins, our natural opioids. The red areas have the most endorphin receptors, followed by the yellow areas. Opioid drugs, such as heroin or morphine, affect our behavior by interacting with these receptors. For many people, love and lust seem close together. Using the biological perspective, however, these two states look related yet distinct. On a neurochemical level, sexual desire or lust is driven primarily by levels of testosterone for males and levels of estrogens for females (Cappelletti & Wallen, 2016), while the bonding we associate with romantic love is largely associated with oxytocin (Diamond, 2004). This suggests that, at least on a hormonal level, it is possible to differentiate the two states. Further understanding of the relationship between lust and love comes from brain imaging research. Of particular interest is the insula, an area of cortex lining the lateral sulcus, which separates the temporal lobes from the frontal and parietal lobes. Pulling the temporal lobe away from the rest of the brain reveals the insula within the sulcus. As shown in Figure 4.28, researchers found that activation of the posterior (back) part of the insula is more closely associated with lust (Cacioppo, Bianchi-Demicheli, Frum, Pfaus, & Lewis, 2012). In contrast, these same researchers found that activation of the anterior or forward part of the insula is more associated with romantic feelings. They conclude that love grows out of and is a more abstract representation of the pleasant sensorimotor experiences that characterize desire (Cacioppo et al., 2012, p. 1048). Areas shown in blue are uniquely active during sexual desire, while areas shown in red are uniquely active during romantic love. One of the areas where activity reliably varies between the states of sexual desire and romantic love is the insula. Activity in the posterior insula (toward the back of the brain) is associated with sexual desire, while activity in the anterior insula (toward the front of the brain) is associated with romantic love. Additional support for the role of the anterior insula in love comes from a unique case study (Cacioppo et al., 2013). A male patient who had experienced damage to his anterior insula was asked to indicate whether a series of attractive individuals could be the object of his lust or his love. Compared to healthy control participants, the patient showed a selective deficit in his decisions about love yet performed normally on decisions about lust. The patient was similar to the controls in the number of people he categorized as objects of love or lust, and his response time to objects of lust appeared normal. However, his response time for categorizing objects of love was slower than that of the healthy controls. Should we conclude that the anterior insula is a love center of the brain? It would be better to conclude, as  Cacioppo et al. (2013) do, that the anterior insula participates in a larger network that helps us figure out internal states, which includes feelings of romantic love. Cyberbullying is unquestionably cruel. Empathy, or our ability to understand and share the feelings of others, usually prevents cruelty. If we imagine our responses to being cyberbullied, we are relatively less likely to cyberbully ourselves. An extreme failure in empathy characterizes the psychopath, discussed in Chapter 14. Empathy is a part of social cognition, or the spontaneous attribution of internal thoughts and emotions to other people, animals, and even objects. When social cognition is not engaged, people might instead respond with disinterest or disgust (Harris & Fiske, 2011). Consider how people respond while passing a homeless person on the street. You will make very different conclusions using social cognition (how would it feel to be that dirty and hungry?) as opposed to not engaging social cognition. Responding with disgust opens the door to dehumanizing the other. The online environment has features that make cruelty more likely (Runions & Bak, 2015). Distance between the bully and the victim diminishes the emotional impact of harming another person. In addition, the online environment offers fewer social cues, perhaps making it easier to dehumanize the victim and to underestimate their pain. These specific features of the online environment might support bullying by a person who would be less likely to cause harm in person. Neuroscientists have made progress in identifying the features of the brain that support social cognition and empathy. Perhaps most relevant to the question of cyberbullying is our capacity for sharing the pain of others, and brain circuits involved in pain are well understood. A circuit involving the somatosensory cortex in the parietal lobe helps us identify the location of a source of pain and its intensity (I just twisted my ankle and it’s a 6 on a pain scale of 1 to 10). Another circuit involving the insula and the anterior cingulate cortex is involved with our emotional responses to pain (Twisting my ankle is bringing tears to my eyes!). In general, these same circuits show similar activation when we empathize with someone in pain (see Figure 4.29). However, our tendency to empathize with others is moderated by a complicated set of cognitions, including judgments about whether the person is responsible for his or her own pain (Decety, Echols, & Correll, 2009). Empathy for others depends on complex cognitions, including how much responsibility a person has for their pain. These participants showed a larger response to observing a video of a person in pain when they were told the pain resulted from acquiring AIDS through a transfusion. Lesser responses were observed when the participants were told that the pain resulted from acquiring a virus while otherwise healthy or from acquiring AIDS through drug use. StructureWhat to rememberNeuronCell body: Contains the nucleus and carries out most housekeeping functions. Axon: Used to send information. Dendrite: Used to receive information. GliaHold neurons in place. reflexes, and body functions (heart rate, etc.)Clean up debris. Form myelin. Blood–brain barrier. Action potentialsPropagated down the axon to the terminal. Resting potential: No incoming signals. Action potential: Occurs when cell reaches threshold. Refractory period: Follows action potential. SynapsePoint of communication between two neurons. NeurotransmittersReleased when an action potential arrives at a terminal. Interact with receptors in the receiving neuron’s membrane. Deactivated if they remain in the synaptic gap. ACh. Norepinephrine. Dopamine. Serotonin. Endorphins.`;\n","export const text = `The rods and cones in the retina begin the process of interpreting the light energy that enters the eye. We like to think we understand reality. After all, we can see, hear, touch, smell, and taste it. We don’t live in some science fiction universe where things are not how they appear. Or do we? The human eye can see many different colors, but what does it mean to  see  a color? Is color something that is a fixed quality of an object? Is the sky really blue? Is an apple really red? Or does the human mind construct these colors from the light reflected from these objects into the eye? Consider the image of the blue/black or white/gold dress that became an Internet sensation in February 2015. A friend of a Scottish bride posted the dress worn by the bride’s mother on her Tumblr blog, leading to a discussion that engaged everyone from Justin Bieber to esteemed neuroscientists. Why do people see this photo so differently? The Dress  became an Internet phenomenon as people debated its true colors. Do you see it as black and blue? White and gold? Something else? Neuroscientists disagree about why the dress produced such different responses. The Journal of Vision prepared an entire issue ( A Dress Rehearsal for Vision Science ) devoted to explaining the dress phenomenon. A survey of 1,400 people found that 57% described the dress as blue/black (which is correct), 30% as white/gold, 11% as blue/brown, and 2% as something else (Lafer-Sousa, Hermann, & Conway, 2015). Older individuals and women were more likely to choose white/gold. These researchers believe that people choose dress colors based on their expectations regarding the lighting. If you think the dress is seen in daylight, you make different conclusions than if you think the dress is seen under artificial light. Other scientists believe there is something special about the color blue due to our considerable experience with natural lighting (Winkler, Spillmann, Werner, & Webster, 2015). Because indirect lighting and shadows are usually blue, participants are more likely to confuse blue objects with blue lighting. If you assume the light falling on the dress is somewhat blue, you will probably see it as white. Would having a color deficiency change the way a person sees the dress? See for yourself. We can reconstruct how the image would look to a person with a rare type of blue-yellow color deficiency (see Figure 5.1). Surprisingly, this has little effect, although the blue looks somewhat gray. Is the reality seen by a person with a color deficiency different from your reality? We’re going to argue that it is not reality that changes but rather the way the brain views that reality. Tritanopes  have a rare type of blue-yellow color deficiency. We can simulate how the mysterious dress would look to a tritanope. The differences are surprisingly subtle, which explains why we use the term  color deficiency  rather than  colorblindness.  This example also reminds us that a single reality (the dress) can be sensed and perceived very differently by individual minds. Humans see only a small part of the electromagnetic energy emitted from the Sun. Some animals see even less. Dogs apparently do fine seeing blues, yellows, and grays, whereas humans have evolved to see a more colorful world. The dog’s view of the world is simulated in the photo on the right. As you’ll see in this chapter, we construct models of reality from the information obtained through our senses. We like to think that we are aware of the world around us, and it is unsettling to realize that the world might be different from the representations of reality formed by the human mind. You will learn how the models built by the human mind have promoted our survival over many generations. Our models of reality are distinct from those built by the minds of other animals, whose survival depends on obtaining different types of information from their environments. Our bodies are bombarded with information during wakefulness and sleep. This information takes many forms, from the electromagnetic energy of the sun to vibrations in the air to molecules dissolved in saliva on our tongues. The process of sensationsensationThe process of detecting environmental stimuli or stimuli arising from the body.            sensation        The process of detecting environmental stimuli or stimuli arising from the body.             brings information to the brain that arises in the reality outside our bodies, like a beautiful sunset, or originates from within, like an upset stomach. Sensory systems have been shaped by natural selection, described in Chapter 3, to provide information that enhances survival within a particular niche. We sense a uniquely human reality, and one that is not shared by other animals. Your dog howls seconds before you hear the siren from an approaching ambulance because the dog’s hearing is better than yours for high-pitched sounds. Horses bolt at the slightest provocation, but they may be reacting to the vibration of an approaching car or an animal that they sense through their front hooves, a source of information that is not available to the rider. Some animals sense light energy outside the human visible spectrum. Insects can see ultraviolet light, and some snakes use infrared energy to detect their prey. Differences in sensation do occur from person to person, such as the need to wear corrective glasses or not, but they are relatively subtle. However, once we move from the process of sensation to that of perceptionperceptionThe process of interpreting sensory information.            perception        The process of interpreting sensory information.            , or the interpretation of sensory input, individual differences become more evident. For example, friends voting for different presidential candidates will come to different conclusions about who won a debate. Everyone watching the exchange sensed similar information, but each person’s perceptions are unique. Some types of snakes (vipers, boas, and pythons) can sense prey using infrared energy. Sensation begins with the interaction between a physical stimulus and our biological sensory systems. A stimulus is anything that elicits a reaction from our sensory systems. For example, we react to light energy that falls within our visual range, as we will see later in this chapter, but we cannot see light energy that falls outside that range, such as the microwaves that cook our dinner or the ultraviolet waves that harm our skin (see Figure 5.1). Before you can use information from your senses, it must be translated into a form the nervous system can understand. This process of translation from stimulus to neural signal is known as transductiontransductionThe translation of incoming sensory information into neural signals.            transduction        The translation of incoming sensory information into neural signals.            . You might think of sensory transduction as being similar to the processing of information by your computer. Modern computers transduce a variety of inputs, including voice, keyboard, mouse clicks, and touch, into a programming language for further processing. We have all had the experience of watching events with others (sensation) and then being shocked by the different interpretations we hear of what just happened (perception). Once information from the sensory systems has been transduced into neural signals and sent to the brain, the process of perception, or the interpretation of the sensory information, begins. Perception allows us to organize, recognize, and use the information provided by the senses. If you think about the most memorable advertisements you have seen lately on television or online, it is likely that they share the features of attention-getting stimuli: novelty (we don’t see talking geckos every day), change (rapid movement, use of changing colors, and the dreaded pop-up), and intensity (the sound is often louder than the program you’re watching). An important gateway to perception is the process of attention, defined as a narrow focus of consciousness. As we discuss in Chapters 6, 9, and 10, attention often determines which features of the environment influence our subsequent thoughts and behaviors. Which stimuli are likely to grab our attention? Unfamiliar, changing, or high-intensity stimuli often affect our survival and have a high priority for our attention. Unfamiliar stimuli in our ancestors’ environment might have meant a new source of danger (an unknown predator) or a new source of food (an unfamiliar fruit) that warranted additional investigation. Our sensory systems are particularly sensitive to change in the environment. Notice how you pay attention to the sound of your heating system cycling on or off but pay little attention to the noise it makes while running. This reduced response to an unchanging stimulus is known as sensory adaptationsensory adaptationThe tendency to pay less attention to a nonchanging source of stimulation.            sensory adaptation        The tendency to pay less attention to a nonchanging source of stimulation.            . High-intensity stimuli, such as bright lights and loud noises, draw our attention because the situations that produce these stimuli, such as a nearby explosion, can have obvious consequences for our safety. We rarely have the luxury of paying attention to any single stimulus. In most cases, we experience divided attention, in which we attempt to process multiple sources of sensory information. Students walk to class without getting run over by a car while texting. These divided attention abilities are limited. We simply cannot process all the information converging simultaneously on our sensory systems. To prioritize input, we use selective attention or the ability to focus on a subset of available information and exclude the rest. These abilities may be disrupted in cases of attention deficit hyperactivity disorder (ADHD;  Wimmer et al., 2015; also see Chapter 14). We refer to the brain’s use of incoming signals to construct perceptions as bottom-up processingbottom-up processingPerception based on building simple input into more complex perceptions            bottom-up processing        Perception based on building simple input into more complex perceptions            . For example, we construct our visual reality from information about light that is sent from the eye to the brain. However, the brain also imposes a structure on the incoming information, a type of processing known as top-down. In top-down processingtop-down processingA perceptual process in which memory and other cognitive processes are required for interpreting incoming sensory information            top-down processing        A perceptual process in which memory and other cognitive processes are required for interpreting incoming sensory information            , we use knowledge gained from prior experience with stimuli to perceive them. For example, a skilled reader has no trouble reading the following sentences, even though the words are jumbled:All you hvae to do to mkae a snetnece raedalbe is to mkae srue taht the fisrt and lsat letrtes of ecah wrod saty the smae. Wtih prcatcie, tihs porcses becoems mcuh fsater and esaeir. Divided attention abilities are limited. Some people believe that heads-up displays for cars assist drivers with divided attention, while others believe the displays are too distracting. How can we explain our ability to read these sentences? First, we require bottom-up processing to bring the sensations of the letter shapes to our brain. From there, however, we use knowledge and experience to recognize individual words. Many students have learned the hard way that term papers must be proofread carefully. As in our example, if the brain expects to see a particular word, you are likely to see that word, even if it is misspelled, a mistake that is unlikely to be made by the literal, bottom-up processing of a computer spell-checker. Selective attention, or our focus on a subset of input, prioritizes incoming information. However, we can sometimes be so focused that we miss important information. An astonishing 20 out of 24 expert radiologists completely missed the image of a gorilla superimposed on scans of lungs while scanning for signs of cancer. Can we predict when the mind will use bottom-up or top-down processing? There are no hard and fast rules. Obviously, we always use bottom-up processing, or the information would not be perceived. It is possible that bottom-up processing alone allows us to respond appropriately to simple stimuli, like indicating whether you saw a flash of light. As stimuli become more complicated, like reading a sentence or recognizing a friend in a crowd, we are more likely to engage in top-down processing in addition to bottom-up processing. Gustav Fechner (1801–1887) developed methods, which he called psychophysicspsychophysicsThe study of relationships between the physical qualities of stimuli and the subjective responses they produce.            psychophysics        The study of relationships between the physical qualities of stimuli and the subjective responses they produce.            , for studying the relationships between stimuli (the physics part) and perception of those stimuli (the psyche or mind part) (see Figure 5.2). Fechner’s careful methods not only contributed to the establishment of psychology as a true science but are still used in research today. Golden  rectangles, named for their proportions rather than color, appear in art and architecture dating back to ancient Greece, but why are they attractive? Gustav Fechner (1801–1887) made many attempts to link physical realities with human psychological responses. He asked people to choose which rectangles are most pleasing or least pleasing. His results indicated that the most pleasing rectangle was fourth from the right. This rectangle is the closest to having golden proportions (1:1.618). Its sides have a ratio of 13:21. The methods of psychophysics allow us to establish the limits of awareness, or thresholds, for each of our sensory systems. The smallest possible stimulus that can be detected at least 50% of the time is known as the absolute thresholdabsolute thresholdThe smallest amount of stimulus that can be detected.            absolute threshold        The smallest amount of stimulus that can be detected.            . Under ideal circumstances, our senses are surprisingly sensitive (see Figure 5.3). For example, you can see the equivalent of a candle flame 30 miles (about 48 kilometers) away on a moonless night. We can also establish a difference thresholddifference thresholdThe smallest detectable difference between two stimuli.            difference threshold        The smallest detectable difference between two stimuli.            , or the smallest difference between two stimuli that can be detected at least 50% of the time. The amount of difference that can be detected depends on the size of the stimuli being compared. As stimuli get larger, differences must also become larger to be detected by an observer. An absolute threshold is the smallest amount of sensation that can be processed by our sensory systems under ideal conditions. Moving from left to right in this image, we see that the absolute threshold for touch is the equivalent of feeling the wing of a fly fall on your cheek from a distance of 0.4 inch (about 1 centimeter), the absolute threshold for olfaction is a drop of perfume in the air filling a six-room apartment, the absolute threshold for sweetness is the equivalent of one teaspoon (about 5 grams) of sugar in two gallons (about 7.5 liters) of water (the absolute threshold for bitter tastes is even more sensitive), the absolute threshold for hearing is the equivalent of the sound of a mosquito 10 feet (about 3 meters) away, and the absolute threshold for vision is seeing a candle flame 30 miles (about 48 kilometers) away on a dark, clear night. Another example of signal detection is a jury’s decision about whether a person is guilty. Based on frequently uncertain and conflicting evidence, jurors must weigh their concerns about convicting an innocent person (false alarm) or letting a real criminal go (miss). Many perceptions involve some uncertainty. Perhaps you’re driving rather fast, and you think a distant car behind you might be a police officer. Do you slow down right away? Or do you wait until the car is close enough that you know for sure it’s a police officer? How do your personal feelings about making mistakes affect your decision? Would the cost of a ticket ruin your budget? This type of decision making can have serious implications, such as in the case of decisions made by radiologists examining the results of mammograms for signs of cancer or by intelligence officers assessing the possibility of an attack. Is there reason for concern or not? This situation is different from the thresholds described earlier because it adds the cognitive process of decision making to the process of sensation. In other words, signal detectionsignal detectionThe analysis of sensory and decision-making processes in the detection of faint, uncertain stimuli.            signal detection        The analysis of sensory and decision-making processes in the detection of faint, uncertain stimuli.             is a two-step process involving the actual intensity of the stimulus, which influences the observer’s belief that the stimulus did occur, andthe individual observer’s criteria for deciding whether the stimulus occurred. Experiments on signal detection provide insight into this type of decision making. In these experiments, trials with a single, faint stimulus and trials with no stimulus are presented randomly. The participant states whether a stimulus was present on each trial. The possible outcomes of this experiment are shown in Table 5.1. In the case of reading mammograms, we can use such experiments to help us understand why two people might respond differently, even if they were sensing the same information. Ideally, a radiologist would identify 100% of all tumors without any false alarms, but mammograms are not that easy to evaluate. A radiologist afraid of missing a tumor might identify anything that looks remotely like a tumor as the basis for more testing. Few cases of cancer would be missed (high hit rate), but many healthy patients would go through unnecessary procedures (high false alarm rate). In contrast, another radiologist might need a higher level of certainty about the presence of a tumor before asking for further tests. This would reduce the number of false alarms, but it would also run a higher risk of overlooking tumors (high miss rate). According to Fechner’s work on the difference threshold, British Olympian Zoe Smith would be more likely to notice the difference between 2- and 4-pound (between 1 to 2 kilograms) weights than the difference between her new record of 121 kilograms (266.76 pounds) in the clean and jerk event and the former record of 120 kilograms (264.56 pounds). Does this mammogram indicate the woman has cancer or not? Many decisions we make are based on ambiguous stimuli. Signal detection theory helps us understand how an individual doctor balances the risks of missing a cancer and those of alarming a healthy patient. Participant ResponseStimulus PresentStimulus AbsentYesHitFalse alarmNoMissCorrect rejectionConceptDefinitionExampleAbsolute thresholdThe smallest amount of stimulation that is detectable. Seeing light from a candle flame 30 miles away on a dark night. Difference thresholdThe smallest difference between two stimuli that can be detected. Being able to detect the difference between two different weights. Signal detectionCorrectly identifying when a faint stimulus is or is not present. A radiologist correctly detecting cancer from a mammogram. VisionVisionThe sense that allows us to process reflected light.            Vision        The sense that allows us to process reflected light.            , the processing of light reflected from objects, is one of the most important sensory systems in humans. Approximately 50% of our cerebral cortex processes visual information, in comparison to only 3% for hearing and 11% for touch and pain (Kandel & Wurtz, 2000;  Sereno & Tootell, 2005). We will begin our exploration of vision with a description of the visual stimulus, and then we will follow the processing of that stimulus by the mind into a meaningful perception. Visible light, or the energy within the electromagnetic spectrum to which our visual systems respond, is a type of radiation emitted by the sun, other stars, and artificial sources such as a lightbulb. As shown in Figure 5.4, light energy moves in waves, like the waves in the ocean. Wavelength, or the distance between successive peaks of waves, is decoded by our visual system as color or shades of gray. The height, or amplitude, of the waves is translated by the visual system into brightness. Large-amplitude waves appear bright, and low-amplitude waves appear dim. The distance between two peaks in a light wave (wavelength) is decoded by the visual system as color and the height, or amplitude, of the wave as brightness. The human visual world involves only a small part of this light spectrum (review Figure 5.1). Gamma rays, x-rays, ultraviolet rays, infrared rays, microwaves, and radio waves lie outside the capacities of the human eye. Human vision begins with the eye. The eye is roughly sphere shaped and about the size of a ping-pong ball. Its hard outer covering helps the fluid-filled eyeball retain its shape. Toward the front of the eye, the outer covering becomes clear and forms the corneacorneaThe clear surface at the front of the eye that begins the process of directing light to the retina.            cornea        The clear surface at the front of the eye that begins the process of directing light to the retina.            . The cornea begins the process of bending light to form an image on the back of the eye. Traveling light next enters the pupilpupilAn opening formed by the iris.            pupil        An opening formed by the iris.            , which is actually an opening formed by the muscles of the irisirisThe brightly colored circular muscle surrounding the pupil of the eye.            iris        The brightly colored circular muscle surrounding the pupil of the eye.             (see Figure 5.5). The iris, which means  rainbow  in Greek, adjusts the opening of the pupil in response to the amount of light present in the environment and to signals from the autonomic nervous system, described in Chapter 4. Arousal is associated with dilated pupils, while relaxation is associated with more constricted pupils. Light entering the eye travels through the cornea, the pupil, and the lens before reaching the retina. Among the landmarks on the retina are the fovea, which is specialized for seeing fine detail, and the optic disk, where blood vessels enter the eye and the optic nerve exits the eye. Directly behind the pupil and iris is the main optical instrument of the eye, the lenslensThe clear structure behind the pupil that bends light toward the retina.            lens        The clear structure behind the pupil that bends light toward the retina.            . Muscles attached to the lens can change its shape, allowing us to accommodate, or adjust our focus to see near or distant objects. Behind the lens is the main chamber of the eye, and located on the rear surface of this chamber is the retinaretinaLayers of visual processing cells in the back of the eye            retina        Layers of visual processing cells in the back of the eye            , a thin but complex network of neurons specialized for the processing of light. Located in the deepest layer of the retina are specialized receptors, the rods and cones, that transduce the light information. However, before light reaches these receptors, it must pass through layers of blood vessels and neurons. We normally do not see the blood vessels and neural layers because of sensory adaptation. As we mentioned previously in this chapter, adaptation occurs when sensory systems tune out stimuli that never change. Because the blood vessels and neural layers are always in the same place, we see them only under unusual circumstances, such as during certain ophthalmology (eye) tests. We can identify several landmarks on the surface of the retina. The blood vessels serving the eye and the axons that leave the retina to form the optic nerve exit at the optic disk. Because there are no rods and cones in the optic disk, each eye has a blind spot. Normally, we are unaware of our blind spots because perception fills in the missing details. However, if you follow the directions in Figure 5.6, you should be able to experience your own blind spot. Toward the middle of the retina is the foveafoveaAn area of the retina that is specialized for highly detailed vision.            fovea        An area of the retina that is specialized for highly detailed vision.            , which is specialized for seeing fine detail. When we stare directly at an object, the image of that object is projected onto the fovea. The fovea is responsible for central vision, as opposed to peripheral vision, which is the ability to see objects off to the side while looking straight ahead. There are no photoreceptors in the optic disk, producing a blind spot in each eye. We do not see our blind spots because our brain fills in the hole. You can demonstrate your blind spot by holding your textbook at arm’s length, closing one eye, focusing your other eye on the dot, and moving the book toward you until the stack of money disappears. The image projected on the retina is upside down and reversed relative to the actual orientation of the object being viewed (see Figure 5.7). You can duplicate this process by looking at both sides of a shiny spoon. In the convex (or outwardly curving) side, you see your image normally. In the concave (or inwardly curving) side, you see your image as your retina sees it. Fortunately, the visual system easily decodes this image and provides realistic perceptions of the actual orientations of objects. The image projected on the retina is upside down and reversed, but the brain is able to interpret the image to perceive the correct orientation of an object. RodsRodsA photoreceptor specialized to detect dim light            Rods        A photoreceptor specialized to detect dim light             and conesconesA photoreceptor in the retina that processes color and fine detail.            cones        A photoreceptor in the retina that processes color and fine detail.             are named after their shapes. The human eye contains about 90 million rods and between 4 million and 5 million cones. Rods and cones are responsible for different aspects of vision. Rods are more sensitive to light than cones, and they excel at seeing dim light. As we observed previously, under ideal circumstances, the absolute threshold for human vision is the equivalent of a single candle flame from a distance of 30 miles (about 48 kilometers; see  Hecht, Shlaer, & Pirenne, 1942). Rods become more common as we move from the fovea to the periphery of the retina, so your peripheral vision does a better job of viewing dim light than your central vision does (see Figure 5.8). Before the development of night goggles, soldiers patrolling in the dark were trained to look to the side of a suspected enemy position rather than directly at their target. In humans, cones, indicated by red, blue, and green dots, become less frequent as you move from the fovea to the periphery of the retina. The colors of the dots representing cones indicate the colors to which each shows a maximum response (see Figure 5.12). Rods (light brown dots) and cones are named according to their shapes. This extraordinary sensitivity of rods has costs. Rods do not provide information about color, nor do they provide clear, sharp images. Under starlight, normal human vision is 20/200 rather than the normal daylight 20/20. In other words, an object seen at night from a distance of 20 feet would have the same clarity as an object seen in bright sunlight from a distance of 200 feet. Cones function best under bright light and provide the ability to see both sharp images and color. The rods and cones are the only true receptors of the visual system. When they absorb light, they trigger responses in four additional layers of neurons within the retina. Axons from the final layer of cells, the ganglion cells, leave the back of the eye to form the optic nerveoptic nerveThe nerve exiting the retina of the eye.            optic nerve        The nerve exiting the retina of the eye.            . The optic nerves cross the midline at the optic chiasm (named after its X shape, or the Greek letter chi). At the optic chiasm, the axons closest to the nose cross over to the other hemisphere, while the axons to the outside proceed to the same hemisphere. This partial crossing means that if you focus straight ahead, everything to the left of center in the visual field is processed by the right hemisphere, while everything to the right of center is processed by the left hemisphere. This organization provides us with significant advantages when sensing depth, which we discuss later in the chapter. Beyond the optic chiasm, the visual pathways are known as optic tractsoptic tractsNerve pathways traveling from the optic chiasm to the thalamus, hypothalamus, and midbrain.            optic tracts        Nerve pathways traveling from the optic chiasm to the thalamus, hypothalamus, and midbrain.             (see Figure 5.9). About 90% of the axons in the optic tracts synapse in the thalamus. The thalamus sends information about vision to the amygdala and the primary visual cortex in the occipital lobe. The amygdala uses visual information to make quick emotional judgments, especially about potentially harmful stimuli. The remaining optic tract fibers connect with the hypothalamus, where their input provides information about light needed to regulate sleep–wake cycles, discussed in Chapter 6, or with the superior colliculi of the midbrain, which manage a number of visually guided reflexes, such as changing the size of the pupil in response to light conditions. Visual information from the retina travels to the thalamus and then to the primary visual cortex in the occipital lobe. The primary visual cortex begins, but by no means finishes, the processing of visual input. The primary visual cortex responds to object shape, location, movement, and color (Hubel & Livingstone, 1987;  Hubel & Wiesel, 1959;  Livingstone & Hubel, 1984). Two major pathways radiating from the occipital cortex into the adjacent temporal and parietal lobes continue the analysis of visual input. The parietal pathway helps us process movement in the visual environment. The temporal pathway responds to shape and color and contributes to our ability to recognize objects and faces. To see something requires the brain to interpret the information gathered by the eyes. How do you know your sweater is red or green, based on the information sent from the retina to the brain? How do you recognize your grandmother at your front door? Most of us think about colors in terms of the paints and crayons we used in elementary school. Any kindergartner can tell you that mixtures of red and yellow make orange, red and blue make purple, and yellow and blue make green. Mixing them all together produces a lovely muddy brown. Colored lights, however, work somewhat differently (see Figure 5.10). The primary colors of light are red, green, and blue, and mixing them together produces white light, like sunlight. If you have ever adjusted the color on your computer monitor or television, you know that these devices also use red, green, and blue as primary colors. Observations supporting the existence of three primary colors of light gave rise to a trichromatic theorytrichromatic theoryA theory of color vision based on the existence of different types of cones for the detection of short, medium, and long wavelengths            trichromatic theory        A theory of color vision based on the existence of different types of cones for the detection of short, medium, and long wavelengths             of color vision. The primary colors of paint might be red, yellow, and blue, but in the world of light, the primary colors are red, green, and blue. Trichromatic theory is consistent with the existence of three types of cones in the retina that respond best to short (blue), medium (green), or long (red) wavelengths. Our ultimate experience of color comes not from the response of one type of cone but from comparisons among the responses of all three types of cones (see Figure 5.11). Our perception of color results from a comparison of the responses of the red, green, and blue cones to light. A 550-nanometer light is perceived as yellow and produces a strong response in green cones, a moderate response in red cones, and little response in blue cones. Color deficiency occurs when a person has fewer than the typical three types of cones. We no longer use the term colorblind, as this is not accurate. Most people with color deficiencies see color differently than someone with all three cone types. Very rarely, individuals have either one type of cone or none. To these people, the world appears to be black, white, and gray. Trichromatic theory does a good job of explaining color deficiency, but it is less successful in accounting for other color vision phenomena, such as color afterimages. For example, if you stare at the yellow, green, and black flag in Figure 5.12 and then focus on the dot within the white rectangle to the right, you will see an afterimage of the American flag in its more traditional colors of red, white, and blue. If you stare at the dot in the center of the yellow, green, and black flag for a minute and then shift your gaze to the dot in the white space on the right, you should see the flag in its traditional red, white, and blue. An opponent process theoryopponent process theoryA theory of color vision that suggests we have a red-green color channel and a blue-yellow color channel in which activation of one color in each pair inhibits the other color            opponent process theory        A theory of color vision that suggests we have a red-green color channel and a blue-yellow color channel in which activation of one color in each pair inhibits the other color             of color vision does a better job than the trichromatic theory in explaining these color afterimages. This theory proposes the existence of color channels: a red–green channel and a blue–yellow channel. We cannot see a color like reddish green or bluish yellow because the two colors share the same channel. The channels are  opponent  or competing. Activity in one color group in a channel reduces activity in the other color group. Returning to our green, yellow, and black flag, how can we use opponent process theory to explain our experience of the red, white, and blue afterimage? By staring at the flag, you fatigue some of your visual neurons. Because the color channels compete, reducing activity in one color group in a channel, such as green, increases activity in the other group, which is red. Fatiguing green, black, and yellow causes a rebound effect in each color channel, and your afterimage looks red, white, and blue. (Black and white also share a channel.) If you stare at an image of a real red, white, and blue flag and then look at a white piece of paper, your afterimage looks like our green, black, and yellow illustration. Which of these two theories of color vision, trichromatic theory or opponent process theory, is correct? The trichromatic theory provides a helpful framework for the functioning of the three types of cones in the retina. However, as we move from the retina to higher levels of visual analysis, the opponent process theory seems to fit observed phenomena neatly. Both theories help us understand color vision but at different levels of the visual system. Monty Roberts, known as the Horse Whisperer, attributes his abilities to observe horse behavior to his complete lack of color vision. This condition is quite rare, occurring in only 1 person out of every 30,000. Now that you have an understanding of color perception, we can consider one of the practical problems associated with individual differences in color vision. Between 7% and 10% of males and about 0.4% of females have a form of red–green color deficiency (see Figure 5.13). Males are more affected than females because the genes for the pigments used by red and green cones are located on the X chromosome, making red–green color deficiency a sex-linked condition (see Chapter 3). Smaller numbers of people lack blue cones (0.0011%) or cones altogether (0.00001%). Given the frequency of color deficiency, making visual materials accessible to people with all types of color vision is a serious concern. The Ishihara Color Test, designed by Shinobu Ishihara in 1917, is a standard method for detecting color deficiency. The test is printed on special paper, so the recreated image here would not be considered a valid basis for diagnosing color deficiency. Color can be an effective tool for designing exciting and engaging websites, but many graphic web designers, who typically have excellent vision, fail to consider how the site might look to a person with a color deficiency. One clue for designing an accessible site can be found in other systems based on color, such as traffic lights. Although most of us rely on the color information from the red, yellow, or green lights, the lights also vary in location. In other words, color should never be the only basis for extracting meaning. A second major concern is contrast, which we discuss in the next section. The strong contrast between black letters on the white pages makes text easy to read for most people. Colored text against a colored background might add interest, but it runs the risk of being harder to read, especially when reds and greens are used. As shown in Figure 5.14, online resources simulate how a web page looks to a person with color deficiency, which helps designers maximize accessibility. Web designers have found colors that work for people with typical color vision and people who have color deficiency. This set of colors shows how different shades would be seen by people with typical vision and by people with three of the most common forms of color deficiency. Even though these colors are seen differently by the three groups, nobody mistakes one shade for another. We asked earlier how your brain uses incoming visual signals to recognize your grandmother. A bottom-up approach assumes that as information moves from the retina to higher levels of visual processing, more complicated responses are built from simpler input. In this hierarchical model, the result would be a hypothetical  grandmother cell,  or a single cell that could combine all previous input and processing to tell you that your grandmother is at the door. Although the hierarchical model is attractive in many ways, it does not fit perfectly with what we know about the visual system. First, we would need a large number of single neurons to respond to all the objects and the events that we can recognize visually. In addition, the hierarchical model is unable to account for top-down processing. Figure 5.15a may appear to be a random pattern of black dots on a white background. Figure 5.15b may not appear to be a recognizable object at all. The sensations produced by these stimuli lead to no meaningful perceptions. However, once we tell you that the first image is a Dalmatian dog and the second image is a cow, you can instantly pick out their shapes. Now that you know what the images are, you will probably never see them again the way you did initially. Recognizing these objects requires knowledge and memory of what Dalmatians and cows look like. It is unlikely that a single cortical cell acting as a Dalmatian or cow detector could incorporate such complex inputs from memory.(a) This might look like a splattering of black dots on a white page until you learn that it represents a Dalmatian dog. (b) Top-down processing ensures that once you know this is a photo of a cow, you can pick out its features easily. If we don’t use single cells to recognize objects, how can we accomplish this task? The visual system might perform a mathematical analysis of the visual field (De Valois & De Valois, 1980). While the hierarchical model implies a reality built out of individual bars and edges, the mathematical approach suggests that we analyze patterns of lines. The simplest patterns of lines are gratings, as shown in Figure 5.16. Gratings can vary along two dimensions: frequency and contrast. High-frequency gratings have many bars in a given distance and provide fine detail, while low-frequency gratings have relatively few bars. High-contrast gratings have large differences in intensity between adjacent bars, like black next to white. The print you are reading in is an example of high contrast because the black letters are quite different from the white background. Low-contrast gratings have subtler differences in intensity between bars, such as dark gray next to black. An alternative to the hierarchical model suggests that the visual system analyzes the visual environment as a collection of patterns, like these gratings. Gratings vary in frequency (number of bars in a given distance) and contrast (the difference in light intensity from one bar to the next). Observing responses to gratings gives us a window into the visual capacities of other species. At a certain point of contrast and frequency, gratings look plain gray. Animals can be trained to make a distinction between gratings and gray circles. For example, if a bird is rewarded with food for pecking at a disk with a grating but not for pecking a uniform gray disk, any performance that is better than 50-50, or chance, indicates that the bird can see the difference between the grating and the gray. We can graph the range of gratings that are visible to the observer as a function of their contrast and frequency. Figure 5.17 illustrates the visible ranges for human adults and cats. Compared to human adults, cats see less detail. However, cats see large (low-frequency), low-contrast objects better than humans do. Large, low-contrast shadows on the wall may get kitty’s attention but not yours. You will think kitty is chasing ghosts again. Using gratings, we get a window into the visual world of the cat. By comparing gratings to a uniform gray disk, we can learn when a grating with a certain contrast and frequency simply looks gray to humans or cats. We can see better detail than kitty, but she sees large shadows that we don’t even notice. As we observed in Chapter 1, a group of German researchers known as the Gestalt psychologists tackled visual perception with a number of ingenious observations. The word Gestalt is derived from the German word for  shape.  These psychologists objected to efforts by Wilhelm Wundt and the structuralists to reduce human experience to its building blocks, or elements. Instead, the Gestalt psychologists argued that some experiences lose information and value when divided into parts. The main thesis of the Gestalt psychologists, as stated by Kurt Koffka, maintains,  It is more correct to say that the whole is something else than the sum of its parts  (Koffka, 1935, p. 176). According to the Gestalt psychologists, we are born with built-in tendencies to organize incoming sensory information in certain ways. This natural ability to organize simplifies the problem of recognizing objects (Biederman, 1987). One organizing principle suggests that we spontaneously divide a scene into a main figure and ground. We frequently assume that the figure stands in front of most of the ground, and it seems to have more substance and shape. It is possible to construct ambiguous images, like the vase on this page, in which the parts of the image seem to switch roles as figure or ground. The Gestalt psychologists believe we naturally see the difference between objects and their background, but this figure is designed to make us switch back and forth from the vase to the background faces. This vase was designed to commemorate an anniversary of Queen Elizabeth II of England (face on the right) and her husband, Prince Philip (face on the left). A second Gestalt principle is proximity (see Figure 5.18). Objects that are close together tend to be grouped together. The dots that make up our Dalmatian in Figure 5.17a are close together, suggesting they belong to the same object. The principle of similarity states that similar stimuli are grouped together. On a close examination of the dog image, the dots that make up the dog are similar to one another and slightly different (more rounded perhaps) than the dots making up the remainder of the image. The set of dots in (a) do not appear to have any particular relationship with one another, but when we color rows in (b), we suddenly see the dots in columns or rows. Moving two columns or rows slightly closer to each other in (c) makes us see the array differently too. The principle of continuity suggests that we assume that points that form smooth lines when connected probably belong together (see Figure 5.19). In our dog picture, continuity helps us see the border of the curb or sidewalk and the ring of shadow around the base of the tree. Continuity is perhaps a little less useful in identifying the dog, although we can pick out the lines forming the legs. The Gestalt principle of continuity says that we perceive points forming a smooth line as belonging to the same object. If you follow this knot, you can see that it is formed by two objects, but our initial perception is of a single form. Closure occurs when people see a complete, unbroken image even when there are gaps in the lines forming the image (see Figure 5.20). We use this approach in viewing the dog in Figure 5.15a when we  fill in the blanks  formed by the white parts of its body. Because of the principle of closure, we  fill in the blanks  to see a single object, the World Wildlife Fund logo, although it is made up of several objects. Finally, the Gestalt psychologists believed in the principle of simplicity, which suggests that we will use the simplest solution to a perceptual problem. This principle may help explain the fun in pictures like that of our Dalmatian dog. It is simpler to assume that this is a random splash of black dots on white background. Finding a hidden picture within the dots is not the simplest solution, which may account for our surprise. An image projected onto the retina is two dimensional, as flat as the sheet of paper or screen on which these words appear. Somehow, the brain manages to construct a three-dimensional (3D) image from these data. Adelbert Ames constructed a room that was named in his honor, the Ames Room, which illustrated vulnerabilities in our depth perceptiondepth perceptionThe ability to use the two-dimensional image projected on the retina to perceive three dimensions            depth perception        The ability to use the two-dimensional image projected on the retina to perceive three dimensions             (Ittleson, 1952). When viewed directly from the front, the room appears to be a rectangle. People within the room, shown in Figure 5.21, seem to be larger or smaller than normal. This distortion of perceived size results from the room’s ability to confuse our judgment of distance. Many distance cues, such as the apparently rectangular windows, conspire to make these two people look different. The person on the right is much closer to us than the person on the left. The diagram shows the actual layout of the Ames Room. To construct a 3D image, we use both monocular cuesmonocular cuesA depth cue that requires the use of only one eye            monocular cues        A depth cue that requires the use of only one eye             (one eye) and binocular cuesbinocular cuesA depth cue that requires the use of both eyes            binocular cues        A depth cue that requires the use of both eyes             (two eyes). Many monocular cues are found in paintings because the artists attempt to provide an illusion of depth in their two-dimensional pieces. The use of linear perspective, or the apparent convergence of parallel lines at the horizon, by Italian artists during the 15th century provided a realism unknown in earlier works. Linear perspective revolutionized the video game and movie industries, beginning humbly with Sega’s Zaxxon in 1982 and advancing to the ever more realistic environments of Halo, Pixar’s animated films, and the 2009 film Avatar. Other monocular cues include texture gradients and shading. We can see more texture in objects that are close to us, while the texture of distant objects is relatively blurry. Shading and the use of highlights can be used to suggest curved surfaces. Among the most powerful monocular depth cues is occlusion, or the blocking of images of distant objects by closer objects. We also use relative size to judge the distance of objects, although this method requires you to be familiar with the real size of an object. We know how big people are. When the retinal image of a person is small, we infer that the person is farther from us than when the retinal image of a person is larger. Several illusions result from our use of monocular cues to judge depth. Relative size helps to explain the moon illusion. You may have noticed that the moon appears to be larger when it is just above the hills on the horizon than when it is straight overhead. The moon maintains a steady orbit 239,000 miles (385,000 km) above the Earth. How can we account for the discrepancy in its apparent size? When viewed overhead, the moon is seen without intervening objects, such as trees and hills, that might provide cues about its size and distance. However, when viewed near the horizon, we see the moon against a backdrop of familiar objects whose sizes we know well. We expect trees and hills to be smaller at the horizon than when they are close to us, and if we group the moon with those objects, we adjust its apparent size as well. The next time you are viewing the full moon as it rises over the hills, form a peephole with your hand, and you will see the moon in its normal small size. Although some researchers argue that atmospheric differences between the two viewpoints may contribute to the illusion, viewing the moon through your hand should demonstrate that most of the effect arises from your use of other objects to judge distance. In the Müller–Lyer illusion, shown in Figure 5.22, we see the line with outward-pointing arrowheads as being farther from our position, even though the main lines project images of equal length on the retina. The Ponzo illusion, shown in Figure 5.23, confounds size and distance judgments in a similar fashion. The parallel lines signal depth, leading us to believe that the upper horizontal line is farther away than the lower line. If both lines project the same image on the retina, the more distant line must be longer. You might find it hard to believe that the two red vertical lines are actually the same length. We perceive depth because of linear perspective, which in turn make us see the upper horizontal bar as more distant than the lower bar. Even though they are the same length, the bar perceived as more distant looks longer. So far, we have discussed monocular cues that involve a person and a scene that is not moving. The introduction of motion can heighten the impression of depth. As you ride in a car, focus your gaze at a distant point. The objects you pass will appear to be moving in the opposite direction of your car, with closer objects appearing to move faster than distant objects. Next, focus on a point about midway between you and the horizon. Now, the closer objects will continue to move in the opposite direction, but more distant objects appear to be traveling with you. This motion parallax has been used to enhance the 3D feeling in video games. One of our most effective depth cues is retinal disparityretinal disparityThe difference between the images projected onto each eye            retinal disparity        The difference between the images projected onto each eye            . Because this cue requires the use of both eyes, we refer to retinal disparity as a binocular cue. Predator species, including ourselves, have eyes placed in the front of the head facing forward. As a result of this configuration, the visual scenes observed by the two eyes are different and overlapping, as shown in Figure 5.24. The differences between the images projected onto each eye are called disparities. These disparities do not tell us how far away an object is. Instead, they provide information about the relative distance between two objects in the visual field. As the distance between the objects increases, disparity increases. To illustrate the sensitivity of this system, you can identify an object as being 1 millimeter (about .04 inch) closer than another at a distance of 1 meter (about 3.3 feet) from your body, or a difference of 0.1% (Blake & Sekuler, 2006). The right and left eyes see slightly overlapping versions of the visual scene in front of us. We can use the retinal disparity, or discrepancy between the locations of two objects on the two retinas, as a sensitive depth cue. Why would this binocular depth system be an advantage to predators? Most prey species do an excellent job of hiding, often aided by an appearance that blends into the nearby environment. However, retinal disparity allows us to spot tiny variations in the depths of objects in the visual field. This might make an animal stand out against its background, even though it is well camouflaged. Retinal disparity has been used to identify camouflaged military equipment and counterfeit currency. Retinal disparity is imitated by cameras used to film 3D movies, which have two lenses separated by about the same distance as our two eyes. Although human infants can’t report what they see, we can take advantage of their longer gazing at patterns than at uniform stimuli, like a patch of a single color. This allows us to construct graphs of the contrasts and the frequencies to which children respond, similar to those we saw previously for cats. Based on these analyses, we know that human infants see everything human adults see but with less detail. To see well, the infant also needs more contrast than the adult. These findings help explain children’s preferences for large, high-contrast objects. Video games, such as Minecraft, incorporate many monocular cues to provide an experience of depth. The standard-sized blocks used to build structures in the game are separated by lines, which when placed in a row provide linear perspective. Texture gradients, shading, and relative size (players understand the size of the blocks well) also contribute to perceived depth. The two lenses of 3D cameras are separated by about 2 inches (about 50 millimeters), mimicking the distance between two human eyes that makes retinal disparity possible. The photographs shown below provide insight into the visual world of the infant. Frequencies that cannot be seen by the infant have been removed from each photograph. Other research shows that infants as young as 4 months not only show binocular disparity, but also show normal adult responses to color (Bornstein, Kessen, & Weiskopf, 1976). Other depth cues discussed previously develop early too. Infants as young as 2 months understand occlusion (Johnson & Aslin, 1995), and the use of the relative size of objects to judge depth appears between the ages of 5 and 7 months (Granrud, Haake, & Yonas, 1985). Infants’ abilities to perceive faces also develop quite rapidly, as 2-day-old newborns spend more time gazing at their mothers’ faces than at a stranger’s face (Bushnell, 2001; also see Chapter 11). Predictable changes occur in other aspects of human vision as we grow older. Accommodation of the lens, which allows us to change focus from near to far objects, becomes slower beginning in middle adulthood. Older adults respond more slowly to changes in brightness, such as leaving a dark theater into the sunlight. The muscles of the iris lose their elasticity, so pupils remain smaller, further reducing vision by limiting the amount of light that enters the eye. The lens of the eye begins to yellow, which protects the eye from ultraviolet radiation but affects the perception of color. At any age, individual differences shape what people see. In addition to the color deficiencies discussed previously, people differ in their abilities to see near and far objects. Those who deviate from the average often wear corrective lenses or undergo laser surgery to reshape the cornea. The most common visual problems result from eyeball length, with elongated eyeballs interfering with a person’s vision for distant objects (nearsightedness) and shortened eyeballs interfering with vision for close-up objects (farsightedness), as in reading. Vision is also affected by astigmatism, which means that the surface of the cornea is uneven. You can test yourself for astigmatism by looking at Figure 5.25. We can filter out the frequencies and contrast that a baby cannot see to simulate what the world looks like to an infant. If you have astigmatism, which results from an uneven surface of your corneas, some spokes of this figure will appear darker than others. In addition to deficits in social skills and language (see Chapter 14), individuals with autism spectrum disorder (ASD) often demonstrate unusual responses to stimuli. In some cases, children seem to barely notice things, but in others, children seem to be hypersensitive to stimulation. Psychologists agree that diagnosing autism as early as possible leads to the best treatment outcomes. Identifying differences in sensation and perception at early ages might lead to better diagnoses (Gliga et al., 2015). The Question: Do superior visual search skills predict autism? Eighty-two infants with older siblings diagnosed with ASD (high-risk) and 27 control infants with no relatives with ASD took part in a visual search task and measures of ASD risk (response to name, eye contact, social reciprocity, and imitation). Eye-tracking was used to measure the child’s attending to the  odd  character within a visual array (Figure 5.26). Children were assessed at 9 months, 15 months, and 2 years of age. Eye-tracking showed that 9-month-old infants at risk for autism spectrum disorder (ASD) were able to find the  odd  character within each display faster than children at low risk for ASD. Performance on this visual search task predicted the severity of autism symptoms at the ages of 15 months and 2 years. Children enjoy extra protection when they serve as research participants. Researchers must present parents with a thorough informed consent form, and great care must be taken to avoid coercion through rewards for participation. Procedures must be suited to the young participants to avoid fatigue. Parents whose children show evidence of risk for ASD should be referred to services. Superior performance on the visual search task at 9 months was correlated with indicators of ASD symptoms at 15 months and 2 years of age. Assessments of visual search might prove useful in identifying infants who may be diagnosed with autism spectrum disorder. The results also emphasize the role of attention and perception in the development of ASD. Previously, much research about ASD has focused on social deficits, but these might in fact emerge from more basic differences in information processing. Psychologists have discovered a number of differences in the ways that East Asians (e.g., people from China, Korea, and Japan) and European/North Americans process information. In general, Eastern cultures process information holistically, which means processing is based on context and relationships. In contrast, Western cultures process information analytically, focusing on salient objects and forming categories. To illustrate this difference, consider the following images (Chiu, 1972). Of the three images in each box, think about which go together, and ask yourself why you think this is correct. Children from East Asian cultures were most likely to group the woman shape with the baby shape and the cow with the grass, whereas children from Western cultures grouped the man and woman shape and the cow and chicken shape. The East Asian children reported grouping the woman and the baby together because the mother takes care of the baby (relationship), whereas the Western children grouped the man and woman together because they shared a category as adults. East Asian children grouped the cow and grass together (context: the cow eats grass), whereas the Western children grouped the chicken and cow together (both are animals). Further research demonstrated that all children begin by using holistic reasoning regardless of culture, but Western children begin to ignore context and focus more on objects beginning around the age of 5 years. What possible aspects of culture might shape thinking in this way? It is possible that child-rearing practices, such as the emphasis on appropriate social behavior in East Asian cultures versus the emphasis on labeling objects in Western culture, could lead to a divergent emphasis on context or object. The roots of this type of thinking might go even deeper. East Asians and Westerners have been found to use different eye movement strategies while scanning faces (Kelly, Miellet, & Caldara, 2010). Eye-tracking shows that Westerners focus on the eye and mouth regions of a face, while East Asians focus on the nose area (Figure 5.27a and b). This difference might arise from social norms. Eye contact is considered rude in East Asian cultures but is expected in Western cultures. However,  Kelly et al. (2010) found the same cultural differences when participants looked at the faces of sheep and at artificial figures known as Greebles.(a) Which Ones Go Together? Western children group the man and woman shapes and the cow and chicken shapes using categorical reasoning. East Asian children group the woman and baby shapes and the cow and grass together based on relationships and context. (b) Western and East Asian Participants Use Different Face Scanning Approaches Eye-tracking shows that Western participants focus on the eyes and the mouth of a face, whereas East Asian participants focus on the nose. To control for possible social norm influences (eye contact is considered rude in some East Asian cultures), the researchers investigated scanning approaches using sheep and make-believe stimuli known as Greebles. The same principles held for these alternate stimuli, possibly reflecting cultural differences in the emphasis on objects and context. While researchers often attribute these processing differences to the effects of living within an individualistic (Western) versus collectivistic (Eastern) culture, we cannot rule out the effects of genetics and biology without further research. Repeating these studies with second or third generation Asian Americans, for example, might provide insight into the extent of the cultural influences. Anyone can tell, simply by looking, whether two tabletops are the same shape, right? For instance, most people would agree that the shapes and sizes of the two tabletops depicted in Figure 5.28 are different. One tabletop appears to be rectangular, while the other appears to be more square. As much as our perceptions may tell us otherwise, these tabletops are identical. To verify this, trace one of the tabletops, and then rotate it to place it above the other. You will be able to prove that these tables have identical tops. It is not our eyes (sensory receptors) that deceive us; it is our brains. You will probably want to use a ruler to prove to yourself that the lines marked  a  are the same and that the lines marked  b  are the same. Roger Shepard combined a profound scientific curiosity with a love of mischief. Shepard developed a number of creative visual illusions, including  Turning the Tables,  which he illustrated himself (Shepard, 1990). The visual illusion produced in this illustration results from our use of a visual system designed to cope with the three dimensions of the physical world on stimuli that have only two dimensions. In Roger Shepard’s words:The drawings … achieve their effects by means of various visual tricks. But to call them tricks is not to imply that they are without psychological significance. The tricks work by taking advantage of fundamental perceptual principles that have been shaped by natural selection in a three-dimensional world. Our ability to make pictures, which emerged only recently on an evolutionary time scale, enables us to present the eyes with visual patterns that systematically depart from the patterns that we and our ancestors experienced in nature. In considering the ways pictures can trick the eye, we can gain insight into the nature and ultimate source of the principles of visual perception. (Shepard, 1990, p. 121)More generally, examples like this underscore the importance of relying on scientific investigation and evidence to unveil how sensation and perception work. FeatureSignificanceCorneaBends light toward the retina. PupilForms an opening in the iris. LensFocuses light onto the retina. RetinaContains rods, cones, and other visual neurons in its layer of cells. Fovea (area of the retina)Processes detailed vision. ThalamusActs as the target for most axons forming the optic tracts. Primary visual cortex (area in the occipital lobe)Receives visual input from the thalamus and performs initial analysis of input. We have spent a considerable amount of time on the sense of vision, which might be considered a dominant source of information for humans. However, when Helen Keller, who was both blind and deaf, was asked which disability affected her the most, she replied that blindness separated her from things, while deafness separated her from people. AuditionAuditionThe sense of hearing.            Audition        The sense of hearing.            , our sense of hearing, not only allows us to identify objects in the distance but also plays an especially important role in our ability to communicate with others through language. Sound begins with the movement of an object, setting off waves of vibration in the form of miniature collisions between adjacent molecules in air, liquid, or solids. Because sound waves require this jostling of molecules, sound cannot occur in the vacuum of space, which contains no matter. Those explosions we enjoy in Star Wars films are great entertainment but not good science. Earlier in this chapter, we described light energy as waves with different amplitudes and frequencies. Sound waves possess the same dimensions. However, in the case of sound, the height or amplitude of the wave is encoded as loudness or intensity and the frequency of the wave is encoded as pitch. High-amplitude waves are perceived as loud, and low-amplitude waves are perceived as soft. High-frequency waves (many cycles per unit of time) are perceived as high pitched, whereas low-frequency sounds are low pitched. In sound, amplitude is measured in units called decibels (dB), and frequency is measured in cycles per second, or hertz (Hz; see Figure 5.29). Like the light energy we see, sound waves are characterized by frequency and amplitude. We perceive frequency as the pitch of the sound (high or low), measured in hertz (Hz), and we perceive amplitude as the loudness of the sound, measured in decibels (dB). In addition to dizziness and nausea, being exposed to infrasound makes people report feelings of chills down the spine, fear, and revulsion, even though they cannot consciously detect the sound. Some scientists believe that infrasound produced in certain places leads people to conclude the places are haunted. As we observed in the case of the light spectrum, parts of the auditory spectrum are outside the range of human hearing. Ultrasound stimuli occur at frequencies above the range of human hearing, beginning around 20,000 Hz (see Figure 5.30). Ultrasound can be used to clean jewelry or your teeth or to produce noninvasive medical images. Infrasound refers to frequencies below the range of human hearing, or less than 20 Hz. Many animals, including elephants and marine mammals, use infrasound for communication. Infrasound is particularly effective in water because it allows sound to travel long distances. Ultrasounds are above the range of human hearing, and infrasounds are below the range of human hearing. Human audition begins with an ear located on either side of the head. The components that make up the ear are divided into three parts: the outer ear, the middle ear, and the inner ear (see Figure 5.31). The human ear is divided into the outer, middle, and inner ear. The fetus has no bubble of air in the middle ear, having never been exposed to air. Because fluids do a better job than air of transmitting sound waves, there is good evidence that the fetus can hear outside sounds, such as mother’s voice, quite well during the final trimester of pregnancy. The outer ear consists of the structures that are visible outside the body. The pinna, the outer visible structure of the ear, collects and focuses sounds, like a funnel. In addition, the pinna helps us localize sounds as being above or below the head. Sounds collected by the pinna are channeled through the auditory canal, which ends at the tympanic membrane, or eardrum, at the boundary between the outer and the middle ear. The boundary between the middle and the inner ear is formed by another membrane, the oval window. The gap between these two membranes is bridged by a series of tiny bones. The purpose of these bones is to transfer sound energy from the air of the outer and middle ear to the fluid found in the inner ear. Sound waves are weakened as they move from air to water. When you try to talk to friends underwater, the result is rather garbled. Without the adjustments provided by these small bones, we would lose a large amount of sound energy as the sound waves moved from air to liquid. The inner ear contains two sets of fluid-filled cavities embedded in the bone of the skull. One set is part of the vestibular system, which we will discuss later in this chapter. The other set is the cochleacochleaThe structure in the inner ear that contains auditory receptors.            cochlea        The structure in the inner ear that contains auditory receptors.            , from the Greek word for  snail.  When rolled up like a snail shell, the human cochlea is about the size of a pea. It contains specialized receptor cells that respond to vibrations transmitted to the inner ear. The cochlea is a complex structure, which is better understood if we pretend to unroll it (see Figure 5.32). The cochlea may be divided into three parallel chambers divided from one another by membranes. Two of these chambers, the vestibular canal and the tympanic canal, are connected at the apex of the cochlea, or the point farthest from the oval window. Vibrations transmitted by the bones of the middle ear to the oval window produce waves in the fluid of the vestibular canal that travel around the apex and back through the tympanic canal. Lying between the vestibular and the tympanic canals is the cochlear duct. The cochlear duct is separated from the tympanic canal by the basilar membranebasilar membraneMembrane in the cochlea on which the organ of Corti is located.            basilar membrane        Membrane in the cochlea on which the organ of Corti is located.            . Resting on top of the basilar membrane is the organ of Cortiorgan of CortiA structure located on the basilar membrane that contains auditory receptors.            organ of Corti        A structure located on the basilar membrane that contains auditory receptors.            , which contains many rows of hair cells that transduce sound energy into neural signals. Each human ear has about 15,500 of these hair cells. Sound waves produce peak responses on the basilar membrane according to their frequencies. Like the strings on a musical instrument, high tones produce the greatest response at the narrow, stiff base of the basilar membrane, while low tones produce the greatest response at the wide, floppy part of the basilar membrane near the apex. Sound waves travel through the cochlea from the oval window, around the apex, and back to the round window. The waves cause movement of tiny hair cells in the cochlear duct, which we perceive as sound. As waves travel through the cochlea, the basilar membrane responds with a wavelike motion, similar to the crack of a whip. The movement of the basilar membrane causes the hair cells of the organ of Corti to move back and forth within the fluid of the cochlear duct. Bending the hair cells stimulates the release of neurotransmitters onto the cells of the auditory nerveauditory nerveThe nerve carrying sound information from the cochlea to the brain.            auditory nerve        The nerve carrying sound information from the cochlea to the brain.            . The basilar membrane needs to move very little before the hair cells are stimulated. If the hairlike structures extending from the top of the hair cells were the size of the Eiffel Tower in Paris, the movement required to produce a neural response would be the equivalent of 1 centimeter, about 0.4 inch (Hudspeth, 1983). As we mentioned earlier, hair cells stimulate axons forming the auditory nerve. One branch of each auditory nerve cell makes contact with the hair cells, while the other branch proceeds to the medulla of the brainstem. From the medulla, sound information is sent to the midbrain, which manages reflexive responses to sound, such as turning toward the source of a loud noise. In addition, the midbrain participates in sound localization, or the identification of a source of sound. The movement of tiny hair cells in the inner ear produces neural signals that travel to the brain. The midbrain passes information to the thalamus, which in turn sends sound information to the primary auditory cortex, located in the temporal lobe. The primary auditory cortex conducts the first basic analysis of the wavelengths and amplitudes of incoming information (see Figure 5.33). Surrounding the primary auditory cortex are areas of secondary auditory cortex that respond to complex types of stimuli, like clicks, noise, and sounds with particular patterns. The auditory cortex is located in the temporal lobe. The primary auditory cortex processes basic features of sound while the surrounding secondary auditory cortex processes more complex sounds, such as clicks and general noise. Now that we have an understanding of the structures and the pathways used to process the sensations that lead to the perception of sound, we turn our attention to the brain’s interpretation and organization of these sounds in terms of pitch, loudness, and spatial localization. Perception of pitch begins with the basilar membrane of the cochlea (see Figure 5.32). Place theory suggests that the frequency of a sound is correlated with the part of the basilar membrane showing a peak response. The base of the basilar membrane, closest to the oval window, is narrow and stiff. In contrast, at its farthest point near the apex, the basilar membrane is wide and flexible. If you are familiar with stringed instruments like guitars, you know that high tones are produced by striking the taut, small strings, and low tones are produced by striking the floppy, wide strings. The same principle holds for the basilar membrane. High-frequency tones produce the maximum movement of the basilar membrane near the base, while low-frequency tones produce maximum movement near the apex. The hair cells riding above these areas of peak movement show a maximum response. Place theory works well for sounds above 4,000 Hz, which is about the frequency produced by striking the highest key on a piano, C8. Below frequencies of 4,000 Hz, the response of the basilar membrane does not allow precise localization. In these cases, we appear to use another principle described as temporal theory, in which the patterns of neural firing match the frequency of a sound. Humans can perceive sounds that vary in intensity by a factor of more than 10 billion, from the softest sound we can detect up to the sound made by a jet engine at takeoff, which causes pain and structural damage to the ear. Table 5.2 identifies the intensity levels of many common stimuli, measured in the logarithmic decibel scale. Our perception of loudness does not change at the same rate as actual intensity. When the intensity of a sound stimulus is 10 times greater than before, we perceive it as being only twice as loud (Stevens, 1960). Source of SoundIntensity (measured in decibels, or dB)Threshold of hearing0 dBRustling leaves10 dBWhisper20 dBNormal conversation60 dBBusy street traffic70 dBVacuum cleaner80 dBWater at the foot of Niagara Falls90 dBiPod with standard earbuds100 dBFront rows of a rock concert110 dBPropeller plane at takeoff120 dBThreshold of pain/machine gun fire130 dBMilitary jet takeoff140 dBInstant perforation of the eardrum160 dBThe frequency of a sound interacts with our perception of its loudness. Humans are maximally sensitive to sounds that normally fall within the range of speech, or between 80 and 10,000 Hz (see Figure 5.34). Sounds falling outside the range of speech must have higher intensity before we hear them as well. One feature that distinguishes an expensive sound system from a cheaper model is its ability to boost frequencies that fall outside our most sensitive range. These functions plot the results of allowing participants to adjust the intensity of different tones until they sound equally loud. Each curve represents the intensity (dB) at which tones of each frequency match the perceived loudness of a model 1000 Hz tone. The stars indicate that a 100 Hz tone at 60 dB sounds about as loud as a 1000 Hz tone at 40 dB because they fall on the same line. Low frequencies are usually perceived as quieter than high frequencies at the same level of intensity. We are especially sensitive to frequencies found in speech. The pinna helps us localize sounds in the vertical plane, or in space above or below the head. Our primary method for localizing sound in the horizontal plane (in front, behind, and to the side) is to compare the arrival time of sound at each ear. As illustrated in Figure 5.35, the differences in arrival times are quite small, between 0 milliseconds for sounds that are directly in front of or behind us to 0.6 millisecond for sounds coming from a source perpendicular to the head on either side. Because arrival times for sounds coming from directly in front of or behind us are identical, it is difficult to distinguish these sources without further information. In addition to arrival times, we judge the differences in intensity of sounds reaching each ear. Because the head blocks some sound waves, a sound  shadow  is cast on the ear farthest from the source of sound. As a result, a weaker signal is received by this ear. We localize sound to the left and right by comparing the differences between the arrival times of the sounds to our two ears. Just as our visual systems can be fooled by certain types of input, our ability to localize sounds is influenced by interactions between vision and audition. Even before the invention of surround sound, which provides many effective sound localization cues, moviegoers perceived sound as originating from the actors’ lips, even though the speakers producing the sound are located above and to the sides of the screen. Our willingness to believe that the sound is coming from the actors’ lips probably results from our everyday experiences of watching people speak. The McGurk effect is an auditory illusion that occurs when we combine vision and hearing. In this demonstration, hearing  ba-ba  at the same time you see a person’s lips making  ga-ga  results in your perceiving  da-da. In our previous discussion of visual perception, we reviewed the grouping principles developed by Gestalt psychologists. Similar types of groupings occur in audition. Sounds from one location are grouped together because we assume they have the same source, whereas sounds identified as coming from different locations are assumed to have different sources. Sounds that start and stop at the same time are perceived as having the same source, while sounds with different starting and stopping times usually arise from separate sources, such as two voices in a conversation. Grouping plays an especially significant role in the perception of music and speech. In these cases, we see evidence of top-down processing as well, because our expectations for the next note or word influence our perceptions (Pearce, Ruiz, Kapasi, Wiggins, & Bhattacharya, 2010). Similarities between the processing of music and language have led researchers to argue for more music instruction in school to assist children with language learning (Strait, Kraus, Parbery-Clark, & Ashley, 2010). Hearing begins before birth and develops rapidly in human infants. Newborns as young as 2 days show evidence of recognizing their mother’s voice (DeCasper & Fifer, 1980) and respond preferentially to their native language (Moon, Cooper, & Fifer, 1993). Infants younger than 3 months show strong startle reactions to noise. By the age of 6 months, infants turn their heads in the direction of a loud or interesting sound. It is likely that their thresholds for sounds are nearly at adult levels by this age (Olsho, Koch, Halpin, & Carter, 1987). By the age of 1 year, children should reliably turn around when their name is called. An important developmental change in audition is age-related hearing loss. Hearing loss occurs first at higher frequencies. After the age of 30, most people cannot hear sounds above 15,000 Hz. After the age of 50, most people cannot hear above 12,000 Hz, and people older than 70 years have difficulty with sounds above 6,000 Hz. Because speech normally ranges up to 8,000 to 10,000 Hz, older adults might begin to have difficulty understanding the speech of others. Among individual differences in hearing is having perfect pitch, which means that you can name a musical tone that you hear. The brains of individuals with perfect pitch are structurally different from those of people who do not have this ability. Areas of the left hemisphere are larger in musicians with perfect pitch (Schlaug, Jancke, Huang, & Steinmetz, 1995). At the same time, extensive early musical training can shape the structure of the brain (Schlaug et al., 2009). Human culture and social life often provide a framework for the interpretation of stimuli. A dramatic example of this type of influence is our reaction to sine wave speech. To produce this stimulus, scientists artificially alter recordings of speech to resemble regular, repeating sine waves, as shown in Figure 5.36 (Davis, 2007). When people hear these artificial sounds without further instructions, they describe them as tweeting birds or other nonlanguage stimuli. However, if people are told the sounds represent speech, they suddenly  hear  language elements (Remez, Rubin, Pisoni, & Carell, 1981). Sine waves are regular and repetitive waveforms, such as the ones we included earlier to show how the height and frequency of light and sound waves are interpreted by the mind. Researchers can record speech sounds and transform the recordings into artificial sine waves, such as those in this image. If the sounds are played without information about their source, most people interpret the sounds as tweeting birds. However, if people are told that the recordings are language, they report  hearing  language, another example of top-down cognitive influences on perception. Sine wave speech shows us how culture in the form of experience with language can shape perception, but in other instances, perception can shape culture. For many people with hearing loss and for their families and friends, being deaf means something other than having a disability. Instead, deafness is viewed as a culture, complete with its own set of attitudes, language, and norms. American Sign Language (ASL) is viewed as being quite distinct from signed English and is difficult for signing people in Great Britain and Australia to understand (Mindess, 2006). SomatosensationSomatosensationThe body senses, including body position, touch, skin temperature, and pain.            Somatosensation        The body senses, including body position, touch, skin temperature, and pain.             (soma comes from the Greek word for  body ) provides us with information about the position and movement of our bodies, along with touch, skin temperature, and pain. Although these senses may not seem as glamorous as vision and hearing, we are severely disabled by their loss. You might think it would be a blessing to be born without a sense of pain, but people who have impaired pain reception often die prematurely because of their inability to respond to injury. Although unpleasant, pain tells us to stop and assess our circumstances, which might have promoted the survival of our ancestors. Unlike the visual and auditory stimuli we have discussed so far in this chapter, somatosensory stimuli arise from within the body or make contact with its surface. As a result, these stimuli provide an organism little time to react. We can deal with a predator seen or heard from a distance using strategies different from those we use for one that is touching us. Nonetheless, the somatosenses provide essential feedback needed for movement, speech, and safety. The vestibular system helps us maintain a steady view of the world, even when riding the most extreme roller coaster. The transition from walking on four legs to walking on two placed selective pressure on the evolution of primate vision and, to some extent, audition. By standing up on two legs, primates distanced themselves from many sources of information, such as smell. If you don’t believe us, try getting down on your hands and knees and smelling your carpet. This transition did not place the same evolutionary pressure on the human somatosenses, which work about the same way in us as they do in other animals. To begin our exploration of the somatosensory systems, we return to the inner ear. Adjacent to the structures responsible for encoding sound, we find the structures of the vestibular systemvestibular systemThe system in the inner ear that provides information about body position and movement.            vestibular system        The system in the inner ear that provides information about body position and movement.            , which provide us with information about body position and movement. The proximity of these structures to the middle ear, which can become congested because of a head cold, is often responsible for those rather unpleasant feelings of dizziness that accompany an illness. The receptors of the vestibular system provide information about the position of the head relative to the ground, linear acceleration, and rotational movements of the head. We sense linear acceleration when our rate of movement changes, such as when our airplane takes off. Like the cochlea, the vestibular receptors contain sensitive hair cells that are bent back and forth within their surrounding fluid when the head moves. When extensive movement stops suddenly, perhaps at the end of an amusement park ride, these fluids reverse course. You may have the odd sensation that your head is now moving in the opposite direction, even though you are sitting or standing still. The movement of these hair cells results in the production of signals in the auditory nerve, the same nerve that carries information about sound. These axons form connections in the medulla and in the cerebellum. You may recall from Chapter 4 that the cerebellum participates in balance and motor coordination, functions that depend on feedback about movement. In turn, the medulla receives input from the visual system, the cerebellum, and other somatosenses. This arrangement provides an opportunity to coordinate input from the vestibular system with other relevant information. The medulla forms connections directly with the spinal cord, allowing us to adjust our posture to keep our balance. Vestibular information travels from the medulla to the thalamus, the primary somatosensory cortex of the parietal lobe, and then the primary motor cortex in the frontal lobe. This pathway allows vestibular information to guide voluntary movement. In humans particularly, information from the vestibular system is tightly integrated with visual processing. As we move, it is essential that we maintain a stable view of our surroundings. To accomplish this task, rotation of the head results in a reflexive movement of the eyes in the opposite direction. This action should allow you to maintain a steady view of the world, even on the most extreme roller coaster. Touch provides a wealth of information about the objects around us. By simply exploring an object with touch, we can determine features such as size, shape, texture, and consistency. These judgments confirm and expand the information we obtain about objects through visual exploration. Touch is not only a means of exploring the environment. Particularly in humans, touch plays a significant role in social communication. Infants who are touched regularly sleep better, remain more alert while awake, and reach cognitive milestones at earlier ages (Ackerman, 1990). We hug our friends and loved ones to provide comfort, pat others on the back for a job well done, and shake hands to greet a colleague or conclude a deal. The contributions of the sense of touch to human sexuality are obvious. Our sense of touch begins with skin, the largest and heaviest organ in the human body. Embedded within the skin are several types of specialized neurons that produce action potentials whenever they are physically bent or stretched. Different types of receptors respond to certain features of a touch stimulus, such as pressure, vibration, or stretch (see Figure 5.37). In addition to their locations in the skin, receptors are located in blood vessels, joints, and internal organs. Unpleasant sensations from a headache or a too-full stomach or bladder originate from some of these receptors. Some receptor fibers wrap around hair follicles and respond whenever a hair is pulled or bent. Others, as we will see later in this section, participate in our senses of pain and skin temperature. Different receptors in the skin help us sense pressure, vibration, stretch, or pain. Information about touch travels from the skin to the spinal cord. Once inside the spinal cord, touch pathways proceed to the thalamus, along with input from the cranial nerves originating in the touch receptors in the skin of the face, the mouth, and the tongue. The thalamus transmits touch information to the primary somatosensory cortex, located in the parietal lobe. A map of the body’s representation in the primary somatosensory cortex, or a sensory homunculus ( little man ), is shown in the statue to the right. This odd figure demonstrates how areas of the body are represented based on their sensitivity rather than their size. Different species show different patterns of cortical organization for touch. Humans need sensitive feedback from the lips and the hands to speak and make skilled hand movements for tool use and other tasks. Rats devote a great deal of cortical real estate to whiskers, whereas lips have a high priority in squirrels and rabbits. A notable area that is missing from the homunculus is the brain, which has neither touch receptors nor pain receptors. We can only assume that for much of evolutionary history, intrusion into the brain was likely to be fatal. Consequently, there would be no advantage to  feeling  your brain. Because of the lack of somatosensation in the brain, neurosurgeons can work with an alert patient using local anesthesia for the skull and tissues overlying the brain. The surgery produces no sensations of pressure or pain. The representation of touch in the primary sensory cortex is plastic, which means that it changes in response to increases or decreases in input from a body part. Many individuals who lose a body part experience a phenomenon known as phantom limb, a term first used by a Civil War physician to describe his patients’ experience of pain from a missing limb. Phantom sensations can result from the reorganization of the somatosensory cortex following the loss of a body part (Borsook et al., 1998). In one case study, touching different parts of a patient’s face produced  feeling  from the patient’s missing hand (Ramachandran & Rogers-Ramachandran, 2000). When his cheek was touched, he reported feeling his missing thumb, along with the expected cheek, while touching his lip elicited feeling from the missing index finger, along with the normal lip sensations. In an even more bizarre example, a patient was embarrassed to report that he experienced a sensation of orgasm in his missing foot. The sensory homunculus illustrates the amount of representation each part of the body has in the sensory cortex. The human homunculus emphasizes the hands and face. Increased input also changes the organization of the somatosensory cortex. When monkeys were trained to use specific fingers to discriminate among surface textures to obtain food rewards, the areas of the cortex responding to the trained fingertips expanded (Merzenich & Jenkins, 1993). A similar reorganization occurs when blind individuals learn to read Braille (Pascual-Leone & Torres, 1993) or when people train extensively on stringed musical instruments (Elbert, Pantev, Weinbruch, Rockstroh, & Taub, 1995). Using your thumbs for text messaging will probably result in adaptations in cortical representation not seen in older generations (Wilton, 2002). Advances in robotics combined with better understanding of how touch is processed in the brain are leading to the development of prosthetics that can feel. Using fMRI, researchers were able to map areas of the sensory cortex that reacted when a participant imagined something touching different parts of the hand. With electrodes implanted in the relevant areas, the participant could then respond accurately to touch applied to the prosthetic hand, even when blindfolded. With this more natural feedback, the prosthetic hand should be able to manage delicate tasks, like picking up an egg. Individuals with autism spectrum disorder (ASD) experience a very different sensory world (see Chapter 14). Many individuals with ASD are oversensitive to touch, leading to rejection of hugs and cuddling. In addition, brain responses to touch of self or others differ between individuals with ASD and healthy controls (Deschrijver, Wiersema, & Brass, 2017). The extent of the differences correlated with the individuals’ reports of sensory and social difficulties. The representation of body parts in the primary sensory cortex changes in response to the amount of input from a body part. Children who study stringed instruments show more space in the sensory cortex devoted to fingers. Given the anguish experienced by patients with chronic pain, it is tempting to think that not having a sense of pain would be wonderful. However, as mentioned earlier, we need pain to remind us to stop when we are injured, to assess the situation before proceeding, and to allow the body time to heal. Free nerve endings that respond to pain are triggered by a number of stimuli associated with tissue damage. Some pain receptors respond to mechanical damage, such as that caused by a sharp object, while others respond to temperature or chemicals. Among the chemicals that stimulate pain receptors is capsaicin, an ingredient found in hot peppers (Caterina et al., 1997). Information about pain is carried centrally to the brain by two types of fibers. Fast, myelinated axons are responsible for that sharp  ouch  sensation that often accompanies an injury. Slower, unmyelinated axons are responsible for dull, aching sensations. Pain fibers from the body form synapses with cells in the spinal cord, which in turn sends pain messages to the thalamus. This information takes a relatively direct route, with only one synapse in the spinal cord separating the periphery of the body and the thalamus in the forebrain. This arrangement ensures that pain messages are received by the brain with great speed. From the thalamus, pain information is sent to the anterior cingulate cortex and the insula, which manage the emotional qualities of pain, and to the somatosensory cortex in the parietal lobe, which manages information about the location and intensity of pain (Wiech, 2016). Ashlyn Blocker was born with a rare condition preventing her from feeling pain. Without complaint, she went several days with a broken ankle after falling off her bicycle. Pain messages traveling to the brain may be modified by competing incoming sensory signals. Many of us spontaneously rub our elbow after bumping it painfully. The gate theorygate theoryThe theory that suggests that input from touch fibers competes with input from pain receptors, possibly preventing pain messages from reaching the brain.            gate theory        The theory that suggests that input from touch fibers competes with input from pain receptors, possibly preventing pain messages from reaching the brain.             of pain accounts for this phenomenon (Melzack & Wall, 1965). According to this model, input from touch fibers (reacting to rubbing your elbow) competes with input from pain receptors for activation of cells in the spinal cord (see Figure 5.38). Activation of the touch fibers effectively dilutes the amount of pain information reaching the brain. According to the gate theory, incoming pain messages can be influenced by factors such as chronic stress (opening the gate wider and producing a greater sensation of pain) or rubbing an injured body part (closing the gate and reducing the sensation of pain). The perception of pain is affected by the descending influence of higher brain centers. Many forebrain structures form connections with the periaqueductal gray of the midbrain. As we observed in Chapter 4, this area is rich in receptors for our natural opioids, the endorphins. The periaqueductal gray is a major target for opioid painkillers, such as morphine. Electrical stimulation of the periaqueductal gray produces a significant reduction in the experience of pain. In one of the most dramatic examples of how stress can interfere with the perception of pain, Guy Gertsch unknowingly ran the final 19 miles (about 30 km) of the 1982 Boston Marathon on a broken leg. Gertsch finished the race with a highly respectable time of 2 hours and 47 minutes. Pain is an actively constructed experience that involves our expectations and past experiences (Wiech, 2016). The power of expectation can be seen in placebo effects, which occur when people experience pain reduction, even though they have been exposed to an ineffective substance or treatment, such as a sugar pill instead of an aspirin tablet. Traditionally, scientists thought placebo effects were due to the ability of people’s belief that they are being treated for pain to initiate a real decrease in pain sensation. However, even when people are told they are receiving a placebo, pain relief can occur as long as they are also told that placebo effects can be powerful (Carvalho et al., 2016). No other sensory modality is as dramatically affected by culture, context, and experience as our sense of pain. The connection between culture and experience of pain is vividly illustrated by the hook-swinging ritual practiced in India (Melzack & Wall, 1983). This ritual, designed to promote the health of children and crops, involves hanging a male volunteer from steel hooks embedded into the skin and muscles of his back. Instead of suffering excruciating pain, as Westerners might expect, the volunteers appear to be in a state of exaltation. Women who have participated in prepared childbirth classes report less pain than women who are uninformed regarding the birth process. Although athletes and nonathletes share similar pain thresholds, these groups are quite different in their tolerance of pain (Scott & Gijsbers, 1981). Compared to nonathletes, athletes in contact sports such as boxing, rugby, and football tolerate higher levels of pain before identifying a stimulus as painful. Patients who are allowed to self-administer morphine for pain require less medication than patients who receive injections from hospital staff (Bennett et al., 1982). The sense of control may reduce anxiety and the need for pain medication. The famous philosopher Immanuel Kant (1798/1978, p. 46) considered olfactionolfactionSee also Chemical senses            olfaction        See also Chemical senses            , or our sense of smell, to be the  most dispensable  sense. Other species rely more heavily on olfaction and gustationgustationSee also Chemical senses            gustation        See also Chemical senses            , or the sense of taste, than humans do. Nonetheless, our chemical senses provide warning of danger, such as smelling smoke from a fire or tasting spoiled food. The chemical senses also contribute a richness to our emotional and social experiences. The smell of perfume or the taste of chocolate may be accompanied by strong emotional reactions. Contrary to Kant’s view, people who have lost their sense of smell because of head injury often experience profound depression (Zuscho, 1983). Sharing a meal has a strong effect on bonding for humans and other primates (Brosnan, 2010;  Wobber, Wrangham, & Hare, 2010). Culture, context, and experience can shape our perception of pain. During a festival dedicated to penance and atonement, Tamil Hindus walked through the streets carrying devices called kavadis that hold hooks that are pierced through the skin. Without this cultural context, it is likely that most people would find this experience excruciatingly painful. Our chemical senses begin with molecules suspended in the air in the case of olfaction and dissolved in saliva in the case of gustation. Olfaction provides more information from a distance, like vision and audition, whereas gustation, like the somatosenses, involves information from contact with the body. Like the somatosenses, the chemical senses are quite ancient in terms of evolution and have undergone little change over time. However, our sense of smell has been influenced by walking on two feet instead of four. Most olfactory stimuli are relatively heavy and tend to fall to the ground. Consider how your dog puts its nose to the ground when tracking something interesting. Air containing olfactory stimuli is taken in through the nostrils and circulated within the nasal cavities connected to the nostrils, where it interacts with olfactory receptors (see Figure 5.39). The receptors are located in a thin layer of cells within the nasal cavity. Unlike most neurons, the olfactory receptors regularly die and are replaced by new receptor cells in cycles lasting 4 to 6 weeks. Cells at the base of the receptors are responsible for producing the mucus surrounding the receptors. One branch of each receptor interacts with molecules dissolved in the mucus. The other branch carries information back to the central nervous system as part of the olfactory nerveolfactory nerveA nerve carrying olfactory information from the olfactory receptors to the olfactory bulbs.            olfactory nerve        A nerve carrying olfactory information from the olfactory receptors to the olfactory bulbs.            . The olfactory nerve fibers synapse in one of the two olfactory bulbsolfactory bulbsOne of two structures below the frontal lobes of the brain that receive input from the olfactory receptors in the nose.            olfactory bulbs        One of two structures below the frontal lobes of the brain that receive input from the olfactory receptors in the nose.            , located just below the mass of the frontal lobes. Although we often hear that human olfaction is not as good as olfaction in other species, human olfactory bulbs have about the same number of neurons as in 24 other mammalian species (McGann, 2017). Receptors in the nose interact with airborne chemicals to begin the sensing of odor. Unlike most other sensory input to the brain, olfactory pathways do not make direct connections with the thalamus before the information reaches the cerebral cortex. Instead, fibers from the olfactory bulbs proceed to the olfactory cortex, located in the lower portions of the frontal lobe extending into the temporal lobe, and to the amygdala. Because of the role these areas of the brain play in emotion, which we described in Chapter 4, these pathways may account for the significant emotional reactions we experience (disgust or pleasure) in response to odor. The olfactory cortex makes connections with the thalamus, which in turn sends information to the orbitofrontal cortex. It is likely that this pathway contributes to the conscious awareness of odors. The most likely original purpose of our sense of gustation, or taste, was to protect us from eating poisonous or spoiled food and to attract us to foods that boost our chances of survival. Although we seem biased toward detecting negative stimuli (Cacioppo & Gardner, 1999), our attraction to certain tastes also reflects our historical past. Because most of our ancestors were constantly facing the threat of famine, we find fatty and sugary foods to be especially tasty. Unfortunately, given the current availability of safe and palatable foods, our sense of taste may drive us to eat more than we need. Most of us are familiar with four major categories of taste: sweet, sour, salty, and bitter. A fifth type of taste has been proposed, known by the Japanese term umami, which roughly translated means  savory  or  meaty (Chaudhari, Landlin, & Roper, 2000). In addition, the tongue and the mouth contain receptors for carbohydrates (Turner, Byblow, Stinear, & Gant, 2014) and capsaicin, an active ingredient in hot peppers. Mice lacking capsaicin receptors happily consumed water containing capsaicin at levels that were rejected by normal mice (Caterina et al., 2000). Taste receptors are located on the tongue and in other parts of the mouth (see Figure 5.40). Contrary to a popular myth (usually accompanied by an equally mythological map of  taste centers  on the tongue), receptors sensitive to all types of taste are equally distributed across the tongue. You are probably aware of the bumpy texture of your tongue, which results from the presence of papillaepapillaeSmall bumps on the tongue that contain taste buds.            papillae        Small bumps on the tongue that contain taste buds.            . Most papillae contain somewhere between 1 and 100 taste budstaste budsA structure found in papillae that contains taste receptor cells.            taste buds        A structure found in papillae that contains taste receptor cells.            . Each taste bud contains between 50 and 150 receptor cells, which extend tiny hairlike cilia into the saliva that interact with dissolved taste stimuli and transduce the resulting information into neural signals. Like olfactory receptors, taste buds have a limited life before they are replaced. If you burn your tongue by drinking hot liquid, your taste is affected for a day or two. However, when the taste buds are replaced, taste should be back to normal. Taste buds are located in the bumps, or papillae, located on the tongue. Information about taste travels from the mouth and the tongue to the medulla. The medulla in turn communicates with the thalamus, which sends taste information to the insula, lower somatosensory cortex of the parietal lobe, and to the orbitofrontal cortex, where the emotional pleasantness or unpleasantness of particular stimuli is processed (Kobayakawa et al., 2005). As we will see in Chapters 7 and 8, taste information interacts with motivation and learning. Olfaction and gustation share three interesting perceptual themes: We can easily identify a number of complex stimuli combining many types of molecules, such as the aroma of coffee;we can detect small differences among similar smells and tastes; andour experience often shapes our perception of an olfactory or gustatory stimulus (Goldstein, 2010). Humans can distinguish among at least 1 trillion odors (Bushdid, Magnasco, Vosshall, & Keller, 2014). An example of the impact of experience and top-down processing on olfaction is the effect of labeling an odor on people’s rating of its pleasantness. If participants smell an onion stimulus labeled  pizza,  they rate the odor as more pleasant than if the identical stimulus is labeled  body odor  (Herz, 2003). About 25% of the population are supertasters, or people who are extrasensitive to taste; 25% are nontasters, or people who are relatively insensitive to taste; and the remaining 50% fall between these two extremes. You can use the following exercises to determine your taste category. Place a mint Life Saver on your tongue, and allow it to dissolve (no chewing please). Rate the following qualities of the Life Saver on a scale of 1 to 5, with 1 being  very intense  and 5 being  not intense :Sweetness12345Smell12345Coolness12345 Rush 12345How to interpret your results: Mint tasters fall into four groups:Group 1: Mint is mild, no rush. Group 2: Mint is moderate, no rush. Group 3: Mint is moderate, rush. Group 4: Mint is intense, rush. Table 5.3 shows further characteristics of these taste groups. Mildly Sensitive Tasters (Group 1)Moderately Sensitive Tasters (Groups 2 and 3)Supertasters (Group 4)Weak to undetectable sensation from mint. Moderate to strong sensation from mint. Very strong sensation from mint. Flavor of food is not that important. Flavor of food is important. Flavor of food is important. Many foods liked, few foods disliked, and not passionate about food. Many foods liked, few foods disliked, and often passionate about food. Great variation in the number of foods liked and often passionate about food. You need a gummed reinforcer (sticky white ring for notebooks), a swab, blue food coloring, and a mirror. Place one reinforcer on the front of your tongue just to the side of midline. Use a swab to apply blue food coloring to the part of your tongue that shows through the center of the reinforcer. The blue food coloring should make your papillae (bumps) more obvious. Count the number of papillae you see in the ring. More than 25 papillae within the reinforcer ring means you’re a supertaster. The number of papillae of mildly or moderately sensitive tasters will be less than 25, but these two groups cannot be distinguished based on this factor. Rate the tastes of the following foods and drinks using a 1-to-5 scale, with 1 being  dislike strongly  and 5 being  like a great deal :Broccoli12345Grapefruit12345Coffee (black)12345Dark chocolate12345As a child, were you ever described by a parent, teacher, or other adult as a  picky eater ? (circle one)YESNOCan you easily tell the difference between the fat content of milk, for example, between whole and 2% milk or between 1% and 2% milk? (circle one)YESNOPlacing blue food coloring on the tongue makes the papillae easier to see. Supertasters have many more papillae, and thus more taste buds, than other people. Supertasters tend to dislike bitter foods; they have many low numbers. Supertasters also tend to be picky eaters as children and are better at detecting differences in fats in foods. The chemical senses interact to provide the perception of flavor. You have probably noticed that food doesn’t taste good when your sense of smell is decreased by a bad cold. If you close your eyes and hold your nose, you are unable to distinguish between a slice of apple and a slice of raw potato. The orbitofrontal cortex plays an important role in the perception of flavor because the pathways serving olfaction and gustation converge in this part of the brain (Rolls, 2000). Young children are notorious for putting things in their mouths that adults would quickly reject based on taste, including poisonous substances such as drain cleaner. However, this propensity does not mean that children lack a sense of taste. Using facial expressions, researchers have demonstrated that newborns differentiate among sweet, bitter, and sour tastes but seem relatively oblivious to salty tastes (Rosenstein & Oster, 1988). As we get older, the overall number of taste buds decreases, reducing the intensity of many tastes and providing a possible explanation for why some strong flavors, such as that of broccoli, are enjoyed more by adults than by children. As we age, our sensitivity to smell also decreases (Cain & Gent, 1991). Because olfaction and taste interact to form the flavor of foods, decreased sensitivity in both senses might affect overall appetite as we age. Like the other sensory modalities discussed in this chapter, the chemical senses vary from person to person. Females are generally more sensitive to smell than are males (Dorries, 1992;  Koelega & Koster, 1974;  Ship & Weiffenbach, 1993). The average person has approximately 6,000 taste buds, but this number may vary widely. Supertasters have unusually high numbers of papillae and, therefore, have more taste buds (Bartoshuk, 2000). Disturbances in the chemical senses are correlated with several psychological disorders. Olfaction and the experience of posttraumatic stress disorder (PTSD) (see Chapter 14) appear to interact in combat veterans. PTSD is often characterized by intrusive, disturbing flashbacks in which the patient essentially relives the traumatic experience. Given the close association between olfaction and memory, researchers hypothesized that some PTSD flashbacks could be initiated by relevant smells. When compared to combat veterans who did not have PTSD, combat veterans with the disorder experienced marked anxiety when exposed to the smell of diesel, accompanied by changes in the activity of the amygdala (Vermetten, Schmahl, Southwick, & Bremner, 2007). The sense of smell might seem to play a secondary role to vision and audition in humans, but people have manipulated scent for religious, medicinal, and personal purposes since ancient times. We can speculate that once people learned to control fire, a recognition that some burning things smelled better than others could not have been far behind, possibly leading to the use of incense in religious rituals. Use of natural materials for medicine and self-adornment provides the historical roots for large, contemporary industries that manufacture scent for a host of consumer products, including perfume, air fresheners,  new car smell  products, and detergents. We mentioned earlier that of the senses we discussed in this chapter, pain was particularly influenced by cognition and context. Can being in a close relationship affect the way you feel pain? The answer appears to be  yes.  Physical contact with a loved one can affect how the brain processes pain. Women who were expecting an electric shock showed reduced activity in parts of the brain associated with the emotional and arousing aspects of pain when they held their husbands’ hands (Coan, Schaefer, & Davidson, 2006). In addition, the amount of reduction of activity in these pain areas of the brain correlated with the quality of the marriage—happily married women experienced greater decreases in activity associated with pain than less happily married women. Perhaps this buffering effect is why you may reach for your partner when frightened during a scary movie. Knowing that such intimacy could reduce a loved one’s pain might compel you to accompany your partner to a doctor’s appointment or visit a friend in the hospital. Our understanding of pain in connection with our interpersonal relationships can help us in tangible ways to create and maintain stronger, healthier relationships. Research shows that physical contact with loved ones reduces the sensation of pain. Although olfaction often seems to run in the background of our other cognitive processes, it is not immune to the effects of culture and experience. Americans spend millions on products that remove or mask body odor, whereas other cultures do not find such odors offensive. One study compared the categorization of odors by French, American, and Vietnamese participants (Chrea et al., 2004). Although the participants sorted odors similarly into broad categories of floral, sweet, bad, and natural, they differed along subtler dimensions. The French and American participants quickly sorted odors into fruit or flower categories, but this separation had little relevance to the Vietnamese participants. French and French-Canadian participants rated the pleasantness of wintergreen quite differently (Ferdenzi et al., 2016). Wintergreen is used in candy in Canada, but in medicine in France. Different cultures can prefer different foods. It is unlikely that you will find fruit bat pie, a delicacy in Palau, in many American restaurants. As we have seen in this chapter, different people watching the same event can reach different conclusions about what just happened. If researchers are to truly understand a problem such as cyberbullying, they must have methods for identifying perceptions of the experience from the perspectives of those involved. There are two major approaches for collecting information about cyberbullying: self-report and peer-nomination (Pellegrini, 2001). Each provides a unique perspective. Self-report asks youth to identify the frequency and the degree of cyberbullying that they have experienced or perpetrated personally, along with their emotional responses to these instances. Because cyberbullying often takes place where adult supervision is scarce, self-report can be more useful than observation. At the same time, self-report can be biased. Peer nomination provides insight into the larger group’s perception of the individuals being bullied or of the aggressors (Figure 5.41). Again, this method does a good job of providing information that is not typically accessible to adults. To conduct a peer nomination, students are provided with photos or a roster of their group, usually their class. Youth are asked to identify which classmates are picked on frequently and which do the bullying. Peer nominations generally show high correlations among the participants. Peer Nomination. The peer nomination method provides information about how individuals are viewed by their peers. This information usually shows high levels of consensus and provides insights about the group that are not usually observable by outsiders such as teachers and parents. Self-report and peer nomination results can be complimentary, providing perceptions of cyberbullying from both the perspective of the individuals involved and of the peers observing the cyberbullying. In both cases, these methods can provide insight into the phenomenon often unavailable to observation by parents, teachers, researchers, and other adults. Experience clearly plays a role in developing an individual’s taste preferences. The effects of experience on taste begin in the prenatal environment. Infants whose mothers consumed carrot juice during pregnancy showed stronger preferences for carrot flavor (Mennella & Beauchamp, 1996;  Mennella, Jagnow, & Beauchamp, 2001). In terms of survival, this result makes perfect sense. Infants are born with a predisposition to like the safe foods available in their environment. Because the food supply historically has varied widely from place to place, delicacies such as fruit bat pie are appreciated in Palau but not necessarily in the United States. StructureFunctionPinnaCollects sound and identifies its location as coming from above or below the head. Tympanic membraneBegins the process of transduction of sound waves to neural signals when movement occurs. CochleaContains auditory receptors. ThalamusReceives auditory input from the brainstem and connects to the primary auditory cortex. Primary auditory cortex (area in the temporal lobe)Receives and performs an initial analysis of auditory input from the thalamus.`;\n","export const text = `Do we have free will? Transcranial magnetic stimulation switched the hand chosen to carry out a task from right to left, but participants insisted that switching hands was their own idea. What does it mean to have free will? this might sound more like a philosophy question than a psychology question, but advances in biological and cognitive psychology have allowed psychologists to tackle it. Usually, we think of free will as the ability to consciously control our actions and decisions. In the children’s game of rock, paper, scissors, we feel like we are making a conscious decision to choose one of the three options, but how would such a decision be made? What types of processes in the brain correspond to choosing scissors over the other two possibilities? Research into voluntary movements, such as forming your hand into scissors, has shown that the brain makes a commitment to a choice as much as 10 seconds before we become aware of the decision (Soon, Brass, Heinze, & Haynes, 2008). Other research shows that the brain can be manipulated to make an unusual choice without disrupting the individual’s sense of free will. Right-handed people normally use their right hand 60% of the time or more when forced to choose to use one hand over the other, but when researchers exposed right-handed subjects’ right hemispheres to transcranial magnetic stimulation (TMS), illustrated in the large image on the previous page, they chose to use their left hand 80% of the time (Ammon & Gandevia, 1990). Despite making the uncharacteristic choice to use the wrong hand, the participants in this experiment reported feeling in control of their hand movements. If the brain has already committed to a movement, what purpose does a conscious sense of having decided to move serve? Insight into this question emerged from the study of split-brain patients described in Chapter 4. These patients have undergone surgery that severs the connections between their right and their left hemispheres, making it possible to expose one hemisphere at a time to a stimulus. For example, you can show the relatively nonverbal right hemisphere the word stand, and the patient usually stands up. However, because of the limitations of the right hemisphere in processing language, the patient is consciously unaware of having read the word. When asked why they stood up, patients responded by saying they felt the need to stretch. In other words, the mind is building a theory about the behavior that was just performed (Gazzaniga, 2012). This type of observation leads to a view of consciousness as an interpreter, making sense out of our actions and the world within and around us. This view helps us understand the observed delay between the brain’s commitment to an action and our conscious awareness of that decision—the interpreter needs time to analyze the situation. Social context plays a role in sleep, even for the fruit fly. Fruit flies living with other fruit flies rather than alone need more sleep, suggesting that one of the functions of sleep is to process learning about social experiences. The larger the group of flies, the longer each fly needed to sleep. In this chapter, we will explore many situations that affect the performance of the interpreter, from the variations found in stages of sleep and waking to the alterations produced by psychoactive drugs and damage to the brain. This journey will require us to zoom in to observe underlying processes in the brain and then zoom out again to understand the influences of the social context on the phenomenon of conscious awareness. Our English word conscious is derived from the Latin conscientia, which means to know. ConsciousnessConsciousnessA state of awareness.            Consciousness        A state of awareness.             refers to knowing or being aware of ongoing experiences occurring both internally and in the world around us. We understand that to be unconscious, perhaps because of a blow to the head, means that we are unaware of what is happening within and around us. However, the term consciousness has multiple meanings. Consciousness can refer to a state of awareness (alert versus drowsy, for example) or the current content of awareness (while reading this page, you hear your neighbor’s music). A special type of consciousness, self-awarenessself-awarenessThe special understanding of the self as distinct from other stimuli.            self-awareness        The special understanding of the self as distinct from other stimuli.            , occurs when we focus on ourselves as individuals. Consciousness has been a favorite topic among psychologists for many years. William James (1899, p. 15) coined the term stream of consciousness to capture the moving, seemingly unbroken flow of conscious awareness. Sigmund Freud used the term consciousness to refer to aspects of the mind that could be retrieved voluntarily, in contrast to the unconscious parts of the mind that remain hidden to voluntary searches for information. Contemporary cognitive psychologists debate why some information becomes conscious while other information does not (Baars, 1988). Most of us would agree that bacteria do not possess consciousness, but people do. This implies that consciousness emerged at some point in the course of evolution. The exact location of that point remains highly debatable. James (1890, p. 141) maintained that consciousness grows the more complex and intense the higher we rise in the animal kingdom. That of a man must exceed that of an oyster. If consciousness evolved as suggested by James, what advantages might account for this development? The question of the emergence of consciousness in animals is complicated by the multiple meanings of consciousness described previously. Most animals show variations in states of awareness. Even the humble fruit fly takes periods of rest, which are believed to be the fruit fly equivalent of sleep (Shaw, Cirelli, Greenspan, & Tononi, 2000). Consciousness defined as an awareness of ongoing sensations is probably less common in the animal kingdom. Many animals would survive quite well by simply responding automatically to the world around them (food or predator) without necessarily thinking I smell food or I see a predator. The self-awareness aspect of consciousness—as exemplified in I statements—might be the rarest of all among living things, as we will see later. Varying states of awareness might allow animals to repair their bodies and conserve energy. Food requirements would be higher if we stayed awake for 24 hours, as you may have noticed while studying for final exams. Another possible benefit of varying states of awareness is maximizing safety. Most animals are specialized for activity in either the light or the dark. Sleeping during the cycle for which an animal is poorly equipped for activity might contribute to its survival. However, being inactive is not a safe thing to do unless you have a safe place to hide (Allison & Cicchetti, 1976). Horses and rabbits are both frequently preyed upon, but horses sleep in the open, while rabbits sleep in burrows. Consequently, wild horses sleep as little as 1 or 2 hours per day, while wild rabbits sleep at least 8 hours. Sleeping habits of different species reflect how likely they are to be preyed upon by other animals and whether they have shelter. The lion has few predators and is likely to fall asleep wherever it is convenient. One possible advantage of being consciously aware of ongoing sensations is the ability to choose responses rather than to respond instinctively (Gazzaniga, 2011). If suddenly faced with a predator, any animal could respond automatically using reflex and instinct, which we discuss in Chapter 8. Horses are notorious for running whenever the slightest stimulus bothers them. It is unlikely that the horse in these cases is thinking, I hear a mountain lion—I should run. Conscious awareness of sensations provides more options. If a conscious animal hears a sound in the distance that might indicate the presence of a predator, it could choose to continue on, flee, or pause while obtaining further information. This flexibility would be likely to increase the animal’s chances of survival. The self-awareness aspect of consciousness could heighten an animal’s drive to survive. Recognition and understanding of being alive is correlated with heightened meaningfulness of self-preservation. Developmental psychologists have provided a technique for demonstrating self-awareness that can be used with many species. Recognizing one’s self in the mirror is believed to be a major developmental milestone in achieving self-awareness. In the so-called rouge test, a dot of rouge or other odorless dye is placed on a child’s forehead, and the child is allowed to look in a mirror (Lewis & Brooks-Gunn, 1979). Before the age of about 18 months, children do not seem to indicate that they understand the image in the mirror is their own. At about that age, they rub at the spot of rouge or turn their bodies to get a better look, thus demonstrating self-awareness. Chimpanzees pass the rouge test by demonstrating behavior similar to that of 18-month-old children (Gallup, 1970). In addition to chimpanzees, orangutans, gorillas raised by humans, bottle-nosed dolphins, Asian elephants, magpies, and parrots have demonstrated self-awareness in this type of experiment (Gallup & Suarez, 1986;  Patterson, 1984;  Patterson & Cohn, 1994;  Plotnik, de Waal, & Reiss, 2006;  Povinelli, 1993; Prior, Schwarz, & Güntürkün, 2008). Other researchers restrict the possibility of self-aware consciousness to species that exhibit complex social behavior. Of what use would self-awareness be if one did not frequently interact with others of the same kind? Social animals must be able to recognize other individuals and respond to them accordingly. Dolphins appear to have special whistles that they seem to use for particular individuals, like names (Janik, Sayigh, & Wells, 2006). Some social animals, including chimpanzees and elephants, show an awareness of death and appear to grieve at the loss of family members. Elephants in particular have been known to remain in the area of a relative’s bones for lengthy periods (McComb, Baker, & Moss, 2006). Babies 18 months and older are able to recognize themselves in a mirror, which psychologists believe indicates a sense of self-awareness. Happy the elephant, a resident of the Bronx Zoo, repeatedly used her trunk to touch a mark that researchers placed above her eye, suggesting that she too can recognize her image in the mirror. Researchers have suggested that even robots might be able to pass the mirror test. Organisms like the army ant stretch our definitions of what it means to be conscious. Individual army ants (Eciton) are not impressive, but colonies of army ants show signs of collective intelligence and sophisticated problem solving, such as using their bodies to build nests of up to 700,000 individuals (Franks, 1989). Extending the question of what creatures are conscious a step further, we can ask whether it is possible to construct an artificial consciousness. From the Terminator to Her’s Samantha, Hollywood loves to tell the story of the machine that develops consciousness. Stephen Hawking, Bill Gates, and Elon Musk warn us that self-aware robots are likely to jeopardize the continuity of the human race. What does it mean for a machine to be conscious? John  Searle (1980) distinguishes between strong and weak artificial intelligence (AI). Weak AI, which all current robots possess, refers to the ability to demonstrate intelligent behavior but without the ability to experience conscious states or subjective awareness. Strong AI refers to machines possessing the full range of human cognition. How would we judge whether a machine has achieved consciousness? In 1950, Alan Turing devised a test of a machine’s ability to think. According to the Turing test, if a human had a conversation with two other beings, one human and one machine, without being able to distinguish between them, then the machine would have passed the test. The 2014 Eugene Goostman program, which impersonated a 13-year-old from the Ukraine, managed to convince 10 out of 30 judges that he was human. The ability to meet Turing’s criteria may be within reach. One Nao robot appears to have the answer to the dilemma posed to the group, which researchers claim is evidence for self-awareness. One type of consciousness discussed in this chapter is self-awareness. Selmer Bringsjord and his colleagues (2015) claim that their robots have demonstrated evidence of self-awareness in the mirror test described in this chapter. In another demonstration, the researchers told three Nao robots that two were given dumbing pills to prevent them from speaking, while one was given a placebo. In actuality, mute buttons were pressed on two of the robots. When asked who had been given the active pills, all three tried to answer, but of course only one was able to say, I don’t know. The robot paused, then stood up and said, Sorry, I know now. I was able to prove that I was not given a dumbing pill. Daniel  Dennett (1991) argues that it might be more interesting to set aside the question of consciousness and simply build the most human robot possible. Robots are becoming more natural both in looks and behavior, and robotics experts hope to make them indistinguishable from humans. Hanson Robotics’ Sophia has cameras in her eyes that allow her to make eye contact and recognize people. She uses Google Chrome voice recognition to process speech, chat with people, and learn. Hiroshi Ishiguro has plans to make his robots suitable for jobs such as a hotel receptionist, a museum tour guide, and a language tutor. Already, 80% of Ishiguro’s participants greeted his robots by saying hello, as they mistakenly believed the robots to be human. Where will this technology lead us? That remains unknown, but the effort to construct artificial consciousness might yield new insights into our own human consciousness. Is it possible to find consciousness in the brain? Consciousness, the mind, and the brain are like a set of nested Russian dolls. The brain, the outside doll, houses the mind but has other functions as well, such as maintaining breathing and body temperature. The mind, the middle doll, houses consciousness, the innermost doll, but also manages unconscious functions such as long-term memory, discussed in Chapter 9. In this chapter, we will examine a number of natural, artificial, and pathological circumstances that correlate with variations in consciousness. As a result of these investigations, we can begin to view consciousness as requiring complex interactions between areas of the cerebral cortex and the thalamus, discussed in detail in Chapter 4. In particular, lesions of the thalamus result in the type of profound unconsciousness typically associated with brain death (Bogen, 1995). We discuss the concept of brain death more fully in a later section of this chapter. Certain structures in the brain, such as the thalamus, might be necessary, but not sufficient, for consciousness. For example, observing that your television screen no longer produces a picture after you remove a component does not necessarily imply that the sole source of the picture is that particular component; it’s likely that other components need to be attached simultaneously and contribute to the creation of the visual image. Likewise, additional brain structures, such as the reticular formation of the brainstem, play an active role in raising or lowering the thresholds of conscious awareness (see Figure 6.1). Because of these variations, you are far more likely to hear and respond to a sound while awake than while deeply asleep. Structures such as the thalamus and reticular formation have been described as enabling consciousness, but they do not produce its content (Crick & Koch, 2003). Among the brain structures that are believed to participate in consciousness are the cerebral cortex, thalamus, and reticular formation, as well as the complex pathways connecting these parts of the brain. These structures make consciousness possible, which we know because damage to these circuits results in unconsciousness, but they do not determine the content of consciousness. Instead of looking for consciousness as resulting from the activity of particular structures, it is more useful to consider consciousness as resulting from activity changes in brain networks. Neuroscientists initially believed that when your thought is unfocused during a state known as mind wandering, the brain would be quiet, but as you focus on a specific task, the brain would light up. This does not seem to be the case. The brain uses only about 5% more energy when people are focused compared to when they are unfocused (Raichle, 2015; Raichle & Snyder, 2011). These observations have led to the identification of a default mode network (DMN), which maintains a high level of unconscious, background activity as it helps prepare the brain for conscious thought (see Figure 6.2). Activity in the DMN actually decreases a bit when the person engages in a conscious task. One group of researchers was able to predict participants’ mistakes in a computer test by watching the activity of the DMN (Eichele et al., 2008). As much as 30 seconds before a mistake occurred, the DMN would take over, and activity in areas involved with focused attention decreased. Activity of the DMN corresponds to mind wandering, thinking about the self, and preparing for conscious thought. Despite average people spending about 50% of their waking hours in the relatively unfocused mind wandering state managed by the DMN, activity in this network is correlated with feelings of relative unhappiness (Brewer et al., 2011). DMN activity is also correlated with thoughts about the self, which we discuss further in Chapter 12. It is unclear how, or even if, these two correlates of DMN activity interact. Aspect of consciousnessDefinitionExampleState of awarenessThe level of conscious awareness of internal states or surroundings, which can be high or low and realistic or distorted. Asleep or awake. Perceiving realistically or hallucinating. Hypnotized or not. Meditating or not. Content of awarenessThe ongoing catalog of internal and external stimuli that are the focus of current attention. Hearing a neighbor’s music. Thinking that you’re hungry and it’s time to eat. Considering the implications of a news report. Self-awarenessThe special understanding of the self as distinct from other stimuli. Knowing that you are looking at yourself in the mirror. Knowing that you are sleeping and dreaming. Our first meaning of consciousness involves variations in an animal’s state of awareness. Awareness changes dramatically across cycles of sleepsleepThe components of sleep characterized by theta and delta wave activity, as recorded by electroencephalogram, and deep physical relaxation. The component of sleep characterized by waveforms resembling waking, as measured by electroencephalogram, accompanied by rapid motion of the eyes, muscular paralysis, and sympathetic nervous system activation.            sleep        The components of sleep characterized by theta and delta wave activity, as recorded by electroencephalogram, and deep physical relaxation. The component of sleep characterized by waveforms resembling waking, as measured by electroencephalogram, accompanied by rapid motion of the eyes, muscular paralysis, and sympathetic nervous system activation.             and wakingwakingA normal state of consciousness characterized by alertness and awareness of external stimuli.            waking        A normal state of consciousness characterized by alertness and awareness of external stimuli.            . These cycles follow circadian rhythmscircadian rhythmsA daily biological rhythm            circadian rhythms        A daily biological rhythm            , or daily rhythms. The term circadian comes from the Latin words for about a day. Circadian rhythms are only one example of a number of important biorhythms. Animals have birth seasons and migration patterns. Human females experience menstrual cycles. Within a single day, we also see regular fluctuations in arousal, with peaks occurring every 90 to 120 minutes. Circadian rhythms respond to a combination of internal and external factors. Internal biological clocksbiological clocksAn internal mechanism that provides an approximate schedule for various physical processes            biological clocks        An internal mechanism that provides an approximate schedule for various physical processes            , controlled by the hypothalamus, provide an approximate schedule for various physical processes, including sleep and waking, body temperature, and hunger. These internal biological clocks interact with external stimuli, known as zeitgebers (zeit means time and geber means to give in German; hence, these stimuli are time givers). Light is one of the most important zeitgebers for humans. Exposure to the rising sun each day helps to reset the internal biological clocks to the correct time. In the absence of light, our internal clocks run a few minutes longer than a 24-hour cycle. Over time, the distortion adds up, and behavior is only occasionally synchronized with normal cycles of day and night. Totally blind people and sailors on submarines experience longer than normal circadian cycles because of their lack of exposure to natural light (Kelly et al., 1999;  Skene, Lockley, & Arendt, 1999). Eating patterns are also significant zeitgebers, as many midnight snackers know. Mice normally sleep during the day and feed at night. However, when mice are fed only during the day, they begin sleeping at night and exploring more during the day (Mieda, Williams, Richardson, Tanaka, & Yanagisawa, 2006). Our human ancestors remained awake during daylight and slept throughout the dark of night, and they lacked the technology needed to travel rapidly across time zones. Technology, particularly the invention of artificial light, has shaped our contemporary sleep–wake patterns by making it easy to be awake at night (see Figure 6.3). Artificial lighting, including that produced by tablets and phones, affects sleep by breaking down melatonin, a hormone released in the evening that regulates sleep cycles. Changes in melatonin release have been implicated in a long list of human diseases, including cancer and heart disease, which are more common among people who work night shifts (Navara & Nelson, 2007). Americans are believed to be somewhat sleep deprived because the average adult spends only 6 hours and 55 minutes in bed, which includes time needed to initiate sleep. Although people try to make up for lost sleep on weekends by sleeping longer, this solution is not very effective. Some occupations, such as hospital and public safety jobs, involve work around the clock, which in turn is correlated with physical and psychological problems. Between 40 and 80% of workers on graveyard shifts (11:00 p.m. to 7:30 a.m.) experience shift maladaptation syndrome, characterized by health, personality, mood, and interpersonal problems (Wagner, 1996). Accident rates in the industrial swing shift (3:00 p.m. to 11:30 p.m.) are higher than in the traditional day shift, and the graveyard shift (11:00 p.m. to 7:30 a.m.) is the most dangerous work period (Hänecke, Tiedemann, Nachreiner, & Grzech-Šukalo, 1998). Not only do shift workers experience more accidents, but they also make more errors. Hospital workers are more likely to make significant errors during evening shifts than during day shifts (Narumi et al., 1999). Our modern ability to cross time zones leads to the experience of jet lag, which produces fatigue, irritability, and sleepiness. The travel is not to blame because north–south travel of equal distance does not produce the symptoms of jet lag (Herxheimer & Waterhouse, 2003). Chronic jet lag has even more serious consequences. Flight attendants with 4 or more years of experience in which time zones were crossed at least once a week had reduced reaction times and made 9% more mistakes on memory tasks than local crews (Cho, Ennaceur, Cole, & Suh, 2000). Some challenges to our circadian rhythms are easier to manage than others. It is easier to adjust when we set the clock to a later point than to an earlier point. The experiences of two people, one traveling from Los Angeles to New York City (setting the clock forward) and the other traveling from New York City to Los Angeles (setting the clock back), are compared in Figure 6.4. Let’s assume that both travelers usually go to bed at 10:00 p.m. and get up at 6:00 a.m. To stay on the same sleep schedule, the LA–NYC traveler must now go to sleep at 7:00 p.m. and wake up at 3:00 a.m., which is difficult, whereas the NYC–LA traveler must stay up until 1:00 a.m. but can sleep until 9:00 a.m., which is easier. Traveling eastward is more disruptive to circadian rhythms than traveling westward. The Los Angeles resident visiting New York feels like he or she is going to bed 3 hours earlier than usual (7:00 p.m. Los Angeles time) and waking up in the middle of the night (3:00 a.m. Los Angeles time). The New Yorker traveling to Los Angeles has to stay up a little later (1:00 a.m. New York time) but then can sleep later to compensate (9:00 a.m. New York time). Most people find the latter scenario easier. Daylight saving time offers another opportunity to observe our responses to abrupt changes in our daily schedules (see Figure 6.5). This energy-saving relic from World War I requires the setting of clocks forward 1 hour in the spring and back 1 hour in the fall. When Canadian traffic accident data were correlated with the two annual daylight saving shifts, the shift back in the fall resulted in a 7% decrease in traffic accidents on the following Monday compared to other fall Mondays. In the spring setting of the clock forward, a comparable 7% increase in traffic accidents occurred (Coren, 1996). Fifteen years of data show that rates of heart attack (a) increase significantly in the days following the spring shift in daylight saving time and (b) decrease following the fall shift. The incidence ratio is computed by comparing the number of heart attacks on the specified day following the shift to the mean number of heart attacks for the same day 2 weeks before and 2 weeks after the shift. The likely reason for the increase is sleep deprivation; it increases sympathetic nervous system activation and inflammation, which can be dangerous to people with existing health problems. When less light is available for setting the body’s internal clocks, approximately 4 to 6% of the population experience major depressive disorder with seasonal patternmajor depressive disorder with seasonal patternA mood disorder in which depression occurs regularly at the same time each year, usually during the winter months; formerly known as seasonal affective disorder (SAD).            major depressive disorder with seasonal pattern        A mood disorder in which depression occurs regularly at the same time each year, usually during the winter months; formerly known as seasonal affective disorder (SAD).            , formerly known as seasonal affective disorder (APA, 2013;  Jepson, Ernst, & Kelly, 1998). During the winter months at higher latitudes, the reduction in daylight hours interferes with circadian rhythms. This disruption may lead to symptoms of depression, discussed in more detail in Chapter 14, that lift once the days become longer again. Rates of seasonal depression vary from 1.4% in Florida to 9.7% in New Hampshire (Jepson et al., 1999). Seasonal depression is typically treated by exposure to bright light, with or without melatonin supplements or antidepressant medications. The lights used in this therapy are stronger than what is normally experienced indoors but not as strong as the light at the beach on an August afternoon. You might believe that the day should start around 2:00 p.m., whereas your roommate pops out of bed smiling at an alarming 6:30 in the morning. What accounts for these individual differences in sleep patterns? Some individual differences become evident in infancy and remain consistent over the life span. Morning people have been called larks, and night people have been called owls (Åkerstedt & Fröberg, 1976). Many people fall somewhere between these two extremes and are at their best in the middle of the day. Research on the genetic basis of circadian rhythms suggests that the different patterns result from different versions of the genes responsible for the activity of our internal clocks (Katzenberg et al., 1998). Other differences appear to be more age related. Nearly everyone acts like an owl during adolescence and young adulthood (Carskadon, Wolfson, Acebo, Tzischinsky, & Seifer, 1998). In their mid to late 20s, many temporary owls revert to their previous lark or medium state. The exact cause for such a shift during and after adolescence and young adulthood is unclear, but researchers suspect that the shift correlates with a burst of brain development that is initiated at puberty. Some researchers view the return to a previous sleep pattern in young adulthood as a reliable indicator that the brain is fully mature (Roenneberg et al., 2004). Varying states of awareness can be described using electroencephalogram (EEG) recordings, which provide a general measure of overall brain activity. During waking, we alternate among three patterns of activity indicated by the recording of beta wavesbeta wavesA waveform of 15 to 30 cycles per second recorded by electroencephalogram that usually indicates alert waking            beta waves        A waveform of 15 to 30 cycles per second recorded by electroencephalogram that usually indicates alert waking            , alpha wavesalpha wavesA waveform of 9 to 12 cycles per second recorded by electroencephalogram that usually indicates relaxed waking            alpha waves        A waveform of 9 to 12 cycles per second recorded by electroencephalogram that usually indicates relaxed waking            , and gamma wavesgamma wavesA waveform of more than 30 cycles per second recorded by electroencephalogram that indicates attention to sensory input            gamma waves        A waveform of more than 30 cycles per second recorded by electroencephalogram that indicates attention to sensory input             in the EEG (see Figure 6.6). During beta activity, a person is actively thinking and alert. Beta activity is characterized by rapid (15 to 30 cycles per second), irregular, low-amplitude waves that indicate the cerebral cortex is processing large amounts of diverse data. A person showing alpha activity is awake but relaxed. Alpha waves are slightly slower, larger, and more regular than beta waves, with a frequency of 9 to 12 cycles per second. Improvements in EEG technology provided the means to record waking waveforms that are faster than 30 cycles per second, known as gamma waves (Miller, 2007). Gamma waves accompany the processing of sensory input, particularly visual stimuli. Modern artificial light breaks down the hormone melatonin, which is released only at night. Disruptions caused by the effects of light on melatonin might be responsible for higher rates of disease, and cancer in particular, among employees who work at night in hospitals. EEG recordings during waking alternate between alpha waves and beta waves. Alpha waves are associated with a relaxed, possibly drowsy state, whereas beta waves are associated with alertness. Gamma waves are seen in response to sensory input. Daydreaming, or mind wandering, refers to spontaneous, subjective experiences in a no-task, no-stimulus, no-response situation (Vaitl et al., 2005, p. 100). Daydreaming is correlated with activity in the DMN, as described earlier in the chapter. EEG studies show that activity in the DMN is most closely correlated with alpha waves (Knyazev, Slobodskoj-Plusnin, Bocharov, & Pylkova, 2011). The function of daydreaming is unclear. We might daydream simply because we evolved the ability to divide our attention, as described in Chapter 5 (Mason et al., 2007). However, during activation of the DMN, we often think about our past experiences and plan our future (Kahn, 2013). This contradicts the commonly held view that daydreaming is a waste of time. Regardless of whether they were larks or owls as children, most teens act like owls by naturally staying up late and sleeping in. Usually by the mid-20s, young adults go back to whichever sleeping pattern they showed before puberty, a change that may mark the final maturity of the brain. We spend one-third of our lives sleeping. Despite all that experience, few of us know what is going on during sleep. By carefully recording and observing the experience of volunteers sleeping in laboratories, psychologists have been able to unravel many mysteries of sleep. On the basis of observations of sleeping volunteers, we divide sleep into two types: rapid eye movement (REM) sleeprapid eye movement (REM) sleepThe component of sleep characterized by waveforms resembling waking, as measured by electroencephalogram, accompanied by rapid motion of the eyes, muscular paralysis, and sympathetic nervous system activation.            rapid eye movement (REM) sleep        The component of sleep characterized by waveforms resembling waking, as measured by electroencephalogram, accompanied by rapid motion of the eyes, muscular paralysis, and sympathetic nervous system activation.            , and non–rapid eye movement (N-REM) sleepnon–rapid eye movement (N-REM) sleepThe components of sleep characterized by theta and delta wave activity, as recorded by electroencephalogram, and deep physical relaxation.            non–rapid eye movement (N-REM) sleep        The components of sleep characterized by theta and delta wave activity, as recorded by electroencephalogram, and deep physical relaxation.            . These types of sleep are differentiated by EEG recordings, muscle tone, autonomic activity, and mental activity (see Figure 6.7). Sleep is divided into two components: rapid eye movement (REM) sleep and non-REM (N-REM) sleep. The EEG during REM sleep is similar to wakefulness. Stages 1 and 2 of N-REM sleep feature theta waves. Special waveforms known as sleep spindles and K complexes, which might represent the brain’s attempts to tune out environmental stimuli, appear in stage 2. Stages 3 and 4 of N-REM sleep feature delta waves and represent the deepest stages of sleep. N-REM sleep is divided into four stages. Stage 1 usually occurs when we first go to sleep. Our examination of the participant’s EEG shows patterns that are difficult to distinguish from those of the drowsy, waking volunteer. Some theta wavestheta wavesA waveform of four to seven cycles per second recorded by electroencephalogram (EEG) that is characteristic of lighter stages of non–rapid eye movement (N-REM) sleep.            theta waves        A waveform of four to seven cycles per second recorded by electroencephalogram (EEG) that is characteristic of lighter stages of non–rapid eye movement (N-REM) sleep.             (4 to 7 cycles per second), which are larger and slower than alpha waves, are now observed. At this stage, people may not be aware that they are sleeping. Frequently, we awaken a friend or a family member who has fallen asleep in front of the television by turning off the program only to have the sleeping person deny being asleep. After 10 to 15 minutes, stage 1 of N-REM sleep gives way to stage 2. Now there is no doubt that the person is asleep. Further reductions in heart rate and muscle tension occur, and the EEG begins to show special waveforms called K complexes and sleep spindles that occur only in sleep. These particular types of activity might reflect the brain’s efforts to keep us asleep while continuing to monitor the external environment. We usually sleep through familiar stimuli, such as the hum of an air conditioner, while waking in response to unexpected stimuli, such as the sound of a door opening. After about 15 minutes in stage 2, we enter stage 3 and then stage 4 N-REM sleep. Both of these stages show delta wavedelta waveA waveform of 1 to 4 cycles per second recorded by electroencephalogram that usually indicates deep non–rapid eye movement sleep.            delta wave        A waveform of 1 to 4 cycles per second recorded by electroencephalogram that usually indicates deep non–rapid eye movement sleep.             activity, which is the largest, slowest (1 to 4 cycles per second) EEG waveform. Stages 3 and 4 differ primarily in the amount of delta activity that occurs, with stage 4 having the most. We are deeply asleep in these stages. Awakening from stage 4 is difficult, and considerable disorientation may occur before a person becomes fully awake. You may have received a telephone call about an hour after you first go to sleep, when you are likely to be experiencing stage 4 N-REM sleep. If you hear the telephone, it may take several seconds to locate the phone and wake up enough to have a decent conversation. College students often neglect sleep time to study, work, and socialize. Sixty percent of students reported staying awake for an entire night at least once since coming to college (Thacher, 2008). Poor sleep habits lead to daytime sleepiness, which can affect how much you learn in your classes, how well you work, and your safety when driving and operating other machinery. How much sleepiness is too much? Try using the Epworth Sleepiness Scale to see how sleepy you tend to be (Johns, 1991). In contrast to just feeling tired, how likely are you to doze off or fall asleep in the situations in the list? Even if you have not done some of these things recently, try to work out how they would have affected you. Use the scale to choose the most appropriate number for each situation. A score greater than 10 indicates excessive daytime sleepiness. If you scored at this level or above, it’s time for some lifestyle changes. The first episode of REM sleep occurs between 90 and 120 minutes after the onset of sleep. This stage is often called paradoxical sleep, reflecting a combination of brain activity resembling waking with the external appearance of deep sleep. During REM sleep, the EEG shows activity similar to waking activity. The eyes make the periodic movements back and forth that give this stage its name. The sympathetic nervous system becomes very active. Heart rate, blood pressure, and breathing become rapid or irregular. Males experience erections, while females experience increased blood flow in the vicinity of the vagina (Hirshkowitz & Moore, 1996). If awakened during this stage, most people report vivid, storylike dreams, which we discuss in more detail in a later section. Major postural muscles are inactive during REM sleep, effectively paralyzing the sleeper, although smaller muscles in the fingers and the toes might twitch. You might have experienced dreams in which something terrible was coming, but you couldn’t move. Your dream probably reflects recognition on the part of the brain that you are paralyzed. This paralysis provides a protective mechanism that prevents you from acting out your dreams. In cases of REM sleep behavior disorder, paralysis fails, and people injure themselves or their sleeping partners. When one patient dreamed of playing football, he knocked everything off his dresser, hit his head against the wall, and banged his knee on the dresser (Schenck, Bundlie, Ettinger, & Mohowald, 2002). Because the body is paralyzed during REM sleep, sleepwalking cannot happen in this stage. Instead, sleepwalking occurs when a person is in stage 3 or 4 of N-REM sleep. Sleep talking is most common during lighter stages of N-REM sleep, but it is also possible during REM sleep. These phenomena become progressively less common with age. The cycling between REM sleep and N-REM sleep in humans follows a characteristic pattern over a period of 8 hours of sleep (see Figure 6.8). The first half of a night’s sleep is characterized by longer periods of N-REM sleep and brief periods of REM sleep. Stages 3 and 4 of N-REM sleep are especially dominant during these first 4 hours. The second half of the night’s sleep switches to a different pattern. REM sleep is now dominant, and N-REM sleep remains in the lighter stages. Stages 3 and 4 may be infrequent or absent during the last 4 hours of sleep. We usually spend the last half hour or so of the night’s sleep in REM sleep and often wake up with the awareness that we have been dreaming. SituationChance of dozingSitting and readingWatching televisionSitting inactive in a public place (e.g., a theater)Riding as a car passenger for an hour without a breakLying down to rest in the afternoonSitting and talking to someoneSitting quietly after lunch without alcoholIn a car, while stopping for a few minutes in trafficTOTAL SCORE0 = Would never doze1 = Slight chance of dozing2 = Moderate chance of dozing3 = High chance of dozingHuman error, and sleep deprivation in particular, frequently contributes to major accidents and disasters. The 1979 nuclear meltdown at Three Mile Island changed the power industry in the United States. The 1986 explosion of the space shuttle Challenger rattled confidence in the U. S. space program. The 1989 oil spill caused by the running aground of the tanker Exxon Valdez energized concerns about environmental protection. More recently, the operator of a commuter train that ran off the tracks up an escalator at Chicago O’Hare International Airport admitted to dozing off. Sleep shows a typical progression over the night. The first 4 hours are dominated by N-REM sleep, and most stage 3 and 4 N-REM sleep occurs at this time. The second 4 hours feature large amounts of REM sleep. Although we still have many unanswered questions about why we spend one-third of our lives sleeping, we know that lack of sleep can have many negative effects. Excess sleepiness on the part of decision makers has contributed to many disasters, including the Three Mile Island nuclear meltdown, the Challenger space shuttle explosion, and the grounding of the oil tanker Exxon Valdez (Coren, 2012). In 2014, a commuter train at Chicago’s O’Hare airport controlled by an operator who admitted to dozing off ran off the tracks and climbed an escalator. Staying awake for 17 to 19 straight hours produces worse reaction time than being legally drunk (Williamson & Feyer, 2000). Sleep deprivation can result in obesity and risk for type 2 diabetes (Benedict et al., 2016). Sleep, and in particular stages 3 and 4 of N-REM sleep, plays an important role in repairing the body. Sleep deprivation slows the healing of injuries (Murphy et al., 2007), reduces the activity of the immune system (Zager, Andersen, Ruiz, Antunes, & Tufik, 2007), and results in the production of fewer new neurons in adult brains (Guzmán-Marín et al., 2003). Nearly all human growth hormone, which plays important roles in repairing the body, is released during stages 3 and 4 of N-REM sleep but rarely during other stages of sleep and waking (Savine & Sönksen, 2000) (see Figure 6.9). Another line of evidence supporting the restorative hypothesis of sleep is the behavior of people following intense physical activity. Runners competing in ultramarathons (races that are twice the length of a normal marathon) experience greater amounts of N-REM sleep the night after their performance. Human growth hormone stimulates growth and repair. Most human growth hormone release occurs during the first 4 hours of sleep each night, primarily during stages 3 and 4 of N-REM sleep. The drop in time spent in stages 3 and 4 among older adults might contribute to reduced abilities to heal compared to those of younger adults. It is possible to selectively deprive volunteers in a sleep laboratory of stage 3 and 4 N-REM sleep. After a night of such deprivation, volunteers typically complain of muscle and joint pain (Moldofsky & Scarisbrick, 1976). Because time spent in N-REM sleep decreases about 30 minutes per decade after the age of 50 (Van Cauter, Leproult, & Plat, 2000), it is possible that reduced N-REM sleep is the source of some of the muscle and joint aches and pains experienced by older adults. Sleep plays a significant role in memory. Staying up all night results in poor memory performance, and two subsequent nights of normal sleep do not make up for the initial sleep deprivation effects (Stickgold & Walker, 2007). Sleep may serve as a special state for consolidating memories, in contrast to the acquisition and initial processing of new memories that occurs during waking (Rasch & Born, 2013). During N-REM sleep, memories are reactivated and transformed into versions that can be stored in long-term memory (see Chapter 9). REM sleep stabilizes the newly transformed memories. Students who wish to retain the material they’ve studied would be well advised to get a good night’s sleep (Mazza et al., 2016). Birds and mammals are the only creatures to show clear evidence of REM sleep, suggesting that this sleep stage is a fairly recent development in the course of evolution. When volunteers are specifically deprived of REM sleep, they show many of the same symptoms as people who experience an overall lack of sleep, including irritability and difficulty concentrating. In addition, REM sleep-deprived individuals show a phenomenon known as REM rebound (Dement, 1960). When allowed to sleep normally, they spend an unusually large amount of their sleep time in REM sleep. You may have experienced REM rebound during a nap following a night with little sleep. You might be aware of having dreamed vividly during the nap, and you’re unlikely to feel rested. These attempts to make up for lost REM sleep suggest that REM sleep has a necessary function in the adult brain. One clue to a possible function of REM sleep is the fact that the proportion of sleep spent in REM sleep is a function of a species’ relative maturity at birth (Siegel, 2001). The human infant, born at an immature stage of brain development relative to other primate species, spends about half of its sleep time in REM sleep (see Figure 6.10), compared to the 20% of sleep time spent in REM sleep by adolescents and adults (McCarley, 2007). Premature infants spend up to 80% of their sleep time in REM sleep. REM sleep may provide at least part of the stimulation necessary to correctly wire the immature brain (Marks, Shaffery, Oksenberg, Speciale, & Roffwarg, 1995). Infants and young children spend greater percentages of their sleep time in REM sleep than do adolescents and adults, suggesting that brain activity during REM sleep might help wire the developing brain. REM sleep and mood are tightly interwoven, providing us with a further clue regarding the possible functions of REM sleep. As we will see in Chapter 14, individuals with depression experience disruptions in their normal sleep patterns. Without treatment, they enter REM sleep earlier in the night than typical participants do, and they spend a greater proportion of their sleeping time in REM sleep. One of the possible mechanisms by which antidepressant medications regulate mood is their suppression of REM sleep. Individuals with bipolar disorder, which is characterized by moods that swing from euphoria to depression, experience an abnormally low need for sleep. These correlations between REM sleep and mood may represent a common underlying biochemical mechanism. Brainstem neurons that release serotonin and norepinephrine are especially quiet during REM sleep, although they are active during waking and somewhat active during N-REM sleep (Gottesman, 2002). These same neurotransmitters have also been implicated in the regulation of mood states, which helps explain links between REM sleep and mood. From the earliest times in our history, people have searched for the significance and meaning of their dreams. In ancient Egypt, many people believed that dreams predicted the future. In his classic book The Interpretation of Dreams, Freud (1900/1953) argued that the unconscious mind expressed itself symbolically through our dreams. These are interesting ideas, but what does science have to say about dreamingdreamingA mental state that usually occurs during sleep that features visual imagery.            dreaming        A mental state that usually occurs during sleep that features visual imagery.            ? The use of EEG technology to track a volunteer’s sleep patterns allowed researchers to awaken volunteers and assess a dream experience scientifically for the first time (Aserinsky & Kleitman, 1953;  Dement & Kleitman, 1957). Improved methods, including brain imaging, magnetic stimulation, and improved EEG, have produced further insights into dreaming (Cipolli, Ferrara, De Gennaro, & Plazzi, 2016). Rather than proposing sharp distinctions between dreaming during sleep and other types of thought, dream activity is seen as being on a continuum extending across focused thought and mind wandering during wakefulness to thought-like and dream-like behaviors during sleep. An activation–synthesis theory of dreaming suggests that the content of dreams simply reflects ongoing neural activity. For example, dreams of being unable to move in a dangerous situation might accurately mirror the muscle paralysis present during the REM state (Hobson & McCarley, 1977). Common dreams of flying or falling may be caused by the unusual activation of the vestibular system during REM sleep (Hunt, 1989). In Chapter 5, we discussed the vestibular system’s role in helping us maintain balance. Dreams with sexual content are consistent with the physical sexual arousal that occurs during REM sleep. Combining results of brain imaging experiments and cognitive psychology leads to yet another view of dreams. Dreaming behavior correlates with activity in circuits that overlap substantially with the DMN (Fox, Nijeboer, Solomonova, Domhoff, & Christoff, 2013). According to this view, daydreaming and dreaming during sleep are on a continuum. Simultaneous changes in brainstem activity result in lower sensitivity to outside stimuli, and reduced overall frontal lobe activity might lead to less self-awareness and logical thought than occurs during daydreaming. Once the dream state is initiated, the brain draws on memories of general knowledge and personal experience to produce a story line (Foulkes, 1999). Most of us are aware that mood and stress can have negative impacts on our sleep, but it might surprise you that loneliness can be a problem for good sleep quality too. Understanding the impact of loneliness on sleep helps explain some of the negative health outcomes associated with feeling socially disconnected. For example, loneliness in older adults predicted lower rates of survival 6 years later in the United States (Luo et al., 2012) and in China (Luo & Waite, 2014). You might think that loneliness affects people by making them less likely to follow good health habits, such as taking their medication and exercising, but research didn’t support this hypothesis. Instead, researchers suspected that loneliness led to poor health in part by disrupting sleep, which plays an important role in healing the body (Cacioppo et al., 2002). The Question: Is sleep quality affected by loneliness? Sixty-four undergraduates at Ohio State University participated in the study. Their levels of perceived loneliness were assessed using the Revised UCLA (University of California, Los Angeles; UCLA-R) Loneliness Scale (Russell, Peplau, & Cutrona, 1980). Their sleep quality was measured using a sleep monitor called Nightcap while they spent the night in the campus’s Clinical Research Center. Sleep quality measures included sleep onset, sleep duration, number of awakenings, and wake time after onset. The participants’ sleep quality was assessed 2 weeks later at home over five consecutive nights. Sleeping while monitored might cause the study participants some discomfort, but the most significant ethical concern in this research is the assessment of depression and loneliness. Because of the need to preserve participant confidentiality, the researchers would not be able to match depression and loneliness scores to individuals, which would allow them to warn those who needed treatment. Instead, the researchers would need to include referrals to services in their informed consent form, hoping that individuals who needed help would seek it. The participants were divided into high-, medium-, and low-loneliness groups on the basis of their scores on the UCLA-R Loneliness Scale. Sleep quality did vary significantly among these groups, with the high-loneliness group experiencing the lowest sleep quality and the low-loneliness group experiencing the best sleep quality. The medium-loneliness group had sleep quality scores between these two extremes. In particular, the loneliest participants were more likely to experience restless sleep, as indicated by their spending more time awake during the night after they had fallen asleep once. The results of this study indicated that lonely people experienced poor quality of sleep. However, there are several ways these results could be explained, as we outlined in Chapter 2. First, it is possible that the experience of loneliness causes poor sleep quality. Second, poor sleep quality could interfere with a person’s ability to function well socially, leading to loneliness. Third, loneliness and sleep quality could influence each other reciprocally. Finally, some third variable or set of variables could lead to both disruptions of feeling socially connected and sleep quality. If you think that a likely candidate for one of these third variables is depression, the researchers controlled for this by assessing depression before the study and excluding potential participants who appeared to be depressed. In addition, these various interpretations were tested in a subsequent longitudinal study. Loneliness was found to decrease sleep quality, and neither sleep quality nor any third variable that was tested (e.g., depressive symptoms) was found to influence loneliness (see review by  Cacioppo et al., 2015). These results indicate that the relationships between loneliness and health involve neural mechanisms and are not simply the result of the likelihood that an individual follows good health behaviors. The activation-synthesis theory of dream content suggests that the content of dreams represents the mind’s efforts to make sense out of real physical sensations. During REM sleep, the vestibular system of the inner ear is quite active. Because the vestibular system normally informs us about the position and movement of the head, vestibular activity during REM sleep might lead to the commonly experienced dreams of flying or falling. In lucid dreaminglucid dreamingA conscious awareness of dreaming accompanied by the ability to control the content of the dream            lucid dreaming        A conscious awareness of dreaming accompanied by the ability to control the content of the dream            , dreamers become aware that they are dreaming and may use this awareness to control or direct the content of the dream. This improved insight during REM sleep is associated with increased activity in the dorsolateral prefrontal cortex, an area of the brain believed to participate in higher-order executive functions, including people’s sense of control and voluntary behavior (Dresler et al., 2013). Lucid dreaming occurs when frontal areas of the brain inexplicably wake up during a dream (Mota-Rolim & Araujo, 2013). Observing what goes wrong in sleep provides additional insights into the natural variations of consciousness we experience. When the content of a REM dream is especially upsetting, we refer to the experience as a nightmare. If lucid dreamers become stuck in a nightmare, they may have the option of changing channels and exiting that line of content. Some researchers report that training in lucid dreaming may serve as an effective treatment for recurring, upsetting nightmares (Mota-Rolim & Araujo, 2013;  Zadra & Pihl, 1997). Nightmares are often mistaken for sleep terrorssleep terrorsA sleep disorder occurring in non–rapid eye movement sleep in which the sleeper wakes suddenly in great distress but without experiencing the imagery of a nightmare            sleep terrors        A sleep disorder occurring in non–rapid eye movement sleep in which the sleeper wakes suddenly in great distress but without experiencing the imagery of a nightmare            , but they are different phenomena. Nightmares are dreams that occur during REM sleep, while sleep terrors occur during N-REM sleep, particularly during the first 3 hours of sleep. In sleep terrors, the usually smooth transition from stage 4 N-REM sleep upward into REM sleep goes awry. The sleeper becomes acutely distressed despite remaining deeply asleep. If awakened, the person shows the disorientation and confusion typically demonstrated when a sleeper is disturbed during deep N-REM sleep. There is usually no memory of the sleep terror the next day. Twin studies suggest that there is a genetic predisposition to sleep terrors (Taheri & Mignot, 2002). After nightmares, the most common sleep problem is insomniainsomniaA sleep disorder characterized by an inability to either initiate or maintain normal sleep.            insomnia        A sleep disorder characterized by an inability to either initiate or maintain normal sleep.            , in which a person has difficulty initiating or maintaining sleep. Although most people seem to need about 7 to 9 hours of sleep per night to function effectively, wider healthy variations exist. In one case of healthy insomnia, an elderly woman slept only 1 hour per night without apparent detrimental effects (Meddis, Pearson, & Langford, 1973). Insomnia occurs in one of two forms. In cases of onset insomnia, a person lies in bed for what seems to be a long period but is unable to go to sleep. Stress and anxiety are frequent causes of this type of insomnia. Maintenance insomnia occurs when sleep is frequently interrupted or early waking occurs. These cases typically result from stress, substance use, or psychological disorders. Insomnia medications, such as zolpidem (Ambien), have minor effects on sleep, producing sleep 15 minutes faster and lengthening it for about 30 minutes, which might not offset significant side effects, including dependence on the medication, driving or walking while asleep, memory loss, and loss of coordination (Buscemi et al., 2007). Because light can interfere with the sleep hormone melatonin, the use of tablets and smartphones right before bed can interfere with sleep quality. Fortunately, exposure to daylight sun for a few hours is enough to offset any negative effects of the later light exposure (Rångtell et al., 2016). Probably the most dramatic sleep disorder is narcolepsynarcolepsyA sleep disorder characterized by the intrusion of rapid eye movement phenomena into waking            narcolepsy        A sleep disorder characterized by the intrusion of rapid eye movement phenomena into waking            . Narcolepsy consists of sleep attacks, in which REM sleep occurs during wakefulness (Dahl, Holttum, & Trubnick, 1994). In addition to sleep attacks, other aspects of REM sleep may intrude into the wakefulness of patients with narcolepsy. Cataplexy occurs when the muscle paralysis normally associated with REM sleep occurs during wakefulness without loss of consciousness. Many patients with narcolepsy also experience sleep paralysis or muscle paralysis that either precedes actual sleep or lingers once the person has awakened. Although upsetting, this paralysis is easy to resolve. Simply touching the person is enough to end the paralysis. Brain imaging can be used to compare activity during waking with activity during sleep. In this image, areas that are blue or purple are less active during REM sleep than during waking, and areas that are red or yellow are more active during REM sleep than during waking. The bizarre content of dreams occurring during REM sleep might be the result of the lower activity in the frontal lobes (purple) at the top of the image. The visual vividness of dreams might be accounted for by the higher amount of activity in the visual areas of the occipital lobes toward the bottom of the image (yellow and red). Cells in the hypothalamus that normally secrete neurochemicals known as orexins are missing or damaged in the brains of patients with narcolepsy (Thannickal et al., 2000). Although researchers have been able to selectively breed for narcolepsy in dogs (Foutz, Mitler, Cavalli-Sforza, & Dement, 1978), the contribution of genetics to human narcolepsy is more complex. Narcolepsy results from a combination of genetic vulnerability and autoimmune processes that attack the cells in the brain that produce orexins (Taheri & Mignot, 2002). These autoimmune processes are likely triggered by exposure to flu viruses (Dauvilliers et al., 2013;  Partinen et al., 2014). We take it for granted that automatic functions such as breathing continue uninterrupted during sleep, but this is not the case in sleep apneasleep apneaA sleep disorder in which the person stops breathing while asleep            sleep apnea        A sleep disorder in which the person stops breathing while asleep            . In a person with sleep apnea, breathing can stop for a minute or two, and the sleeper awakens abruptly gasping for air. Not only does this lack of air disrupt the sleep quality of the person with sleep apnea, but it can increase the risk for heart disease (Kendzerska, Gershon, Hawker, Leung, & Tomlinson, 2014). Because most cases of sleep apnea occur in people who are obese and who snore (obstructive sleep apnea hypopnea), losing weight and surgical correction of airway blockages are common treatments. Other cases occur when brainstem neurons responsible for maintaining breathing during sleep malfunction (central sleep apnea). In these cases, the use of a machine that regulates airflow during sleep is an inconvenient but effective solution. Sudden infant death syndrome (SIDS)Sudden infant death syndrome (SIDS)A sleep disorder in which an otherwise healthy infant dies while asleep            Sudden infant death syndrome (SIDS)        A sleep disorder in which an otherwise healthy infant dies while asleep            , occurs when a healthy infant simply dies while asleep. Approximately 3,500 infants die from SIDS in the United States each year (Moon & Task Force on Sudden Infant Death Syndrome, 2016). Rates prior to the 1990s were even higher. Between 1992 and 2003, rates of SIDS in the United States were cut in half when the American Academy of Pediatrics (AAP) began telling parents to put their infants to sleep on their backs instead of on their stomachs (NICHHD, 2003). As we will see in Chapter 11, young infants cannot turn over easily for the first few months of life, making suffocation on blankets and stuffed animals a greater risk. Despite her suspected narcolepsy, Harriet Tubman (1820–1913) helped hundreds of slaves escape through the Underground Railroad during the Civil War era. Tubman appears on the left in this photo with some of the former slaves whom she helped. Unfortunately, narcoleptic attacks are often brought on by stress. Tubman had a number of close escapes when she experienced sleep attacks while being pursued by Confederate troops and irate slave owners. Findings that new narcolepsy cases resulted from either infection with or vaccination for the H1N1 flu virus support the idea that narcolepsy is linked to activity in the immune system. American vaccines provoking weaker immune responses were less likely to result in narcolepsy than stronger European vaccines and the disease itself (Dauvilliers et al., 2013;  Dauvilliers et al., 2010). Despite these improvements, SIDS remains the most common cause of death in the first 6 months of life in industrialized nations, even as the cause of these cases remains elusive (Krous, 2014). Although some cases might include biological vulnerabilities in serotonin function (Paterson et al., 2006), most affected infants have additional risk factors, such as exposure to tobacco smoke (Ostfeld, Esposito, Perl, & Hegyi, 2010). Current recommendations for preventing SIDS from the American Academy of Pediatrics include putting infants to sleep on their backs; using a firm sleep surface; room-sharing without bed-sharing; avoidance of soft bedding and overheating; avoidance of exposure to smoke, alcohol, and illicit drugs; breastfeeding; routine immunization; use of pacifiers; and skin-to-skin care for newborns (Moon & Task Force on Sudden Infant Death Syndrome, 2016). Restless legs syndrome (RLS)Restless legs syndrome (RLS)A disorder characterized by the involuntary movement of an extremity, usually one leg            Restless legs syndrome (RLS)        A disorder characterized by the involuntary movement of an extremity, usually one leg            , occurs when one of a person’s limbs, usually a leg, experiences a tingling feeling and moves at regular intervals. RLS appears to be quite common, with 15% of a large sample of adults in the United States reporting these symptoms (National Sleep Foundation, 2009). RLS results from a gene variant that is active only during early prenatal development in the basal ganglia, a part of the brain involved with voluntary movement (Spieler et al., 2014) (see Chapter 4). Of special interest to psychologists is the high frequency of RLS among children and adults with attention deficit hyperactivity disorder (ADHD) and the potential genetic links between these two conditions (Schimmelmann et al., 2009). As we will see in Chapter 14, ADHD is correlated with differences in dopamine functioning, a characteristic shared by RLS (Taheri & Mignot, 2002). Sleep researcher Emmanuel Mignot is holding Bear, one of his dogs with narcolepsy. Research with dogs like Bear showed that narcolepsy is a genetic disorder often triggered by exposure to flu viruses. Sadly, Bear passed away in 2014. The number of deaths from SIDS dropped dramatically when the American Academy of Pediatrics began telling parents in 2003 to put their babies to sleep on their backs instead of on their stomachs. SIDS still occurs, however, so sleeping position is only one of many risk factors, including exposure to cigarette smoke in the home. Stage of waking or sleepBrain activity recorded by EEGObservationsWakingAlpha, beta, gammaAlternating periods of alertness and relaxation. Logical thought. Voluntary movement. Responsive to stimuli. N-REM sleepStage 1Some thetaLogical thought. Reduced heart rate and muscle tension. Some responsiveness to stimuli. Stage 2Larger amounts of thetaLess responsiveness to stimuli. Further reductions in heart rate and muscle tension. Stage 3Some deltaFurther reductions in heart rate and muscle tension. Stage 4Larger amounts of deltaProfound reductions in heart rate and muscle tension. Very low responsiveness to stimuli. REM sleepSimilar to wakingRapid motion of the eyes. Muscle paralysis. Irregular autonomic activity. Vivid dreams. If consciousness is a function of the brain, we should be able to identify aspects of consciousness that change reliably when the brain is damaged. Lessons about consciousness have emerged from the clinical study of patients with damage to specific parts of the nervous system. The effects of brain damage on face recognition illustrate how the brain integrates sensory information, memory, and emotion to form a conscious experience. Prosopagnosia is a condition that affects a person’s ability to recognize faces (Bauer, 1984). One patient was unable to identify familiar faces but was still able to show differing autonomic responses to familiar versus unfamiliar faces. Separate pathways process the features of faces, emotional responses to faces, and recognition of faces. Prosopagnosia results from damage to a part of the temporal lobes on the bottom of the brain known as the fusiform face area (FFA). In this scan using functional magnetic resonance imaging (fMRI), the FFA activity indicates that the person is looking at the image of a face as opposed to a landscape or building. In a situation that is reminiscent of many horror film classics, patients with Capgras syndrome are convinced that imposters have taken the place of familiar people. In certain tragic instances, patients with Capgras even try to harm the imposters. Capgras syndrome and prosopagnosia produce opposite effects. In Capgras syndrome, the ability to recognize faces remains intact, but the sense of emotion and familiarity is distorted. The damage associated with Capgras syndrome is not thoroughly understood, but it probably involves connections between visual areas of the brain and the amygdala (Ramachandran, 1998). Consciousness is altered not only by damage in particular areas of the brain but by the severity of damage as well. Our word coma comes from the Greek word for deep sleep. Coma produces a profound state of unconsciousness in which the person does not have sleep–wake cycles, cannot be awakened, does not respond to pain or light, and is incapable of voluntary behavior. Coma results from damage to both cerebral hemispheres or to the reticular formation (Liversedge & Hirsch, 2010). A stroke affecting a single hemisphere typically does not lead to coma. Comas last anywhere from a few days to several weeks. After a period of 2 to 4 weeks, most patients recover, move into a vegetative state (described later in this section), or die. Among those who recover, improvement is usually slow and gradual. EEG recordings of comatose patients vary depending on the problem that initially led to coma. Following resuscitation from cardiac arrest, many patients show a distinctive pattern of alpha rhythms, especially in the frontal lobes, known as alpha coma (Chatrian, 1990). These alpha patterns do not change when the patient is exposed to various types of stimulation. Other patterns feature a low level of overall activity occasionally interrupted by spikes. None of these patterns are found in the typical, healthy EEG described previously in our section on sleep and waking (see Figure 6.11). Even though the patient with the normal EEG recording is drowsy, which should produce a dominant pattern of alpha waves, the normal recording is quite different from the more uniform alpha activity recorded from a patient in a coma. Coma can be distinguished from similar conditions, such as vegetative state (VS)vegetative state (VS)An abnormal state following brain injury featuring wakefulness without consciousness.            vegetative state (VS)        An abnormal state following brain injury featuring wakefulness without consciousness.             and brain death. VS often follows a period of coma. VS is characterized by waking without consciousness. In contrast to the patient with coma, patients with VS look rather normal. Their eyes open periodically, and they grind their teeth, scream, smile, or cry. Some are responsive to pain. The presence of conscious awareness in VS remains debatable (Celesia, 2013; see Figure 6.12). Using functional magnetic resonance imaging (fMRI), a small number of patients in a vegetative state (VS) showed brain activation to specific tasks that closely resembled the activation of healthy control participants. The yellow-red areas were active when thinking about movement, and the bluish areas were active when thinking about a spatial task. This research emphasizes the need to develop new ways of assessing the cognitive correlates of brain injuries so that patients can receive the best care possible. With the advent of technologies that can artificially maintain bodily functions, including respiration and blood circulation, we have been placed in the odd position of requiring a new definition of death. Brain death is now defined as a complete and irreversible lack of measurable brain activity, as evidenced by two flatline EEG recordings taken 24 hours apart or lack of blood circulation to the brain (see Figure 6.13). In brain death, the patient shows no responses to external stimuli, including pain, and no reflexes related to the cranial nerves described in Chapter 4. Brain death is often determined by a study of blood circulation. In this image, blood can be seen flowing into the neck, scalp, and face (particularly the nose, which leads to the phrase hot nose sign). However, no blood flow is seen in the brain itself, which remains dark. A careful determination of brain death is required because of the common practice of organ donation following death. Because of improvements in technology that literally bring people back to life, more than 8 million Americans have reported a near-death experiencenear-death experienceAn altered state of consciousness reported by people who were close to death because of cardiac or other medical problems that features out-of-body experiences, light-at-the-end-of-a-tunnel perceptions, and a state of calmness            near-death experience        An altered state of consciousness reported by people who were close to death because of cardiac or other medical problems that features out-of-body experiences, light-at-the-end-of-a-tunnel perceptions, and a state of calmness             (Mauro, 1992). Patients who recover following a cardiac arrest or some other life-threatening condition often report similar experiences. These experiences typically feature an out-of-body experience in which people feel they are viewing their body from a floating position above it. In addition, many people report a light at the end of the tunnel visual experience, which may include images of deceased friends, relatives, or deities. These visual sensations are accompanied by a peaceful, calm emotional state and followed by a reluctant return to the body. The similarities among documented cases of near-death experiences suggest that the dying-revival experience produces consistent responses in the human brain (Negovsky & Gurvitch, 1995). In animal models of cardiac arrest, a surge of brain activity occurs near death, suggesting a state of heightened consciousness (Borjigin et al., 2013). Strong out-of-body sensations and other near-death phenomena occur in volunteers using ketamine (Jansen, 1996). A damaged brain may release its own chemicals, similar to ketamine, in an effort to minimize cell loss. SeizuresSeizuresAn abnormal level of brain activation with a sudden onset.            Seizures        An abnormal level of brain activation with a sudden onset.             are uncontrolled electrical disturbances in the brain that often produce changes in consciousness, providing further evidence that cortical activity is correlated with consciousness. People with recurring seizures are diagnosed with epilepsy. Seizures occur as a result of brain injury or infection but can also appear without an obvious cause. Disturbances in the activity of the inhibitory neurotransmitter GABA might account for many seizures. Drugs that inhibit GABA reliably produce seizures, while drugs that enhance GABA activity, such as barbiturates, prevent or control seizures. Seizures are classified as partial or generalized. Partial seizures originate in a particular part of the brain known as the focal area and are often accompanied by an aura or a premonition that a seizure is about to occur. Generalized seizures do not arise from a focal area. Instead, these seizures are characterized by the abnormal activation of circuits connecting the cortex and the thalamus. Generalized seizures are typically not accompanied by an aura (see Figure 6.14).(a) Partial seizures originate in a focal area and spread to cortical and subcortical structures. (b) Generalized seizures do not originate in a focal area. Once they begin, generalized seizures spread through the brain symmetrically via connections between the thalamus and the cortex. Partial seizures originating in the temporal lobe often produce distortions of consciousness, leading to the experience of déjà vu (French for already seen), a feeling that one is reliving the past, or jamais vu (French for never seen), a sense that one’s familiar circumstances are suddenly foreign or strange. Consciousness is lost in generalized seizures. The two major categories of generalized seizure are tonic-clonic and absence seizures. Tonic-clonic seizures begin with a loss of consciousness, cessation of breathing, and intense muscular contraction. This phase usually gives way to violent, rhythmic contractions that may result in broken bones or other physical injuries. These phases are followed by a period of coma, lasting about 5 minutes. In an absence seizure, the person loses consciousness and awareness of surroundings, and motor movements are limited to blinking, head turns, and eye movements. Humans have intentionally altered consciousness in religious, recreational, and healing contexts. Methods may vary dramatically, but we will see that some common themes of these efforts emerge. Individuals who deliberately alter their state of consciousness may be seeking insight, positive emotion (bliss or ecstasy), or a sense of unity or oneness. The universality of such efforts, which often underlie religious behavior, strongly suggests that these states confer some advantages, although what those advantages may be remains unknown. Historically, humans have gone to great lengths to obtain psychoactive drugspsychoactive drugsAny drug with the capability of altering a person’s state of consciousness.            psychoactive drugs        Any drug with the capability of altering a person’s state of consciousness.            , or substances that alter consciousness. Humans may be unique among animals in their willingness to voluntarily seek alterations in consciousness through the deliberate use of psychoactive substances. Most psychoactive substances are quite poisonous (livestock eating tobacco leaves in the field will die) and usually have the bitter taste associated with most poisons. On the rare occasions when nonhuman animals consume psychoactive substances (cats and catnip or elephants and alcohol), we have no way of knowing whether this consumption is motivated by a desire to alter consciousness. Before exploring commonly used drugs, we will outline several general principles related to their use, including tolerance, withdrawal, and addiction. Most animals avoid psychoactive substances, but this Swedish moose managed to get stuck in a tree after eating fermented apples that had fallen on the ground. We have no way of knowing whether the moose was motivated by a desire to experience an altered state of consciousness or it simply liked the taste of the fruit. ToleranceToleranceThe need to administer greater quantities of a drug to achieve the same subjective effect            Tolerance        The need to administer greater quantities of a drug to achieve the same subjective effect             occurs when a person must take larger quantities of a drug to produce the desired effects. The nervous system seeks to maintain homeostasis, a steady internal balance or equilibrium discussed further in Chapter 7. When a person takes a drug repeatedly, the nervous system attempts to compensate for the drug’s effects. These compensations are both biological and learned (via mechanisms described in more detail in Chapter 8). When some habitually used drugs are no longer available, the user might experience withdrawalwithdrawalPhysical responses to the removal of some habitually administered drugs            withdrawal        Physical responses to the removal of some habitually administered drugs            , a set of effects that are the opposite of those produced by the drug. Withdrawing from alcohol, which inhibits the nervous system, produces a rebound effect characterized by so much excess brain activity that life-threatening seizures often occur. Withdrawing from stimulants such as caffeine can make the user sluggish and lethargic. Not all drugs that are abused produce significant withdrawal. For example, heroin and nicotine produce powerful withdrawal effects, but cocaine usually does not (Coffey, Dansky, Carrigan, & Brady, 2000). AddictionAddictionA compulsive physical or psychological dependence on a substance or activity that continues in spite of negative consequences.            Addiction        A compulsive physical or psychological dependence on a substance or activity that continues in spite of negative consequences.             is a compulsive physical or psychological dependence on a substance or activity that continues in spite of negative consequences. Although tolerance and withdrawal are two hallmark symptoms of physical dependence on a drug, not all addictive drugs display these features. For example, cocaine is quite addictive despite its lack of withdrawal symptoms. The defining features of an addiction are the compulsive repetition of drug use or an activity and the inability to abstain from the addictive behavior despite serious negative consequences. Addiction overwhelms normal control of behavior, distorts typical systems of reward, and interferes with the recognition of problems. Should we be concerned about military veterans returning from service in Afghanistan with opium addictions? If the experiences of returning Vietnam veterans are repeated, the answer will be no. Even though heroin was readily available in Vietnam and large numbers of soldiers used heroin while serving, nearly all quit using upon their return to the United States. These observations remind us that addiction is a complex phenomenon with multiple causes. In addiction, people make choices on the basis of short-term outcomes (getting high) instead of on the basis of long-term outcomes (family, finances, job, and staying out of jail). This disruption in normal, logical decision making can occur because of distortions in one or more of three related neural systems: an impulsive system involving dopamine pathways connecting the midbrain and the basal ganglia, a reflective system in the frontal lobes that weighs the pros and the cons of a decision and controls impulses, and a craving system involving the insula (Noël, Brevers, & Bechara, 2013) (see Figure 6.15). Activity in the insula is correlated with participants’ craving for their drug of choice, and strokes that damage the insula eliminate the urge to use some addictive drugs (Verdejo-Garcia, Clark, & Dunn, 2012). Excess activity in the impulsive system, inadequate activity in the reflective system, and increased desire appear to set the stage for maintaining compulsive addictive behaviors. Addiction is correlated with activity in a number of brain circuits. (a) An impulsive, reward-seeking system that uses the neurochemical dopamine connects the midbrain (ventral tegmental area and substantia nigra) with the basal ganglia, including the nucleus accumbens. A reflective system involving the frontal lobes controls impulses and makes decisions. (b) A craving system involves the insula. Cannabis, from the Cannabis sativa plant, has a long human history. It was included in the pharmacy written by Chinese emperor Shen Neng nearly 5,000 years ago. Marijuana, the smoked form of cannabis, remains the most commonly used federally illegal substance in the United States today. The behavioral effects of cannabis are often so subtle that people may report no changes in response to its use. Most individuals experience some excitation, vivid imagery, and mild euphoria. Others respond with depression and social withdrawal. At somewhat higher doses, cannabis produces hallucinations, leading to its formal classification as a hallucinogenhallucinogenA drug that stimulates the experience of false perceptions            hallucinogen        A drug that stimulates the experience of false perceptions            . Cannabis contains more than 50 psychoactive compounds, known as cannabinoids. The most important of these is tetrahydrocannabinol (THC). THC produces some of its behavioral effects by interacting with receptors for endogenous cannabinoids, natural substances that are similar to THC in chemical composition. One type of cannabinoid receptor is located in parts of the brain involved with pain, appetite, learning, reasoning, and movement (see Figure 6.16). Another type is found elsewhere in the body, particularly in structures associated with the immune system. Cannabinoids, such as THC, interact with receptors located in a number of parts of the brain. In particular, cannabinoid receptors are numerous in the hippocampus and the prefrontal cortex. Our main endogenous cannabinoid, anandamide, removes unnecessary memories and slows movement, leading to relaxation. Unlike THC, though, anandamide’s effects last only briefly. Anandamide acts by preventing the inhibition of dopamine release by other neurochemicals. In other words, anandamide promotes dopamine activity. Because THC shares anandamide’s ability to stimulate dopamine activity, cannabis has the potential to produce dependency (Merritt, Martin, Walters, Lichtman, & Damaj, 2008). As this textbook goes to press, 29 states and the District of Columbia in the United States have laws allowing the recreational or medicinal use of marijuana (cannabis) (See Figure 6.17). Other states have decriminalized the possession of small amounts. Cannabis is already commonly used, with about 5 million daily users worldwide (Radhakrishnan, Wilkinson, & D’Souza, 2015). In spite of its common use, scientific investigations into cannabis effects are surprisingly underdeveloped (Choo & Emery, 2016). Use of marijuana (cannabis) is legal in 29 states and the District of Columbia in the United States, making a scientific understanding of its effects a priority. Scientists express mixed opinions regarding the health outcomes of using cannabis, as many variables play significant roles, such as potency of drug used, age at when use begins, extent of use, and individual genetic factors. In general, however, several public health concerns have been raised: impairment of learning and memory (even when nonintoxicated), decreased motivation, and risk for psychosis (Volkow et al., 2016). Perhaps the most controversial of these possible outcomes is the relationship between cannabis and psychosis, including schizophrenia (see Chapter 14). Evidence for a positive correlation between adolescent cannabis use and psychosis is quite strong (Volkow et al., 2016), but as you know from your study of Chapter 2, correlations do not give us the ability to point to causes. So while it might be true that cannabis use is a risk factor for psychosis, it is also quite possible that people who are prodromal, or heading in the direction of developing psychosis without yet showing full symptoms, are more likely to use cannabis. Complex interactions might occur, in which people with a particular genotype (see Chapter 3) or who smoke tobacco first, and then use cannabis, might be more vulnerable to psychosis (Gage & Munafò, 2015). Given the consequences of psychosis for public health, it is important for researchers to take an objective, unemotional look at this issue to provide individuals with the evidence they need to make informed choices. In 1938, researcher Albert Hoffman reported some unusual sensations after working with lysergic acid diethylamide (LSD). Hoffman deliberately ingested some of the chemical and reported vivid, colorful visual hallucinations. LSD is chemically similar to serotonin, and it interacts with serotonin receptors (Gonzalez-Maeso et al., 2008; Gonzalez-Maeso et al., 2007). LSD’s ability to produce hallucination remains poorly understood. One of the interesting cognitive outcomes of LSD use is the experience of flashbacks, intrusive and involuntary hallucinations, even after the drug has been discontinued (Halpern & Pope, 2003). Further research is necessary to identify the mechanisms for this experience. Lysergic acid, a metabolite of LSD is similar in chemical structure to naturally occurring serotonin. Although the exact mechanism responsible for producing hallucinations is poorly understood, we do know that lysergic acid interacts with serotonin receptors. Caffeine is a stimulantstimulantAny drug that increases the activity of the nervous system.            stimulant        Any drug that increases the activity of the nervous system.             found in a range of sources, including coffee, tea, cola, energy drinks, and many over-the-counter pain relievers (see Figure 6.18). Caffeine interferes with the inhibition normally produced by adenosine in the brain. If you keep an inhibitory substance such as adenosine from doing its job, the result is increased excitation and alertness. Although most people think immediately of coffee when they hear the word caffeine, caffeine can be found in many beverages, foods, and over-the-counter medications. Caffeine produces a withdrawal syndrome characterized by headaches and fatigue. Some people experience problems with heart rhythms related to caffeine use. Caffeine crosses the placenta easily and also enters breast milk, leading to reduced rates of growth and other complications in the fetus and breastfed infant of a mother using the drug (Bakker et al., 2010). On the positive side, caffeine use is correlated with a lower risk of Parkinson’s disease, which causes difficulty moving (Ross & Petrovitch, 2001), as well as Alzheimer’s disease and other forms of dementia (Eskelinen & Kivipelto, 2010). Because these are correlations, we don’t know whether the apparent protection against these conditions is because of some specific action of caffeine in the brain or is simply the result of the characteristics of people who enjoy large amounts of caffeine. As we mention in this chapter, many historical efforts to achieve altered states of consciousness emerged in religious contexts. These methods for producing altered states frequently move out of their original contexts, such as the use of drum circles in contemporary men’s gatherings. Use of drugs, and hallucinogens in particular, have a long history in religious rituals. When used in religious contexts, hallucinogens are often referred to as entheogens, loosely translated as generating the divine within. In South America, people were using a brew known as ayahuasca centuries ago as part of their religious rituals, but the practice has since spread globally. Ayahuasca contains the hallucinogen N, N-dimethyltryptamine (DMT), which like most hallucinogens is chemically very similar to serotonin. DMT, which occurs naturally in small amounts in the brain, produces vivid visual hallucinations and euphoria but also intense vomiting. In contrast to the acceptance of ayahuasca in South America, DMT use is illegal in most countries. However, legal exceptions have been made for use of ayahuasca in religious settings. In 2006, the U. S. Supreme Court decided to allow ayahuasca churches to continue to use tea during their ceremonies. A popular interest in ayahuasca has led to the development of ayahuasca retreats in Peru, Colombia, and Brazil. This commercialization of the practice does bring money to poor regions but threatens the original context of the ritual. Pseudo-shamans are quite ready to make money from gullible tourists. The death of several ayahuasca tourists has raised concerns about the safety of the ayahuasca retreats. Many hallucinogens have their historical roots in religious practice. In South America, a hallucinogenic tea known as ayahuasca has been used for centuries. Interest on the part of tourists has led to the establishment of ayahuasca retreats, but opinions about the commercialization of historical religious practices are mixed. Outside the religious context, scientists continue to be interested in possible applications of the main ingredients of ayahuasca for treating depression and other psychological disorders (Domínguez-Clavé et al., 2016;  Sessa & Johnson, 2015). After caffeine, the most commonly used stimulant in the United States is nicotine, usually in the form of smoked or chewed tobacco (see Figure 6.19). Nicotine mimics the action of the neurotransmitter acetylcholine, increasing heart rate and blood pressure, reducing fatigue, and improving cognitive performance while at the same time producing muscular relaxation. Normally, our bodies feel somewhat tense when we are mentally alert, so nicotine’s combination of relaxation with alertness is unique. Drug effects depend on the concentration of the drug in the blood supply, and some methods of administration produce effective concentrations faster than others. In the case of nicotine, smoking a cigarette (a) produces a faster increase in blood nicotine concentration than produced by chewing an equivalent dose of tobacco (b). However, chewing tobacco produces higher sustained concentrations of nicotine than smoking does. Despite repeated warnings of the dangers of using nicotine, it continues to be a widely used substance (see Chapter 16). Nicotine use typically begins in adolescence, and adolescents become more rapidly dependent on the substance than do adults (Dani & Harris, 2005). People with psychological disorders, schizophrenia and depression in particular, are very likely to use nicotine (Dani & Harris, 2005). Again, because these are correlations, we do not know whether nicotine has any causal role in the development of psychological disorders or whether people vulnerable to disorders are simply more likely to enjoy the effects of nicotine. The behavioral effects of cocaine and amphetamines are quite similar. At moderate doses, cocaine and amphetamines produce alertness, elevated mood, confidence, and a sense of well-being. At higher doses, these drugs can produce symptoms that are quite similar to schizophrenia (see Chapter 14). Users experience hallucinations, such as a feeling of bugs running on the skin, and delusional fears that others are trying to harm them. Some users experience repetitive motor behaviors, such as chewing movements or grinding the teeth. Methamphetamine, the current commonly abused form of amphetamines, is particularly likely to lead to symptoms of psychosis (McKetin, McLaren, Lubman, & Hides, 2006; see Figure 6.20).(a) Methamphetamine users are likely to experience psychotic symptoms, probably because of substantial loss of brain volume in areas related to memory, emotion, and reward. (b) Methamphetamine also stimulates clenching and grinding of teeth, leading to a pattern of dental decay known as meth mouth. Although they produce similar types of behavior, cocaine and amphetamines act somewhat differently at the synapse. Both drugs distort the action of dopamine transporters, gates that move previously released dopamine back into the presynaptic cell (see Chapter 4), but they do so differently. Methamphetamine mimics dopamine, leading it to be moved out of the synaptic gap into neurons by dopamine transporters. Once inside the neuron, methamphetamine enters synaptic vesicles, pushing the dopamine molecules out into the intracellular fluid of the axon terminal. The presence of large amounts of dopamine outside of vesicles makes the transporters begin to work in reverse, pushing dopamine out of the cell. With the transporters working in reverse and unable to retrieve the released dopamine, the molecules of dopamine become trapped in the synaptic gap, stimulating receptors continuously. Cocaine’s action on the dopamine transporters is simpler. Cocaine simply blocks the transporters, keeping all previously released dopamine active in the synaptic gap. Because of their direct actions on the dopamine reward systems in the brain, these drugs are among the most addictive available substances. As we will see in Chapter 15, many children diagnosed with ADHD are treated with stimulant medications. The two most common choices are methylphenidate or a combination of amphetamine salts (Adderall). Both of these drugs boost the activity of dopamine and norepinephrine, which in turn increases users’ ability to stay alert and concentrate. At clinical doses, these drugs do not seem to produce dependence, although they affect sleep and appetite. It is a common misconception that only people diagnosed with ADHD respond to these drugs with increased concentration. Everybody reacts to these drugs in similar ways (Agay, Yechiam, Carmel, & Levkovitz, 2010), which has led to an increase in their nonprescribed use by college students and others wishing to get a competitive edge in intellectual work. Before World War I, many commercial products contained cocaine. Freud originally believed that cocaine was an effective antidepressant. The ability of cocaine to produce rapid addiction eventually changed people’s minds about the safety of the drug.3,4-methylenedioxymethamphetamine (MDMA, or Ecstasy) is a relative of amphetamines and the hallucinogen mescaline that increases heart rate, blood pressure, and body temperature. MDMA increases sociability by stimulating the activity of serotonin and oxytocin (Thompson, Callaghan, Hunt, Cornish, & McGregor, 2007). MDMA mimics serotonin and is actually taken up more readily than serotonin itself by serotonin transporters. Once inside the cell, MDMA interferes with the storage of serotonin in vesicles and causes the transporters to begin acting in reverse, resulting in the release of large amounts of serotonin. Serotonin becomes trapped in the synaptic gap by the malfunctioning transporters, allowing it to continue to interact with receptors. Side effects of MDMA can include dehydration, exhaustion, hyperthermia (high core body temperature), convulsions, and death (Pilgrim, Gerostamoulos, & Drummer, 2011). MDMA has a smaller effect on dopamine than on serotonin, but the drug can still produce some dependency. MDMA (Ecstasy) appears to have long-term detrimental effects on neurons that release serotonin, associated with mood, appetite, and sleep. Use of MDMA is controversial, with some scientists expressing concern about its apparent detrimental effects on neurons that release serotonin (Biezonski & Meyer, 2011;  Capela et al., 2007). Some therapists, however, argue that MDMA might be beneficial to individuals with posttraumatic stress disorder (PTSD). Strong research evidence for such benefits is lacking (Parrott, 2007), but studies using participants with PTSD who had not responded to conventional treatments showed some promise (Mithoefer, Wagner, Mithoefer, Jerome, & Doblin, 2011;  Sessa, 2016). Alcohol is one of the earliest psychoactive drugs used by humans, dating back into our prehistory. In addition to seeking the relaxation produced by alcohol, early humans might have turned to fermented beverages as a safety precaution against contaminated water supplies because alcohol has natural antiseptic qualities. At low doses, alcohol dilates blood vessels, giving people a warm, flushed feeling. It reduces anxiety, which makes relatively shy people more outgoing at a party. At higher doses, alcohol’s inhibition of the higher levels of the brain leads to aggression, risky behaviors, and poor motor coordination. At very high doses, alcohol can produce coma and death, either from suppression of breathing or aspiration of vomit. Alcohol produces its main behavioral effects by boosting the effects of GABA, the main inhibitory neurochemical in the brain. By boosting an inhibitor, alcohol depresses the activity of the brain, particularly that of the cerebral cortex. At the same time, alcohol blocks receptors for the brain’s major excitatory neurotransmitter, glutamate. Finally, alcohol also stimulates reward pathways that release dopamine, which probably accounts for its addictive potential. Further discussion on the detrimental effects of alcohol on the developing fetus will be discussed in Chapter 11. Alcohol’s impact on health in general is discussed in Chapter 16. Natural or synthetic substances that interact with endorphin receptors are known as opioids. Opioids that are derived from the opium poppy are known as opiates. The opium poppy is the source of several psychoactive opiates, including morphine and codeine. Heroin can be synthesized through further processing of morphine. These substances are effective because they imitate the action of endorphins (short for endogenous morphine). Neurons possessing endorphin receptors are located in parts of the brain involved with pain, stress, and attachment. The activity of both natural endorphins and opioids results in the release of large amounts of dopamine, leading to feelings of well-being and reduction of pain. Opioids have legitimate medical purposes, including pain control, cough suppression, and control of diarrhea. At low doses typical in medical practice, opioids produce a sense of euphoria, pain relief, reduced anxiety, muscle relaxation, and sleep. The higher doses characteristic of opioid abuse affect consciousness by producing a strong, rapid onset feeling of euphoria, or a rush. With even higher doses, opioids exert a strong depression on breathing, occasionally leading to death. Among the current frequently abused forms of opioids is the relatively new painkiller oxycodone hydrochloride (OxyContin), reportedly used by more than 8% of recent 12th graders (Johnston, O’Malley, Bachman, & Schulenberg, 2011). In 2010, the U. S. Food and Drug Administration (FDA) approved a new formulation of OxyContin designed to reduce rates of abuse. Unfortunately, efforts to reduce OxyContin abuse usually result in compensatory increases in heroin use (Cicero, Ellis, & Surratt, 2012) (see Figure 6.21). Abuse of painkillers in general continues to be a serious concern in public health. Although the U. S. Food and Drug Administration (FDA) approved a new formulation of OxyContin designed to be less prone to abuse, the overall levels of painkiller abuse were not affected much. Instead, a compensating increase in heroin use was observed beginning in 2010 as users apparently switched their drugs of choice. HypnosisHypnosisAn altered state of consciousness characterized by relaxation and increased suggestibility            Hypnosis        An altered state of consciousness characterized by relaxation and increased suggestibility             is a state of consciousness involving focused attention and reduced peripheral awareness characterized by an enhanced capacity for response to suggestion (Elkins, Barabasz, Council, & Spiegel, 2015, p. 6). Misunderstandings about what hypnosis can and cannot do are common. Early research suggested that hypnotized people were more likely to comply with requests to do dangerous things, such as pick up a poisonous snake or throw a beaker of what they believed to be acid into an experimenter’s face (Rowland, 1939;  Young, 1952). However, later experiments showed that equal compliance could be obtained with participants who were not hypnotized (Orne & Evans, 1965). In these situations, the participants’ strong belief that the experimenters would be unwilling to harm them overrode any measurable influence of hypnosis. Contrary to popular belief (and a number of television crime shows), hypnosis does not sharpen memory and is instead likely to make it worse (Register & Kihlstrom, 1987). Hypnosis does have legitimate uses in therapy, however, especially in cases of habit change (quitting smoking, etc.) and pain relief. Frustrated licensed psychotherapist Steve Eichel demonstrated the lack of regulation of hypnotherapists by acquiring a number of impressive-looking credentials for Zoe D. Katze, who happens to be Eichel’s cat. Zoe’s name is a wordplay on the German die Katze, which literally means the cat. One study that illustrates both the power and the limitations of hypnosis used a task known as the Stroop test (Raz, Kirsch, Pollard, & Nitkin-Kaner, 2006;  Stroop, 1935). In this test, participants are simply asked to respond to the color of the letters they are shown (see Figure 6.22). When the ink color and the word are congruent (e.g., the word red appears in red ink), reaction time is significantly faster (100–200 milliseconds faster) than when the ink color and the word are incongruent (e.g., the word red appears in green ink). Nearly everyone who can read shows a robust Stroop effect, and extensive training does not make the effect go away. This effect is typically explained in terms of a well-practiced skill (reading) taking precedence over a less practiced skill (naming the colors of letters). When instructed to ignore the practiced skill, attention is compromised, and reaction time becomes slower. Under normal circumstances, results of the Stroop test show that participants identify ink colors faster when (a) the color of the letters matches the word being read (the word red appears in a red font) than when (b) it doesn’t match (the word red appears in a yellow or green font). Highly suggestible hypnotized participants who were told that they were reading nonsense words instead of real words did not show this typical difference. Their reaction time for naming the ink color was the same whether it matched the color name or not. Using a posthypnotic suggestion,  Raz et al. (2006) told their participants to respond to a cue by viewing the stimuli as gibberish instead of real words. Participants were assessed for their individual differences in suggestibility, or the ease with which someone can be hypnotized. Participants who were rated as having low suggestibility performed in a typical manner on the Stroop test. In contrast, participants rated as having high suggestibility demonstrated little difference between the amount of time needed to identify the ink colors in the congruent and the incongruent conditions. The ability of posthypnotic suggestion to minimize the Stroop effect is remarkable because other efforts to modulate the effect have all failed. Clearly, hypnotic suggestion has the ability to reorganize cognitive processes, at least in highly suggestible individuals. MeditationMeditationA voluntary alteration of consciousness characterized by positive emotion and absence of thought            Meditation        A voluntary alteration of consciousness characterized by positive emotion and absence of thought             techniques vary widely and may occur in either religious or nonreligious contexts. When experienced practitioners are asked to describe the subjective experience produced by meditation, they describe a conscious state without thought, accompanied by a blissful emotional state (Travis, 2001). A variation on meditation known as mindfulness meditation has been used more frequently over recent years to treat substance abuse and a variety of psychological disorders (Hölzel et al., 2011). Mindfulness refers to nonjudgmental awareness of experiences in the present moment (Hölzel et al., 2011, p. 537). Meditation incorporating mindfulness also appears to produce benefits in immune function, blood pressure, stress hormone levels, and cognitive functioning. EEG recordings during meditation feature increased proportions of alpha waves, which we described earlier as being characteristic of an awake but relaxed state. In participants who had meditated more than 3 years, increased theta activity, normally found in lighter stages of sleep, was observed over the frontal lobes of the brain (Aftanas & Golocheikine, 2001). Theta activity in the prefrontal areas of the brain has been correlated with positive emotions (Aftanas, Varlamov, Pavlov, Makhnev, & Reva, 2001). Imaging studies using functional magnetic resonance imaging (fMRI), suggest that meditation represents a voluntary regulation of attention and autonomic functions (Lazar et al., 2000). Brain imaging studies have discovered that experienced meditators showed less activity in the brain’s DMN, which was described earlier in this chapter (Brewer et al., 2011). As we noted previously, activity in the DMN is usually correlated not only with mind wandering and thinking about the self but also with unhappiness. Relative inhibition of the activity of the DMN through meditation might be responsible for the feelings of bliss associated with this practice (see Figure 6.23). Meditation is associated with a decrease in activation in the prefrontal cortex and posterior cingulate gyrus, which are important parts of the DMN, of experienced meditators compared to control participants who were introduced to the meditation methods right before being scanned. The amount of decrease in signal strength is shown (a) in the prefrontal cortex and (b) in the posterior cingulate cortex. The colored bars represent three methods of mindfulness meditation: choiceless awareness is shown in green, loving kindness is shown in red, and concentration is shown in blue. Meditation appears to result in physical changes in the brain (Fox et al., 2014). Meditation has been correlated with increased cortical thickness (Lazar et al., 2005) and increased gray matter in the brainstem (Vestergaard-Poulsen et al., 2009). The increased cortical thickness was more noticeable in older participants, suggesting that meditation might offset typical age-related thinning of the cortex. A variety of environmental conditions have significant effects on consciousness, including exposure to high altitudes during mountain climbing, underwater conditions in scuba diving, starvation, sleep deprivation, sensory deprivation, twirling, and rhythm-induced trance. These conditions may lead to hallucination and other consistent changes in cognition and emotion. Several of these environmental factors characterize vision quests, which typically take place within religious contexts. In the traditional Native American vision quest, preparation typically involved 3 to 4 days of starvation and sleep loss. The individual would travel to an isolated location where no shelter could be built. The seeker would then be instructed to concentrate and watch for the appearance of a guardian. This combination of events reliably led seekers to experience the following phenomena: a distorted sense of time, a sensation of oneness, the appearance of light, the emotion of ecstasy or bliss, and a sense of improved insight (Steiger, 2000). We have all observed young children experiment with their bodies by spinning. Although we do not know the precise reason for such spinning behavior, other than that it is fun, spinning probably represents the child’s efforts to alter sensory and cognitive states. Spinning is also used as a meditation-inducing method by the Sufis, a mystic Muslim sect. Sufis may whirl for an hour or more before falling to the ground, where they seek a feeling of oneness with the Earth. Sufis belong to a mystic Muslim sect that uses spinning as a means of inducing a state of meditation. Spinning can last an hour or more, after which the Sufi typically falls to the ground and experiences a sense of oneness with the Earth. It is likely that dancing has been a part of human cultures for thousands of years, both for recreational and for religious purposes. Accompanied by the equally ancient practice of drumming, dance is capable of initiating trancelike states in the participants (Vaitl et al., 2005). A trance state typically refers to an induced state of consciousness in which a person is less responsive to external stimuli (Tassi & Muzet, 2001). The altered states induced by drumming and dancing include distortion of time, vivid imagery, and strong positive emotions such as ecstasy and bliss (Vaitl et al., 2013). Participants listening to rhythmic drumming showed more theta wave activity in their EEGs, normally associated with light N-REM sleep, than did participants listening to unstructured beat sequences (Winkelman, 2003). Modern youth are fond of combining dance and psychoactive drugs to alter consciousness, particularly in the context of raves, which are large parties featuring electronic music, darkness, and the use of hallucinogens or MDMA. The raves feature some themes common to the contexts we have described previously. Unity, or oneness, and positive emotion are again primary goals reported by the participants. We engage in a variety of unconscious behaviors, and not just when we’re asleep. Among the unconscious behaviors that have been identified by psychologists is one that is particularly relevant to relationships—the effect of mimicry on liking and feeling close to others (Lakin, Chartrand, & Arkin, 2008). A substantial body of research supports the idea that we unconsciously copy the behavior of others and that when we do copy another person’s behavior, that person likes us better without necessarily knowing why. Under what circumstances are we most likely to copy the behavior of other people? One possible answer is that we mimic others to avoid or repair social exclusion. Jessica Lakin and her colleagues (2008) made their participants feel socially excluded (temporarily) and then observed that these excluded participants were more likely to mimic another person than were participants who had not been socially excluded. This process extends to group membership as well. Participants excluded by an in-group were subsequently more likely to mimic an in-group member than an out-group member. These results suggest that mimicry developed as an automatic behavior that we could use to respond to threats to our social connectivity, which as we have stated many times previously, could have been a matter of life or death to our hunter–gatherer ancestors. We often imitate the behavior of others without being consciously aware that we’re doing so. Imitation seems to enhance liking and social inclusion. Because this mimicry is quite automatic and unconscious, we are usually not aware that we are behaving this way. Knowing now that this is an adaptive behavior, look more closely at how you behave when encountering situations in which you feel excluded. It could be a social situation, such as at a party or an organizational event, where you are trying to mingle or initiate a dialogue with others. Or perhaps you observe mimicry when you are around someone you like romantically. Do you feign interest in things you wouldn’t necessarily be interested in otherwise? Are you more apt to align yourself with the personality of the person you find interesting? Do you see this kind of behavior in friends or family? Attorneys often remind us that what is right is not always legal, and what is legal is not always right. One of the tragic outcomes of cyberbullying is suicide by the victim, leading to the coining of the term bullycide. In spite of the obvious role the bully is playing in these cases, the U. S. legal system is not always able to prove that the victim’s suicide was caused by the bully rather than an act of free will. Several recent cases illustrate the legal challenges in cyberbullying cases. In 2010, Tyler Clementi jumped to his death in the Hudson River after realizing his roommate and a friend had livestreamed images of Clementi having sex with another man. Although the roommate was found guilty of all the charges brought by the prosecution, his sentence was 30 days in jail. In 2007, 47-year-old Lori Drew played a terrible trick on one of her daughter’s friends, Megan Meier, after the two girls had a falling out. Drew impersonated a boy online and convinced Megan the boy liked her. Subsequently, Drew posted a note to Megan saying that the world would be a better place without you. Megan hung herself, but in the absence of any relevant statutes, Drew went completely unpunished. Rutgers first-year student Tyler Clementi, a gifted violinist, committed suicide when he discovered that his roommate had secretly filmed him having sex with another man and had broadcast the film on the Internet. Because the U. S. legal system views suicide largely as an act of free will, the roommate was sentenced to only 30 days in jail. Finding a person guilty of homicide in the United States requires a strict legal standard of causality that quickly becomes complicated in cases where someone has driven another to suicide. The current legal system, based on long traditions of free will, does not hold bullies responsible for a victim’s suicide unless they inflict severe physical injury or make the victim mentally irresponsible (Rogers, 2014). Changes in legal interpretations that consider the special nature of cyberbullying might result in more intuitively fair outcomes. If the legal system takes into account the imbalance of power that commonly exists between a cyberbully and victim, the nature and severity of the bullying, and the ability of the victim to avoid the bullying, it might become more likely for perpetrators like Lori Drew to be held accountable for their actions. Drug classActs onLSDSerotoninMarijuanaEndogenous cannabinoidsCaffeineAdenosineNicotineAcetylcholineCocaine, amphetamines, and methamphetamineDopamineMethylphenidateDopamineMDMA (Ecstasy)Serotonin, oxytocinAlcoholGABA, dopamineOpioidsEndogenous endorphins`;\n","export const text = `Axons from the sympathetic nervous system form connections in the gut, contributing to those butterflies we feel at times of excitement. Emotion and motivation, the topics of this chapter, involve neural circuits that often operate below the level of our conscious awareness. We can zoom in to look at these neural circuits, like these sympathetic axons (in blue in the large image on the previous page) forming connections with the gut. We have all had the feeling of butterflies in our stomach when we are excited, and these types of neural pathways are responsible for such feelings. Zooming out, we can examine emotion and motivation in the larger context of the individual using the example of elite athletes at the Olympic Games. The 2016 Rio Games featured 11,544 athletes from 205 countries, a tiny fraction of the millions of people who compete in athletics worldwide. To stand out among these elites takes extreme motivation, not to mention talent and hard work. Many athletes competing at this level have focused on their sport to the exclusion of most other activities since they were in elementary school, yet of the more than 11,000 participants, only 972 (or 8%) went home with a medal. Given the odds of obtaining a medal, you would think that any athlete winning one would be ecstatic, but that is not always the case. As you can see in the photograph of Olympic swimmers on the medal stand, the athletes are showing a range of emotions. Look for a moment at the way the three athletes are holding their flower bouquets. The gold and bronze medalists are holding their bouquets straight up, but the silver medalist is close to dropping his bouquet. His entire demeanor says dejection and disappointment. Why would a silver medalist be disappointed with such an exceptional achievement? To answer this question, we must zoom out even farther from the individual to consider the social context. Psychologists have found that the reactions of these swimmers are quite typical (McGraw, Mellers, & Tetlock, 2005;  Medvec, Madey, & Gilovich, 1995). Silver medalists are more likely to compare themselves to gold medalists, which leads to disappointment, while bronze medalists compare themselves to the fourth-place finishers who do not get a medal, which leads to joy. Emotions are spontaneous, automatic responses to our ongoing perceptions and thoughts. In this chapter, we will explore the mechanisms responsible for our emotions and motivations, beginning with the underlying physical mechanisms and zooming out to look at individual and, ultimately, social influences on these behaviors. Emotion and motivation share the experience of subjective feelings and engage similar processes and structures in the brain. However, psychologists find it useful to distinguish between these two concepts. An emotionemotionA combination of arousal, physical sensations, and subjective feelings that occurs spontaneously in response to environmental stimuli.            emotion        A combination of arousal, physical sensations, and subjective feelings that occurs spontaneously in response to environmental stimuli.             is a brief combination of physical sensations, such as a rapid heartbeat, and conscious, subjective feelings, like feeling afraid, that prepares an individual for action. Emotions are spontaneous, automatic responses to our ongoing perceptions and thoughts. We communicate our emotions to others through behaviors such as facial expression, body language, gestures, and tone of voice. If we are thirsty following a tough workout, we are motivated to seek a drink of water. It is unlikely that a thirsty person would be motivated to find a hamburger instead. Psychologists have debated whether emotions should be viewed as discrete stand-alone states or points on a continuum. The discrete approach views emotional states such as happy, sad, or angry as being like primary colors of red, yellow, or blue, while the continuous view sees emotions like a rainbow, with particular emotions being located closer to some than others and even blending together (Russell, 1980; see Figure 7.1). Like many of the either/or discussions that we encounter in psychology, a compromise might be the best strategy. If we think instead of emotions as being stars, we can imagine a constellation containing clusters of positive emotions such as happiness and pride and another constellation containing clusters of negative emotions such as anger and sadness. Russell (1980) suggested that emotions could be viewed along dimensions of pleasant/unpleasant and high arousal to low arousal instead of as isolated states. This approach to emotion suggests that some emotions are closer together than others. Emotions can be distinguished from moods. A mood is a more general state than an emotion. You can be in a good mood while feeling a variety of specific emotions, such as happiness, pride, or relief. A mood generally lasts longer than a single emotion. For example, when we discuss psychological disorders in Chapter 14, we will see that criteria for depression specify that depressed mood should characterize at least half a day every day for 2 weeks (American Psychiatric Association [APA], 2013). Because emotions are responses to the ongoing and ever-changing flow of environmental information, it is unlikely that any single emotion would last this long. Many animals are motivated to explore their surroundings even when they have no immediate needs because being familiar with your neighborhood saves time when a need does arise, whether that is food, water, shelter from a storm, or a gas station. Today, many people rely on technology to help them navigate in unfamiliar locations. MotivationMotivationA process that arouses, maintains, and guides behavior toward a goal.            Motivation        A process that arouses, maintains, and guides behavior toward a goal.             is defined as a process that arouses, maintains, and guides behavior toward a goal. For example, we are motivated to seek a drink of water in response to thirst. The process of motivation is accompanied by distinct emotional states. Thirst is generally quite distressing, and taking a drink of water can produce positive emotions like relief and happiness. Emotion and motivation share the ability to arouse an organism and stimulate behavior, but motivation does so more directly and precisely than emotions do. People who feel motivated by thirst are likely to do one thing: seek something to drink. In contrast, experiencing the emotion of sadness stimulates behavior, but that behavior can take many forms. Some people respond to sadness by crying in a room by themselves, while others seek the company of friends. Whether we’re experiencing happiness or sadness, anger or disgust, an emotion combines a physical sensation, such as a rapid heartbeat, and a conscious, subjective feeling, like joy or sadness. Psychologists have asked questions about how the physical sensations and subjective feelings might relate to one another. Several major theories of emotion describe the relationship between the physical responses and subjective feelings experienced during an emotion. In other words, what are the connections between those butterflies in your stomach and the knowledge that you’re feeling afraid? These theories of emotion are similar in many ways to efforts to explain perception in Chapter 5. Some attempt to work from the bottom up (from physical sensation to cognitive appraisal), while others work from the top down (from appraisal to physical sensation). Working independently, William James and Danish physiologist Carl Lange developed similar descriptions of the relationship between physical sensations and subjective feelings in emotion (James, 1890; Lange, 1885/1912). Consequently, the James-Lange theoryJames-Lange theoryA theory of emotion that proposes that physical sensations lead to subjective feelings.            James-Lange theory        A theory of emotion that proposes that physical sensations lead to subjective feelings.             bears the names of both men (see Figure 7.2). Each of these theories of emotion— (a) the James-Lange theory, (b) the Cannon-Bard theory, and (c) the Schachter-Singer two-factor theory—attempts to explain how physical responses and subjective feelings occur when we experience an emotion. James believed that emotions could arise from several sources, including the recall from memory of events that were emotional. However, his greatest interest was in emotions that have a distinct bodily expression (James, 1884/1969, p. 189). At the core of the James-Lange theory is the idea that this type of emotion is the result of a sequence of events. Once the individual perceives a stimulus, such as a grizzly bear, that person experiences a physiological response. This physiological response is subsequently interpreted by the individual, giving rise to a conscious awareness of a subjective feeling. The James-Lange theory assumes that physical states related to each type of feeling (sadness and happiness, for instance) are distinct from one another. The theory also assumes that we are capable of correctly labeling these distinct physical states as separate feelings. As James wrote, We feel sorry because we cry, angry because we strike, afraid because we tremble (1890, p. 1006). James’s statement may seem counterintuitive to you. Shouldn’t a good cry make us feel less sad, not more? Many people also believe that holding anger inside instead of expressing it somehow makes feelings of anger build up and become worse. These common sense notions support the concept of catharsiscatharsisA theory of emotion that views emotion as a reservoir that fills up and spills over; it predicts that expressing an emotion will reduce arousal.            catharsis        A theory of emotion that views emotion as a reservoir that fills up and spills over; it predicts that expressing an emotion will reduce arousal.            . In catharsis, emotions are viewed as filling an emotional reservoir. When the reservoir fills up, the emotion will spill over in an uncontrolled manner. For example, we talk about a buildup of hostility or anger that somehow overflows, producing a more extreme response than is warranted by the situation. Catharsis theorists recommend that we avoid this buildup by regularly expressing our emotions. In other words, it might be better to express some anger regularly than to repress your anger and explode. Catharsis and the implications of the James-Lange theory are contradictory. One suggests that expressing an emotion reduces that feeling, while the other argues that expressing an emotion leads to the subjective feeling. Which is correct? If we are feeling sad, does having a good cry make us feel better or more aware of our sadness? One clue to our dilemma comes from an experiment in which participants were directed to make specific movements of the face (Levenson, Ekman, & Friesen, 1990). For example, they might be instructed to raise their eyebrows and form their mouth into the shape of an O. They were not informed that they were to make any particular facial expression, nor were they allowed to view their face in a mirror. They did, however, report feeling the emotion they had portrayed; you might have experienced a feeling of surprise if you followed the same directions. This result is consistent with the notion that expressing an emotion leads to the subjective feeling, rather than reducing it in ways predicted by catharsis. This experiment also supports the James-Lange approach by suggesting that feedback from the body helps us consciously determine the state of our emotions. Consistent with the James-Lange theory of emotion, the physical expression of an emotion might lead to subjective feelings. Traditional Maori warriors prepared for battle by performing a haka. The New Zealand All Blacks rugby team also uses the haka before a game. The James-Lange theory would predict that these men would feel more aggressive after performing the haka. The catharsis approach is further challenged by the phenomenon of psyching up. Traditional Maori warriors of New Zealand prepared for battle by performing a haka, featuring ritual grimaces, vocalizations, and battle moves. A contemporary rugby team, the New Zealand All Blacks, uses the traditional haka of the players’ homeland to prepare for games. Not only do these performances contribute to the athletes’ physical and emotional preparation for participation in a rather violent sport, but they no doubt intimidate the opposing team as well. One implication of the James-Lange theory is that we might be able to influence our subjective feelings by changing our physical sensations. By appearing to be happy, we might begin to feel happier. Another practical outcome of this approach is the impact of modeling facial expression on empathy. The early back-and-forth imitations of facial expressions by adults and infants may contribute to the capacity to read others’ emotions. When students are asked to determine the emotions expressed in the photos on page 245 during a classroom presentation, it is not uncommon to see some individuals spontaneously copy the facial expression in question to identify it correctly. John Stibbard, son of the owner of the Capilano Suspension Bridge, poses on the bridge with the Olympic torch after completing his leg of the torch run in preparation for the 2010 Winter Olympics in Vancouver, Canada. We might not be as good at reading our physical states as the James-Lange theory requires us to be. One experiment that challenged our ability to distinguish correctly among the physical states of different emotions was carried out in Capilano Canyon, located in British Columbia (Dutton & Aron, 1974). Capilano Canyon may be crossed at one of two locations. The first crossing involves a 450-foot-long bridge with low handrails, suspended about 230 feet above rocks and rapids. Farther upstream, the canyon may be crossed on a solid wooden bridge that is less frightening. In the study, an attractive female experimenter approached single men crossing one of the two bridges. After completing a short interview, the experimenter gave each man her telephone number, in case he had further questions about the experiment. Not only did the men on the frightening bridge include more sexual content in their interviews, but they were about four times as likely to telephone the female researcher later. Although the study is flawed by a failure to randomly assign men to bridges (bolder men might be both more likely to choose the scary bridge and to telephone a stranger), it stimulated further thinking about the James-Lange theory. The results of this study suggest that its participants were not very good at discriminating the physical sensations associated with fear and sexual arousal. We can assume that the attractiveness of the female would be constant, suggesting that the setting in which she was seen largely accounted for the differences in sexual content in interviews and the likelihood of seeking further contact with her. One might speculate that similar mistakes underlie the popularity of horror films and amusement parks with impossibly high roller coasters as dating activities. Scare your date to death, and he or she may interpret feelings of fear as love. These results suggest that in some circumstances, we might not be as good at interpreting our physical sensations as predicted by the James-Lange theory. Walter Cannon disagreed with the James-Lange theory and proposed his own theory, which was later modified by Philip Bard, resulting in the Cannon-Bard theoryCannon-Bard theoryA theory of emotion featuring the simultaneous and independent occurrence of physical sensations and subjective feelings during an emotional experience.            Cannon-Bard theory        A theory of emotion featuring the simultaneous and independent occurrence of physical sensations and subjective feelings during an emotional experience.             (Bard, 1934;  Cannon, 1927). The James-Lange theory proposes a sequence of events, from physical sensations to subjective feeling, but the Cannon-Bard theory proposes that both factors occur simultaneously and independently. How are these theories different? Let’s assume that you are innocently reading your textbook in your room when a bear walks in the door. According to the James-Lange theory, the sight of the bear would immediately set off physical sensations that you would then cognitively interpret as fear. According to the Cannon-Bard theory, the sight of the bear would immediately and simultaneously trigger a subjective feeling of fear (whoa, there’s a bear in my room) and physical sensations (probably the autonomic nervous system’s fight-or-flight response in this example). Unlike the James-Lange theory, the Cannon-Bard theory does not assume that the experience of a subjective feeling depends on physical sensations. The Cannon-Bard theory fares somewhat better in explaining the Capilano Canyon results. Cannon and Bard would be comfortable with the notion that fear and sexual arousal would produce similar physical sensations and that study participants may simply have erred in their cognitive assessment of the situation. Instead of saying, I’m really scared because I’m on this bridge, the participants would say, I think this interviewer is very attractive. According to this model, a person’s cognitive assessments of an emotional situation work independently of any physical sensations that might occur. Stanley Schachter and Jerome Singer (1962) proposed a two-factor theory of emotion that adds an intermediate step between physical sensations and subjective feelings. Emotional arousal signals us to make a conscious, cognitive appraisal of our circumstances, which then allows us to identify the emotion we’re experiencing. The same physical sensations may lead to several interpretations, based on the way an individual assesses a situation. Returning to our example of the bear entering your room, we can see how the Schachter-Singer two-factor theorySchachter-Singer two-factor theoryA theory of emotion in which general arousal leads to assessment, which in turn leads to subjective feelings.            Schachter-Singer two-factor theory        A theory of emotion in which general arousal leads to assessment, which in turn leads to subjective feelings.             differs from the James-Lange and Cannon-Bard approaches. For Schachter and Singer, the sight of the bear would initiate a general state of arousal. To identify the source of your arousal, you would assess your situation, attribute your arousal to the presence of a bear in your room, and identify your feelings as fear (with considerable accuracy, we would assume). Schachter and Singer (1962) directly tested this approach by injecting volunteers with epinephrine (adrenalin), which causes a high level of arousal. The participants had been told that they were getting an injection of a vitamin and that their vision would be tested as soon as another volunteer arrived to participate in the experiment. The second volunteer was actually an actor employed by the researchers. In half of the trials, the actor behaved in a happy, silly manner; in the other half, he acted angry and stomped out of the room. The results supported a role for cognitive appraisal of the environment in the identification of emotion. Participants who were exposed to the happy actor rated themselves as feeling happy, while participants exposed to the angry actor felt more negative. Importantly, when the participants were accurately informed that they were getting a drug that produced arousal instead of a vitamin, the behavior of the actor did not influence their assessment of the situation. Instead, they attributed their feelings to the drug. Botox, a deactivated form of the same toxin that causes botulism poisoning from spoiled food, helps people with migraine headaches and neurological disorders of movement, but it is also used for cosmetic purposes. Botox reduces wrinkling by paralyzing the small muscles used in facial expressions. The ability of Botox to prevent facial movement provides an interesting test for the James-Lange theory (Neal & Chartrand, 2011). Without feedback from our facial expressions, can we identify our own emotions? Without the ability to imitate the facial expressions of others, can we still identify how they’re feeling? The Question: How would increasing and decreasing the ability to move facial muscles affect participants’ judgments of the emotions of others? In a first experiment, 31 female participants were recruited from cosmetic surgery clinics. Of those participants, 16 had been treated with Botox, and the other 15 had been treated with a dermal filler material, which does not restrict the movement of facial muscles. Participants viewed photographs of eyes and the surrounding area and were asked to choose which of four emotional adjectives (happy, sad, and so on) best fit the expression they saw (see Figure 7.3). In a second experiment, a gel that forced participants to put more effort into their facial movements was placed on the faces of half of a group of 95 participants and on the arms of the other half as a control. Participants completed the same test of identification of emotion as before, along with two control tests: identifying emotion in voices, which is unlikely to be affected by facial expression, and a set of arithmetic questions. Actress Rachel Weisz, star of The Mummy and The Bourne Legacy, is not a fan of Botox. She told an interviewer, Acting is all about expression; why would you want to iron out a frown? Following treatment for wrinkles with either Botox or an injected dermal filler (Restylane), study participants were asked to identify the emotional expression of faces presented on a computer screen. The dermal filler group was significantly better than the Botox group at correctly identifying the emotions. This result suggests that by reducing the ability to imitate another person’s facial expressions, Botox limits a person’s perception of the emotions of other people. Because the participants were voluntarily seeking the Botox or dermal filler treatments, their participation in the experiment did not increase their risk. Their confidentiality must be strictly maintained, and a debriefing about the findings should be available. In Experiment 1, the Botox group performed significantly more poorly on the identification of emotions task than the control group. In Experiment 2, participants with the facial gel performed better than those with the gel on their arms. Both experiments support the idea that feedback from facial expressions of emotion influences the ability to identify emotional states in other people. The participants in the first experiment who used Botox had less facial mobility than the participants who used the dermal filler. The lack of facial mobility caused by Botox interfered with the participants’ ability to accurately assess another person’s facial expression of emotion. In the second experiment, making the muscles work harder to imitate a facial expression by applying a restrictive gel actually improved the participants’ ability to identify the facial expressions of others. These results are quite consistent with the James-Lange theory’s emphasis on feedback from the body as important to the subjective identification of emotion, not just in ourselves but also in other people. These results suggest that an important side effect of Botox might be a reduction in social competence and empathy because of an inability to read the emotions of other people correctly. A later attempt to replicate Schachter and Singer’s study did not succeed (Marshall & Zimbardo, 1979). As we discussed in Chapter 2, a failure to replicate a study casts serious doubts on the results. Nonetheless, Schachter and Singer’s theory has provided considerable insight into how we identify our subjective feelings. Schachter and Singer account easily for the Capilano Canyon bridge study. Once aroused by being on the scary bridge, the male subjects assessed their circumstances and attributed their feelings to sexual attraction. Additional support is provided by research that demonstrates that arousal produced by one emotion can transfer to and intensify a second emotion. People who have just exercised (producing physiological arousal) are more likely to become angry or sexually aroused when exposed to relevant stimuli (Reisenzein, 1983). Each of these classic theories offers important insights into our experience of emotion, yet none definitively resolves our original question regarding the relationship between physical sensations and subjective feelings. The same patterns of physical activity may occur during a variety of emotions, and a single emotion may be associated with a variety of physical states. That mistakes can and do happen is certain. Nonetheless, we seem to manage quite well at interpreting our emotions in most situations. A more contemporary model reconciles the debate regarding the impact of physical sensations on subjective feelings (Cacioppo, Berntson, Klein, & Poehlmann, 1997). The somatovisceral afference model of emotion (SAME)somatovisceral afference model of emotion (SAME)A model of emotion in which a range of physical sensations from precise to general requires varying degrees of cognitive processing prior to subjective feelings            somatovisceral afference model of emotion (SAME)        A model of emotion in which a range of physical sensations from precise to general requires varying degrees of cognitive processing prior to subjective feelings             begins with a recognition that physical responses to a stimulus can range from quite specific to quite general. For example, the physical sensations associated with disgust can be more precise than the physical sensations associated with pride. The initial degree of specificity of the physical response leads to different cognitive processing. A highly specific physical response leads to unambiguous recognition of a subjective feeling. This is the situation that appeared most interesting to James. A bear walks in, you react physically, and you know you’re scared. At the other extreme, instead of specific physical responses, a situation might produce general arousal, which requires significant cognitive processing and evaluation. For example, a valedictorian giving a graduation speech might not understand her arousal until she sees her parents and other members of the audience clapping and realizes the emotion she is feeling is pride. This scenario is closest to that proposed by Schachter and Singer (1962). SAME provides middle ground between the James-Lange and the Schachter and Singer theories. If a physical response is only partially specific, an ambiguous message is sent forward that is clarified by the presence of additional cues or information (see Figure 7.4). The model correctly predicts that emotional responses range from immediate to delayed based on the amount of cognitive processing that is required. Emotions that are associated with a precise set of physical responses, such as fear, occur quickly, but emotions that are associated with more general arousal, such as pride, occur relatively slowly. According to appraisal theory, a person who values wealth and a person who values family might experience different emotions in response to the same event. This ambiguous image (you should find yourself alternating between seeing a young woman and seeing an old woman) can serve as a model for how the mind processes a range of emotional situations, from simple to complex. If we look at a simple image, we need little cognitive assessment to respond appropriately. As images become more complex, we require increasing amounts of cognitive assessment before we respond. Processing the simple emotion of fear is similar to processing an unambiguous image. Processing more complex, ambiguous emotions, like pride, requires more cognitive assessment, just like viewing this ambiguous image does. Contemporary cognitive approaches to emotion place little emphasis on the physical correlates that are featured so prominently in the classic theories. Instead, these approaches emphasize the role of appraisalsappraisalsThe detection and assessment of stimuli that are relevant to personal well-being.            appraisals        The detection and assessment of stimuli that are relevant to personal well-being.            , or the detection and assessment of stimuli with relevance for well-being, in eliciting emotion (Moors, Ellsworth, Scherer, & Frijda, 2013). For example,  Ellsworth (1994) expresses some frustration with the approach taken by James when she says, Bears do not automatically cause us to run or tremble. Instead, the interpretation of the stimulus, the bear in this case, forms the critical starting point for the emotional cascade. Appraisal theory further assumes that interpretation is a continuous process rather than a single decision about a stimulus (Moors et al., 2013). For example, attention may be aroused by some change in the environment. A recognizable emotion has not yet occurred, and if the appraisal of the stimulus determines it to be of no significance, arousal returns to baseline. However, if the stimulus is found to have positive or negative value to the individual making the appraisal (a source of food or a predator, for instance), feelings and physiological responses change yet again. An emphasis on appraisal as a starting point for emotion may help us account for the vast range of emotional reactions that individuals might have to the same event (Moors et al., 2013). One person may view material wealth as essential to happiness, whereas another may view successful relationships with others as a primary goal. These different cognitive patterns should produce different emotional responses to the same event. A letter to an advice columnist described a situation in which a man’s wife had wrecked his expensive antique automobile in a serious accident (Dear Abby, January 14, 2006). Our first person, who values money, is likely to respond with dismay at the loss of the expensive car. Our second person, who values relationships, is likely to be happy that his beloved wife survived the accident in one piece. Emotional behavior is not unique to humans. Charles Darwin (1872) made a careful study of the facial expressions produced by humans and other primates. He concluded that all primates form facial expressions using the same muscles, which in turn led him to state that emotional facial expression must have evolved. Although we are frequently cautioned about anthropomorphism, or the attributing of human qualities to other animals, it is easy to identify with the anger of a mother bear defending her cubs or the fear of a gazelle trying to outrun a cheetah. Darwin discovered that humans and other primates use the same facial muscles to produce emotional expressions, leading him to hypothesize that emotions evolved. Can you figure out what emotion is being expressed by both the chimpanzee and the man? Try imitating the expression and think about how you feel. Still no luck? The emotion being expressed is disappointment. The evolution of a behavior implies that it makes survival more likely. A major advantage provided by emotion is the ability to produce arousal. The word emotion is derived from the Latin word meaning to move. The arousal produced by an emotion stimulates action, which might be lifesaving. According to the classic Yerkes-Dodson lawYerkes-Dodson lawA description of the relationships among task complexity, arousal, and performance            Yerkes-Dodson law        A description of the relationships among task complexity, arousal, and performance            , the ideal amount of arousal interacts with the complexity of a task (Yerkes & Dodson, 1908). For simple tasks, such as outrunning a predator, greater arousal leads to greater performance. For more complex tasks, such as taking a difficult exam, arousal levels that are too high can begin to interfere with performance (see Figure 7.5). We have all had the experience of choking. or performing badly because of too much arousal. In addition to providing general arousal, each emotion is associated with a positive or negative quality or valence, which moves us in the correct direction of either approach or avoidance (Davidson & Irwin, 1999). The negative emotion of disgust tells us to avoid rotting food, and positive feelings of happiness cement our social bonds. The Yerkes-Dodson law predicts that the complexity of a task interacts with an organism’s arousal level to determine the quality of performance. For simple tasks, such as running to escape a predator, greater arousal leads to a relatively steady improvement in performance. For difficult tasks, such as completing an exam, moderate amounts of arousal lead to the best performance, and high arousal can lead to weaker performance. In addition to producing beneficial arousal, emotions enhance survival by providing an important means of communication. Nonverbal forms of communication such as facial expression, calls, and body language were used to provide information to others long before humans began using spoken words (see Figure 7.6). If one person assumes a body posture indicating fear, the emotion ripples through a crowd to its edges in a short amount of time (de Gelder, Snyder, Greve, Gerard, & Hadjikhani, 2004). Human infants communicate a range of needs to their parents before developing the ability to speak. You have no difficulty interpreting the facial expressions, body language, and gestures of a driver behind you who doesn’t approve of your lane-change technique. One of the advantages of emotions is the ability to communicate nonverbally. Even with these simple stick figures, it is easy to interpret how each character feels. An additional and welcome benefit of having emotions is our ability to enjoy the arts. Evidence for a coordinated development of language and emotional communication remains in our contemporary brain, which uses the same pathways for language and for the perception and appreciation of music (Schön et al., 2010). Without the capacity to feel, we would find it difficult to appreciate a Picasso or the moving strains of Yesterday by the Beatles. We wouldn’t enjoy a good cry at the end of Pride and Prejudice or be glued to the edge of our seats while watching Mother! The universal nature of human responses to many poems, pieces of music, and other artistic accomplishments points to shared underlying mechanisms, shaped through our evolutionary past. Shared enjoyment of the arts contributes greatly to social bonding in human societies. When we dance together, sing together, or watch plays and other visual displays of art, shared emotional responses contribute to feelings of community and closeness. Long before infants learn to speak, they can communicate with adults by using facial expressions. We can identify further advantages of emotion by studying people whose emotional lives have been affected by brain damage. In Chapter 4, we reviewed the case of Phineas Gage, whose frontal lobe damage changed him from a responsible, well-liked member of the community to an impulsive, indecisive, and emotionally volatile individual who had difficulty keeping a job. More recently, Antonio  Damasio (1994) described the case of a young man named Eliot, who had frontal lobe surgery to remove a tumor. Like Gage, Eliot maintained his intelligence and other skills after his surgery, with the notable exception of his ability to establish priorities. Damasio hypothesized that emotions provide a bridge to past experiences that can be used to set priorities such as approach and avoidance. In the absence of normal feedback from the body that can be used to identify emotional states (my heart is beating fast, so I must be scared), Eliot can no longer use these past experiences to guide his current decisions. The mind’s priority for processing negative events has been described in earlier chapters. For example, in Chapters 1 and 5, we discussed how we are far more sensitive to bitter tastes because these are often associated with poisons than we are to sweet tastes, which are rarely life threatening. In Chapter 4, we identified several nervous system structures that participate in our emotional lives. Emotional responses combine bottom-up processing, in which lower parts of the nervous system send alerts to the cerebral cortex, with top-down processing, in which cortical executive functions such as attention and appraisal modify the activity of the lower structures. The autonomic nervous system, which controls many activities of our glands and organs, participates in the general arousal associated with emotional states. In particular, activity of the sympathetic division of the autonomic nervous system is correlated with our fight-or-flight response to perceived danger. The autonomic nervous system is under the immediate control of the hypothalamus, a structure that also plays an important role in most motivated behavior. Researchers have asked whether specific patterns of autonomic activity occur during particular emotional states. In a large meta-analysis, autonomic measures did not reliably predict particular emotional states, such as happiness or sadness (Cacioppo, Berntson, Larsen, Poehlmann, & Ito, 2000). However, the analysis indicated that the autonomic responses associated with negative emotions are stronger than those associated with positive emotions. Fear, with its rapid heartbeat and sweaty palms, provokes a more dramatic set of physical reactions than feelings of happiness. In its direct management role over the autonomic nervous system, the hypothalamus participates in any of the emotional states that also engage that system. Is that the only role for the hypothalamus in emotion? It is clear that stimulating the hypothalamus in animals can elicit many behaviors, including sexual behaviors, eating, drinking, and aggression, but it is unclear whether these artificially induced behaviors are accompanied by the feeling part of an emotion. Renewed interest in the hypothalamus and its role in emotion has resulted from experiments using deep brain stimulation, delivered through surgically implanted electrodes, to relieve various conditions from obesity to depression. During stimulation of the hypothalamus, one patient experienced both the physical and subjective feelings normally associated with a panic attack (see Chapter 14) (Wilent et al., 2010). The physical responses, such as shortness of breath and rapid heartbeat, were not surprising given our knowledge of hypothalamic functions, but the patient’s subjective feelings of overwhelming anxiety were unexpected. Additional research following up on this case study might further illuminate the role of the hypothalamus in emotion. The amygdala identifies emotional stimuli, both positive and negative, and initiates responses to the perception of these stimuli (see Figure 7.7). The amygdala’s role in the identification and classification of emotional stimuli is carried out through its participation in a circuit that includes the frontal lobes of the cortex, the cingulate cortex, and the insulainsulaRegions of cortex located at the junction of the frontal and temporal lobes.            insula        Regions of cortex located at the junction of the frontal and temporal lobes.             (see Figure 7.8). People with amygdala damage, like Patient S. M., do not always use information about people’s eyes to judge their emotions. Eye tracking shows (a) how a healthy control participant scans an emotional image and (b) how a person with amygdala damage might scan the same face. (c) For all emotions studied, Patient S. M. showed fewer eye fixations than normal controls (NC). The amygdala not only detects emotional stimuli, but it also forms a bridge between these stimuli and the appropriate responses. The amygdala initiates responses through its tight connections with the hypothalamus, which in turn affects autonomic, hormonal, and behavioral processes. For example, when the amygdala perceives danger from environmental stimuli, it initiates a hormonal cascade involving the pituitary gland and the adrenal glands that produces an increased release of neurochemicals in the brain, leading to an increase in overall arousal. We will discuss this process in greater detail in Chapter 16. One of the first clues to the importance of the amygdala to emotion resulted from an experiment conducted in 1939 in which researchers removed both temporal lobes, which include the amygdalae, from rhesus monkeys (Klüver & Bucy, 1939). After recovery, the normally hard-to-handle adult rhesus monkeys became tamer, and their emotions were less intense. They allowed themselves to be picked up and stroked, and they appeared to be relatively oblivious to stimuli that normally elicit intense fear, such as snakes. They made fewer fear-related grimaces and vocalizations. We now understand that whenever the amygdala is damaged, animals respond inappropriately to danger. Rats with damaged amygdalae fail to learn to fear a tone or other stimulus that reliably predicts the onset of electric shock (LeDoux, Cicchetti, Xagoraris, & Romanski, 1990). Rhesus monkeys with lesioned amygdalae show less fear of rubber snakes and less restraint around unfamiliar monkeys, a potentially dangerous way of behaving in a species that enforces strict social hierarchies (Emery et al., 2001;  Mason, Capitanio, Machado, Mendoza, & Amaral, 2006). Insight into the role of the amygdala in humans has been provided by case studies in which disease has damaged this structure. Patient S. M. experienced damage to both amygdalae because of a rare disease (Adolphs, Tranel, Damasio, & Damasio, 1994). Although Patient S. M. can recognize the emotions of happiness, sadness, and disgust portrayed in photographs, she has selective difficulty identifying fear correctly. As shown in Figure 7.7, Patient S. M. has particular difficulty using information from the eye region of the face in judging emotion, although when she was instructed to pay attention to eyes, her performance improved (Adolphs et al., 2005). Individuals with autism spectrum disorder, discussed in Chapter 14, share Patient S. M.’s reluctance to make eye contact and difficulty identifying other people’s emotions, especially fear. One of the most consistent biological correlates of autism spectrum disorder is abnormal development of the amygdala (Amaral, Schumann, & Nordahl, 2008). These findings suggest that the amygdala participates in both the active exploration of the social environment and the interpretation of the results of that exploration (Adolphs, 2007). The insula, found in the fold between the junction of the temporal lobes with the frontal and parietal lobes, plays an important role in our subjective experiences, or feelings (Damasio, 2003). Activity toward the back of the insula is associated with many internal sensations, including pain and itch, that can be localized as originating from particular parts of the body. This area is also associated with many of the physical sensations we associate with emotional feelings, including changes in blood pressure, breathlessness, and feelings of butterflies in the stomach. In contrast, activity in the forward areas of the insula is associated with more global feelings, including a particularly strong reaction to disgust (Gasquoine, 2014;  Papagno et al., 2016). The cingulate cortex, shown in Figure 7.9, serves as a major gateway between the amygdala and other subcortical structures and the frontal areas of the cortex. In particular, the cingulate cortex participates in processing the emotional quality of physical pain. The cingulate cortex forms circuits with the frontal cortex, the amygdala, and other subcortical structures involved with emotional processing. The basal ganglia are part of the brain’s voluntary movement systems and help coordinate movement in response to assessments of emotion. The basal ganglia show particularly strong activation that correlates with the emotion of disgust. We often dislike our own photographs even when other people seem to think they look fine. Part of the problem is that we rarely see ourselves the way others do—we usually see ourselves in a mirror, which reverses our emotional expressions. Because our facial expressions are not symmetrical, due to the stronger role of the right hemisphere in emotion, that crooked smile looks wrong when we see it on the opposite side of our face. The anterior cingulate cortex, along with the dorsomedial prefrontal cortex, contributes to the more conscious appraisal of threat compared to the less conscious signals arising from the amygdala (Kalisch & Gerlicher, 2014). As we will see in Chapter 14, some individuals are unusually anxious. Worrying and catastrophizing (focusing on the worst possible outcomes) by anxious people involves exaggerated appraisals of threat. Unusually high levels of activation in the anterior cingulate cortex have been observed during episodes of catastrophizing and worry. As we discussed in Chapter 4, the basal ganglia are large, subcortical structures that participate in the generation of voluntary movement. These structures coordinate movements in response to emotional stimuli. Strokes that damage the basal ganglia result in an overall reduction in the experience of emotional intensity (Paradiso, Ostedgaard, Vaidya, Ponto, & Robinson, 2013). The nucleus accumbens, which is part of the basal ganglia, is particularly associated with pleasure and reward. The basal ganglia also show considerable activity in response to facial expressions of disgust (Phan, Wager, Taylor, & Liberzon, 2002). This finding is consistent with clinical observations of people with diseases that damage the basal ganglia, such as Huntington’s disease. These people have particular difficulty recognizing facial expressions of disgust, although their recognition of other major emotional facial expressions appears intact (Hennenlotter et al., 2004). As we observed in Chapter 4, damage to the frontal lobes produces changes in emotional behavior. People with frontal lobe damage often experience a reduction in fear and anxiety, which may contribute to their engagement in impulsive, risky behaviors. They seem unable to experience gut feelings that serve as warnings in threatening situations (Damasio, 1994). Without such warnings, these individuals forge ahead impulsively where others would pause. We do not appear to have happy centers or sad centers in the cerebral cortex. Instead, the experience of different emotions produces different patterns of cortical activation. When people are asked to recall a specific event from their past associated with the emotions of anger, sadness, fear, and happiness, brain imaging scans identify distinct patterns of activity for each emotion (Damasio et al., 2000). However, the same brain regions usually participate in more than one of these states. Humans express emotion with their entire bodies. People who are feeling threatened or scared generally cross their arms and hunch forward. Angry, aggressive people typically lean forward, rise onto the balls of their feet, and raise their shoulders to appear larger and more intimidating. However, humans rely most heavily on the face for expressing emotion. This man has a tumor in his right primary motor cortex that prevents him from voluntarily smiling on the left side of his face when asked to do so, as shown in the photo on the left. In contrast, he can smile spontaneously and naturally in response to a genuine, involuntary emotion, as shown in the photo on the right. These observations support the hypothesis that voluntary and spontaneous emotional expressions are managed by different areas of the brain. The smiles, frowns, and other facial expressions we make are influenced by the way the brain controls the tiny muscles of our faces. These muscles receive input from the motor areas of the cerebral cortex, which control voluntary movement, as well as from subcortical areas, including the basal ganglia. The cortical input allows us to voluntarily smile for the camera. The subcortical input is responsible for more spontaneous expressions of emotion, such as laughing at a funny joke. It is possible to lose one type of input without affecting the other. The young man shown in the photo above has a tumor that affects his right motor cortex. When asked to smile voluntarily, his smile appears quite crooked because of the importance of the cortex in voluntary expression. When he smiles spontaneously to a joke, his smile appears more natural because his subcortical areas can still react normally. People with Parkinson’s disease, which damages the subcortical emotional pathways, show the opposite pattern. They may be unable to smile spontaneously in response to a joke, but they can smile voluntarily when asked to do so. Do we learn how to express emotions, or are these behaviors built in? Darwin (1872) believed that human emotional expression had been shaped through evolution. If he is correct, we would expect humans in all parts of the world to share common means of expression. Some major emotional expressions appear to be universal across cultures (Keltner & Ekman, 2000), although not all researchers agree (Gendron, Roberson, van der Vyver, & Barrett, 2014). The primary colors of emotional expression, including anger, sadness, happiness, fear, disgust, surprise, contempt, and embarrassment, are identified correctly by most people, regardless of the culture in which they live. These photos represent the possibly universal expressions of happiness, anger, surprise, disgust, sadness, and fear. Further support for a view of emotional expression as being innate comes from the predictable developmental course shown by young children. Infants’ social smiles (as opposed to smiles resulting from gas, etc.) emerge near the same age (around 3 months), regardless of whether an infant can see faces or is born blind (Freedman, 1964). Monozygotic (identical) twins are more similar than dizygotic (fraternal) twins in the ages at which they first show fear of strangers. As shown in Figure 7.10, children raised in diverse cultures (African bush, urban Guatemala, rural Guatemala, and Israeli kibbutz) showed similar age-related distress at being separated from their mothers (Kagan, Kearsley, & Zelazo, 1978). Children in diverse cultures show the strongest emotional reactions to separation from their mothers around the same stage in development, suggesting that this emotional behavior has a biological origin. Developmental timelines for emotional behavior are also characteristic of nonhuman primate species. Rhesus monkeys raised in isolation performed typical fear responses when shown pictures of other monkeys engaging in threatening behaviors. The isolated monkeys demonstrated these fear responses at the same stage in development as monkeys raised in normal social conditions (Sackett, 1966). Other researchers point to the universality of words for different emotional states as evidence of emotion’s common biological source. Words for emotions have been found to be quite similar in a sample of 60 of the world’s languages (Hupka, Lenton, & Hutchison, 1999). The exceptions, however, are also intriguing. The Japanese term ijirashi refers to a feeling that occurs when we see another person overcoming an obstacle. No equivalent term exists in English. Despite the possibility of a shared biological basis for emotion, we can observe significant individual differences in emotional expressiveness. When young children are exposed to strong, novel stimuli, such as the odor of alcohol on a swab, some react strongly to the stimuli, while others appear to ignore them. Still others show a moderate response to the stimuli (Kagan, 1997). As we will see in Chapters 11 and 14, children who are highly responsive often develop into cautious, anxious adults. In contrast, children who are relatively oblivious to stimulation may develop into bold, fearless risk takers. If their insensitivity extends to social signals produced by others, this lack of sensitivity to stimuli may lead to antisocial behavior. Psychopaths incarcerated for murder show significantly reduced reactions to slides of pleasant, neutral, and unpleasant situations when compared to typical control participants (Herpertz et al., 2001). Individuals also differ in their ability to regulate emotions (Gross, 1998). Two major strategies are used in emotion regulation: suppression, or inhibiting expression, and cognitive reappraisal, or modifying the meaningfulness of an event (English & John, 2013). David Matsumoto carefully compared thousands of photographs from the 2004 Olympic and Paralympic Games and found that all competitors, sighted or blind, displayed the same expressions in response to winning or losing. These results suggest that, as Darwin suggested, some emotional expressions are innate to the human species. The ability to suppress emotional expression can be learned. For example, medical personnel undergo training that affects their expression of disgust. Most patients would not appreciate a physician or nurse who said oh, gross during an examination. Military personnel learn to maintain their composure when a superior is yelling at them face to face, a skill that may be important when a soldier is captured by enemy forces. Many cultures have display rulesdisplay rulesA cultural norm that specifies when, where, and how a person should express an emotion.            display rules        A cultural norm that specifies when, where, and how a person should express an emotion.            , or norms that specify when, where, and how a person should suppress emotion (see Figure 7.11). The reactions of Japanese and American university students were recorded as they watched emotional movies either alone or with a group of students they did not know (Ekman, Friesen, & Ellsworth, 1972). The Japanese students showed more emotion when alone than when in the group of strangers. In contrast, the American students were about as emotional in either case. If anything, Americans tend to show more emotion when in a group than when alone. Americans made more intense expressions of disgust in response to an odor when in a group than when they were alone (Jancke & Kaufmann, 1994). Despite the strong evolutionary and biological heritage we have for spontaneous emotional expression, we can learn to control our emotional expressions to fit a situation. This Marine recruit is expected to remain calm while his drill instructor applies a correction. This skill is believed to help military personnel maintain composure in highly emotional situations. Although cultures worldwide are fairly similar in their emotional expressiveness, variations occur. Switzerland, with a suppression score of 3.05, shows emotion most freely of all these countries, and Hong Kong, with a suppression score of 4.72, shows the least emotion. Reappraisal as a strategy for regulating emotion involves thinking differently about the stimulus that elicits an emotional response rather than attempting to hold in a response. For example, we might be angry when another driver behaves aggressively, perhaps putting us in danger. To manage our anger through suppression, we might simply use willpower to avoid acting in angry ways ourselves. To manage our anger using reappraisal, we focus instead on the fact that the other driver might be having a tough day and that nobody was actually hurt. People who use the reappraisal strategy more than the suppression strategy experience less depression and higher life satisfaction (Gross, 2013). Psychologist Jerome Kagan described individual differences in emotionality based on children’s responsiveness to the environment, ranging from the cautious child to the bold. Human adults are quite accurate in their ability to read emotions. When research participants watched only 10 seconds of a videotaped interaction between a teacher and an off-camera student, they were able to judge whether the teacher liked the student with considerable accuracy (Babad, Bernieri, & Rosenthal, 1991). We might think we do a good job of hiding our feelings, but the subtleties of emotional expression often give us away. At the same time, our ability to distinguish between genuine and fake expressions, while good, is far from perfect (Ekman, Davidson, & Friesen, 1990;  Frank & Ekman, 1993). Individual differences in the ability to perceive the emotions of ourselves and others do occur. Twin studies demonstrate that individual differences in the ability to read emotional expression are heavily influenced by genetics (Anokhin, Golosheykin, & Heath, 2010). These differences form one of the key aspects of emotional intelligence (Salovey & Mayer, 1990). Emotional intelligence refers to our abilities to perceive, use, understand, and manage emotions. Individual differences in emotional intelligence predict the success of both work and personal relationships. This short questionnaire (Gross & John, 2003) will assess how much you use suppression and reappraisal strategies to regulate emotion. For each item, select a number on a scale of 1 (strongly disagree) to 5 (strongly agree). I control my emotions by changing the way I think about the situation I’m in. When I want to feel less negative emotion, I change the way I’m thinking about the situation. When I want to feel more positive emotion, I change the way I’m thinking about the situation. When I want to feel more positive emotion (such as joy or amusement), I change what I’m thinking about. When I want to feel less negative emotion (such as sadness or anger), I change what I’m thinking about. When I’m faced with a stressful situation, I make myself think about it in a way that helps me stay calm. I control my emotions by not expressing them. When I am feeling negative emotions, I make sure not to express them. I keep my emotions to myself. When I am feeling positive emotions, I am careful not to express them. Items 1 through 6 indicate your use of reappraisal, and items 7 through 10 indicate your use of suppression. It might surprise you to learn that these scales are independent. In other words, a person who uses the suppression strategy quite a bit is no more or less likely to use reappraisal strategies than a person who rarely uses suppression (Gross & John, 2003). Gross and  John (2003) reported that men are more likely than women and non-European-Americans are more likely than European-Americans to use suppression. You might see evidence for these general findings (or not!) among your classmates. Keep in mind that college students are not a representative sample of the adult public. This chapter asks whether emotional expression is universal for the human species or more influenced by culture and learning. Some aspects of emotional expression do seem to be species-wide, while others are clearly influenced by environmental factors. What types of processes might account for cultural differences in emotional suppression? We noted that Western cultures are more expressive than Eastern cultures, perhaps due to the contrast between individualism and collectivism. However, another factor that might explain variations in display rules is a culture’s history of immigration. In countries such as the United States, which has a long history of people immigrating from many different places, emotional expressivity appears to be higher than in countries with less immigration from fewer sources (Rychlowska et al., 2015). In a relatively unpredictable social situation lacking shared language and norms, somewhat exaggerated emotional expression might help avoid misunderstandings. Rychlowska et al. (2015) also found that cultural heterogeneity was predictive of how people interpreted smiling (see Figure 7.12). Smiling can be interpreted as a means of social bonding or friendliness, or as a sign of dominance. People from cultures with high levels of historical immigration from many places were more likely to endorse the bonding interpretation of smiling, while people from relatively homogeneous cultures rated dominance as the most likely reason to smile. Psychologists have usually interpreted cultural differences in emotional suppression as being the result of a culture’s degree of individualism or collectivism. This explanation might be too simple. The amount of immigration experienced by a culture is related to the proportion of the population that interprets smiling as an attempt to reward or bond as opposed to an attempt to dominate. These results emphasize the importance of considering not just present cultural environments, but also historical influences on culture, when attempting to explain cultural differences in behavior. In addition to providing display rules, cultures influence interpretations of emotional expressions. Japanese and American study participants were asked to correctly identify emotional expressions with varying levels of intensity (Matsumoto, 2002). Individuals from both cultures identified the emotion being expressed with considerable accuracy, even at low levels of intensity. However, intensity was interpreted quite differently by the Japanese and the Americans. When emotional expression occurred at low intensities, Japanese participants assumed that people were feeling more than they were willing to express. In contrast, when emotional expression was highly intense, the American participants assumed that people were exaggerating their feelings. Although fake and genuine smiles look similar, we usually do a good job of distinguishing between them. Which of these smiles do you think is real? Which is fake? A person’s ability to read others’ emotional expressions can be reduced by certain psychological disorders (see Chapter 14). Individuals with schizophrenia and bipolar disorder have normal abilities to process facial features but perform worse than typical controls on tasks requiring them to distinguish among facial expressions (Kohler, Walker, Martin, Healey, & Moberg, 2009). People diagnosed with autism spectrum disorder and those with antisocial personality disorder have specific difficulties recognizing expressions of fear (Marsh & Blair, 2008). Even though emotional expression is a major means of communication, our abilities to read emotions are not perfect. Juries are made up of human beings, who detect deception in face-to-face encounters only slightly better than chance (Ekman & O’Sullivan, 1991). Polygraph methods are rejected for use in court because of their poor record of accuracy (Kleinmuntz & Szucko, 1984). Functional magnetic resonance imaging (fMRI) eventually might improve our ability to detect honesty, and fMRI results are beginning to be admitted to U. S. courts of law (see Figure 7.13) (Langleben & Moriarty, 2013). Some researchers consider fMRI to be superior to the traditional polygraph for detecting deception (Langleben et al., 2016), while others are less convinced (Monteleone et al., 2009). Until better technologies are available, what do psychologists know about lying that can help the legal system identify deception? During a standardized task known as the modified Guilty Knowledge Test, which is used to assess deception, participants’ brains showed significantly different activation when lying compared to when they told the truth. Red areas represent brain regions that are more active when the participant is lying, and blue areas represent brain regions that are more active when the participant is telling the truth. So far, laboratory studies have used small numbers of law-abiding participants rather than real criminals and concrete tasks such as reporting which playing card is being held (e.g., ace of spades). It is unknown whether this technology will advance to the point where results will be admitted in a court of law on a regular basis. Lying is difficult, especially in high-stakes situations featuring high arousal. While aroused, people betray their lying in predictable ways (Ekman, 1996). Inappropriate smiling and nervous laughter might reflect the person’s high arousal. A deceptive person stiffens the upper body, nods the head less frequently, and uses fewer hand gestures than normal. In contrast to the reduced mobility of the upper body, the feet swing. An unwillingness to make eye contact might indicate dishonesty. However, in many cultures, making eye contact is considered impolite, and failure to make eye contact should be interpreted cautiously. Liars experience more cognitive demand than truthful people as they must multitask while assembling their deception (Vrij, 2015). The liar may stumble verbally, adding um and uh while seeking to construct a plausible lie. Liars must manage how credible they appear to others. They may be distracted by their goals for lying, such as preventing embarrassment, avoiding criminal charges, or making money. They must actively suppress the truth to avoid slips. If the interviewer can add to the liar’s cognitive load by requesting continued eye contact and asking unanticipated questions, the liar is more likely to slip. An excellent signal that someone is lying is the use of scripted stories, which are lacking in detail. People telling the truth add 20 to 30% more detail to a story compared to liars (Colwell, Hiscock-Anisman, Memon, Taylor, & Prewett, 2007). Another reliable way to identify lying is to ask people to tell their story backward in time (Fisher & Geiselman, 2010). A person telling the truth can simply move from one event to the next, but constructing a false story backward quickly overwhelms a person’s memory for details. Name of theoryMajor featuresJames-Lange theorySpecific physical sensations lead to an identification of the subjective feeling. Cannon-Bard theoryPhysical sensations and subjective feelings occur simultaneously and independently. Schachter-Singer two-factor theoryGeneral arousal, not specific physical states, leads to an appraisal of our surroundings, which allows us to identify our subjective feelings. SAMEDifferent emotional states produce physical responses ranging from specific to ambiguous. Ambiguous responses require more appraisal than specific responses before a subjective feeling is identified. Appraisal theoryEmotions begin with the detection and assessment of stimuli that are relevant to personal well-being. Animals, including humans, have limited time and resources, and a state of arousal is expensive in terms of the energy it requires. Motivational systems allow an animal to be aroused only when necessary, such as when it needs food, and then reduce arousal following the solution of a problem, such as after a meal. Preventing the waste of precious energy resources provides a significant survival advantage. Motivation also provides the benefit of helping an animal prepare to meet future needs. Most animals are motivated to explore their environments because familiarity with an environment allows them to act more effectively when a need arises. We can think of motivation as a process that maintains homeostasishomeostasisA steady internal balance, or equilibrium.            homeostasis        A steady internal balance, or equilibrium.            , a term introduced by psychologist Walter  Cannon (1932) to describe a steady internal balance or equilibrium. To achieve homeostasis, organisms actively defend certain values known as set points. Under normal circumstances, we carefully regulate such variables as core body temperature, fluid levels, and body weight around set pointsset pointsA value that is defended to maintain homeostasis.            set points        A value that is defended to maintain homeostasis.            . Deviations from these set points stimulate behavior by the organism that is designed to reestablish the original values. You might think about this process as analogous to your home’s temperature control. A set point of air temperature is established using your thermostat. If your home’s temperature drops below that set point, the furnace is activated until the set point is again established. If your home’s temperature rises above the set point, the air-conditioning system is activated until the set point is regained. Similarly, if your core body temperature drops below, your body initiates several processes designed to increase its temperature, such as producing heat by the muscle contractions we know as shivering. If your core body temperature rises above its set point, cooling mechanisms are activated. You sweat, and the evaporating moisture cools your skin. Blood is diverted to the outer parts of the body, leading to a flushed appearance. Motivation begins with a stimulus, arising from either the internal or the external environment of the organism, that serves as a cue for motivated behavior. Stimuli that are important to survival, such as the presence of a predator or a deficit in body fluids, generate arousal and tension, a state frequently called drivedriveA state of tension and arousal triggered by cues important for survival.            drive        A state of tension and arousal triggered by cues important for survival.             (Hull, 1943). Being in a drive state propels the organism into some sort of action related to the stimulus, whether that means running from the predator to safety or pulling a bottle of water from a backpack to quench thirst. If actions are successful in regaining equilibrium, we experience drive reductiondrive reductionThe state of relief and reward produced by removing the tension and arousal of the drive state.            drive reduction        The state of relief and reward produced by removing the tension and arousal of the drive state.            , accompanied by a rewarding feeling of relief. Drive theories of motivation are often described as push theories because drive is seen as pushing an organism toward a goal. However, not all psychologists agree that motivation requires the push of drive. Instead, they suggest that rewards or incentivesincentivesA reward that pulls an organism’s behavior in a particular direction            incentives        A reward that pulls an organism’s behavior in a particular direction             have the capacity to pull an organism in a particular direction. According to this view, animals are viewed as naturally inclined to act on their environment rather than wait passively for a need to arise (Deci & Ryan, 2000). In incentive theories, no reference to unpleasant internal drive states is required to explain motivated behavior. Incentives or rewards may be intrinsic or extrinsic. Intrinsic rewardsIntrinsic rewardsA reward that arises internally.            Intrinsic rewards        A reward that arises internally.             arise internally, such as feelings of accomplishment when a goal is met. Extrinsic rewardsExtrinsic rewardsA reward from an outside source.            Extrinsic rewards        A reward from an outside source.             come from outside sources, such as money for completing work or praise from a supervisor. These different types of reward can interact in complex ways (see Figure 7.14). In some cases, extrinsic rewards can have negative effects on intrinsic motivation. For example, if a child who enjoys reading suddenly gets paid for each book completed, the child’s enjoyment of reading might decrease because the motivation shifts from intrinsic, the love of reading, to extrinsic, the love of reward money (DeCharms, 1968). Harvard University economist Roland Fryer overcame a tough childhood in Daytona, Florida, to become the youngest tenured African American professor in the history of the university. Drawing on his personal experiences, Fryer experimented with different incentives for a variety of school-related behaviors. His results suggest that the relationships among intrinsic rewards, extrinsic rewards, and behavior can be quite complex. Although previous research has shown that extrinsic rewards can undermine intrinsic motivation, it is important to remember that this result occurs only when behavior is intrinsically motivated in the first place. If children do not intrinsically enjoy reading, Fryer suggests paying them to read might work. Unfortunately, Fryer has been the target of death threats for suggesting this simple solution to illiteracy. LocationDALLASCHICAGOWASHINGTONNEW YORK CITYWhat students were paid forReadingGradesVariousTest scoresGrade level participatingSecond-gradersNinth-gradersSixth-, seventh-, and eighth-gradersFourth- and seventh-gradersHow much$2 per book$50 for A’s$35 for B’s$20 for C’sUp to $100 every two weeks$25 (fourth-graders) to $50 (seventh-graders) per testAverage student earned$13.81$695.61$532.85$139.43 (fourth-graders)$231.55 (seventh-graders)Study size1,780 from 22 schools4,396 from 20 schools3,495 from 17 schools8,320 from 63 schoolsResultsVery Positive Paying children to read dramatically boosted readingcomprehension scores. Mixed Children cut fewer classes and got slightly better grades. Standardized test scores did not change. Positive Rewarding five different actions, including attendance and behavior, seemed to improve reading skills. No Effect Paying children for higher test scores did not lead to more learning or better grades — or any measureable changes. Psychologists have studied a range of motives, from the mostly physical motives of temperature control and thirst to the more cognitive and social motives to affiliate with others and to achieve. We will explore this range by discussing some specific motives in detail, including hunger, sexuality, affiliation, and achievement. After discussing these examples, we will examine the ways humans set priorities when faced with competing motives. A combination of metrics that varied from school to school but always included attendance and behaviorNot including control groupsHunger is a complex motive. In comparison to the regulation of body temperature through processes such as sweating or shivering, the reduction of hunger through eating is more heavily influenced by emotion, learning, and culture. Humans take in a remarkable variety of nutrients. The traditional diet of the Inuit people living in Arctic regions contains little plant material, whereas vegans adhere to a diet that contains no animal material. Humans consume a range of diets, from the nearly all-meat diet of the traditional Inuit cultures to the vegan diet. We respond to combinations of external and internal cues that make us feel hungry. External cues for hunger include time of day, the sights and smells of favorite foods, admonitions that we should clean our plates, or the social settings in which food is presented. These external cues may encourage us to eat when our bodies do not need nutrients or to eat more food than we require. Individuals who are very responsive to external cues might find maintaining a healthy weight to be somewhat challenging. Internal cues for hunger are generated when our bodies are genuinely short on nutrients. Short on nutrients applies to the maintenance of current, not necessarily ideal, weight. A morbidly obese person is likely to feel as hungry as a thin person when sufficient nutrients are not available to maintain the status quo. Walter Cannon believed that stomach contractions were an important component in the detection of hunger, and to prove his point, he persuaded his colleague A. L. Washburn to swallow a balloon attached to an air pump (Cannon & Washburn, 1912). The balloon allowed Washburn’s stomach contractions to be monitored while Washburn indicated feelings of hunger by pushing a telegraph key (because he couldn’t talk with a tube down his throat). Although Washburn’s key presses correlated with his stomach contractions, these signals do not tell the whole story. People who have had their stomachs surgically removed still experience a sense of hunger in the general region where the stomach used to be. Most of us start eating long before our stomachs begin to growl, indicating that additional types of signaling must be at work (see Figure 7.15). Walter Cannon (1871–1945) and his assistant, A. L. Washburn, performed one of the earliest experiments on the relationship between stomach contractions and feelings of hunger. The large waves at the top were Cannon’s recordings of Washburn’s stomach contractions, and the smaller waves at the bottom represent Washburn’s key taps indicating hunger. Although these two events are correlated, there is more to hunger than a growling stomach. An important hunger cue is a low level of circulating sugars, particularly glucoseglucoseA type of sugar that plays an important role in hunger levels.            glucose        A type of sugar that plays an important role in hunger levels.            . Glucose concentrations in the blood are highest just following a meal. As glucose levels drop over time, a person begins to feel hungry again. Glucose levels are intimately connected with levels of the hormone insulin. Insulin, released by the pancreas, moves circulating glucose from the blood into cells awaiting nutrients. Typically, glucose and insulin levels are positively correlated. Right after a meal, both are high. Glucose levels rise as glucose is extracted from the food that has been consumed, and insulin is high because its release is triggered by the anticipation and consumption of food. As time goes by without more food, glucose moves from the blood into other tissues, and insulin is no longer released, leading to lower levels of both substances in the blood. These lower levels of glucose and insulin should signal the need for more food (see Figure 7.16). Following a meal (time 0), a healthy person experiences a large spike in insulin levels. In contrast, a person with type 2 diabetes has a slower and less dramatic release of insulin. The person with diabetes will not be able to move glucose out of the blood into cells requiring nutrients. The appetite-suppressing action of insulin will be less effective, and the person with diabetes will remain hungry. This is an overly simplistic conclusion, however. High glucose levels occur in poorly managed diabetes because of either a lack of insulin production (type 1 diabetes) or the body’s resistance to insulin (type 2 diabetes). Without sufficient insulin activity, circulating glucose is unable to move out of the blood into the cells that need nutrients. If hunger results from low levels of circulating glucose, we would expect people with untreated diabetes to not feel very hungry, but this is not the case. Most people with diabetes who have high blood sugar report feeling hungry all the time. This makes sense when you consider that their cells are starving because of their inability to obtain glucose from the blood. It is more accurate to say that the amount of glucose that is available to cells is an indicator of hunger (Mayer, 1955). Hunger results whenever cells are unable to obtain the glucose they require. Hunger also occurs in response to low levels of stored fats (Kennedy, 1953). If you maintain a healthy weight, you are carrying sufficient body fat to survive 5 to 6 weeks of total starvation. The heavier the person, the longer that individual can survive without food. One person survived a medically supervised fast of 382 days, during which time his weight dropped from 455.4 pounds (207 kilograms) to 179.5 pounds (81.6 kilograms) (Stewart & Fleming, 1973). The body monitors fat stores by assessing levels of the hormone leptinleptinA hormone secreted by fat cells that helps the body maintain an appropriate level of stored fat.            leptin        A hormone secreted by fat cells that helps the body maintain an appropriate level of stored fat.            . Leptin, from the Greek word leptos, or thin, is produced and secreted by fat cells (Zhang et al., 1994). Because fat stores and leptin levels are positively correlated, leptin levels provide a measure for the amount of fat that has been stored. As shown in Figure 7.17, leptin levels and their associated fat stores initiate a cascade of events that influence eating behavior. When fat stores and leptin levels are low, brain areas that include the lateral hypothalamus (LH) initiate feeding. When the LH is lesioned, rats fail to eat and die of starvation unless force-fed (Anand & Brobeck, 1951). Stimulation of the LH typically initiates immediate eating. Low fat stores and leptin levels also activate the parasympathetic division of the autonomic nervous system, enhancing the body’s ability to digest and store nutrients. Metabolism, or the chemical reactions required by life, slows down, allowing nutrients to be stored rather than used up right away. Activation of the parasympathetic nervous system, initiation of feeding behavior, and reduction in metabolic rate allow the animal to find, eat, and store nutrients. As fat stores return to normal, leptin levels increase, and the feeding cycle tapers off. Unfortunately for the person trying to lose weight, the leptin system does a good job of defending a set point. As the dieter successfully decreases body fat levels, the associated low levels of leptin initiate a feeding cycle. The dieter feels constantly hungry, making the maintenance of weight loss often more difficult than the initial loss. Feeding is stimulated by two additional hormones: ghrelin, which is released by the pancreas and the lining of the stomach (Howard et al., 1996), and orexins, produced in the LH (de Lecea et al., 1998;  Sakurai et al., 1998). Ghrelin release increases with time since the last meal and appears to contribute to the rewarding aspects of feeding. Orexin activity contributes to feeding because it is negatively correlated with blood glucose levels (Cai et al., 1999). Orexins also participate in sleep (see Chapter 6), suggesting that they might link feeding, activity levels, and sleep. People of normal weight carry enough fat to survive 5 to 6 weeks of total starvation, although this is obviously not a good thing to do for one’s health. Baba Ramdev, a yoga guru, was hospitalized after losing 12 pounds by the seventh day of a hunger strike against corruption. It is likely that Ramdev was already quite thin at the beginning of his fast. We reach the point of satietysatietyA sense of feeling full and not requiring further food            satiety        A sense of feeling full and not requiring further food             or fullness long before the nutrients we have eaten can make their way to waiting cells. Just as stomach contractions serve as signals for hunger, an obvious sign of satiety is a feeling of stomach fullness. In extreme cases of obesity, some people choose to have a portion of the stomach stapled or banded to produce this feeling of fullness after less food has been eaten. In the brain, the ventromedial hypothalamus (VMH) participates in sensing satiety. As shown in Figure 7.18, lesions of the VMH produce a syndrome of large weight gains (Hoebel & Teitelbaum, 1966). Although the VMH plays a significant role in satiety, it is overly simplistic to assume that it works alone and directly. Among the many indirect effects of VMH lesions are boosts in insulin production, which in turn produce low blood glucose levels and hunger. Following lesioning of the VMH, rats nearly tripled their normal weight, suggesting that the VMH plays a significant role in satiety. However, satiety is too complex to be explained as the function of a single part of the brain. Satiety also results from the release of the gut hormone cholecystokinin (CCK) by the digestive system in response to the arrival of food, especially fatty foods. CCK not only acts in the digestive tract but also serves as a chemical messenger in the brain. CCK limits meals by activating pathways that connect the hindbrain to the digestive system (Blevins et al., 2009). Drugs that inhibit CCK’s action in the brain increase eating behavior, further supporting the hypothesis that CCK normally acts to inhibit further eating (Cooper & Dourish, 1990). Earlier we observed the cascade of events initiated by low levels of stored fat and leptin that led to feeding and the storage of nutrients. What happens when fat and leptin levels rise again? With increased storage of fat, leptin levels rise, initiating a sequence of events that inhibit feeding (see Figure 7.19). The sympathetic division of the autonomic nervous system is activated, leading to the expenditure of energy, metabolic rate increases, and inhibition of feeding. As time without food progresses, fat and leptin levels drop and the feeding sequence is reinitiated. Because of leptin’s role in satiety, scientists initially believed it might be helpful as a dieting aid. Mice genetically engineered to lack genes that produce leptin become obese. Injections of leptin allow them to lose weight (Halaas et al., 1995). Unfortunately, people do not respond like mice; obese humans have very high circulating leptin levels but appear to become resistant to the satiety messages this state should produce (Friedman & Halaas, 1998). Adding further leptin is unlikely to lead to weight loss. Our ancestors were much more likely to starve than to be troubled by having too much food. Those who lived and reproduced exhibited characteristics best suited for surviving famine. As we will see in the next sections, the same motivational mechanisms that evolved to prevent starvation among hunter-gatherers might not work well in a society featuring cultural influences on body image and an ample and calorie-rich food supply. What do we mean by obesity? A well-respected approach to body weight is the body mass index (BMI)body mass index (BMI)A height-to-weight ratio used to identify healthy weight, underweight, overweight, and obesity            body mass index (BMI)        A height-to-weight ratio used to identify healthy weight, underweight, overweight, and obesity            , shown in Figure 7.20. The BMI is a simple height-to-weight ratio computed by dividing weight in kilograms by the square of height in meters. We have converted Figure 7.20 to inches and pounds for convenience. A BMI score between 18.5 and 24.9 is considered healthy. A BMI score between 25 and 30 is considered overweight, and a BMI score above 30 is considered obese. This system works well for most people, but it does not account for those who have unusually heavy skeletons or musculature. Many elite athletes would score in the overweight or even obese range despite being quite fit. Using a BMI score of 30 or above as a criterion, rates of obesity in the United States tripled from 12% of the adult population in 1991 to 37% in 2014 (Ogden, Carroll, Fryar, & Flegal, 2015). In 1990, no state reported that more than 15% of its adult population was obese, but by 2015, no state reported fewer than 20% of its adult population was obese, and four states reported that over 35% of their adult population was obese (see Figure 7.21). Obesity rose in all age groups. How can we account for this rapid increase in obesity over the last two and a half decades? It is unlikely that current obesity levels are the result of a single cause. Genetic predispositions, lifestyle issues, and social comparisons all make significant contributions to obesity. In 1990, no state reported that more than 15% of its population was obese, and the overall national rate of obesity was 12%. By 2015, no state reported that less than 20% of its adult population was obese, and four states reported that more than 35% of their adult population was obese. Genes can influence factors such as set point and rate of metabolism, and twin studies indicate that some people are more likely than others to become obese (Livshits, Kato, Wilson, & Spector, 2007). Although genes are unlikely to have changed dramatically over a few decades, complex interactions between genes and environmental factors, such as the bisphenol A in plastic products discussed in Chapter 3, could affect current rates of obesity (Dolinoy, Huang, & Jirtle, 2007). Genes might also interact with the bacteria populations in the gut, which differ considerably between obese and lean humans (Million et al., 2013). Manipulations of bacteria through antibiotics and probiotics have been used to promote weight gain in livestock for decades. Transplantation of gut bacteria from obese to lean animals led to obesity (Turnbaugh et al., 2006). The bacteria found in the obese animals appears to be more efficient at harvesting energy from the animal’s diet. Beginning in the 1800s, in an effort to prevent starvation, initiatives leading to increased cheap sources of high-calorie foods were adopted worldwide. Currently, global agriculture produces 2,600 calories per person per day, which is expected to rise to 3,000 calories per person per day by 2030 (Caballero, 2007). With such a plentiful food supply, we can see why obesity is becoming a global problem even as starvation remains a serious concern in many parts of the world. A cultural contribution to obesity is our contemporary, sedentary lifestyle, with many people spending hours sitting in front of televisions or computers. Humans use energy efficiently. At rest, we use only 12 kilocalories (usually described simply as calories) per pound per day. This means that the average 150-pound couch potato needs only about 1,800 calories of food per day. Even the most demanding activities require a remarkably small investment in resources. Cyclists competing in the challenging Tour de France use an average of 5,900 calories per day (Armstrong & Jenkins, 2000). Meals at many chain restaurants contain more than 2,500 calories, the total daily requirement for a person who weighs 208 pounds and has a typical level of activity. To avoid starvation, our ancestors developed strong preferences for calorie-rich foods containing sugars and fats. When food supplies are limited, these preferences ensured that humans would spend the greatest amounts of time and energy seeking these rich sources of calories. Unfortunately, we retain these preferences today and continue to gravitate to sugary, fatty foods. Zooming out from individual factors to the larger social context, we see that social factors also contribute to obesity. We apparently keep track of how we are doing by comparing our size to that of those in our social circles. Having an obese spouse raises your risk of obesity by 37%, and having obese friends raises your risk by 57% (Christakis & Fowler, 2007). The path to a healthy weight can be difficult. Unfortunately, weight loss for our ancestors usually meant they were one step closer to death by starvation, and we are well prepared to prevent that occurrence. Once a person is obese, a new set point is established and subsequently staunchly defended. Even the surgical removal of fat, or liposuction, does not result in permanent weight loss because the fat removed by the process is replaced or redistributed to other parts of the body within one year (Hernandez et al., 2011). The average adult weighing 150 pounds needs only 1,800 calories per day. When a single dessert item such as cheesecake provides about 70% of that daily requirement, it becomes easy to see how people eating out frequently could become overweight or obese. It is also likely that before eating the cheesecake a diner might have an appetizer, salad, entrée, and beverage. This doesn’t include meals and snacks consumed at other times throughout the day. Calorie-reducing diets work, but dietary changes cannot be viewed as something to do until a goal weight is reached. If the dieter returns to previous eating habits, the lost weight is quickly regained. Successful diets should be viewed as lifestyle changes that are sustainable for an individual. For example, removing one sugared soft drink per day should result in about a 10-pound weight loss over the next year. It is essential for the dieter to avoid triggering mechanisms designed to prevent starvation, such as lowering metabolic rates. By keeping activity levels up and restricting weight loss to a pound or two per week at most, it is possible to slide weight loss under the radar of our vigilant weight maintenance systems. Efforts to use chemicals to control weight have been discouraging. During the 1950s and 1960s, it was not uncommon for American doctors to prescribe amphetamines for weight loss. Although amphetamines suppress appetite, their psychoactive and addictive properties make this approach less than desirable. More recently, several new medications have been approved for the treatment of obesity. One, orlistat (Xenical), reduces the absorption of fats by the digestive tract. The remaining approved medications target appetite mechanisms in the hypothalamus or prevent reabsorption of glucose by the kidneys. Medications do not work on their own but instead make it somewhat easier for obese individuals to maintain lifestyles that are more consistent with a healthy weight (Apovian et al., 2015). Contrary to popular opinion, many people lose weight by dieting and are successful in maintaining their weight loss for years. The National Weight Control Registry tracks more than 5,000 case studies of successful maintainers, including Drew Saur. Saur lost more than 150 pounds and has maintained his loss since 2005. He began by counting the calories he was consuming and was astonished to find that he was eating about 5,000 calories per day, not surprising given his starting weight of 325 pounds. By gradually reducing calories and waiting to eat until he felt hungry, Saur lost about 2 to 3 pounds per week. He began walking, and when his weight loss permitted more activity, he switched to running. Successful dieters and maintainers such as Saur do not view dieting as something drastic to do before a wedding or class reunion but as a gradual lifestyle change that is livable. Some individuals who become discouraged with diets and medication turn instead to surgical interventions, including stomach stapling and gastric bypass procedures, which literally reduce the amount of nutrients that can be consumed or processed. While the resulting weight loss can be dramatic (the average gastric bypass patient loses 90 pounds within 3 months of surgery) (Maggard et al., 2005), these procedures represent major surgery and often result in complications. Although it is feasible that current research into brain mechanisms for hunger and satiety may eventually lead to better treatments for obesity, lifestyle changes leading to healthier eating and exercise habits remain the most reliable approach. Coexisting with our ongoing obesity epidemic are eating disorders characterized by unusual patterns of eating and distortions of a person’s body image. Normally, we have an accurate view of what we look like. People with distorted body image can be convinced they are obese when they are in danger of starvation. Traditionally, eating disorders have been viewed as a problem for women and as less typical for men. However, increasing numbers of men are experiencing eating disorders and body dissatisfaction. The lifetime incidence of eating disorders in males is approximately 0.17%, compared to about 3% in females (Pedersen et al., 2014). Rates of eating disorders are about six times higher among homosexual and bisexual men compared to heterosexual men, although sexual orientation does not seem to be associated with additional risks for eating disorders among women (Matthews-Ewald, Zullig, & Ward, 2014). Anorexia nervosaAnorexia nervosaAn eating disorder characterized by the maintenance of unusually low body weight and a distorted body image.            Anorexia nervosa        An eating disorder characterized by the maintenance of unusually low body weight and a distorted body image.             is characterized by the maintenance of unusually low body weight, an intense fear of gaining weight, and a distorted view of the body as obese (APA, 2013). Anorexia literally means loss of appetite. People with anorexia can show two patterns of behavior (APA, 2013). In the restricting type, people simply eat little food. In the bingeing-purging type, people combine restricted eating most of the time with behaviors associated with bulimia nervosa, which we discuss later. Anorexia nervosa is dramatic but rare, affecting about 0.4% of females and perhaps 0.04% of males (APA, 2013). Anorexia nervosa is one of the few psychological disorders that can kill, with up to 5% of people with the disorder dying per decade from causes related to the condition, which includes an elevated risk for suicide (APA, 2013). Other symptoms include interruption of normal menstruation in females, very dry and yellow skin, fine downy hair (lanugo) on the face and other parts of the body, increased sensitivity to cold, and cardiovascular and gastrointestinal problems. Bulimia nervosaBulimia nervosaAn eating disorder characterized by bingeing, purging, and having feelings of depression, disgust, and lost control.            Bulimia nervosa        An eating disorder characterized by bingeing, purging, and having feelings of depression, disgust, and lost control.             is characterized by cycles of binge eating, in which unusually large amounts of food are consumed, and purging through the use of vomiting or laxatives. Bingeing is often followed by feelings of depression, disgust, and a sense of lost control. Bulimia is more common than anorexia, affecting 1.5% of women and between 0.15% and 0.5% of men (APA, 2013;  Hudson, Hiripi, Pope, & Kessler, 2007). Binge-eating disorderBinge-eating disorderAn eating disorder characterized by eating abnormally large amounts of food at one sitting and feeling that eating is out of control, without compensatory behaviors such as induced vomiting or the use of laxatives.            Binge-eating disorder        An eating disorder characterized by eating abnormally large amounts of food at one sitting and feeling that eating is out of control, without compensatory behaviors such as induced vomiting or the use of laxatives.             is similar to bulimia nervosa in regard to eating abnormally large amounts of food at one sitting and feeling that eating is out of control, but it does not include compensatory behaviors such as induced vomiting or the use of laxatives (APA, 2013). People with binge-eating disorder eat rapidly, eat until they are uncomfortably full, eat when they are not hungry, and often eat alone because of embarrassment about the quantities they consume. Compared to other eating disorders, differences between males and females with binge-eating disorder are smaller. Approximately 1.6% of adult females and 0.8% of adult males meet the criteria for this disorder (APA, 2013). We have defined healthy weight as a BMI score between 18.5 and 24.9, yet the typical runway model held up as a cultural standard for beauty has an average BMI score of only 16.5. In response to several deaths of ultrathin models, some European countries now require models to maintain a minimum BMI score of 18.5. The United States has not followed suit, and American designers have complained that this standard would require a 6-foot-tall model to balloon to 136 pounds. In contrast, a 6-foot-tall model with a BMI score of 16.5 would weigh a scant 121 pounds. It is not surprising that young women exposed to these standards of beauty occasionally develop distorted images of their own bodies. Environmental factors, especially cultural attitudes toward beauty, can play a significant role in the development of eating disorders. Anne Becker and her colleagues were observing eating patterns in the Fiji Islands when American television became available in 1995 (Becker, Burwell, Herzog, Hamburg, & Gilman, 2002). Prior to this time, Becker reported that dieting was unknown in this culture, which valued a robust, well-muscled body for both men and women. The Fijian language has a term for going thin that is used to express concern about someone who may be losing weight because of health problems. In the United States, people may say, Have you lost weight? You look great! In Fiji, a person is more likely to say, Are you okay? You look like you’re going thin. These cultural norms underwent nearly overnight change with the introduction of American television, with its frequent images of glamorous, ultrathin actresses. Suddenly, 74% of the adolescent girls in Becker et al.’s study reported themselves as being too big or too fat. Teens began to report dieting with the same frequency as their American counterparts, a radical shift from the researchers’ observations since 1988. Because Becker et al.’s data are correlational, we cannot conclude that watching television produced disordered eating in Fiji. However, the results are suggestive of strong cultural influences on patterns of disordered eating. Freedom of speech is a cherished right, especially in the United States, and we decide to restrict access to data reluctantly, as in the case of movie and video game rating systems. Despite these strong values, many health professionals cringe when viewing so-called pro-ana sites online. Some of these communities are designed to help people with eating disorders, but many are unabashedly in favor of promoting thinness at any cost. One example, thinintentionsforever.com, features tips such as Friends will only get in the way. Avoid them until you reach your goals. Comments on the site include ones like this: I need to lose weight, and fast. I am 11 years of age, and am over 110 pounds. I want to get under 100. I cannot buy laxatives or anything, and my mom is against ana. HELP! The site is one of the first to show up on a Google search for pro-ana sites. What do we know about the influence of these sites? Teens diagnosed with eating disorders visit pro-ana sites regularly, and few of their parents were aware of either the existence of these sites or their teens’ use of the site (Wilson, Peebles, Hardy, & Litt, 2006). Individuals with eating disorders who frequent pro-ana sites show more disturbed body image and eating habits than people who view medical information sites about their disorders (Rodgers, Lowy, Halperin, & Franko, 2016). Further research could discover how the use of these sites influences recovery from eating disorders. In the meantime, raising awareness among parents, peers, and health advisers of the existence and content of these sites might increase our ability to help those who have eating disorders. It is likely that general personality characteristics that increase a person’s risk for eating disorders may be heritable, not the disorders themselves (Farstad, McGeown, & von Ranson, 2016). Evidence from twin studies has been mixed, with some authors reporting heritability of 48 to 88% for eating disorders and others finding no evidence of genetic influences (Hinney & Volckmar, 2013). People might have a genetic vulnerability to disordered eating in general but not toward a specific type of eating disorder (Bulik, Kleiman, & Yilmaz, 2016). Once an eating disorder is established, biological factors contribute to maintaining abnormal patterns of eating. Even after people with anorexia nervosa regain normal weight, some still show evidence of elevated levels of hormones that typically raise metabolism and inhibit feeding (Stanley et al., 2003). The binge-purge cycling of bulimia involves processes similar to those of addiction (Avena, Rada, & Hoebel, 2008). When food-deprived rats are given access to sugar water, they tend to binge by consuming larger than normal amounts. Subsequently, if the bingeing rats are given naloxone, a chemical that blocks the action of opioid drugs (discussed in Chapter 6), they respond as if they had been addicted to opioids. People who fast and then binge on sweets may set up a similar addictive process that is difficult to stop. A questionnaire known as the Yale Food Addiction Scale 2.0 shows that nearly all individuals with bulimia act as though they have a food addiction (Vries & Meule, 2016). Before the introduction of American television programming to the islands of Fiji in 1995, the cultural ideal for both men and women was a robust, well-muscled body, as evidenced by this traditional dancer. Dieting and eating disorders were unknown. A short time after American television was available, however, Fijian rates of dieting and eating disorders grew to match American rates and ideal beauties became slimmer. Treating anorexia nervosa can be challenging because the therapist is facing a person who is terrified of gaining weight. The first priority is keeping the person alive, and this effort typically involves hospitalization and careful monitoring of food intake. Cognitive behavioral therapy may be used to address distortions of body image (see Chapter 15), but no known medications are effective in treating anorexia (Dold, Aigner, Klabunde, Treasure, & Kasper, 2015). About 50% of people with anorexia make a full recovery, but nearly 20% fail to respond. Treating bulimia and binge-eating disorder is somewhat more encouraging. Antidepressant medications, particularly selective serotonin reuptake inhibitors (SSRIs), are usually quite effective, especially when combined with cognitive behavioral treatment (Johnson, Tsoh, & Vanrado, 1996;  Kass, Kolko, & Wilfley, 2013). Although sexual behavior is essential for the survival of a species, unlike eating, it is not essential to the survival of an individual organism. However, individuals’ sexual behavior appears to be maintained by processes similar to the life-sustaining motivations we have already discussed. Sexuality is an extremely complex aspect of human behavior. In this chapter, we will focus on the more physical aspects of sexual motivation. In Chapter 11, we will explore sexuality across the life span, and in Chapter 13, we will discuss phenomena related to attraction and maintaining relationships. The females of most mammalian species undergo estrus, a period of hours or days during which the female is receptive or willing to have sex. In these species, hormone levels and sexual behavior are tightly linked. In humans and Old World primates, menstrual cycles replace periods of estrus. The sexual behavior of estrus and nonestrus species is quite different (Rushton et al., 2001). Human females show receptivity throughout the menstrual cycle. However, women who are not using chemical contraception report feeling more interest in sex around the time of ovulation (Cappelletti & Wallen, 2016). This situation has obvious advantages for the reproduction of the species because this is when sexual behavior is most likely to lead to pregnancy. Traditionally, female sexual interest has been viewed as correlated with levels of testosteronetestosteroneA male hormone.            testosterone        A male hormone.            . Women typically produce about one-tenth the amount of testosterone that men produce. Women’s adrenal glands produce up to half of their circulating androgens or male hormones, including testosterone, with the ovaries producing the rest (Abraham, 1974). An early influential study found that women who have had their adrenal glands removed decreased their sexual activity (Waxenberg, Drellich, & Sutherland, 1959). This led to decades of research and treatment that assumed that adrenal hormones, and testosterone in particular, were important to female sexual desire. However, more contemporary research points to the sex hormones released by the ovaries as being responsible for female sexual desire. The relative contributions of testosterone and estradiol, the major female sex hormone, remain the subject of considerable debate (Cappelletti & Wallen, 2016). Testosterone is clearly correlated with sexual interest in males. However, as long as a man’s testosterone falls within a normal range, it does not provide a strong predictor of an individual’s sexual frequency (Gray et al., 2005;  Mazur & Booth, 1998). Below-normal levels of testosterone, as in castration, are usually accompanied by very low sexual desire and activity. Male testosterone levels fluctuate over the course of a day and are influenced not only by a man’s biology but also by several environmental factors, notably situations perceived as competitive. Male collegiate athletes experience increases in testosterone in anticipation of upcoming competitive events. Following a competition, testosterone continues to rise among the winners and temporarily decreases among the losers (Booth, Shelley, Mazur, Tharp, & Kittok, 1989). Simply observing a competition may influence testosterone levels. Men cheering for the successful Brazilian soccer team at the 1994 World Cup experienced increased testosterone levels, whereas men supporting the losing Italian team experienced a decrease (Bernhardt, Dabbs, Fielden, & Lutter, 1998). These fluctuations are more dramatic when taking the location of the venue into account. Losing at home produces a more dramatic decrease in testosterone than losing a game as the visiting team. This response to losing a competition at home may represent a remnant of our ancestors’ need to defend their territory. We can assume that losing a battle is catastrophic in any circumstance, but losing near your family and home makes them especially vulnerable to destruction. Male testosterone levels are also influenced by relationship status. Men in stable, long-term relationships have lower testosterone levels than single men or men who are within a few years of divorce (Mazur & Michalek, 1998). Men’s testosterone levels also drop significantly after the birth of a child (Gettler, McDade, Feranil, & Kuzawa, 2011). These changes might reflect a feedback loop in which reproductive success signals a decrease in testosterone (Puts et al., 2015). High testosterone levels interfere with pair-bonding and parental care by stimulating continued competition for mates and also have costs related to immune system functioning. It makes sense that the achievement of reproductive goals would lead to a reduction in the competitive behaviors and physical costs associated with high testosterone levels (see Figure 7.22). In males, higher testosterone levels are associated with psychological factors including increased competitiveness and sexual desire, which then lead to increased sexual behavior. As reproductive goals (e.g., mating and producing children) are achieved, male testosterone levels drop again. High testosterone levels are not compatible with pair-bonding and parenting and have detrimental effects on the immune system, so this decrease represents an effective adaptation. We have seen how sexual desire is correlated with testosterone for men and testosterone and estradiol for women, but romantic love is associated with two other hormones, oxytocin and vasopressin (see Figure 7.23). In humans, both hormones are active in the brain, but vasopressin is expressed more by males, and oxytocin is expressed more by females (Ishunina & Swaab, 1999; van Londen et al., 1997). In both sexes, oxytocin enhances bonding. Women release oxytocin during breastfeeding and even respond to a brief hug from their partners with a spike in oxytocin release (Light, Grewen, & Amico, 2005). Oxytocin is released at orgasm in both sexes. However, because of the greater overall expression of oxytocin in the brains of females, along with the 10 times higher testosterone levels in men, it appears that women are more likely than men to equate sexual desire with feelings of romantic love (Diamond, 2004). We do not mean to imply that men do not bond, but high testosterone levels are likely to make sexual activity highly salient for young men, with or without the bonding associated with oxytocin release. The discovery of the importance of oxytocin and vasopressin to bonding originated in the study of two types of rodents: the prairie vole and the montane vole. Although similar in many ways, the two types of vole differed in both their bonding to a partner and their parenting behaviors. Prairie voles are monogamous for life, but montane voles are not. Prairie voles share parenting duties, but montane voles do not. Differences between the two types of vole in oxytocin and vasopressin were closely associated with the behavioral differences, with the prairie voles showing higher levels of both hormones than the montane voles. In one type of study, voles are allowed to mate, and then they are placed in a test box. On either side of the male, who can move freely, are two females—a stranger and the partner—who are tethered in their boxes and can’t move. The chart shows how much time the average male prairie vole and montane vole spend with the partner, with the stranger, or alone. As you can see, the prairie voles prefer to be with their partners, but the montane voles prefer to be alone. Although cultural values often link sexual desire and romantic love, these two functions represent distinct biological and emotional states. Romantic love promotes the establishment of long-term relationships, whereas sexual desire promotes mating (Cacioppo, 2016). Both romantic love and sexual desire involve the insula. Sexual desire is correlated with activity toward the rear of the insula, consistent with other observations that the back of the brain is more involved with current and short-term sensory experiences. In contrast, romantic love is associated with activity toward the front of the insula, consistent with future-thinking about more abstract concepts (Cacioppo & Cacioppo, 2013). Romantic love and sexual desire often occur along a time continuum, with the pleasant sensorimotor experiences associated with sexual desire leading to the more cognitive feelings of romantic love (Cacioppo, Bianchi-Demicheli,  Frum, Pfaus, & Lewis, 2012). A study of a rare patient with a lesion in the insula suggests that activity in the insula does not simply correlate with romantic love and sexual desire but actually contributes to differences between these processes (Cacioppo et al., 2013). Humans show wide variations in sexual and emotional satisfaction within relationships (see Figure 7.24). Cross-cultural studies show that subjective sexual satisfaction depends on a number of factors, including relative equality between men and women, mental and physical health, and the importance of sexual behavior for an individual (Laumann et al., 2006). In the United States, more committed relationships produce the highest levels of both sexual and emotional satisfaction (Mark, Garcia, & Fisher, 2015). Individuals who expect to continue a relationship are more attentive to their partners, both emotionally and sexually. In contrast, one-night stands receive the lowest sexual and emotional satisfaction scores from participants and promote the strongest feelings of regret (Fisher, Worth, Garcia, & Meredith, 2012). A survey of nearly 30,000 people in 29 countries showed wide variations in sexual satisfaction. In most countries, there was a strong correlation between relational satisfaction (defined as a combination of physical pleasure with a partner and emotional satisfaction with a partner) and sexual satisfaction. Gender differences in satisfaction with sexual function were the smallest in Cluster 1 countries. Historically, research on sexual satisfaction has focused on heterosexual couples. Relatively few studies have considered sexual satisfaction across sexual orientations. Mark and her colleagues (2015) found no differences in sexual and emotional satisfaction among heterosexual, bisexual, and homosexual individuals in committed relationships but did observe differences in casual sexual contexts. Individuals of all sexual orientations reported more emotional and sexual satisfaction from committed relationships than from casual sex situations. However, gay and bisexual men reported experiencing higher amounts of sexual and emotional satisfaction from casual sex contexts than heterosexual individuals, while lesbian women reported less, although the size of these differences was relatively small. A homosexual orientation is carefully defined as a stable pattern of attraction to people of the same gender. Homosexual behavior can occur with or without a homosexual orientation. Among the Sambian people of New Guinea, young males are separated from their mothers and expected to engage in homosexual behavior with older males. The Sambian people believe that obtaining semen in this manner transforms them into powerful warriors (Herdt, 1987). After marriage, the now-adult man is no longer expected to engage in homosexual behavior. Individual differences in human sexuality are substantial and normal. One of the ways individuals vary is in their sexual orientationsexual orientationA stable pattern of attraction to members of a particular sex.            sexual orientation        A stable pattern of attraction to members of a particular sex.            , which refers to a stable pattern of attraction to members of a particular sex. A person’s sexual orientation can incorporate behavior, attraction, and identity (Ward, Dahlhamer, Galinsky, & Joestl, 2014). These three components are often congruent, but not necessarily so. Many people engage in same-sex behavior and fantasy while maintaining a strong heterosexual orientation. In the Sambia tribal culture of New Guinea, all adolescent males are expected to engage in same-sex behavior before marrying women (Stoller & Herdt, 1985). The adult sexual orientation of these men remained overwhelmingly heterosexual. The U. S. National Health Interview Survey incorporated carefully crafted questions about sexual orientation beginning with Which of the following best represents how you think about yourself? Choices for males included gay, straight (that is, not gay), bisexual, something else, and I don’t know the answer. Choices for females included lesbian or gay, straight (that is, not lesbian or gay), bisexual, something else, and I don’t know the answer. Individuals who chose something else or I don’t know the answer were asked follow-up questions but were not included in the overall report of the data. Results showed that 97.7% of women and 97.8% of men identified themselves as straight, 1.5% of women and 1.8% of men identified themselves as homosexual, and 0.9% of women and 0.4% of men identified themselves as bisexual (Ward, Dahlhamer, Galinsky, & Joestl, 2014). It is likely that male and female sexual orientations emerge along different pathways (Diamond, 2012). Sexual orientation in males tends to be stable, acting like a compass, whereas females experience more fluidity in their sexual attractions over time. Women are more likely than men to experience physical arousal and desire for both sexes and to report changes in their degree of same-sex or other-sex attraction over time (Diamond, 2012). Genes appear to influence sexual orientation, although the exact mechanisms with which they do so are not well understood and are likely to be quite complex. Like most human behaviors, sexual orientation is probably the result of an interaction between genetic factors and other aspects of the individual’s experience, which can include prenatal factors. To assess the genetic contribution to sexual orientation, researchers have compared rates of homosexuality in twins. If one identical male twin is homosexual, his twin has about a 50% chance of also being homosexual (Kirk, Bailey, & Martin, 2000). Analyses of twin reports of the number of lifetime same-sex partners showed that sexual orientation is somewhat heritable for both males (0.34–0.39) and females (0.18–0.19) (Långström, Rahman, Carlström, & Lichtenstein, 2010). Genome-wide linkage scans compare complete sets of DNA for individuals that differ in a trait. This type of study shows that male sexual orientation is associated with chromosome 8 and an area of the X chromosome known as xq28 (Sanders et al., 2015). As genetic methods continue to improve, it is likely that we will have a better understanding of the role of genes in sexual orientation in the future. Among the other possible biological variables affecting sexual orientation are levels of prenatal, but not adult, sex hormones. Women with a condition known as congenital adrenal hyperplasia (CAH) experience high prenatal levels of male hormones. Consequently, they are often born with masculinized external genitalia, are more likely to describe themselves as tomboys, and are more likely to engage in lesbian or bisexual behavior (Meyer-Bahlburg, Dolezal, Baker, & New, 2008). However, most women with CAH are heterosexual, just as most lesbian and bisexual women do not have a history of CAH or similar conditions. An interesting clue to the origin of sexual orientation arises from the observation that birth order influences the odds that a man is homosexual. Men who have older brothers from the same mother are more likely to be gay than men who have no siblings, younger siblings only, or older sisters (Balthazart, 2011;  Blanchard, 1997). Theoretically, carrying a male fetus could provoke a mother’s immune response, which would become stronger with each successive male fetus she carries. Her immune response could influence the development of her fetus by altering significant hormonal processes. Several structures in the brain are known to be different in males and females and also seem to differ between homosexual and heterosexual males. Among these is a small cluster of neurons located in the hypothalamus known as the interstitial nucleus of the anterior hypothalamus (INAH; see Figure 7.25). There are four separate nuclei in INAH. Two of these, INAH-2 and INAH-3, are notably larger in males than in females (Allen, Hines, Shryne, & Gorski, 1989). The exact function of these nuclei in humans is unknown, although animal research suggests that they participate in sexual behavior. Building on this research about differences between the brains of men and women, Simon LeVay asked whether INAH-3 might be different in heterosexual and homosexual males. After examining the brains of more than 40 individuals, LeVay (1991) concluded that INAH-3 in homosexual men was about two to three times smaller than in heterosexual men, or about the same size as typically found in women. Simon LeVay reported that an area of the hypothalamus known as INAH-3 differs between heterosexual and homosexual men. If you compare the samples in (b) and (c), you can see that INAH-3, indicated by the four dark arrows, appears to be larger in the heterosexual male sample than in the homosexual male sample. The impact of this size difference on adult sexual behavior or sexual orientation remains unknown, however, because these are correlational data. LeVay was quite cautious in his interpretation of his results. The homosexual individuals in his sample had all died from AIDS. Otherwise, there would be little reason to note sexual orientation on a person’s medical records. Although it is unlikely that AIDS would produce this type of change in the brain, LeVay could not rule out that possibility based on his data. LeVay also raised the possibility that engaging in homosexual behavior might influence INAH-3, even in the adult, although he gave little credence to that possibility. Nonetheless, LeVay’s research had a profound effect on the way psychologists and the public looked at sexual orientation. Before the 1990s, the medical and psychological research literatures used the term sexual preference instead of today’s sexual orientation. Because of the difficulties arising from this type of research in humans, others have investigated the correlations between brain structure and sexual behavior in animals. Among domestic sheep, 6 to 8% of rams (males) mate exclusively with other males. The sheep equivalent of INAH-3 is about the same size in ewes (females) as in the rams that mated with other rams. This structure is larger in rams that mated with ewes (Roselli, Larkin, Resko, Stellflug, & Stormshak, 2004). So far, our discussion has focused on motivational behavior that is central to maintaining life. In addition, humans experience a range of complex cognitive and social motives. A lengthy list of these types of complex motives published in 1938 included achievement, affiliation, autonomy, nurturance, dominance, play, and order (Murray, 1938). More recent work focuses on needs for competence and relatedness (Deci & Ryan, 2000). Although some psychologists retain strong distinctions between biological and psychological motives, the dividing line is surely blurred. When an individual’s need for affiliation is not met, the consequences of the resulting loneliness include biological factors, such as increased rates of illness and death (Cacioppo et al., 2015). Humans are a social species. Our need to belong might not seem to be in the same category to you as our need for nutrients and sexual behavior, but the results of many studies show that affiliation works similarly to the other motivations we have discussed so far (Baumeister & Leary, 1995). When people are asked to identify which pleasures are most important to their happiness, the overwhelming majority rate love, intimacy, and social affiliationaffiliationBeing associated with other people.            affiliation        Being associated with other people.             above wealth, fame, and even physical health (Cacioppo & Patrick, 2008). Not only do people value affiliation, but a perceived lack of connection with others can have devastating effects. Solitary confinement is viewed as one of the worst punishments humans inflict on one another. The effects of social isolation can be as detrimental to good health as high blood pressure, lack of exercise, obesity, or smoking (House, Landis, & Umberson, 1988). Not all behaviors associated with feeling isolated are self-destructive. In some cases, feeling isolated can result in outwardly hostile behavior (see Figure 7.26). Students told in an experiment that a personality test had shown that they were the type likely to end up alone later in life showed less empathy and more aggression toward other students than did students told that they would enjoy rewarding relationships throughout life (Twenge, Baumeister, Tice, & Stucke, 2001). It is probably not an accident that most individuals responsible for mass shootings are described as outcasts or loners. People who were told that a personality test indicated they were likely to spend their future alone without significant relationships were far less helpful across different types of tasks than participants who were told that they were likely to have a future featuring acceptance and belongingness. The experience of loneliness appears to serve as an aversive signal that, like hunger, thirst, and pain, evolved to warn humans that they are facing a threat to their survival and that their social connections are in need of repair (Cacioppo & Patrick, 2008). Social connections are especially important to the survival of humans. Compared to other species, humans require the greatest amount of parenting to survive to adulthood and reproduce. In addition, as we discuss in Chapter 13, humans are not well equipped to survive in isolation. Our ancestors formed hunter-gatherer groups because the cooperative sharing of responsibilities enhanced the survival of all group members. Individuals differ in the amount of social connection they desire. Early work on affiliation by Henry  Murray (1938) and David  McClelland (1985) viewed the need for others as similar to a personality trait, as we discuss further in Chapter 12. Some people are relatively happy working in a cubicle for hours on end, while this lack of social contact would be intolerable for others. Still others might feel lonely even when surrounded by large numbers of caring friends and family members. We appear to have a set point for social activity that operates similarly to the set points we mentioned in our discussion of hunger. When we sense a gap between our actual social connectivity and what we desire, we experience the unpleasant state of loneliness, which in turn motivates us to seek more social experiences. Twin studies show that this set point, like the others we have discussed, is influenced by our genetics (Boomsma, Willemsen, Dolan, Hawkley, & Cacioppo, 2005). Knowing one identical twin’s need for affiliation helps predict the other twin’s need. Individual predispositions interact with people’s situations to predict desire for affiliation. We frequently join others to share good events, whether that means a birthday, a wedding, a promotion, or simply the end of another round of final exams. We have all enjoyed the camaraderie of staying in the stands long after our team has won an important game, celebrating with nearby fans and exchanging high fives. People affiliate not only to share happy occasions, such as the introduction of a royal baby, but also to reduce stress in sad times, such as when family members of victims of the September 11, 2001, terrorist attacks gathered to throw flowers in the memorial pools at Ground Zero. Affiliation is not just for good times, however. People often find that the company of others helps to reduce stress. In a classic series of experiments, Stanley  Schachter (1959) showed that people expecting to be given a painful electric shock were more likely to seek the company of other study participants than to wait for their shock alone. Not all stressful situations are equally likely to produce affiliation, however. In one study, participants were told that they would be performing embarrassing behaviors, such as sucking on large pacifiers (Sarnoff & Zimbardo, 1961). These participants made different choices than the participants expecting shock in the previous study: They preferred to be alone. Exactly what does the company of other people contribute to reducing stress? It appears that we appreciate the company of others when we expect them to reduce our stress. In many cases, other people can reduce our stress by providing information. Hospital patients awaiting dangerous surgeries preferred roommates who had already been through the procedure to those who had not yet had surgery (Kulik, Mahler, & Moore, 1996). Achievement motivationAchievement motivationA desire to excel or outperform others            Achievement motivation        A desire to excel or outperform others            , or the desire to excel or outperform others, has been a topic of interest to psychologists beginning with William  James (1890). Using measures of individual achievement motivation, people who are high in achievement motivation have been shown to seek and typically succeed in competitive, entrepreneurial occupations (McClelland, 1985). The role of individual achievement motivation in predicting performance has been of great interest in the fields of education, management, and athletics. Many psychologists have recognized that individual differences in achievement motivation seem similar to personality traits, which we discuss in Chapter 12 (Elliot & Murayama, 2008). A trait is a stable characteristic that shows relatively little variation over time. However, traits also typically interact with situations. An introverted person might seem more introverted at a big party than when out to dinner with one friend. According to this more complex view, a person’s trait achievement motivation interacts with the opportunities present in the environment (Hart & Albarracin, 2009). For example, achieving excellence might be viewed as an alternative to having fun or enjoying leisure time. High-achieving people do not dislike fun but are prepared to avoid or postpone some fun to meet their achievement goals. Low-achieving people may be perfectly capable of success but, for a variety of reasons, have simply prioritized fun alternatives and inhibited the achievement alternatives. A person’s trait level of achievement motivation interacts with the opportunities to achieve available in the environment. Carol  Dweck (2012) argues that achievement motivation is influenced by people’s beliefs about their own abilities. Individuals with a growth mindset, or a belief that capacities can be developed, outperform individuals with a fixed mindset, or a belief that capacities do not change. In other words, a person with a fixed mindset might say I’m not good at math, while a person with a growth mindset might say math is hard, but I’m getting better at it. Individuals with a fixed mindset avoid challenges, become defensive when criticized, and feel envious when they see someone performing better. In contrast, individuals with a growth mindset focus on the processes that lead to success, such as hard work and trying out new strategies. Instead of catastrophes, mistakes are viewed as important sources of information eventually leading to a solution. A number of practical implications emerge from the study of achievement motivation. Many teachers, managers, and coaches emphasize competition, such as sales contests, as a way to promote achievement. While this competitive approach works well for people who are high in achievement motivation, it might not work so well for those who are lower in this trait (Hart & Albarracin, 2009). For lower achievers, messages that emphasize the fun and relaxing aspects of a task might be more likely to elicit the best outcomes. Dweck cautions that her work is often misinterpreted by teachers and parents, who claim everyone is smart or praise children for working hard even when they’re not making any progress. She advises that praising strategies and tying those to the outcome of the child’s efforts is more effective (You tried different things and figured out the solution). We have discussed several different motivations, from hunger to sex to affiliation to achievement. If faced simultaneously with many drives and demands, how do we decide which motivations to follow first? As we observed in the case of achievement motivation, situations often present us with multiple, overlapping opportunities and goals (Elliot, 1999). Approach and avoidance differ in valence or natural attractiveness, with approach involving positive and desirable outcomes and avoidance involving negative and undesirable outcomes. A particular situation might stimulate us to engage in a behavior to approach an outcome, avoid one, or possibly do both simultaneously. When you think about your motivation for studying this textbook right now, you might think you are working to get a good grade in the course (approach), avoid getting a poor grade (avoidance), or do some combination of the two. In any case, you are motivated to study. This example is only one of several forms conflicts among motives can take. In the example mentioned previously of the student facing a choice between doing well in school and enjoying fun and leisure time, the conflict occurs between two positive choices, or approach-approach. The individual must decide which goal has priority. In still other cases, choices occur between two unpleasant alternatives (avoidance-avoidance). If you don’t stay late at work, your boss will think you’re a lazy employee, but if you do stay late at work, your partner will be angry. In most real-world situations, choices typically have multiple, overlapping approach-avoidance aspects, making the prioritization of goals essential. An early effort to provide a model for prioritizing motives was contributed by Abraham Maslow in 1943. Maslow viewed motivation as a hierarchy of needs, in which lower levels must be satisfied before the individual can pursue higher-level needs. As shown in Figure 7.27, Maslow’s model is typically illustrated as a pyramid. Maslow’s classic pyramid depicting his hierarchy of needs is shown in (a).  Kenrick et al. (2010) have proposed an updated version of the pyramid (b) that incorporates developmental and evolutionary perspectives. Schoolteachers often face students who come to school hungry or sick because of poverty or neglect. According to Maslow, it is difficult for teachers to interest these children, whose basic needs are unmet, in learning about long division and other subjects. At the lowest level of the pyramid, we find physiological needs, including food, water, and shelter. These basic needs must be met daily; otherwise, life will be threatened. Consequently, if meeting these needs is a challenge for a person, Maslow predicted that the person will not pursue needs at higher levels of the hierarchy. In one classic study, young healthy men were given approximately 1,500 calories per day for 6 months, which resulted in a loss of about 25% of their normal body weight (Keys, Brozek, Henschel, Mickelsen, & Taylor, 1950). Not only did the men become obsessed with food, but their interest in sex declined dramatically. Once physiological needs are met, Maslow suggests that we turn our attention to safety and then to belongingness, represented by the love and affection of others. For Maslow, these three lower categories are essential to human life. Unfortunately, we don’t need to look too far in the daily news to read about large numbers of the world’s population who do not attain these basic needs. Miller (1944) shocked rats while they were eating a food reward they had earned by running down a maze alley, setting up a conflict between approach and avoidance toward the food box. Subsequently, the rat would run down the alley but stop without entering the food box. Miller could adjust where the rats stopped (a point of equilibrium between approach and avoidance) by changing either the rats’ hunger levels or the intensity of the shock. Freed from the challenges of meeting basic needs, we begin to seek esteem, or the respect we receive from other members of the community. At the pinnacle of human striving, however, is the goal of self-actualizationself-actualizationA state of having fulfilled your potential            self-actualization        A state of having fulfilled your potential            . People seeking self-actualization desire to fully meet their potential, as suggested by the U. S. Army slogan Be all you can be. Cultures differ dramatically in their emphasis on self-actualization, with individualistic nations like the United States embracing this value more than collectivist nations in Southeast Asia, where harmony and belongingness are highly valued (Hofstede, 1984). Abraham Maslow did not believe that everyone achieves self-actualization. Among the select few he believed had reached this pinnacle of motivation were Abraham Lincoln and Mahatma Gandhi—as well as U. S. First Lady Eleanor Roosevelt, shown in this photograph with Madame Chiang Kai-shek. Maslow’s self-actualized people shared common attributes, including independence and a good sense of humor. Who would you add to this list of self-actualized people? Maslow’s classic theory received a recent modification that retained the overall hierarchical organization but added three new perspectives: the evolutionary functions of motives, the development of motives over the life span, and the cognitive priorities assigned to motives in response to environmental stimuli (Kenrick, Griskevicius, Neuberg, & Schaller, 2010). This modified pyramid, illustrated in Figure 7.15, replaces self-actualization with mate acquisition, mate retention, and parenting. The authors of the revision noted that self-actualization was interesting, but they could not find an evolutionary explanation for why we would seek to reach this level. Many of the activities described by Maslow as helping people to reach self-actualization, such as art and poetry, might be better explained as efforts to gain status, which in turn would attract mates. The idea that good relationships are characterized by positive emotions and that bad relationships feature many negative emotions sounds obvious. However, it is unrealistic to think that happy couples never experience conflict. They do, but the way they fight can have a big influence on the likelihood that they will stay together. John Gottman uses a combination of heart rate, facial expression, and analysis of the way people talk about their relationships to each other and to others to make accurate predictions about whether a relationship will last (Gottman, 2011). A key observation is the ratio of positive to negative comments in a couple’s discussion of a problem. Happy couples make five times more positive comments about each other and their relationship during these discussions (e.g., we laugh a lot versus we never have fun). In several places in this textbook, we have emphasized how the human mind is skewed toward the negative, such as noticing bitter tastes over sweet. This slant suggests that it is easy to focus on your partner’s negative qualities, leading to negative emotions and conflict. If we put our relationships on evolutionary cruise control, the ratio of positive-to-negative comments might drop to a point where the relationship is in danger. Maintaining a positive outlook on your partner requires attention and work. Try observing how people talk about their partners, or watch your own behavior, if you’re brave enough. Are you achieving Gottman’s 5:1 ratio of positive to negative comments? Research by John Gottman shows that happy couples do experience negative emotions with each other. However, for every negative incident, they tend to experience five times as many positive emotional interactions. What motivates someone to cyberbully another person? Answers to this question might emerge from what we know about the motivations of face-to-face bullies. Bullying is aggressive, which means that it involves the deliberate intent to harm another. Aggression takes many forms, but bullying research focuses on two types: reactive and proactive aggression (Crick & Dodge, 1996). Reactive aggression involves retaliation for a perceived harm, whereas proactive aggression occurs without provocation and is usually goal-oriented. The goals might be dominating another person or enhancing one’s own reputation or popularity among peers. Research on face-to-face bullying shows that proactive aggression is the norm (Camodeca, Goossens, Terwogt, & Schuengel, 2002). Will this finding also be characteristic of cyberbullying? In cyberbullying, reactive aggression might take the form of retaliation for behaviors or online content that is viewed as insulting. This process can lead quickly to an escalating flame war. Proactive aggression might take the form of hijacking a person’s identity or setting up a fake social media account used to harm the person. Although relatively little research has been done in this area, existing studies point to reactive motivation as the most common form of aggression in cyberbullying (Shapka & Law, 2013). In this sense, then, cyberbullying is not similar to its face-to-face counterpart. Interventions aimed at reducing proactive aggression in cyberbullying, based on similar interventions for face-to-face bullying, might not produce the desired effects. Research has shown that most cyberbullying involves reactive rather than proactive aggression. Here we see that a simple statement from an EPSN publicist provokes a critical reaction from a college football coach. In response, the ESPN publicist fires back with an emotional response. TermMeaningExampleMotivationA process that arouses, maintains, and guides behavior to a goal. Being thirsty leads you to seek a drinking fountain and get a drink of water. HomeostasisA steady internal balance or equilibrium. We are motivated to maintain a constant internal temperature. If we feel cold, we shiver and put on more clothing. Set pointA value actively defended to maintain homeostasis. It is hard to lose weight because we actively defend our previous weight by slowing our metabolism when fewer calories are consumed. DriveA state of arousal or tension resulting from stimuli that are important to survival. Being hungry is unpleasant. Drive reductionThe feelings of relief and reward following a return to equilibrium. Quenching your thirst on a hot day is rewarding. IncentivesRewards that motivate behavior without the experience of any unpleasant drive state. You enjoy throwing a great surprise party for a friend’s birthday.  `;\n","export const text = `Learning something new produces structural changes in neurons. Although you may not have had an opportunity to learn to surf, we’re assuming that in your role as a student, you are familiar with the process of learning. But knowing how to learn is different from understanding how and why learning occurs. What is going on in the minds of the people in the photo on this page as they learn to do something new? A behavior like learning to surf is complicated, so scientists interested in learning often begin their examinations using animals that are simpler than humans, observing as they learn to do behaviors simpler than surfing. Scientists have zoomed in to observe changes that happen in single neurons, like the image on the left, found in simple animals like the Aplysia californica sea slug. The sea slug is capable of learning to anticipate an electric shock that happens every time it is touched, and it demonstrates this knowledge by protectively withdrawing its body in response to touch before the shock is administered. Because the slug has such a simple nervous system compared to ours, scientists have been able to identify which neurons are involved in this learning and to describe the changes in a neuron’s functioning that make new behaviors—like withdrawing prior to shock—possible. Is it possible that human learning has anything in common with learning in slugs? The answer to this question is yes and no. Zooming out, we will see that some types of learning are common across animals with vastly different evolutionary histories and complexities, like humans and sea slugs. In other cases, we see interactions between nature and nurture. Experience, or nurture, frequently interacts with the nature of the organism exposed to the experience. You can talk to your kitten and to an infant, but only the infant will respond to this experience by learning to understand what you’re saying and how to reply. Each species brings unique and innate building blocks of learning, accumulated over many generations, to any given situation. Animals respond to the environment with reflexes, instinctive behaviors, and learned behaviors. Animals, like this ferocious kitten, use a reflex called piloerection to make themselves look bigger. The evolutionary purpose of this behavior is to appear so menacing that they can avoid conflict. Zooming out still more, we will see how learning in groups can often be different from learning as an individual. Even the lowly sea slug learns differently when alone than when in a group of fellow slugs. Slugs in isolation seem to have a terrible time learning to stop trying to eat food that is too tough to swallow (Schwarz & Susswein, 1992), but they learn this task easily when in the presence of other slugs (Susswein, Schwarz, & Feldman, 1986). Apparently, slugs can communicate with one another using pheromones, and the presence or absence of these chemicals has a powerful effect on their ability to learn (Susswein, Schwarz, & Feldman, 1986). The people on the previous page, who are learning to surf in a group, might have had a different experience if they were taking lessons individually. Animals, including ourselves, behave in response to their environments. Behavior can take the form of either externally observable actions or internal processes, such as emotions, thoughts, and physiological responses. These behaviors fall into three broad categories: reflexes, instincts, and learned behaviors. Each type of behavior plays a role in helping us survive, but each differs dramatically in its ability to adapt to a changing world. ReflexesReflexesAn inevitable, involuntary response to stimuli.            Reflexes        An inevitable, involuntary response to stimuli.             are inevitable, involuntary responses to stimuli. In casual conversation, we sometimes attribute a baseball player’s high batting average or our ability to step on the brake in time to avoid an automobile accident to great reflexes, but in fact, these examples involve learned behaviors that have become fast and automatic as a result of lots of practice. Nobody is born knowing how to hit baseballs or use the brakes of a car, so these behaviors do not meet this definition of a reflex. Goose bumps are what is left of piloerection in humans. These signals of arousal are remnants of a time when we had enough hair to make this an easily noticeable response. In humans, most reflexes are controlled by nervous system circuits located in the spinal cord and brainstem, as described in Chapter 4. Your physician checks one of these reflexes by tapping your knee with a hammer. The tap stretches your leg muscles, and the stretch is sensed by neurons in the spinal cord. Motor neurons in the spinal cord tell your thigh muscle to contract to compensate for the stretching, and your foot kicks out. No experience with knee-tapping is necessary to produce this behavior, nor can you voluntarily prevent it. By the time your brain realizes your knee was tapped, you have already reacted. Other reflexes pull our bodies away from painful stimuli, such as when we step on a tack or piece of glass or touch a hot stove, turn our heads in the direction of loud sounds, and help us stand upright and walk. Although some scientists disagree, contagious yawning might represent an instinct for a number of species, including our own. In one study, puppies over the age of 7 months were susceptible to contagious yawning in response to seeing humans yawn. Reflexes produce fast, reliable responses that promote your welfare. If you’ve ever touched a hot stovetop and found yourself pulling your hand back seemingly before you even knew what you had done, you know at least one benefit of reflexes. Reflexes have the disadvantage of being inflexible. For example, we respond to stress or cold by forming Goose bumps (bumps on the skin). This reflex appears to be left over from when our species had more body hair. Goose bumps raise each strand of hair, which in times of stress makes an individual look larger, scaring off predators or competitors, and, in response to cold, traps more insulating air near the skin. As humans lost most of their body hair over time, the advantages of this reflex decreased, but we still retain the behavior. InstinctsInstinctsAn inborn pattern of behavior elicited by environmental stimuli; also known as a fixed action pattern.            Instincts        An inborn pattern of behavior elicited by environmental stimuli; also known as a fixed action pattern.            , also called fixed action patterns, are inborn patterns of behavior elicited by environmental stimuli. Once they begin, instinctive behaviors run until completion. Instincts share reflexes’ reliability and lack of dependence on experience, but the resulting behaviors are more complex, requiring many more neurons than the number involved in a reflexive kick of your foot. Instinctive behaviors occur in the mating and parenting behaviors of many species (Tinbergen, 1951). For example, a mother dog instinctively licks clean her first litter of pups immediately after birth. An example of a human instinct is yawning. Once a yawn is initiated, it is difficult to stop. Psychologists define learninglearningA relatively permanent change in behavior or the capacity for behavior due to experience.            learning        A relatively permanent change in behavior or the capacity for behavior due to experience.             as a relatively permanent change in behavior (or in the capacity for behavior) due to experience. The core of this definition is the phrase change in behavior. After learning, we can do something new that we couldn’t do before, providing us with enormous advantages in surviving a changing world. Not all changes in behavior take place because of learning, however. Our behavior changes as we mature from infancy through adulthood, as we will see in Chapter 11. Behavior can be changed by brain damage or by having a psychological disorder. So our definition of learning limits the changes we consider to be learned to those that result from experience. The other qualification in this definition, relatively permanent, prevents the labeling of brief or unstable changes—such as when we experience different moods or suffer from an illness—as learning. The extent to which learning is really permanent will be discussed in Chapter 9. In the 20th century, psychology was dominated by the beliefs that, compared to other animals, humans have relatively few reflexes and instincts and that most human behavior results from learning. William James argued that humans have more instincts than other animals, although we are usually unaware of them (James, 1887). He believed that our behavior simply appears more complex and thoughtful because we often face the need to choose among competing instincts. Animals with fewer instincts experience fewer conflicts, so their behavior appears to be more automatic and less thoughtful. James’s approach to instinct and learning is echoed in the writings of contemporary evolutionary psychologists, who argue for an innate learning instinct that prepares humans to learn certain things in particular ways based on our evolutionary history (Cosmides & Tooby, 1997). Cognitive psychologists also revive the flavor of James by suggesting that learned behavior resulting from experience can look automatic and instinctive (Bargh & Chartrand, 1999). For example, prejudice toward a group of people requires learning, but prejudiced behavior often occurs without much conscious awareness, as explored further in Chapter 13. People who consciously believe that they are without prejudice toward members of a minority group nonetheless sit farther from an individual from that group than from members of the majority (Dovidio & Gaertner, 2005). Interactions between instinct and learning provide an explanation for another observation: experience has different effects at different times in an organism’s life span, as discussed further in Chapter 11. We do not attempt to teach philosophy to 2-year-old children because they are not yet capable of learning this material. Imprinting, the tendency of young animals to bond with and follow an adult, depends on timing. Birds that see a human immediately upon hatching follow that person everywhere, but birds that first see a human after having been hooded for their first few days out of the shell attempt to flee in terror (James, 1887). Imprinting, in which a young organism bonds with adults, provides an example of how experience has different effects at different times in the life span. These baby swans are following their parents because the parents were the first things they saw when they hatched. After a delay of a day or two, exposure to the parents would not lead to following. If the baby swans saw a human experimenter instead of their parents upon hatching, they would follow the human instead. Organisms usually show imprinting by the time they are mobile, which for swans means the first day of life, but for human children means closer to 1 year. Because these children have imprinted on their primary caregivers instead of their preschool teacher, the teacher needs a bit of technological help to encourage his charges to follow him on a walk. Learning is traditionally divided into three categories: associative, nonassociative, and observational. More than one type of learning can operate simultaneously in the same situation. Associative learningAssociative learningThe formation of associations, or connections, among stimuli and behaviors.            Associative learning        The formation of associations, or connections, among stimuli and behaviors.             occurs when we form associations, or connections, among stimuli, behaviors, or both. In other words, if A happens, then B is likely to follow. This type of learning helps us to predict the future based on past experience. The ability to anticipate the future provides enormous survival advantages because through it, animals gain time to prepare. Two important types of associative learning are classical conditioning and operant conditioning. In classical conditioningclassical conditioningA type of learning in which associations are formed between two stimuli that occur sequentially in time.            classical conditioning        A type of learning in which associations are formed between two stimuli that occur sequentially in time.            , we form associations between pairs of stimuli that occur sequentially in time. If a child sees a bee for the first time and then gets stung, the child forms a connection between seeing bees and the pain of being stung. The next time a bee flies by, the child is likely to feel quite frightened. In operant conditioningoperant conditioningA type of learning in which associations are formed between behaviors and their outcomes.            operant conditioning        A type of learning in which associations are formed between behaviors and their outcomes.            , we form associations between behaviors and their consequences. If you study hard, you will get good grades. We will discuss these forms of associative learning in more detail in later sections of this chapter. Nonassociative learningNonassociative learningLearning that involves changes in the magnitude of responses to stimuli.            Nonassociative learning        Learning that involves changes in the magnitude of responses to stimuli.             involves changes in the magnitude of responses to a single stimulus rather than the formation of connections between stimuli. Two important types of nonassociative learning are habituation and sensitization. HabituationHabituationA simple form of learning in which reactions to repeated stimuli that are unchanging and harmless decrease.            Habituation        A simple form of learning in which reactions to repeated stimuli that are unchanging and harmless decrease.             reduces our reactions to repeated experiences that have already been evaluated and found to be unchanging and harmless. For example, you might sleep better the second night than the first in the same hotel because you have adjusted to the unfamiliar noises in that environment. Sometimes we habituate to things that we should, ideally, still be noticing. A major concern about exposing children to violent media is the possibility that their emotional responses to violent images will habituate, leading to higher tolerance for violent behavior (Grizzard et al., 2015). It is likely that the first time this dog’s owner attempted to dress it up, the dog was a bit upset. After repeated experiences of being dressed up, however, the dog probably has habituated, which means that it has learned that no harm results from the process. Now it remains calm. In contrast to habituation, sensitizationsensitizationAn increased reaction to many stimuli following exposure to one strong stimulus.            sensitization        An increased reaction to many stimuli following exposure to one strong stimulus.             increases our reactions to a range of stimuli following exposure to one strong stimulus. Following an earthquake, people experience exaggerated responses to movement, light, or noise. If you are awakened by a loud crash, even if you figure out it’s just your roommate coming home late at night, it might be harder to get back to sleep because of your suddenly increased state of arousal. Every little sound now seems magnified. Many universities offer special programs for students who are the first in their extended families to attend college in recognition of the need to level the playing field with students who enter college having already learned a great deal about college life from observing their college-educated family members. Why would we show habituation to some stimuli and sensitization to others? In general, habituation occurs in response to milder stimuli, whereas sensitization occurs in response to stronger stimuli. Habituation ensures that we do not waste precious resources monitoring low-priority stimuli. On the other hand, sensitization is useful in dangerous situations. After detecting one harmful stimulus, raising our overall level of responsiveness should improve reaction time should other dangers arise. Observational learningObservational learningLearning that occurs when one organism watches the actions of another organism; also known as social learning or modeling.            Observational learning        Learning that occurs when one organism watches the actions of another organism; also known as social learning or modeling.            , also known as social learning or modeling, occurs when one organism learns by watching the actions of another organism. If your knowledge of table manners does not extend to the many forks, knives, and spoons at a fancy dinner, you might want to watch what others do before diving into your food. Observational learning provides the advantage of transmitting information across generations within families and cultures. Following an earthquake, this little boy is likely to be extra jumpy for a while in response to other stimuli, like loud noises, because of sensitization. Watching others is an efficient way to learn new skills, like dancing. Imagine for a moment how difficult it would be to write a description of how this dance should be performed. Watching others is also a useful way to learn what is harmful to do. Associative learningClassical conditioning Signal → Important eventSnakes → SnakebiteOperant conditioning Behavior → ConsequencesPractice → Successfully riding a wave in surfingNonassociative learningHabituation Responses to repeated, unchanging, irrelevant stimuliReduced response to neighbor’s loud television every eveningSensitization Responses to many stimuliJumpiness in response to many stimuli following an earthquakeObservational learningWatch → Imitate Copy new dance moves from your favorite music videoWatch → Avoid imitating Watch friend get sick from alcohol—don’t drink as muchAs discussed in Chapter 1, Ivan Petrovich Pavlov (1849–1936) is so tightly connected to the study of classical conditioning that the phenomenon is frequently called Pavlovian conditioning. Pavlov switched his interests from the study of digestion to the study of learning after noticing that his dogs had learned to anticipate the arrival of food. Instead of salivating when presented with food, his dogs began to salivate as soon as the lab assistant retrieved them from the kennel or strapped them into their experimental harnesses in the laboratory. Most people would probably not have noticed the differences in the dogs’ behavior. Pavlov not only noticed but realized the full significance of his observations: the dogs had formed an association between the stimuli preceding the food and the arrival of the food. In other words, the dogs had learned that certain stimuli served as signals for the eventual appearance of food (see Figure 8.1).(1) and (2) Before conditioning, food (unconditioned stimulus, or UCS) reliably produces salivation (unconditioned response, or UCR), and the sound of the metronome produces no reliable responding. (3) During conditioning, the sound of the metronome is followed by the food (UCS), which again produces salivation (UCR). (4) After conditioning, the sound of the metronome (conditioned stimulus, or CS) by itself is sufficient to produce salivation (conditioned response, or CR). Learning has occurred. If you are like most people reading about classical conditioning for the first time, you are probably wondering why psychologists spend so much time and effort discussing salivating dogs. If classical conditioning were that limited in its scope, it probably wouldn’t warrant more than a small footnote in the history of psychology. Instead, classical conditioning explains many of our learned emotional responses to our environment. It forms the basis for many practical applications, from prepared childbirth methods to the treatment of drug addiction and unrealistic fears. In describing the process of classical conditioning, Pavlov distinguished between conditioned and unconditioned stimuli and responses. Conditioned refers to something that must be learned, while unconditioned refers to factors that are reflexive or that occur without learning. Therefore, a conditioned stimulus (CS)conditioned stimulus (CS)An environmental event whose significance is learned through classical conditioning.            conditioned stimulus (CS)        An environmental event whose significance is learned through classical conditioning.             refers to an environmental event whose significance is learned, while an unconditioned stimulus (UCS)unconditioned stimulus (UCS)A stimulus that elicits a response without prior experience.            unconditioned stimulus (UCS)        A stimulus that elicits a response without prior experience.             has innate meaning to the organism. In a typical experiment by Pavlov, dogs heard the sound of a ticking metronome just before food appeared through a small door. Dogs do not have an innate response to the sound of ticking metronomes, but they generally are born knowing what to do with food. By turning on the metronome before delivering food, Pavlov established the ticking sound as the CS, while the food was the UCS. A UCS may be pleasant, like food, but it may also be unpleasant, like an electric shock. The important features of a UCS are its innate biological significance and its reliable ability to elicit a response without prior exposure. Just as dogs don’t require training to salivate in response to meat powder, few of us need experience with electric shock before we respond with fear and other negative emotions. Conditioned responses (CRs)Conditioned responses (CRs)A response learned through classical conditioning.            Conditioned responses (CRs)        A response learned through classical conditioning.             are learned reactions, while unconditioned responses (UCRs)unconditioned responses (UCRs)A response to an unconditioned stimulus that requires no previous experience.            unconditioned responses (UCRs)        A response to an unconditioned stimulus that requires no previous experience.             don’t need to be learned; they appear without prior experience with a stimulus. Salivating when food is put in your mouth is a UCR because we do this reflexively, without prior experience, but salivating to ticking metronomes occurs only as a result of experience. Our definition of learning requires behavior to change, so the appearance of a CR tells us that learning has occurred. Once learning has taken place, the organism now responds to CSs that reliably predict the arrival of the UCS. As this section will show, understanding classical conditioning illuminates a range of behaviors that you might have seen in yourself or others—from the avoidance of foods that you associate with feeling sick, to the butterflies you feel in your stomach before a big performance, to the development of social prejudices. Pavlov and those following in his footsteps have extended the usefulness of classical conditioning by exploring its features and development in more detail. AcquisitionAcquisitionThe development of a learned response.            Acquisition        The development of a learned response.             refers to the development of a CR. Pavlov argued that acquisition requires contiguity, or proximity in time between the CS and the UCS. If the CS occurs long before the UCS, the organism may not view the two stimuli as related. A CS that occurs simultaneously with a UCS or, worse yet, following a UCS is not a useful signal. A dinner bell sounded after food has been served is not helpful (see Figure 8.2). With each pairing of the conditioned stimulus and unconditioned stimulus, conditioned responses become more likely. Acquisition also requires contingency, or a correlation between the CS and the UCS. Learning about a reliable signal is easier than learning about a signal that occurs only sometimes. To demonstrate the contingency factor, Robert  Rescorla (1968) exposed rats to sound followed by a mild electric shock, which quickly produced fear of the sound. For some rats, a shock was administered only after a sound, while for other rats, shocks were administered following the sound on some occasions and without any sound on others. All the rats had the same number of contiguous sound–shock pairings. They differed in the correlation between sound and shock. Learning was faster for rats experiencing signaled shocks 100% of the time. As the percentage of signaled shocks decreased, learning about the signal slowed. If your migraine headaches are always preceded by exposure to bright light, you are more likely to fear bright lights in the future than if your headaches follow exposure to bright lights only once in a while (see Figure 8.3). Imagine the experience of the two dogs in this experiment. Food (unconditioned stimulus) always follows hearing the bell (conditioned stimulus) for both dogs, which demonstrates contiguity. However, the first dog receives food only after the bell, while the food is not always signaled by the bell for the second dog. The dog in the first case would learn the association between bell and food faster than the dog in the second one, because the first dog’s bell is a more reliable signal for food (contingency). CRs disappear, or undergo extinctionextinctionThe reduction of a learned response. In classical conditioning, extinction occurs when the unconditioned stimulus (UCS) no longer follows the conditioned stimulus (CS). In operant conditioning, extinction occurs when the consequence no longer follows the learned behavior.            extinction        The reduction of a learned response. In classical conditioning, extinction occurs when the unconditioned stimulus (UCS) no longer follows the conditioned stimulus (CS). In operant conditioning, extinction occurs when the consequence no longer follows the learned behavior.            , if the association between the CS and the UCS is broken. When Pavlov continued to expose his dogs to the ticking of the metronome without providing food, the dogs eventually stopped salivating in response to the sound of the metronome. Pavlov believed that extinction is not the same thing as forgetting; rather, it is actually new learning that overrides old learning. As evidence for this belief, Pavlov pointed to the occurrence of spontaneous recoveryspontaneous recoveryDuring extinction training, the reappearance of conditioned responses (CRs) after periods of rest.            spontaneous recovery        During extinction training, the reappearance of conditioned responses (CRs) after periods of rest.            , or the reappearance of CRs following periods of rest between sessions of extinction training. Even if a dog has stopped salivating to the sound of the metronome by the end of an extinction session, conditioned salivation reappears at the beginning of the next session. In other words, the CR decreases during a session of extinction training not because the dog is forgetting the relationship between ticking and food, but because the dog is now learning that ticking no longer predicts food, and it may take several sessions for this new learning to replace the old (see Figure 8.4). Addiction often involves the association of conditioned stimuli (CSs), such as the syringe, with the unconditioned stimulus (UCS) of a drug. Recovering addicts can be exposed to extinction—viewing the syringe (CS) without receiving the drug (UCS). This process reduces conditioned responses (CRs) that might be contributing to cravings for the drug. However, extinction typically requires multiple sessions. When the recovering addict returns for the next extinction training session after a period of rest, that person is likely to show CRs again. Eventually, with enough training, extinction is complete, and no further spontaneous recovery is observed. Without the ability to extinguish CRs, adjusting to further changes in the environment would be difficult, if not impossible. We would not be able to learn to enjoy dogs again after being bitten by one. An addict whose associations between needles and the effects of using heroin never extinguished would have an even harder time overcoming addiction. So far, we have been discussing examples of excitatory classical conditioning, in which the organism learns that a CS predicts the occurrence of a UCS. Pavlov was also quite interested in the classical conditioning of inhibitioninhibitionA feature of classical conditioning in which a conditioned stimulus (CS) predicts the nonoccurrence of an unconditioned stimulus (UCS).            inhibition        A feature of classical conditioning in which a conditioned stimulus (CS) predicts the nonoccurrence of an unconditioned stimulus (UCS).            , in which a CS predicts the nonoccurrence of a UCS (Pavlov, 1927). To demonstrate inhibition, we can begin by establishing excitatory conditioning by pairing a signal—a light—with shock. After some experience with this pairing, a rat learns to fear the light. Now we continue to present light–shock pairings, but we add other training trials that include the inhibitory CS—a sound—by presenting the light and sound together, followed by no shock. Even though the light is present, the rat learns that it is not going to be shocked in the presence of the sound, and it shows no fear. For a vulnerable animal in the wild, it is important to know not only that a predator at a water hole is in hunting mode (the sight of a prowling predator is an excitatory CS eliciting fear), but also that a predator relaxing after a recent kill is unlikely to kill again soon (the sight of a predator calmly drinking water is an inhibitory CS that tells the animal that the predator is unlikely to attack soon and therefore it is safe). For drug addicts, establishing inhibitory CSs associated with the lack of an expected drug effect might provide a more powerful method for rehabilitation (Kearns, Weiss, Schindler, & Panlilio, 2005). If an addict learns that drugs are never available in the presence of a certain signal, turning the signal on whenever the addict’s resolve is weak might help prevent a relapse. Inhibition learning helps organisms behave adaptively when they’ve learned that something important will not occur. These zebras may have learned that when lions act a certain way, they are already full and unlikely to hunt again. These inhibitory signals tell the zebras that they can drink in safety—at least for a little while. Once a CR is successfully acquired, organisms often show a tendency to respond to stimuli that are similar to the CS. For example, the child who learned to be afraid of bees after being stung might also begin to fear wasps and yellow jackets, a process that Pavlov called generalizationgeneralizationThe tendency to respond to stimuli that are similar to an original conditioned stimulus (CS).            generalization        The tendency to respond to stimuli that are similar to an original conditioned stimulus (CS).            . Generalization has obvious survival value. If our ancestors had one bad experience with a lion, it would make sense to avoid all lions, as well as other animals with lionlike characteristics. Unfortunately, our tendency to generalize can also have negative outcomes. For example, a soldier traumatized in combat might react with unnecessary fear to sounds that are similar to gunfire on the battlefield, such as the backfiring of a car back at home. Counteracting our tendency to generalize is another learning process known as discriminationdiscriminationA learned ability to distinguish between stimuli.            discrimination        A learned ability to distinguish between stimuli.            , which allows us to make fine distinctions between the implications of stimuli. In the laboratory, if you present food following a high tone but never following a low tone, a dog initially learns to salivate following both tones because of generalization. As learning progresses, the dog eventually learns to discriminate, or differentiate, between the abilities of the two stimuli to predict food. As a result, salivation to the high tone continues, but salivation to the low tone stops (see Figure 8.5). If generalization had led the soldier to react with fear to the sound of a backfiring car, further experience with the sound would help distinguish it from real gunfire in combat. Because the sound of a backfiring car is not followed by any fear-producing UCSs, it eventually loses its ability to elicit fear. The dog is receiving food after the high tone, but nothing after the low tone. Early in training, the dog salivates after both tones because of generalization. Further along in training, however, the dog learns to discriminate between the abilities of the two tones to signal food and salivates only after the high tone. We have seen how CRs spread to similar stimuli through generalization. In addition, CRs can occur in response to stimuli that predict the CS, a process known as higher-order conditioninghigher-order conditioningLearning in which stimuli associated with a conditioned stimulus (CS) also elicit conditioned responses (CRs).            higher-order conditioning        Learning in which stimuli associated with a conditioned stimulus (CS) also elicit conditioned responses (CRs).            . Higher-order conditioning allows us to make even more distant predictions about the occurrence of significant events. A person who was bitten by a dog might show fear the next time the dog is seen because the sight of the dog (CS) is now associated with the pain of the bite (UCS). Subsequently, the sight of the dog (CS) might begin to act more like a UCS, producing fear in response to other stimuli (seeing the dog’s yard or doghouse or hearing the dog bark) that might signal the appearance of the dog. Higher-order conditioning occurs when stimuli associated with a conditioned stimulus (CS) gain the ability to elicit conditioned responses (CRs) on their own. If a child has learned to fear dogs (CS) because of a previous bite (unconditioned stimulus, or UCS), anything that signals dog might now produce fear too, including the sight of a doghouse, a dog’s feeding bowl, or a chew toy. Pavlov’s dogs had probably never heard a metronome before participating in his experiments. What happens when you are already familiar with a CS? The answer is that you will take a longer amount of time learning to respond to it. It takes more time to learn about a familiar CS than about an unfamiliar CS, a phenomenon known as latent inhibitionlatent inhibitionThe slower learning that occurs when a conditioned stimulus (CS) is already familiar compared to when the CS is unfamiliar.            latent inhibition        The slower learning that occurs when a conditioned stimulus (CS) is already familiar compared to when the CS is unfamiliar.             (Lubow & Moore, 1959). The phenomenon is latent in the sense that its effects are not seen right away (when the stimuli are first presented), but emerge later when the rate of learning is examined. The inhibition part of the term refers to the relatively poor learning that occurs in response to familiar stimuli. If you have eaten lots of pizzas over time (familiar CS) but get sick after eating one, you are unlikely to associate the pizza with feeling ill. In contrast, if you get sick the first time you eat chocolate-covered ants (unfamiliar CS), you’ll quickly associate eating ants with feeling ill. Early behaviorists concentrated their study of learning on external behaviors that they could observe directly. As new technologies became available, such as brain imaging methods and more powerful computers used to model thinking and reasoning, psychologists interested in learning began to explore internal processes, leading to revolutionary advances in our understanding of cognition and biology. Early behaviorists also limited most of their studies to simple animals rather than humans. This restriction resulted from strong beliefs that behavior followed the same rules in all organisms, which meant that it was safe to apply experimental results from studies using rats to the behavior of humans and that having more control over your experimental subjects (food, housing, etc.) led to better science. Although the latter may be true, psychologists discovered that treating the learning animal as some kind of interchangeable black box was overly simplistic. Some learning processes have clearly been conserved over the course of evolution, allowing us to make conclusions relevant to humans about the changes at a synapse that accompany classical conditioning in the Aplysia californica sea slug (Carew & Kandel, 1973). In other cases, species bring their unique biology into the learning situation, as we will see in a later section on taste aversion learning. Psychologists have learned to be cautious about generalizing their conclusions across species. In our earlier discussion of the acquisition of classical CRs, we talked about the contributions of contiguity (closeness in time) and contingency (the correlation between the CS and the UCS). It should be easy to learn about a signal that both precedes and is predictive of an important event. The bell on your microwave both precedes and predicts the availability of food, and we would not be surprised if your mouth starts watering a bit whenever you hear it. What happens, however, if you already possess one good signal but add another one that also precedes and predicts a UCS? Based on past experience, you have learned that if your computer monitor suddenly goes dark (CS), something terrible (UCS) has happened to your computer. Whenever you see a dark screen, you feel extremely stressed (CR). Let’s assume that the next time a dark screen appears, the computer also emits a funny sound. Even though the sound both precedes and predicts a hard drive crash, it’s unlikely that you will learn much about the sound and its relationship with hard drive crashes. We don’t bother to learn much about new signals that provide no additional information, even if they meet our requirements for contiguity and contingency (Kamin, 1968,  1969). Because of latent inhibition, classical conditioning proceeds more slowly when a new conditioned stimulus is familiar than when it is unfamiliar. If you got sick after eating a familiar food (perhaps pizza), you are less likely to associate your illness with the food than if you got sick after eating an unfamiliar food, like this scorpion lollipop that one of your textbook authors is about to try. How can we account for this failure to learn under circumstances that should produce strong learning? Rescorla and Wagner (1972) proposed a model of classical conditioning in which learning occurs as a function of how surprising the association between the CS and the UCS appears. Early in training, more learning takes place because the relationship between the CS and the UCS is relatively unexpected. Later in training, less learning takes place with each exposure to the now-familiar relationship between the CS and the UCS (see Figure 8.6). According to the Rescorla–Wagner model, the rate of learning about a conditioned stimulus (CS) depends on how new or surprising the association between the CS and the unconditioned stimulus (UCS) appears to be. Early in training, learning proceeds rapidly (25 units per block or set of trials) because the association is new and surprising. Later in training, gains in conditioning strength (measured by how often a conditioned response occurs) level off because the association between the CS and the UCS is now familiar and no longer surprising (4 units per block of trials). We can’t imagine early behaviorists using terms like expect, surprise, and predict, because these represent internal states they believed were impossible to investigate scientifically. But today, it seems reasonable to discuss some aspects of learning using this vocabulary. We have already used some examples of classical conditioning involving food that you dislike because you got sick after eating it. These classically conditioned taste aversions result when the sight, smell, or flavor of the food (CS) has been paired in the past with illness (UCS). Taste aversion, or dislike of the food, is the resulting CR. Taste aversion isn’t just another interesting example of classical conditioning. Its demonstration led to a substantial rethinking not only of classical conditioning, but of behaviorism in general. Pavlov believed that stimuli that met the CS or UCS criteria could be successfully paired to produce classical conditioning. He made no provisions in his theory for special interactions between particular types of stimuli. John Garcia, who had a background not only in psychology but also in biology, did not believe that stimuli were so interchangeable. In what became known as a classic taste aversion study, Garcia and Koelling (1966) demonstrated that the types of stimuli used as CSs and UCSs matter, and that some combinations are learned faster than others. Garcia and Koelling presented groups of rats with either saccharin-flavored water (tasty water) or plain water. When the rats consumed the plain water, their drinking triggered a light and a clicking sound, which the researchers called bright-noisy water. After drinking either tasty or bright-noisy water, half the rats were given an injection of lithium chloride, which produces strong sensations of nausea, and the other half received an electric shock (see Table 8.1). Conditioned stimulusUnconditioned stimulusBright-noisy waterTasty waterLithium chloride (LiCl)Weak learningStrong learningShockStrong learningWeak learningThis type of experiment should look familiar to you by now as an example of classical conditioning. Tasty water or bright-noisy water served as CSs, while shock or lithium chloride served as UCSs. Garcia and Koelling subsequently presented tasty and bright-noisy water to see whether either would be avoided, with avoidance serving as an indication of the CR of disgust with or dislike of the water. Rats immediately learned associations between tasty water and subsequent illness but had difficulty learning to use the bright-noisy water as a signal for illness. Conversely, bright-noisy water, but not tasty water, became an effective signal for shock. After all, if you feel sick, you are more likely to decide that your illness was a result of eating mystery leftovers for breakfast rather than the flickering of the fluorescent lights in your classroom. However, if you receive an electric shock, you might be more suspicious of the flickering lights than of your breakfast. This experiment had far-ranging implications. Not only did these findings challenge Pavlov’s views of the relative interchangeability of stimuli, but they also prompted a renewed interest in the biological predispositions of organisms, or their preparedness to learn certain things. Although rats readily formed associations between taste and illness, but not between visual stimuli and illness, birds easily formed associations between visual stimuli and illness, but not between taste and illness. Rats see poorly, so they usually depend on taste and smell for identifying food. Birds have excellent vision and typically identify food sources using visual cues, such as the markings of particular species of butterfly. John Garcia and Robert Koelling’s work on taste aversion explains why the monarch butterfly (on the top) is often rejected by birds as food. The milkweed on which the monarch caterpillar feeds contains cardiac glycosides, which are maintained in the mature insects’ bodies. Ingested glycosides make the birds ill, so they avoid feeding on monarchs, as would be predicted by classical conditioning. Some scientists believe that by looking similar to the monarch, the viceroy butterfly (on the bottom) might escape being eaten too. Recall that visual stimuli (bright—noisy water) were not very effective signals for illness in rats, but apparently they work just fine for birds. The natural capacities of a species prepare it to learn some things better than others. The willingness of behaviorists to treat organisms as interchangeable black boxes with irrelevant internal features was severely challenged by this work, paving the way to an abandonment of the rigid behaviorism that had dominated psychology for most of the 20th century. Many learning theorists closed down their rat and pigeon labs and turned their attention to emerging cognitive, biological, and evolutionary approaches. Although strict behaviorism no longer dominates research in psychology, it still provides powerful explanations of human behavior and effective therapeutic tools for producing change. The pervasiveness of classical conditioning in everyday life is quite remarkable. When your palms get sweaty and you feel butterflies in your stomach before a big exam, awaken just before your alarm clock goes off, or feel more awake just because you smell coffee brewing, you can blame your responses on classical conditioning. If you are a clinician working with a traumatized combat veteran who is frightened by the smell of diesel fuel or a coach working with an athlete to overcome choking in big games, classical conditioning gives you some answers that you need to produce positive change. Psychologists have long been curious about what happened to Albert after he left Watson and Raynor’s laboratory. One hypothesis suggests that Albert was Douglas Merritte, the child of one of the foster mothers living at the Harriet Lane home in 1920. Merritte died at age 6 of hydrocephalus. Another strong candidate is William Albert Barger, whose name and physical health appear to some to be a better match for the baby shown in Watson’s films of his experiment. Barger died in 2007, at the age of 87. If Barger is indeed the famous Albert, we hope that his strong dislike of animals as an adult was not a remnant of Watson’s experiment. In 1920, John B. Watson and Rosalie Raynor conducted an experiment with a 9-month-old infant named Albert. By today’s standards, this experiment hardly appears well designed or ethical, but the results of the experiment led to research that shed a great deal of light onto human fear. While Albert played with a tame, white laboratory rat (CS), Watson and Raynor made a loud noise (UCS) by hitting a steel bar with a hammer. Albert was quite frightened by this noise (UCR). A week later, Albert was again offered the rat, but this time, he was afraid (CR). His fear generalized to other white, furry objects, including a rabbit, a dog, a fur coat, and a Santa Claus mask. Watson and Raynor had successfully demonstrated that fears could result from classical conditioning. Although Albert left Watson and Raynor’s laboratory without treatment for his fear, one of Watson’s students, Mary Cover Jones, demonstrated how classical conditioning procedures could be used to reduce learned fears. Her experiment featured a 3-year-old named Peter, who had a serious phobia, or intense, unrealistic fear, of rabbits (Jones, 1924). Could classical conditioning provide a way to reduce Peter’s fears? One possible approach would be to use extinction. As mentioned previously, CRs extinguish if the CS is presented alone, without the UCS. Treating phobias by exposing people to fear-producing stimuli in a manner that is safe until they no longer respond (i.e., extinction) is known as exposure therapy or flooding. Although exposure therapy works, it is often traumatic. Being exposed to a stimulus you find frightening until you are no longer afraid is not fun. Imagine forcing a person afraid of heights to bungee jump until the fear of heights has gone. Instead of using exposure therapy, Jones treated Peter with counterconditioning, or the substitution of one CR for another, opposite response. Jones associated food, a new UCS, with the presence of a rabbit (CS). Eventually, Peter was able to stroke the rabbit while eating. While not particularly hygienic, this achievement represented a big improvement in Peter’s life. Understanding classical conditioning provides insight into many situations where our emotional responses seem to be triggered by the environment, as in the case of performance anxiety. Counterconditioning has many useful applications. Aversion therapyAversion therapyAn application of counterconditioning in which a conditioned stimulus (CS) formerly paired with a pleasurable unconditioned stimulus (UCS) is instead paired with an unpleasant UCS.            Aversion therapy        An application of counterconditioning in which a conditioned stimulus (CS) formerly paired with a pleasurable unconditioned stimulus (UCS) is instead paired with an unpleasant UCS.             can be used to replace inappropriate positive reactions to a stimulus with negative reactions. For example, a compound containing silver interacts with nicotine to make a tobacco cigarette taste terrible. Substituting a negative outcome for a positive outcome of smoking helps some smokers quit more easily (Rose, Behm, Murugesan, & McClernon, 2010). A variation of counterconditioning used to treat fear is known as systematic desensitizationsystematic desensitizationA type of counterconditioning in which people relax while being exposed to stimuli that elicit fear.            systematic desensitization        A type of counterconditioning in which people relax while being exposed to stimuli that elicit fear.             (Wolpe, 1958). Associations between a phobic stimulus and fear are replaced by associations between the phobic stimulus and relaxation. The person undergoing treatment is first trained to achieve a state of physical and mental relaxation, usually by tensing and relaxing muscle groups from head to toe. If you would like to try this progressive relaxation technique yourself, we outline the procedure in Chapter 15. Once relaxation is achieved, the fear stimulus is gradually introduced, either in physical form, through guided imagery, in which the person is asked to imagine the stimulus, or by using virtual reality. If relaxation falters at any point, the person retreats to an earlier stage of exposure to the fear stimulus until he or she can relax again. In addition to the influences on addiction discussed in Chapter 6, classical conditioning can contribute to dependence on a drug or behavior. Stimuli associated with drug use often become CSs for the effects of a drug. For example, peak caffeine levels occur about 45 minutes after drinking a cup of coffee (Liguori, Hughes, & Grass, 1997), yet most coffee drinkers report feeling more awake as soon as they take that first sip in the morning (or even smell the coffee brewing). John B. Watson and Rosalie Raynor observe Little Albert’s generalization to a bunny mask worn by Watson. The mask is similar to the original conditioned stimulus in their experiment—a white laboratory rat—which stimulated Albert’s conditioned fear. One of the challenges faced by people recovering from addiction to substances is that environmental cues (i.e., CSs) associated with the effects of substance use (i.e., UCSs) continue to elicit cravings (i.e., CRs) for the drug of choice. Most treatments for substance abuse involve simply abstaining from a drug, as opposed to extinction or counterconditioning. There is nothing about avoiding the use of a drug that substantially weakens the previously formed associations surrounding its use. Consequently, being exposed to established CSs, including the people with whom one did the drugs previously or the context in which one used the drugs, often leads to a former addict’s relapse (Chaudhri, Sahuque, & Janak, 2008). The sight, smell, or taste of a recovering alcoholic’s favorite drink, or even a visit to a favorite bar, is often enough to undermine the person’s abstinence from drinking. Reducing these associations through extinction can help the addict continue to abstain. While still a graduate student, Mary Cover Jones used classical conditioning principles to reduce a small child’s fear of rabbits. As a result, she is often called the Mother of Behavior Therapy. Classical conditioning contributes to the formation and change of attitudes (Cacioppo & Berntson, 2001). After all, advertisers have been using classical conditioning for years to influence consumer attitudes about products. By forming associations between the products and other stimuli we value, like celebrities, advertisers hope that our opinions of their product will improve. Prejudice, which is discussed further in Chapter 13, is a negative attitude about a group of people. Like other attitudes, it is influenced by classical conditioning, although it has many other roots as well. In particular, latent inhibition can contribute to the development of negative attitudes (Cacioppo, Marshall-Goodell, Tassinary, & Petty, 1992). Consider the following. Because of latent inhibition, if children grow up with little exposure to people outside their own race, people of their own race are more familiar, and learning to associate their race with other attributes should be slow. In contrast, the children have had less preexposure to people of other races. Latent inhibition would predict that children exposed to news reports about crime would form stronger associations between people of unfamiliar races and crime than between people of their own race and crime. Product placement in movies is expensive, but marketers hope that audiences will form associations between their products and the positive brand of a character. If the hero drinks Coca-Cola, maybe drinking Coca-Cola will make you feel heroic, too. By reducing the amount of learning that occurs in response to familiar stimuli, latent inhibition helps us focus our energy on novelty and change in our environments. Less latent inhibition is seen in creative people and in people diagnosed with schizophrenia than is seen in general (Baruch, Hemsley, & Gray, 1988a, 1988b;  Lubow, Ingberg-Sachs, Salstein-Orda, & Gewirtz, 1992). This difference means that creative people and people with schizophrenia form new associations with familiar stimuli faster than most people do. They make connections under circumstances in which most of us would not. Reduced latent inhibition might account for the creative person’s ability to see familiar things in new ways, which is a positive outcome, but it also might lead to the tendency of a person with schizophrenia to make odd, inappropriate connections among ideas. Chapter 14 refers to this tendency as a loosening of associations. For example, a person with schizophrenia might suggest that a painting has a headache. This is not the type of association between stimuli that most people would make. While reading about Ivan Petrovich Pavlov’s experiments with salivating dogs, it might be difficult to grasp the full significance of Pavlov’s results, but there were some good reasons that his laboratory continued to receive considerable resources during difficult times of war and revolution (Gantt, 1928). As demonstrated in this chapter, the possible applications of classical conditioning range widely, from treating addictions and phobias to understanding prejudice and schizophrenia. One of the less typical applications was the result of work by John Garcia and one of his graduate students, Carl Gustavson (Gustavson, Kelly, Sweeney, & Garcia, 1976). Gustavson was a child of the rural West, and he had grown up watching the ongoing battles among sheep ranchers, coyotes, and wolves. He asked whether the taste aversion work pioneered by Garcia might provide a useful solution. With encouragement from Garcia, Gustavson set up an outdoor training facility with six coyotes and two wolves. The coyotes were fed rabbit meat laced with lithium chloride (LiCl), while the wolves were fed sheep meat with LiCl. He even experimented with a caged cougar that refused to eat deer meat after a single meal of venison laced with LiCl. Some of the researchers’ observations of the captive animals were especially dramatic. When a lamb was tethered in the wolves’ pen, the wolves approached in a normally threatening manner. The lamb froze, which is also species-typical behavior in this situation. Then, something interesting happened. The wolves sniffed at the lamb and then retreated, showing signs of defensive puppy play (crouching with ears and tail down). In response to these submissive displays, the lamb began to charge the wolves and even chased them around the pen. Use of conditioned taste aversion to provide a nonlethal method of predator control is being explored in efforts to save the endangered Mexican wolf, shown here. Dan Moriarty and his colleague Lowell Nicolaus laced meat with the fungicide thiabendazole. Although the fungicide makes the wolves ill, it causes no permanent damage. If the program is successful in the wild, it could reshape the way that endangered predators are managed worldwide. Would these observations transfer to the real outdoors? The researchers distributed sheep-flavored bait and sheep carcasses laced with LiCl across a 3,000-acre sheep ranch in Washington State. A comparison of the ranch’s loss records demonstrated a 30% to 60% reduction in sheep killed by coyotes following the treatment. A larger-scale test of 10 herds over a 3-year period in Saskatchewan demonstrated a reduction in predator control costs of between 86% and 90% per year. Where does the practice stand today? Unfortunately, when wildlife experts attempted to apply taste aversion to predator control, they made numerous procedural mistakes. They viewed the LiCl as a repellant, as opposed to a UCS. The bait was overdosed with LiCl, which ensured that the predator would link salty tastes with illness instead of the taste of the prey with illness, leading to the discarding of the procedure as ineffective. Contemporary coyote management techniques include propane cannons, horns, sirens, radios, and strobe lights (we are left wondering what these stimuli do to the sheep …), but aversion conditioning is still described by wildlife experts as speculative (Wilbanks, 1995, p. 163). Gustavson’s technique has recently made a comeback in an effort to save the endangered Mexican wolf. When released in the wild, the wolves, like Gustavson’s coyotes, naturally run into conflict with ranchers and farmers. Professors Lowell Nicolaus and Dan Moriarty’s initial results using conditioned taste aversion with captive gray wolves convinced U. S. government wildlife agencies to prepare to replicate the study with captive Mexican wolves (U. S. Fish and Wildlife Service, 2014). A total of 23 Mexican wolves were treated between 2011 and 2013. Two animals treated in 2011 were retested in 2012, and both still showed a conditioned aversion to cattle (U. S. Fish and Wildlife Service, 2014). Classical conditioning phenomenonDescriptionExampleAcquisitionGradual development of conditioned responses (CRs)A dog salivates on a higher percentage of trials as training progresses. ExtinctionReduction of CRs when a conditioned stimulus (CS) is presented without being followed by an unconditioned stimulus (UCS)If the metronome is no longer followed by food, the dog stops salivating to the metronome. Spontaneous recoveryReappearance of CRs following periods of rest between extinction training sessionsThe dog shows no salivation at the end of the day’s extinction training, but after a night of rest in the kennel, the dog salivates at the beginning of the next extinction session. InhibitionA CS’s prediction of the nonoccurrence of a UCSSo long as a gauge is in the green, your equipment will not explode. You do not feel fear. GeneralizationResponses to stimuli that resemble the CSAlbert’s fear of white rats generalized to a Santa Claus beard. DiscriminationResponding to the CS, but not to similar stimuli that have not been paired with the UCSA combat veteran learns to distinguish between the sound of gunfire and the backfire from a car. Higher-order conditioningCRs to stimuli that predict the occurrence of a CSA child who has been bitten by a dog begins to fear the street where the dog lives. Latent inhibitionSlower development of CRs to a familiar CS than to an unfamiliar CSAn American forms a taste aversion faster to fruit bat pie than to hamburgers. The discussion of behaviorism in Chapter 1 introduced you to Edward Thorndike and his law of effect. To recap, Thorndike had observed the learning that took place when a cat tried to escape one of his puzzle boxes. According to Thorndike, the cats learned to escape by repeating actions that produced desirable outcomes and by eliminating behaviors that produced what he called annoying outcomes, or outcomes featuring either no useful effects or negative effects (1913, p. 50). Consequently, the law of effect states that a behavior will be stamped into an organism’s repertoire depending on the consequences of the behavior (Thorndike, 1913, p. 129). The association between a behavior and its consequences is called operant or instrumental conditioning. In this type of learning, organisms operate on their environment, and their behavior is often instrumental in producing an outcome. B. F. Skinner extended Thorndike’s findings using an apparatus that bears his name—the Skinner box, a modified cage containing levers or buttons that can be pressed or pecked by animals (see Figure 8.7). A specially adapted cage called a Skinner box, after behaviorist B. F. Skinner, allows researchers to investigate the results of reinforcement and punishment on the likelihood that the rat will press the bar. Operant conditioning differs from classical conditioning along several dimensions. By definition, classical conditioning is based on an association between two stimuli, whereas operant conditioning occurs when a behavior is associated with its consequences. Classical conditioning generally works best with relatively involuntary behaviors, such as fear or salivation, whereas operant conditioning involves voluntary behaviors, like walking to class or waving to a friend. As we all know from experience, some types of consequences increase behaviors, while others decrease behaviors. Skinner divided consequences into four classes: positive reinforcement, negative reinforcement, positive punishment, and negative punishment. Both types of reinforcement increase their associated behaviors, whereas both types of punishment decrease associated behaviors (see Table 8.2). Add stimulus to environmentRemove stimulus from environmentIncrease behaviorPositive reinforcementNegative reinforcementDecrease behaviorPositive punishmentNegative punishmentWe all have unique sets of effective reinforcers and punishers. You might think that getting an A in a course is reinforcing, making all those extra hours spent studying worthwhile, but top grades may be less meaningful to the student sitting next to you, who came to college for the social life. A parent might spank a child, believing that spanking is an effective form of punishment, only to find that the child’s unwanted behavior is becoming more rather than less frequent. For some children, the reward of getting the parent’s attention overrides the discomfort of the spanking part of the interaction. In other words, the identity of a reinforcer or punisher is defined by its effects on behavior, not by some intrinsic quality of the consequence. The only accurate way to determine the impact of a consequence is to check your results. If you think you’re reinforcing or punishing a behavior but the frequency of the behavior is not changing in the direction you expect, try something else. By definition, a positive reinforcement increases the frequency of its associated behavior by providing a desired outcome. Again, each person has a menu of effective reinforcements. In a common application of operant conditioning, children with autism spectrum disorder are taught language, with candy serving as the positive reinforcement. Benjamin Lahey tells of his experience trying to teach a child with autism spectrum disorder to say the syllable ba to obtain an M&M candy (Lahey, 1995). After 4 hours without progress, Lahey turned to the child’s mother in frustration, asking her what she thought might be the problem. The mother calmly replied that her son didn’t like M&Ms. Lahey switched to the child’s preferred treat, chopped carrots, and the child quickly began saying ba. Chopped carrots are probably not the first reinforcer you would try with a 4-year-old boy, but in this case, they made all the difference. Edward Thorndike’s Law of effect stipulates that behaviors followed by positive consequences are more likely to be repeated in the future, and that behaviors followed by negative consequences are less likely to be repeated. Why, then, do large numbers of people, particularly in adolescence, engage in self-injury, or deliberate physical damage without suicidal intent (Klonsky & Muehlenkamp, 2007)? Up to 25% of teens have tried self-injury at least once (Lovell & Clifford, 2016). Most initiate self-injury while in middle school (grades 6 through 8), and approximately 6% of college students continue to self-injure. As this chapter has detailed, reward and punishment are in the eye of the beholder. The first challenge that we face in our analysis of self-injury is the assumption that pain is always a negative consequence. For most of us, it is. However, adolescents who engage in self-injury report feelings of relief or calm, despite the obvious pain that they inflict on themselves. Such feelings probably reinforce further bouts of self-injury. Self-injury often occurs in response to feelings of anger, anxiety, and frustration, and alleviation of these negative feelings might reward the injurious behavior (Klonsky, 2007;  Klonsky & Muehlenkamp, 2007). Finally, injury is associated with the release of endorphins, our bodies’ natural opioids. The positive feelings associated with endorphin release also might reinforce the behavior. Self-injury frequently occurs in people diagnosed with psychological disorders, such as depression, anxiety disorders, eating disorders, or substance abuse, which are discussed further in Chapters 7 and 14. Others engaging in the behavior have a history of sexual abuse. Observations that captive animals in zoos and laboratories are often prone to self-injury might provide additional insight into the causes of this behavior (Jones & Barraclough, 1978). Treatment usually consists of therapy for any underlying psychological disorders, along with avoidance, in which the person is encouraged to engage in behaviors that are incompatible with self-harm. To assist these individuals further, we need to be able to see reward and punishment from their perspective, not just our own. If the consequences of a behavior influence how likely a person is to repeat the behavior in the future, how can we explain the prevalence of self-injury? Why don’t the painful consequences of the behavior make people stop? In situations like this, operant conditioning tells us that we need to look for possible reinforcers for the behavior that override the painful outcomes. In the case of self-injury, people report feeling calm and relief. To treat such behaviors effectively, psychologists need to understand what advantages they provide from the perspective of the person doing the behavior. The Premack principle can help you maintain good time management. If you prefer socializing to studying, use the opportunity to socialize as a reward for meeting your evening’s study goals. If everyone has a different set of effective reinforcers, how do we know which to use? A simple technique for predicting what a particular animal or person will find reinforcing is the Premack principle, which states that whatever behavior an organism spends the most time and energy doing is likely to be important to that organism (Premack, 1965). It is possible, therefore, to rank people’s free-time activities according to their priorities. If Lahey had been able to observe his young client’s eating habits before starting training, it is unlikely that he would have made the mistake of offering M&Ms as reinforcers. The opportunity to engage in a higher-priority activity is always capable of rewarding a lower-priority activity. Your grandmother may never have heard of Premack, but she knows that telling you to eat your broccoli to get an ice cream generally works. Both Thorndike and Skinner agreed that positive reinforcement is a powerful tool for managing behavior. In our later discussion of punishment, we will argue that the effects of positive reinforcement are more powerful than the effects of punishment. Unfortunately, in Western culture, we tend to provide relatively little positive reinforcement. We are more likely to hear about our mistakes from our boss than all the things we’ve done correctly. It is possible that we feel entitled to good treatment from others, so we feel that we should not have to provide any reward for reasonably expected behaviors. The problem with this approach is that extinction occurs in operant, as well as in classical, conditioning. A behavior that is no longer reinforced drops in frequency. By ignoring other people’s desirable behaviors instead of reinforcing them, perhaps with a simple thank-you, we risk reducing their frequency. According to the Premack principle, a preferred activity can be used to reinforce a less preferred activity. Most children prefer candy over carrots, so rewarding a child with candy for eating carrots often increases carrot consumption. One little boy with autism spectrum disorder, however, preferred carrots to M&Ms, and his training proceeded more smoothly when carrot rewards were substituted for candy rewards. Some reinforcers, known as primary reinforcers, are effective because of their natural roles in survival, such as food. Others must be learned. We are not born valuing money, grades, or gold medals. These are examples of conditioned reinforcersconditioned reinforcersA reinforcer that gains value from being associated with other things that are valued; also known as a secondary reinforcer.            conditioned reinforcers        A reinforcer that gains value from being associated with other things that are valued; also known as a secondary reinforcer.            , also called secondary reinforcers, that gain their value and ability to influence behavior from being associated with other things we value. Here, we see an intersection between classical and operant conditioning. If you always say good dog before you provide your pet with a treat, saying good dog becomes a CS for food (the UCS) that can now be used to reinforce compliance with commands to come, sit, or heel (operant behaviors). Classical conditioning establishes the value of good dog, and operant conditioning describes the use of good dog to reinforce the dog’s voluntary behavior. Serena Williams loves her Wimbledon trophy not for its intrinsic value (you can’t eat it, etc.), but because trophies have become conditioned reinforcers. Many superstitious behaviors, like wearing your lucky socks, can be learned through operant conditioning. Operant conditioning does not require a behavior to cause a positive outcome to be strengthened. All that is required is that a behavior be followed by a positive outcome. Unless you suddenly have a string of bad performances while wearing the lucky socks, you are unlikely to have an opportunity to unlearn your superstition. Humans are capable of generating long chains of conditioned reinforcers extending far into the future. We might ask you why you are studying this textbook right now, at this moment. A psychologist might answer that you are studying now because studying will be reinforced by a good grade at the end of the term, which in turn will be reinforced by a diploma at the end of your college education, which in turn will be reinforced by a good job after graduation, which in turn will be reinforced by a good salary, which will allow you to live in a nice house, drive a nice car, wear nice clothes, eat good food, and provide the same for your family in the coming years. Negative reinforcementNegative reinforcementA method for increasing behaviors that allow an organism to escape or avoid an unpleasant consequence.            Negative reinforcement        A method for increasing behaviors that allow an organism to escape or avoid an unpleasant consequence.            , which sounds contradictory, involves the removal of unpleasant consequences from a situation to increase the frequency of an associated behavior. Negative reinforcement increases the frequency of behaviors that allow an organism to avoid, turn off, or postpone an unpleasant consequence; these are sometimes called escape and avoidance behaviors. Let’s look at a laboratory example of negative reinforcement before tackling real-world examples. If a hungry rat in a Skinner box learns that pressing a bar produces food, a positive consequence, we would expect the frequency of bar pressing to increase. This would be an instance of positive reinforcement. However, if pressing the bar turns off or delays the administration of an electric shock, we would still expect the frequency of bar pressing to increase. This would be an instance of negative reinforcement. Be careful to avoid confusing negative reinforcement with punishment, which is covered in the next section. By definition, a punishment decreases the frequency of the behaviors that it follows, whereas both positive and negative reinforcers increase the frequency of the behaviors that they follow. Returning to our Skinner box example, the rat’s bar pressing increases following both positive reinforcement (food) and negative reinforcement (turning off a shock). If we shocked the rat every time it pressed the bar (punishment), it would stop pressing the bar quickly. Many everyday behaviors are maintained by negative reinforcement. We buckle up in our cars to turn off annoying beeps or bells, open umbrellas to avoid getting wet, scratch an insect bite to relieve the itch, take an aspirin to escape a headache, apply sunscreen to avoid a sunburn or skin cancer, and apologize to avoid further misunderstandings with a friend. In many real-world cases, positive and negative reinforcement act on behavior simultaneously. A heroin addict uses the drug to obtain a state of euphoria (positive reinforcer) but also to eliminate the unpleasant symptoms of withdrawal (negative reinforcer). You might study hard to achieve high grades (positive reinforcers) while also being motivated by the need to avoid low grades (negative reinforcers). A punishmentpunishmentA consequence that eliminates or reduces the frequency of a behavior.            punishment        A consequence that eliminates or reduces the frequency of a behavior.             is any consequence that reduces the frequency of an associated behavior. Positive punishmentPositive punishmentA consequence that eliminates or reduces the frequency of a behavior by applying an aversive stimulus.            Positive punishment        A consequence that eliminates or reduces the frequency of a behavior by applying an aversive stimulus.             refers to applying an aversive consequence that reduces the frequency of or eliminates a behavior. As described previously, we can demonstrate that a rat will quickly stop pressing a bar if each press results in an electric shock. Negative punishmentNegative punishmentA method for reducing behavior by removing something desirable whenever the target behavior occurs.            Negative punishment        A method for reducing behavior by removing something desirable whenever the target behavior occurs.             involves the removal of something desirable. In the Skinner box, we can change the rules for a rat that has learned previously to press a bar for food. Now, food is made available unless the rat presses the bar. Under these conditions, the rat will also quickly stop pressing the bar. Putting up an umbrella to avoid getting wet from the rain is an example of a negatively reinforced behavior. Thorndike and Skinner were in agreement about the relative weakness of punishment as a means of controlling behavior. Part of the weakness of punishment effects arises from the difficulties of applying punishment effectively in real contexts. Three conditions must be met for punishment to have observable effects on behavior: significance, immediacy, and consistency (Schwartz, 1984). As we observed with reinforcement, consequences have to matter to the person or animal receiving them (i.e., significance). If we use a punisher that is too mild for a particular individual, there is little incentive for that person to change behavior. College campuses usually charge a fairly significant amount of money for parking illegally. However, there will typically be some students for whom that particular punishment is not sufficient to ensure that they will park legally. How high would a parking fee have to be to gain complete compliance on the part of the university community? What if you risked the death penalty for parking illegally? We can be fairly certain that most people would leave their cars at home rather than risk that particular consequence. The point is that punishment can work if a sufficiently severe consequence is selected, but using the amount of force needed to produce results is rarely considered practical and ethical. Free societies have long-standing social prohibitions against cruel and unusual punishments, and these conventions are incompatible with using the force that may be needed to change the behavior of some individuals. Immediate punishment is more effective than delayed punishment (i.e., immediacy). For the rat in the Skinner box, delays of just 10 seconds can reduce the effectiveness of electric shock as a punisher. Humans are more capable than rats at bridging long intervals (Kamin, 1959). Nonetheless, the same principle holds true. Delayed punishment is less effective than immediate punishment. We should not be too surprised that the months or years that are required to try and convict a serious criminal may greatly reduce the impact of imprisonment on that person’s subsequent behavior. The likelihood of getting a ticket influences drivers’ behavior. At an intersection with cameras, drivers are unlikely to run a red light, but at other intersections, behavior might be determined by whether a police officer is nearby. Our final requirement for effective punishment is its uniform application (i.e., consistency). College students are a prosocial, law-abiding group as a whole, yet many confess to determining their highway speed based on the presence or absence of a police car in their rearview mirrors. The experience of exceeding the speed limit without consequence weakens the ability of the possibility of tickets and fines to influence behavior. However, at intersections known to be controlled by cameras, compliance is generally quite high. If you are certain that running a red light will result in an expensive ticket, it would be foolish to test the system. To reduce undesirable behaviors, Skinner recommended extinction as an alternative to punishment (Skinner, 1953). In the discussion of classical conditioning earlier in this chapter, the term extinction was used to refer to the disappearance of CRs that occurs when the CS no longer signals the arrival of a UCS. Extinction in operant conditioning has a similar meaning. Learned behaviors stop when they are no longer followed by a reinforcing consequence. Attention is a powerful reinforcer for humans, and ignoring negative behavior should extinguish it. Parents and teachers cannot look the other way when one child is being physically aggressive toward another, but in many other instances, Skinner’s approach is quite successful in reducing the frequency of unwanted behaviors (Brown & Elliot, 1965). Although ignoring a child’s tantrums can be embarrassing for many parents, this can be an effective strategy for reducing their frequency. In this chapter, we have learned that rewards increase and punishments decrease the frequency of associated behaviors. But how do these consequences compare to each other? Do people learn faster when rewarded or punished? Is the relative effectiveness of reward and punishment the same for people of all ages? Participants were asked to choose the correct item from a pair of abstract symbols, and that their overall points (11 for a correct choice and 21 for an incorrect choice) would determine a final monetary prize (Palminteri, Kilford, Coricelli, & Blakemore, 2016). The Question: Do adolescents and adults respond differently to consequences? In this British study, 38 people (20 adults between the ages of 18 and 32 and 18 adolescents between the ages of 12 and 17) participated. They received 5 pounds (about $6.20) for participating, plus anywhere from zero to 10 pounds ($12.40) based on their performance in the task. They viewed pairs of abstract items (the Agathodaimon alphabet). As shown in Figure 8.8, their choices were either rewarded (happy smiley and +1 point) or punished (unhappy smiley and −1 point). Participants viewed these slides in order. First, they fixated on the cross. Next, they see both figures and select one. Finally, they receive feedback in the form of a smiley and point total (in this case, our participant was correct). Although the topic of punishment might make it sound like this study raises more than the typical ethical concerns about research, facing an unhappy smiley and failing to earn 10 pounds ($12.40) are unlikely to cause the participants significant distress. The risks of the study would be spelled out for prospective participants in an informed consent process. The participation of adolescents would require parent or guardian approval plus participant assent. In other words, adolescents can’t be forced to participate by their parents, but they cannot participate without parental approval. The adults learned equally well from punishment and reinforcement. In contrast, the adolescents were more likely to learn from reinforcement than from punishment. In other words, the adolescents were more likely to seek rewards than to try to avoid punishments. This research demonstrated that decision making based on the probabilities of reward and punishment continues to develop through adolescence into young adulthood. Unlike the adults, who were as likely to seek rewards as they were to try to avoid punishments, the adolescents appeared to pay more attention to rewards. Among the many implications of this study, we might conclude that using positive feedback to support learning in adolescents is likely to produce more benefits that using negative feedback. In other words, teens might learn more at school about what they’re doing correctly than what they’re doing incorrectly. College students, in contrast, are likely to benefit from both types of feedback. Reinforcing a behavior every time it occurs is known as continuous reinforcement. Although it is highly desirable to use continuous reinforcement when a new behavior is being learned, it is inconvenient to do so forever. Dog owners want their dogs to walk with a loose leash, but once this skill is learned, the owners don’t want to carry dog treats for reinforcement whenever they walk their dogs. Once we deviate from continuous reinforcement, however, the manner in which we do so may have a dramatic impact on the target behavior. To obtain the results we want, it is helpful to understand what happens when we use partial reinforcementpartial reinforcementThe reinforcement of a desired behavior on some occasions, but not others.            partial reinforcement        The reinforcement of a desired behavior on some occasions, but not others.            , or the reinforcement of the desired behavior on some occasions, but not others. Concerns about the effects of piecework on worker well-being contributed to the Fair Labor Standards Act of 1938, which included a provision for a minimum hourly wage. Psychologists have identified many ways to apply partial reinforcement, but we will concentrate on two variations: ratio schedules and interval schedules. In a ratio schedule of partial reinforcement, reinforcement depends on the number of times a behavior occurs. In an interval schedule of partial reinforcement, reinforcement depends on the passage of a certain amount of time. Either type of schedule, ratio or interval, can be fixed or variable. In fixed schedules, the requirements for reinforcement never vary. In variable schedules, the requirements for reinforcement are allowed to fluctuate from trial to trial, averaging a certain amount over the course of a learning session. A fixed ratio (FR) schedulefixed ratio (FR) scheduleA schedule of reinforcement in which reinforcement occurs following a set number of behaviors.            fixed ratio (FR) schedule        A schedule of reinforcement in which reinforcement occurs following a set number of behaviors.             requires that a behavior occur a set number of times for each reinforcer. Continuous reinforcement, discussed earlier, is equivalent to a FR of 1. If we now raise our requirement to two behaviors per reinforcer, we have a FR schedule of 2, and so on. Using the Skinner box, we can investigate the influence of FR schedules on the rate at which a rat will press a bar for food. To do so, we track cumulative responses as a function of time. FR schedules produce a characteristic pattern of responding. In general, responses are fairly steady, with a significant pause following each reward. As the amount of work for each reward is increased, responding becomes slower (see Figure 8.9). The schedule used to deliver reinforcement has a big impact on the resulting behavior. In general, the variable schedules produce higher rates of responding than do their fixed counterparts. The fixed interval schedule produces a characteristic pattern of low rates of responding at the beginning of the interval and accelerated responding as the end of the interval approaches. Psychologists typically recommend against the use of physical punishment with children, largely due to research showing a relationship between the use of physical punishment and increased aggressiveness on the part of a child. In addition, as you have seen in this chapter, there are many alternative ways to manage behavior successfully. As is the case with many types of psychological research, however, the classic studies on physical punishment and child aggression were conducted with middle-class, white American families. How representative are these samples of families in general? Some researchers believe that they are not representative and that physical punishment effects depend very much on cultural context (Deater-Deckard & Dodge, 1997). These researchers believe that physical punishment in cultures in which it is considered normal has a much less detrimental effect than in cultures where it is considered less normal. Racial and ethnic groups vary in the frequency with which they use physical punishment (Gershoff, Lansford, Sexton, Davis-Kean, & Sameroff, 2012). In a sample of over 11,000 U. S. families with kindergarten-aged children, rates of spanking were generally high (about 80%) across all groups, with 89% of African Americans, 79% of whites, 80% of Hispanics, and 73% of Asians reporting having spanked their child. When asked if they had spanked their child in the previous week, 40% of African Americans, 28% of Hispanics, 24% of whites, and 23% of Asians reported that they had done so. Does spanking have different effects on children’s behavior across the racial and ethnic groups due to these different frequencies?  Gershoff et al. (2012) concluded that it does not. Across all racial and ethnic groups, spanking was associated with higher levels of aggressive behaviors in the child, which in turn leads to more spanking in a coercive cycle of parenting. Psychologists have wondered if spanking had different effects within different racial and ethnic contexts, but it does not. Spanking is associated with higher levels of child aggression, regardless of racial or ethnic context. While many lines of research benefit from considerations of diversity, the message here is not modified by racial or ethnic identity—spanking children not only fails to decrease negative behaviors, but it actually appears to increase them. In early industrial settings, workers were often paid by the piece, a real-world example of the use of an FR schedule. In other words, workers would be paid every time they produced a fixed number of products or parts on an assembly line. Most workers find this system less than ideal. If the equipment malfunctions, the worker cannot earn money. Lunch breaks would also then be viewed as loss of income rather than a helpful time of rest. Some examples of piecework remain today, including the work of most physicians, who get paid by the procedure, and service workers like plumbers or hairstylists, who get paid for finishing a specific task. As in FR schedules, variable ratio (VR) schedulesvariable ratio (VR) schedulesA schedule of reinforcement in which reinforcement occurs following some variable number of behaviors.            variable ratio (VR) schedules        A schedule of reinforcement in which reinforcement occurs following some variable number of behaviors.             also involve counting the number of times that a behavior occurs. However, this time the required number of behaviors is allowed to fluctuate around some average amount. In the Skinner box, we might set our VR schedule to 10 for a 1-hour session. This means that over the course of the session, the rat must press an average of 10 times to receive each food pellet. However, this schedule may mean that only 1 press delivers food on one trial, but 30 presses are required on the next trial. The rat is unable to predict when reinforcement is likely to occur, leading to a high, steady rate of responding in our cumulative record. We do not see the characteristic pausing observed following reinforcement in the FR schedule because the rat cannot predict when the next reward will occur. One of the most dramatic real-world examples of the VR schedule is the programming of slot machines in casinos. Slot machines are human versions of Skinner boxes that use VR schedules. The casino sets the machine to pay off after some average number of plays, but the player doesn’t know whether a payoff will occur after one coin or thousands are inserted. You don’t have to observe the behavior of people playing slot machines long to see a demonstration of the high, steady responding that characterizes the VR schedule. The programming of slot machines can be sophisticated. Slot machines that are located in places where people are unlikely to return (airports and bus stations) pay off less frequently than those in places where people are more likely to play regularly. Unlike ratio schedules, reinforcement in interval schedules depends on the passage of time rather than the number of responses produced. In a fixed interval (FI) schedulefixed interval (FI) scheduleA schedule of reinforcement in which the first response following a specified interval is reinforced.            fixed interval (FI) schedule        A schedule of reinforcement in which the first response following a specified interval is reinforced.            , the time that must pass before reinforcement becomes available following a single response is set to a certain amount. In the Skinner box, a rat’s first bar press starts a timer. Responses that occur before the timer counts down are not reinforced. As soon as the timer counts down to zero, the rat’s next bar press is reinforced, and the timer starts counting down again. In the FI schedule, the interval is the same from trial to trial. Animals and people have a good general sense of the passage of time, leading to a characteristic pattern of responding in FI situations. Reinforcement is followed by a long pause. As the end of the interval is anticipated, responding increases sharply. A graph of the number of bills passed by the U. S. Congress as a function of time looks similar to the rat’s performance on an FI schedule in the Skinner box (Weisberg & Waldrop, 1972). Few bills are passed at the beginning of a session, but many are passed at the end (see Figure 8.10). Workers in the garment industry are often paid by the piece, or with a set amount of money for each finished garment. This compensation system is an example of a fixed ratio (FR) schedule. Because workers cannot make money when their equipment breaks down and they tend to view lunch and other breaks as costing them money, this schedule is not considered to be fair to workers. As the end of a congressional session approaches, the U. S. Congress begins to pass more bills in patterns that look similar to the behavior of rats on FI schedules in Skinner boxes. As you already may have guessed, the variable interval (VI) schedulevariable interval (VI) scheduleA schedule of reinforcement in which the first response following a varying period is reinforced.            variable interval (VI) schedule        A schedule of reinforcement in which the first response following a varying period is reinforced.             is characterized by an interval that is allowed to fluctuate around some average amount over the course of a session. This time, our bar-pressing rat experiences intervals that range around some average amount (say, 2 minutes). On one trial, the rat may obtain reinforcement after only 30 seconds, whereas the next trial may involve an interval of 5 minutes. Over the session, the average of all intervals is 2 minutes. As in the VR situation, we see a high, steady rate of responding. You are probably quite familiar with VI schedules, in the form of pop quizzes administered by your professors. Your professor might tell you that there will be five quizzes given during the term, but the timing of the quizzes remains a surprise. You might have the first two only 1 day apart, followed by a 2-week interval before the next quiz. Your best strategy, like the rat in the Skinner box on a VI schedule, is to emit a high, steady rate of behavior. Many parents have regretted the day that they unintentionally put an unwanted behavior on a partial reinforcement schedule by uttering the words, OK, just this once. Perhaps a parent is strongly opposed to buying candy for a child at the supermarket checkout counter (where, because of John Watson and his applications of psychology to advertising, candy is displayed conveniently at child’s-eye height). Then comes the fateful day when the parent is late coming home from work, the child is hungry because dinner is delayed, and unintentionally, the parent gives in just this once, putting begging for candy on a variable schedule. Subsequently, when the parent returns to the previous refusal to buy candy, a high, steady rate of begging behavior occurs before it finally extinguishes. Most casinos feature a large number of slot machines, which are essentially Skinner boxes for people. The slot machine is programmed on a variable ratio (VR) schedule, which means that the player cannot predict how many plays it will take to win. In response, players exhibit the same high, steady rate of responding that we observe in rats working on VR schedules in the laboratory. In the laboratory once more, we compare the behavior of two rats in Skinner boxes. One is working on a continuous (or FR 1) schedule of reinforcement. The other is working on a partial schedule of reinforcement (perhaps a VR 3 schedule). After several sessions of training, we stop reinforcement for both rats. It may surprise you to learn that the rat working on the continuous schedule will stop pressing long before the rat accustomed to the VR 3 schedule. In other words, extinction occurs more rapidly following continuous reinforcement than following partial schedules. This outcome is known as the partial reinforcement effect in extinctionpartial reinforcement effect in extinctionThe more rapid extinction observed following continuous reinforcement compared to that following partial reinforcement.            partial reinforcement effect in extinction        The more rapid extinction observed following continuous reinforcement compared to that following partial reinforcement.            . The partial reinforcement effect probably occurs because of one of two factors, or a combination of both. First, the transition from continuous reinforcement to extinction is more obvious than the transition from a partial schedule to extinction. If you are accustomed to being paid for a babysitting job every time you work, you will definitely notice any of your employer’s failures to pay. In contrast, if your neighbor typically pays you about once a month for raking his yard each weekend, you might not notice right away that he hasn’t paid you for a while. Second, partial schedules teach organisms to persist in the face of nonreinforcement. In a sense, partial schedules teach us to work through periods in which reinforcement does not occur. Consequently, we might view extinction as just another case where continuing to perform might eventually produce reinforcement. When positive behavior is occurring, such as working on your senior thesis regularly despite a much-delayed grade, persistence is an enormous advantage. However, as shown in our earlier example of begging for candy, placing an undesirable behavior on partial reinforcement makes it more difficult to extinguish. Fishing works according to a variable interval (VI) schedule of reinforcement. Fish (the reinforcers) are caught after periods of waiting for fish to bite that vary in length. As in laboratory demonstrations of the VI schedule, fishing usually produces a steady rate of responding. What happens if you are exposed to two or more schedules of reinforcement at the same time? This scenario is realistic because we face these types of choices every day. Which is a more rewarding use of my time: studying for my midterm or making some extra money by working overtime? In making these types of choices, animals and people follow the matching law, which states that the relative frequency of responding to one alternative will match the relative reinforcement for responses on that alternative (Herrnstein & Heyman, 1979). The law powerfully accounts for the effects on behavior of frequency, magnitude, and delays in reward. Time spent playing online video games provides an interesting example of the effects of simultaneous schedules of reinforcement. The millions of users of massively multiplayer online games, such as League of Legends and Crossfire, spend an average of 22 hours per week on their games (Yee, 2006). What compels these people to make such a lopsided choice between online interactions and real-life social experiences? One clue to this choice is that substantial numbers of players report that the most rewarding or satisfying experience they had over the last 7 or 30 days took place while gaming. We would assume that if the frequency and magnitude of rewards available in gaming were higher than those in real-life socializing, people would choose to spend their time accordingly. So far, this discussion of operant conditioning has centered on increasing or decreasing the frequency of a particular behavior. What happens if you want to increase the frequency of a behavior that rarely or never occurs? Most parents would like to teach their children to use good table manners, but you could wait a long time for the opportunity to reward young children for using the correct utensil to eat their food. Fortunately, we have a method for increasing the frequency of behaviors that never or rarely occur. Using the method of successive approximationsmethod of successive approximationsA method for increasing the frequency of behaviors that never or rarely occur; also known as shaping.            method of successive approximations        A method for increasing the frequency of behaviors that never or rarely occur; also known as shaping.            , or shaping, we begin by reinforcing spontaneous behaviors that are somewhat similar to the target behavior that we want to train. As training continues, we use gradually more stringent requirements for reinforcement until the exact behavior that we want occurs. You can think of shaping as a funnel. Using the table manners example, parents start out with generous criteria for reinforcement (Thank you for picking up the spoon) and gradually narrow the criteria (Thank you for putting the spoon in the food) until they are reinforcing only the target behavior (Thank you for using the spoon to eat your applesauce). One of the most positive features about the shaping process is that behavior doesn’t have to be perfect to produce reinforcement. Whether training imaginary raptors in Jurassic World or real animals, using the method of successive approximations (shaping) can lead to the reliable performance of otherwise low-frequency behaviors. Like Chris Pratt’s character, most trainers use a combination of classical conditioning (clicker sound leads to food) and operant conditioning (approximation of desired behavior leads to clicker sound). The rats in Skinner boxes that have been described in this chapter do not spontaneously start pressing levers. We have to teach them to do so. We begin by making sure the hungry rat understands that food is available in the Skinner box. Using a remote control, we activate the food dispenser a few times. Quickly, the rat forms a classically conditioned association between the sound of the food dispenser (CS) and the arrival of food (UCS) in the cup. However, if we continue to feed the rat in this manner, it is unlikely that it will ever learn to press the bar. Indeed, there is no reason for it to do so because it already is obtaining the food it needs. So, we narrow our criteria for obtaining food from simply existing in the box to standing in the corner of the box that contains the bar. If we press our remote control every time the rat is in the correct corner, it will begin to stay there most of the time. Now we want the rat to rear on its back feet so that it is likely to hit the bar with its front feet on the way down. If we begin to reinforce the rat less frequently for staying in the corner, it will begin to explore. Eventually, the rat is likely to hit the bar with its front feet while exploring, pressing the bar. Now it will begin to press the bar on its own. In the hands of an experienced trainer, this process takes about half an hour. Shaping involves a delicate tightrope walk between too much and too little reinforcement. If we reinforce too generously, learning stops because there is no incentive for change. If your piano teacher always tells you that your performances are perfect, you will stop trying to improve them. However, if we don’t reinforce frequently enough, the learner becomes discouraged. Reinforcement provides important feedback to the learner, so insufficient reinforcement may slow or stop the learning process. Teaching a complex behavior requires chaining, or breaking down the behavior into manageable steps. Chaining can be done in a forward direction, such as teaching the letters of the alphabet from A to Z, or in a backward direction, such as teaching the last step in a sequence, then the next to the last, and so on. Chaining can be useful when teaching new skills, such as working independently on academic projects, to children with special needs (Pelios, MacDuff, & Axelrod, 2003). Backward chaining is used by most trainers of animals used in entertainment. For example, dogs have been taught to perform complex dances like the Macarena (Burch & Bailey, 1999). The trainer uses a verbal, gestural, or clicker cue while shaping the last step in the dance. When the dog performs this last step reliably, the trainer adds the next-to-last step, and so on until the entire complex sequence is mastered. Even the most radical behaviorists, including Skinner, did not deny the existence of cognitive, social, or biological influences on learning (Jensen & Burgess, 1997). Instead, behaviorists believed that internal processes followed the same rules as externally observable behavior. As  Skinner (1953) wrote, We need not suppose that events which take place within an organism’s skin have special properties…. A private event may be distinguished by its limited accessibility but not, so far as we know, by any special nature or structure (p. 257). However, as we saw in the case of classical conditioning, the results of some operant conditioning experiments stimulated greater interest in the cognitive, social, and biological processes involved in learning. One of the important principles of operant conditioning is that consequences are required in order for learning to occur. Edward  Tolman (1948) challenged this notion by allowing his rats to explore mazes without food reinforcement. Subsequently, when food was placed in the goal boxes of the mazes, the previously unreinforced rats performed as well as the rats that had been reinforced all along. Tolman referred to the rats’ ability to learn in the absence of reinforcement as latent learninglatent learningLearning that occurs in the absence of reinforcement.            latent learning        Learning that occurs in the absence of reinforcement.            . He argued that the rats had learned while just exploring, but that they did not demonstrate their learning until motivated by the food reward to do so. We usually judge whether learning has occurred by observing outward behavior. Tolman’s rats remind us that there is a difference between what has been learned and what is performed. Students are all too familiar with the experience of performing poorly on exams despite having learned a great deal about the material. In addition to challenging the role of reinforcement in learning, Tolman disputed traditional behaviorist explanations of the nature of the learning that occurred in mazes. He believed that instead of learning a simple operant turn right for food association, rats learned This is where I can find food (Tolman, 1948,  1959). After training rats to follow a path in a maze to find food, Tolman blocked the path but allowed the rats to choose from a number of additional paths. If the rats had learned a simple turn left–get food response, they should have chosen the paths that were most similar to the training path. Instead, they showed evidence of choosing paths that required them to turn in a different direction from their previously trained path, but one that led them directly to the goal (Tolman, Richie, & Kalish, 1946; see Figure 8.11). Edward Tolman did not believe that rats wandering around a maze learned turn left for food in the way that early behaviorists believed that they did. Instead, Tolman believed that the rats were learning a more cognitive map of where they could find food. He provided evidence for his approach by blocking a learned pathway to food. If the behaviorists were right, the rats should choose the path most similar to the trained one. However, the rats did not do that. They showed evidence of having formed cognitive maps and were willing to turn in a different direction if that path led to food. To account for his results, Tolman suggested that the rats had formed cognitive maps, or mental representations of the mazes. Map formation was viewed as a unique, nonassociative learning process that didn’t follow the previously established rules of associative learning (O’Keefe & Nadel, 1978). For example, in contrast to the gradual acquisition of learning that usually occurs in classical and operant conditioning, cognitive maps are instantly updated when new information becomes available. Chimpanzees show considerable ability to form cognitive maps (Menzel, 1978). After being carried around a circuitous route in their one-acre compound as nine vegetables and nine fruits were placed in 18 locations, chimpanzees were released in the center of the compound. They not only navigated to each food location using the shortest pathways, but, given their preference for fruit over vegetables, they visited the spots containing fruit first. They showed no indication that they were attempting to retrace the pathway over which they were carried as the food was put in place. Just as the work of Garcia and Koelling highlighted the need to consider biological limitations on classical conditioning, biological boundaries in operant conditioning were described by Keller and Marion Breland, two of Skinner’s former students. In their 1961 article titled The Misbehavior of Organisms (a wordplay on Skinner’s classic book, The Behavior of Organisms), the Brelands outlined some challenges they encountered while using operant conditioning to train animals for entertainment. In one instance, the researchers described how they sought to train a pig to pick up large wooden coins and deposit the coins in a large, wooden piggy bank. Initially, all went well. The pig would quickly learn to deposit four or five coins (an example of an FR schedule) for each food reward. Eventually, however, the pig began to work slower and slower, to the point where it couldn’t obtain enough food for the day. Instead of taking the coins to the piggy bank, the pig would repeatedly toss them in the air and sniff around to find them. Raccoons trained with the coins ultimately tried to wash them instead of depositing them in the bank. The animals’ natural approach to food (the rooting by the pigs and the washing by the raccoons) began to interfere with their handling of the coins. You may already have suspected that the coins had become the object of some higher-order classical conditioning because of their relationship with food. The Brelands concluded that these animals are trapped by strong instinctive behaviors, and clearly we have here a demonstration of the prepotency of such behavior patterns over those which have been conditioned. We have termed this phenomenon ‘instinctive drift’ (Breland & Breland, 1961, p. 683). Keller Breland observes the performance of one of the star pupils of the I. Q. Zoo attraction he developed with his wife, Marian. Unfortunately, the Brelands discovered that the animals’ instinctive behaviors often interfered with their training. The Brelands referred to this phenomenon as instinctive drift. So far, this discussion of classical and operant conditioning has focused on the individual in isolation. Learning can take place when people or animals are alone, but it often occurs in the presence of others, especially in a species as social as ours. As we will see in a later section, people are particularly likely to learn by observing others. What do we know about the impact of others on our operant learning? The presence of others may not just promote learning; it also may be necessary for learning. Human infants learn more about language when they are listening to another person face to face than when they are watching a person speak on television (Kuhl, 2007;  Meltzoff, Kuhl, Movellan, & Sejnowski, 2009). Although operant conditioning alone cannot account for language learning, as discussed in Chapters 10 and 11, these results emphasize the importance of social interaction in producing the arousal, focus, and motivation that contribute to effective learning. Experienced whale trainer Dawn Brancheau was killed by one of her favorite killer whales during a 2010 show at Sea World in Orlando, Florida. Animal experts believed that the whale had simply reverted to normal whale behavior, similar to the instinctive drift observed by Keller and Marian Breland. As mentioned previously in the discussion of cognitive maps, learning and the performance of learned behavior are not always identical. Our performance of learned behaviors varies depending on an interaction between the presence of others and the complexity of the learned task. For simple tasks, like pedaling a bicycle or reeling in a fishing line, the presence of others makes us perform faster, a phenomenon known as social facilitation (Triplett, 1898; also see Chapter 13). In complex tasks, such as taking a difficult college entrance exam, the presence of others can make our performance slower and poorer. Again, this effect is not restricted to complex organisms like ourselves; the same results can be observed in the lowly cockroach (Zajonc, 1965). In a straight maze leading to food, cockroaches with an audience of other cockroaches ran quickly. In a more complex maze involving several turns, the cockroaches responded to an audience by running more slowly. Important applications of operant conditioning may be found in contemporary approaches to psychotherapy, education, advertising, politics, and many other domains. Possibly one of the oddest applications of operant conditioning was Skinner’s secret World War II defense project, code-named Project Pigeon. Lagging well behind the Nazis in the area of guided missile technology, the United States invested $25,000 (worth about $400,000 in today’s dollars) in Skinner’s organic homing device (Capshew, 1993). Skinner, who had considerable experience training pigeons to peck at visual stimuli in his laboratory, now trained them to peck at a projected image of a missile’s target. Riding in a chamber within the missile, the pigeon’s pecks would be translated into updated commands for correcting the path of the bomb. Unfortunately for Skinner (but fortunately for his pigeons), Project Pigeon elicited laughter from military officers instead of approval (Skinner, 1960). Although never implemented, Project Pigeon stimulated Skinner and his intellectual descendents to look outside the laboratory for useful extensions of their work on learning. B. F. Skinner’s Project Pigeon was one of the more bizarre applications of operant conditioning research. Pigeons enclosed in this capsule were trained to peck at projected images of bomb targets. Even though Skinner’s device was superior to other World War II missile guidance systems, it was never implemented. A widely used application of operant learning is the token economytoken economyAn application of operant conditioning in which tokens that can be exchanged for other reinforcers are used to increase the frequency of desirable behaviors.            token economy        An application of operant conditioning in which tokens that can be exchanged for other reinforcers are used to increase the frequency of desirable behaviors.            . Money, in the form of coins, bills, bitcoins, or bank statements, is fairly useless. You can’t eat it, wear it, or shelter in it. Nonetheless, people value it because it takes on secondary reinforcing qualities due to its history of association with other things that have intrinsic value. The use of money to buy things of personal value is an example of a token economy. You earn money for doing certain things, and then you have the opportunity to trade the money you earned for items of value to you. This system meets the best practices criteria that have been described in the context of positive reinforcement. Each person can obtain reinforcement that has unique personal value. One friend may spend discretionary money on going out to dinner, while another invests in the stock market. Both find money reinforcing for doing work. Token economies can be effective ways of managing behavior. Tokens, including money, can be traded for a valued reinforcement of the worker’s choice. This woman’s purchase might motivate her work, but another worker might use the same paycheck to buy a motorcycle or go on vacation. An informed approach to compensating employees should include consideration of learning principles. Menu approaches to employee benefits provide an excellent example of this application. Historically, employers offered a set program of health, retirement, and other benefits to their entire workforce regardless of individual needs. We would expect this approach to be minimally reinforcing because it does not match reinforcers to worker priorities. Catering a benefits package to individual needs is more sensible. A young worker in good health might be more motivated by a benefits package that includes childcare, while a more mature worker may worry about long-term care in the event of a disability. By allowing workers to select their benefits from a menu, everybody can find something worth earning. All of us respond positively to token economies, but they are especially useful in educational and institutional settings. Teachers provide frequent rewards in the form of checks, stars, or tickets that can be exchanged later for popcorn parties or a night without homework. The key to an effective token economy is to offer ultimate rewards that are truly valuable to the people you wish to motivate. If students don’t care about popcorn parties, offering them will have little effect. Token economies are equally useful in prison settings and in institutions serving people with an intellectual disability or mental illness. As we will discuss in Chapter 15, learning theories also have been applied successfully to the clinical setting in the form of behavior therapies, also called applied behavior analysis. After all, our formal definition of learning states that it involves a change in behavior, and changing behavior is precisely what therapists usually seek to do. In addition to the extinction and counterconditioning applications of classical conditioning, behavior therapies use operant conditioning concepts such as extinction, reward, and on rare occasions, punishment. An important application of operant conditioning principles is their use in behavior therapies for conditions like autism spectrum disorder. Operant conditioning can be used to increase the frequency of language use and socially appropriate behaviors, like eye contact. Coupled with cognitive methods designed to address the way that people think about their circumstances, these methods comprise the most popular and effective means for treating many types of disorder, from substance abuse to depression. One of the most dramatic applications of behavior therapy is the treatment for autism spectrum disorder pioneered by O. Ivar Lovaas (Lovaas, 1996;  Lovaas et al., 1966). Autism spectrum disorder is characterized by language and social deficits. Although behavior therapy doesn’t cure autism spectrum disorder, behavioral interventions, like the use of chaining described previously, typically improve an individual’s level of functioning. We all have behaviors that could use some improvement. Maybe we eat poorly, drink too much, smoke, or lash out angrily at others. An understanding of the processes of learning provides us with powerful tools for changing behavior. Let’s assume that your eating habits, like those of many students, do not meet the My mom would approve standard. Yet you are learning in your psychology course that good health habits are essential tools for managing stress (see Chapter 16). How do you bring about the necessary changes? Before doing anything to produce change, you need to understand your current behavior. Many people have a poor understanding of what they actually eat during a day, so you could start by keeping a diary. What foods, and how much of them, do you eat? What else is going on when you eat well or poorly? What possible reinforcers or punishers are influencing your eating patterns? For example, let’s say that you observe a tendency to eat high-calorie snacks late at night while studying, even when you are not hungry. Your goal, then, is to eliminate these late-night snacks. Your baseline shows that your snacking is a social behavior. You consume these extra foods only when studying with a group. The social camaraderie and good taste of the food serve as powerful reinforcers for the behavior. Now that you have a better understanding of your problem behavior, you are in a good position to construct a plan. An important part of your plan is to choose appropriate consequences for your behavior. Again, it is essential that we design consequences that are meaningful to individuals. As we have argued in this chapter, positive reinforcement has many advantages over punishment. You might try placing the money you’re saving on junk food in a designated jar to buy a special (nonfood) treat at the end of a successful week, or allow yourself an extra study break each night that you meet your goals. If you are convinced that the only way that you will change is through punishment, you could take an alternative approach. Although some people have successfully stopped smoking through the use of positive punishment, including electric shock (Law & Tang, 1995), this sounds quite unpleasant. Instead, a negative punishment, such as losing texting privileges for the day following a failure to meet one’s goals, might work. As you implement your program, track your progress and make any modifications that seem necessary. In addition to the improvement of your target behavior, a beneficial side effect of applying learning methods to your behavior is the knowledge that given the right tools, you can control your behavior. ScheduleFeaturesIn the labEveryday exampleFixed ratioReinforcement occurs after a set number of responses. A rat presses 3 times for each food pellet. A garment worker is paid for finishing 10 shirts. Variable ratioReinforcement occurs after a variable number of responses, which average a set number for a session. A rat is fed on average after three responses, but the number of responses required for obtaining food varies between 1 and 15. People play slot machines and win, sometimes on the first play and other times after thousands of plays, on a schedule determined by the casino. Fixed intervalEach response begins an interval during which no reinforcement is available. The first response after the interval is reinforced. A rat’s first press after each 1-minute interval has timed out is reinforced. Students study more hours right before finals than at the beginning of the term (although unlike in the rat’s case, this behavior contributes to reinforcement). Variable intervalEach response begins an interval of varying length, with the average length for the session set by the experimenter. A rat’s first press after an interval is reinforced. The interval averages 1 minute over the session, but reinforcement could be obtained after intervals ranging between 10 seconds and 3 minutes. A fisherman trails his line behind the boat, and at various time intervals, a fish is caught. The ability to learn by watching others, known as observational learning, provides considerable advantages, especially in a social species like our own. Learning occurs without personally experiencing negative consequences. This ability to learn from observing others greatly expands our learning capacity, especially when we then generalize from these concrete examples (watching successful students) to produce effective rules (good time management is important to being a successful student). Observational learning also can have a dark side, as will be seen in our later discussion of the pioneering work by Albert Bandura and his colleagues (Bandura, Ross, & Ross, 1963;  Bandura, 1965) on the modeling of aggression by children. Not only do we learn by observing others, but it appears that observational learning can override other influences on behavior, possibly because we are such a social species. Parents learn (often the hard way) that children are more likely to pattern behavior after what they observe their parents doing than after what they hear their parents say. Various behaviors, both positive and negative, appear to be influenced by observation, including aggression, achievement motivation, language development, phobias, cognitive development, moral judgment, and suicidal behavior. People benefit greatly from exposure to positive role models, especially those with whom they can identify. We worry about the relatively small number of women in university math and science faculties, not only because of the possibility of discrimination, but also because seeing women in these positions might inspire young girls to follow in their footsteps. However, the use of performance-enhancing drugs by sports heroes might lead to role modeling of a different sort by young people. Babies learned more Chinese when listening to a person face to face than while watching the same person speak on a television monitor. It is easy to find examples of observational learning in our daily lives. New college students identify successful, more experienced students in their classes and copy their behavior. Stumped by your new computer software, not to mention the manual that came with it, you watch as a tech-savvy friend shows you how to make it work. Young athletes pore over films of superstars to perfect their technique. Popular cooking shows on television teach you to prepare a special meal. Cross-generational cycles of domestic violence persist as children continue the patterns of aggressive behavior they observe in their parents and grandparents. The goal of this section is to identify the circumstances in which this type of learning occurs and the variables that affect its outcomes. Bandura’s work on the observational learning of aggression provides one of the strongest arguments against exposing children to violent media (Bandura, Ross, & Ross, 1963;  Bandura, 1965). He was interested in imitationimitationCopying behavior that is unlikely to occur naturally and spontaneously.            imitation        Copying behavior that is unlikely to occur naturally and spontaneously.            , which is defined as the copying of behavior that is unlikely to occur naturally and spontaneously (Thorpe, 1963). Observational learning can provide a quick and easy way to learn without having to go through individual trial and error. In a series of classic studies of the imitation of aggression in children, Bandura observed children’s interactions with a Bobo doll, an inflatable toy clown with sand in the bottom, which allows it to rock back and forth when pushed. A number of 3- to 5-year-olds watched an adult model physically and verbally assault an unfortunate Bobo doll (Bandura et al., 1963). The adult yelled, Pow, right in the nose! when punching the doll in the face; Sockeroo, stay down! when hitting the doll with a mallet; Fly away! when kicking the doll around the room; and Bang! when throwing a ball at the doll. Subsequently, when the children were allowed an opportunity to play with a Bobo doll, they displayed a significant amount of aggression. In a later study, one group of children saw the adult being rewarded for aggression with candy and soda (Bandura, 1965). A second group of children saw the adult being verbally reprimanded, spanked, and threatened. The third group did not see any positive or negative consequences for the adult’s actions. The group that witnessed the punishment of the adult model showed slightly less aggression than children in the other groups did. After watching an adult model assault the Bobo doll, young children copied the adult’s movements and verbalizations. Bandura identified four necessary cognitive processes in the modeling of others’ behavior: attention, retention, reproduction, and motivation. Models that get our attention are more likely to elicit imitation. A person must retain a memory of what the model did. We must be able to reproduce the behavior. Many of us enjoy watching elite athletes perform, but no matter how long and often we watch Serena Williams or LeBron James, few of us have the talent to duplicate their movements. If you happen to play tennis or basketball, however, you can learn to improve your game if you carefully observe these superstars. Finally, a person must have a motivation for imitating the behavior. Either past or anticipated reinforcement encourages us to model another person’s behavior. In vicarious reinforcement, witnessing somebody else getting reinforced for a behavior raises the likelihood that we will imitate the behavior. At the same time, witnessing the other person getting punished for the behavior should reduce the likelihood that you will copy it. An individual’s learning may serve that person well throughout a lifetime, but the invention of culture provides opportunities to pass the benefits of experience along for many generations. A society is a group of people living together. Culture, in contrast, consists of all the socially transmitted information used by the group of people, including ideas, concepts, and skills. Observational learning in particular provides a powerful tool for transmitting this information over time. Humans are not the only species to show evidence of imitation. Richard  Dawkins (1976) envisioned a way to break culture down into observable parts. He referred to the basic unit of cultural transmission as a meme. Memes, he said, are transmitted by observational learning from one person to another and can take the form of ideas, symbols, or practices. Melodies, religious beliefs, catchphrases, and the technology for building arches are examples of memes. Dawkins viewed memes as the cultural equivalents of genes—they replicate from one person to the next and they respond to selection pressure. Memes that provide an advantage, such as knowledge of the use of fire, are likely to continue. Those that do not confer much advantage, such as some fads, are likely to die out quickly. Still others, such as pagers, are abandoned when more effective replacements (cell phones) emerge. Among the most social of memes are Internet memes, which are inside jokes passed to others using technologies such as social networking sites and e-mail. Special websites that chronicle Internet memes allow viewers to provide updates of their favorite memes, which contributes to their popularity. Richard Dawkins defined meme as the basic unit of cultural transmission passed to others by observational learning. Internet memes are often unpredictable. Chewbacca Mom Candace Payne never imagined that her short video made for friends would become a viral sensation. Knowledge of the way that people learn can improve your social life, and possibly even your love life. Operant conditioning can help you decrease unwanted behaviors and increase desired behaviors toward you by people with whom you interact. Your behavior influences the way that others behave toward you. If you regularly find that you are treated poorly in relationships, understanding the learning perspective provides powerful tools for change. This chapter has recommended an emphasis on regularly noticing and rewarding desired behaviors. It is easy to fall into the trap of feeling entitled to good behavior from the people who are close to us, which can lead to these behaviors being taken for granted and ignored. Without positive reinforcement, these good behaviors might be extinguished. It takes little time and energy to thank people for the nice things they do for us, and this simple courtesy can increase the frequency of positive interactions in the future. Ignoring annoying behavior can be difficult, but you don’t want to reinforce it with attention. When undesirable behaviors inevitably occur, many people turn to punishment. Skinner believed that part of the love affair that we have with punishment is because of the reinforcing properties of punishment to the punisher.  Skinner (1971) stated, We ‘instinctively’ attack anyone whose behavior displeases us—perhaps not in physical assault, but with criticism, disapproval, blame, or ridicule (p. 190). Punishing a partner for bad behavior might make you feel better, but this comes at a significant cost. These punishing behaviors are not endearing, and frequent use of them is likely to end relationships. If punishment is off the table, what then do we recommend you do when you experience negative behavior from a partner? If possible, try to ignore negative behaviors, putting them on extinction. Unfortunately, some people would rather have negative attention from you than no attention and prefer punishment from you to being ignored. This is particularly likely to be the case if you have forgotten to reinforce positive behaviors. If you combine positive reinforcement of good behavior and extinction of negative behavior, you should notice quite an improvement. Some behaviors like aggression cannot be ignored and require either a complete end to the relationship or professional counseling. Thoughtfully observing the way that you treat other people and their reactions to your behavior, using the learning principles described in this chapter, should provide you with the understanding that you need to improve your relationships. In this chapter, we defined observational learning as the ability to learn by watching others. Both prosocial and antisocial behaviors, including aggression, can be learned this way. We also reviewed Albert Bandura’s conditions for observational learning: attention, retention, reproduction, and motivation. To what extent can we use these principles to explain the occurrence of cyberbullying? A study conducted with over 4,000 teens supported the idea that cyberbullying was influenced by observational learning (Hinduja & Patchin, 2013). Having friends who also engaged in cyberbullying was a strong predictor of being a cyberbully (see Figure 8.12). A total of 62% of young people who indicated that all or most of their friends had cyberbullied in the previous six months admitted to cyberbullying themselves, in contrast to only 4% of the participants who had no friends who cyberbullied. Observing your friends engaging in cyberbullying might be similar to seeing Bandura’s adults assaulting the Bobo doll—the behavior is normalized. It is also possible, however, that birds of a feather flock together, or that antisocial teens tend to congregate in peer groups. Still, using the particular outlet of cyberbullying for antisocial tendencies requires the attention and exposure identified as important variables by Bandura. Teens who know that their friends participate in cyberbullying are far more likely to cyberbully themselves than teens who do not report having friends who cyberbully. While this might be yet another example of birds of a feather flocking together, it is also possible that observing others being aggressive leads to acting aggressively yourself, as predicted by Albert Bandura. Hinduja and Patchin (2013) also presented evidence supporting Bandura’s principle of motivation in observational learning. Bandura argued that anticipated rewards and punishments influence the likelihood that we will copy a behavior. The teens who reported a strong likelihood of punishment from either their parents or school for cyberbullying were the least likely to engage in the practice, although this effect was smaller for the teens who associated with other bullies. Among the bullies, rewards related to peer acceptance might outweigh the negative sanctions from adults to some extent. Nonetheless, understanding the nature of the group support enjoyed by cyberbullies and the importance of anticipated sanctions can give schools and communities better tools for reducing cyberbullying. Establishing a peer climate that clearly labels cyberbullying as deviant, undesirable behavior, coupled with reliable, significant sanctions, might put a much-needed dent in the frequency of this behavior. FeatureDescriptionExampleAttentionWe are more likely to model the behavior of people who get our attention. Children wear the jerseys of the best players in the sport. MemoryWe must retain a memory of the behavior to be imitated. A student re-creates from memory a math proof demonstrated earlier that day by a professor. ReproductionWe must have the ability to reproduce the behavior. An athlete works on her technique after watching films of an elite athlete in her sport. MotivationPast or anticipated reinforcement for a behavior motivates us to perform it too. One student received extra credit for participating in an experiment, so his friends also signed up to participate.  `;\n","export const text = `The hippocampus plays an important role in memory. In the case of Henry Molaison, discussed in this chapter, surgical removal of a large part of the hippocampus reduced his seizures but left him with serious memory deficits. In Chapter 2, we introduced you to one of the most famous case studies in psychology—the case of Henry Molaison (1926–2008), who is known in the scientific literature as the amnesic patient H. M. To reduce Molaison’s severe seizures, possibly resulting from a minor head injury caused by a childhood accident, his neurosurgeons carried out an experimental procedure that would be considered radical today but was even more so in 1953. As shown in the large image to the left, they removed most of Molaison’s hippocampus in both hemispheres, along with some of the surrounding neural tissue. As a result, Molaison’s memory functions declined dramatically, although his personality and intellect remained intact. Zooming in to consider the results of this type of surgery, we can ask what scientists learned from their study of Molaison. Brenda Milner, followed by her student Suzanne Corkin, studied Molaison for more than four decades. His case produced a string of surprises, which we discuss in more detail later in this chapter. For example, certain structures in the brain are important for memory, but more so for some types of memories than for others. Some tasks we can perform automatically; that is, we can remember how to do something without remembering that we know how to do it. Photograph of Henry Molaison. Zooming out to Molaison the individual, shown to the right we can see that his story is more extensive than the study of his brain. According to people who knew him, he was a nice man who loved animals, watching trains, and doing crossword puzzles. Even though Molaison was the subject of decades of study, many questions about him remain. Why did he seem less affected by pain than other people? Why was he accurately able to rate the intensity of an odor but unable to tell the difference between a rose and something rotten? Although his ability to learn new facts following his surgery was remarkably impaired, why did he know scattered facts from the following decade? Which of these observations shows the effects of his surgery? Of his accident? Of his seizures? Zooming out still farther, we ask ourselves how our social interactions would be affected by being unable to remember new people we have met. Molaison remembered people, such as his surgeon, whom he had met before his surgery, but he never learned to recognize Milner, Corkin, or the other scientists who met him later, even though he saw them regularly for decades. He retained a genuine concern for other people and took great pride in his case helping science, but it is hard to imagine that his life was not somewhat lonely and socially isolated. The careful slicing of Henry Molaison’s preserved brain took more than 53 straight hours and was broadcast live on the Internet. We often think of memory as something we need to succeed on exams, but in this chapter, you will learn that memory means much more to our human experience. Our memories allow us to form new connections with people, places, and things; to experience a new acquaintance, over time, becoming an old friend; and to experience the continuity of time rather than repeatedly living life as a single, isolated moment. MemoryMemoryThe ability to retain knowledge.            Memory        The ability to retain knowledge.             is defined as the ability to retain knowledge. The problems encountered by Molaison provide us with a glimpse of how our lives would be altered without certain memory abilities, but what else do we know about how memory typically functions? Can we make more general conclusions about how memory helps animals adapt and survive? Take a minute and write down the five most important things you need to remember today. How would your life be affected if you couldn’t remember these things? Cognitive psychologists see memory as part of a continuum of information processinginformation processingA continuum including attention, sensation, perception, learning, memory, and cognition            information processing        A continuum including attention, sensation, perception, learning, memory, and cognition             that begins with attention, sensation, perception, and learning, which we explored in previous chapters, and progresses to the use of stored information in thinking, problem solving, language, and intelligent behavior, which we discuss in Chapter 10 (see Figure 9.1). Information flows in both directions along this continuum, leading to the bottom-up and top-down processing we observed in Chapter 5. Memories of the characteristics of Dalmatian dogs, for example, are gained by learning about Dalmatians through experience with them. These memories helped you identify the photograph of one in Chapter 5 and should help you interact with one appropriately (thinking and problem solving). Memory is located on a continuum of information processing that flows both from the bottom up and from the top down. We use our memories of Dalmatian dogs to recognize one in an ambiguous photograph and interact with a new one we happen to meet. Memory can be divided into three steps: encoding, storage, and retrieval. EncodingEncodingThe transformation of information from one form to another.            Encoding        The transformation of information from one form to another.             refers to the process of acquiring information and transferring it into memory. In Chapter 5, we described how the sensory systems translate or transduce electromagnetic energy, sound waves, pressure, and chemical stimulation into action potentials that can be processed by the nervous system. These transduced signals can then be encoded in different forms in memory, such as visual codes, acoustic or sound codes, or semantic or meaningful codes. For example, when you meet your friend on the way to class, you encode her appearance visually, the sound of her voice acoustically, and how much you value her friendship semantically. Encoded information needs to be retained, or stored. As we will see later in this chapter, storagestorageThe retention of information.            storage        The retention of information.             of memories in the brain can last anywhere from fractions of a second (sensory memory) to several seconds (short-term and working memory) to indefinitely (long-term memory). Storage in the mind differs from storage of information in your computer in one important respect. Computers store encoded information in reliable and unvarying ways, such as putting socks in a drawer or papers in a file. What you retrieve is identical to what was stored. In contrast, human memory does not generate exact records. Instead, bits of information are stored that are later reconstructed into usable memories. Although this process typically results in a useful memory, errors and distortions can occur. The culmination of the memory process is the retrievalretrievalThe recovery of stored information.            retrieval        The recovery of stored information.             of stored information. As you have no doubt experienced, storing information is no guarantee that you can find the information again when you need it. Later in this chapter, we will discuss ways that memory retrieval can fail in detail. Two of the most common causes of retrieval failure are interference and stress. For example, we seem to know all the answers when watching Who Wants to Be a Millionaire? in the safety of our homes, but when put on the spot, we might be lucky to remember our names. Fortunately, understanding the strengths and weaknesses of the brain’s memory functions may be one of the more practical topics for students that we cover in psychology. Once you understand how memory works, you will have an easier time ensuring that your study habits maximize your performance in school. Evolutionary psychologists view memory as a component of a neural machine designed to use information acquired in the past to coordinate an organism’s behavior in the present (Klein, Cosmides, & Tooby, 2002, p. 308). The evolution of memory allowed animals to use information from the past to respond quickly to immediate challenges, a monumental advance in the ability to survive. Instead of reacting to each predator or source of food as a new experience, an animal with the ability to remember past encounters with similar situations would save precious reaction time. As we discussed in Chapter 3, useful adaptations often come with a price, such as the unwieldy antlers of the male deer that require energy to build yet help the deer fight successfully for mates. The development of a memory system is no exception to this rule. Forming memories requires energy. For memory systems to flourish within the animal kingdom, the survival advantages needed to outweigh the energy costs. Given the 81 years or so of human life expectancy, it would be difficult to demonstrate the energy costs of memory in people, but we can observe the costs in a simpler organism, the fruit fly (Drosophila), which has a life expectancy of only 10 to 18 days. Fruit flies are capable of learning classically conditioned associations between odors and electric shock (see Chapter 8). After experiencing pairings of odor and shock, the flies fly away from the odor 24 hours later (Mery & Kawecki, 2005). However, to form memories about odor and shock, the flies must use more energy than they use for activities that do not require memory. The flies that remembered how to avoid shock died about 4 hours earlier than flies that did not form memories. Nearly all animals have the capability of forming memories despite the high-energy costs, which is a testament to memory’s benefits to survival. Atkinson and Shiffrin (1968, 1971) proposed one of the most influential models of memory. According to this classic information processing model, data flow through a series of separate stages of memory (see Figure 9.2). Contemporary cognitive psychologists have continued to modify this original model while retaining the basic ideas that memories can be stored for different lengths of time and that control processes influence the system. According to this classic model of memory, information flows through a series of stages. If memory is not transferred to the next stage, it is permanently lost. To illustrate the flow of information in this model, let’s consider what happens when you use your memory to complete a specific task—remembering a phone number provided to you by a new acquaintance. While settling into your seat before class, you converse with a classmate about studying together for an upcoming exam. Your classmate gives you her cell phone number so that you can arrange a good time to meet. This incoming information, the auditory signals of your classmate’s voice in this case, is processed in a first stage of the information processing model, the sensory memorysensory memoryThe first stage of the Atkinson–Shiffrin model that holds large amounts of incoming data for brief amounts of time.            sensory memory        The first stage of the Atkinson–Shiffrin model that holds large amounts of incoming data for brief amounts of time.            . This stage holds enormous amounts of sensory data, possibly all information that affects the sensory receptors at one time. However, the data remain for brief periods, usually a second or less, that only last as long as the neural activity produced by a sensation continues. The information held in sensory memory has been compared to a rapidly fading echo of the real input. You can demonstrate the duration and fade of sensory memory information by rapidly flapping your hand back and forth in front of your eyes. When you do this, you can see where your fingers were at a previous point in time. Encoding failure is one of the most common memory problems faced by students. If we don’t encode information because we’re daydreaming during a lecture, there will be no memory of the information to retrieve later. Sensory input is translated or transduced into several types of codes or representations. A representation of a memory refers to a mental model of a bit of information that exists even when the information is no longer available. Visual codes are used for the temporary storage of information about visual images (Baddeley, Eysenck, & Anderson, 2009). Haptic codes are used to process touch and other body senses. Acoustic codes represent sounds and words. Input from different sensory systems remains separate in sensory memory, and although these different sensory streams are processed similarly, there are also some differences. Acoustic codes, or echoic memories, last longer than visual codes, or iconic memories, possibly to meet our needs to hear entire words and phrases before we can understand spoken language. Both brains and computers feature the ability to store memories with one critical difference: The computer stores exact copies of data, but the brain does not. Instead, the brain stores bits of data that are reconstructed later for use. This photo shows the high-security computer memory storage at the Swedish Bahnhof, a facility located 100 feet underground in a concrete bunker. The facility manages servers for many secretive organizations, including WikiLeaks. George Sperling demonstrated the duration of iconic memories by testing recall for matrices of 12 to 16 letters that were presented for as few as 15 milliseconds. Participants were usually able to identify four or five letters. However, the process of verbally instructing participants to do this task takes time, during which the sensory memory for the matrix fades rapidly. If different tones were used to signal a row of the matrix to be recalled instead of verbally asking for a response, participants demonstrated recall for as many as 12 of the original 16 items (Averbach & Sperling, 1961;  Sperling, 1960). If the tone was sounded less than a quarter of a second after the presentation of the matrix, participants could usually recall all four letters in a row. After a quarter of a second delay or more, recall fell to one letter (see Figure 9.3). George Sperling presented a matrix of letters to study participants for as few as 15 milliseconds. When asked to recall the whole matrix (a), most participants could remember about four or five letters. However, when tones instead of verbal commands were used to signal which row to remember (b), participants were able to recall whole rows at a time, which implies they could remember more than four or five items. Sperling concluded that they could see the entire matrix in sensory memory briefly, which allowed them to respond correctly. Why do we have a sensory memory? Only a small subset of this incoming data is processed by the next stage. It is likely that we need to collect incoming data until they make enough sense to process further. The first number in your classmate’s phone number might be simple (2), but it still contains two speech sounds (t and oo) that must be combined to make sense. Encoding and storing memories do not guarantee that they can be retrieved when you need them. Stress can make retrieving even the simplest of memories surprisingly difficult. A tiny amount of information in the sensory memory moves to the next stage of the information processing model, short-term memory (STM)short-term memory (STM)The second stage of the Atkinson–Shiffrin model that holds a small amount of information for a limited time.            short-term memory (STM)        The second stage of the Atkinson–Shiffrin model that holds a small amount of information for a limited time.            , for further processing. If you focus on your new friend’s phone number, the information moves from sensory memory to short-term memory. Consider all the other information that might be processed by your sensory memory at the same time. Perhaps your professor’s first Microsoft PowerPoint slide has appeared on the screen, the heater in the classroom cycled on, and your stomach growled because you didn’t have time for breakfast. None of these bits of information are processed in short-term memory unless you pay attention to them. If you are distracted by one of these, it is likely that you will need to ask your friend to repeat her number. Even though processing memory requires energy, the benefits to survival far outweigh the costs. Without memory, this squirrel would be unable to retrieve the acorns stored weeks ago. Short-term memory, like the sensory memory that precedes it, appears to have remarkable limitations in duration. Without additional processing, information in short-term memory lasts 30 seconds at most (Ellis & Hunt, 1983). In a classic experiment, participants were shown stimuli consisting of three consonants, such as RBP (Peterson & Peterson, 1959). After seeing one of these triplets, participants counted backward by 3s for periods of 0 to 18 seconds to prevent further processing of the consonant triplet. As shown in Figure 9.4, accuracy dropped rapidly. It is likely that the Petersons’ task overestimates the length of time that material is stored in short-term memory. The study’s participants were aware in advance that they would be tested on the items, and despite the distraction of counting backward, they may have made deliberate efforts to retain the triplets in memory. In a classic study by Peterson and Peterson (1959), participants were given triplets of letters, such as XPJ or BTP, to recall. Their ability to remember the letters decreased as the time between presentation and recall increased. These results led the Petersons to propose that short-term memory might last up to 18 seconds, but more contemporary scientists using different methods think that it lasts as little as 2 seconds. You are probably thinking right now that you know what to do to prevent this loss of information. If you repeat the information over and over, a process known as rehearsalrehearsalRepetition of information.            rehearsal        Repetition of information.            , information stays in short-term memory indefinitely, so long as you don’t think about anything else. While you are entering your classmate’s number into your phone, you can rehearse the number in your short-term memory. However, if your attention is diverted from rehearsing the number when the professor calls on you, the phone number will be gone. The incoming information of the professor’s question pushes the previous data out of the system. Data in short-term memory are easily displaced by new, incoming bits of data. If rehearsing the information has been insufficient for moving it into the next stage, long-term memory, the data will be lost. Our ability to see what we have written with a sparkler results from the remaining traces in our sensory memories. The actual light is long gone. In addition to limitations of duration, short-term memory is characterized by severe limitations in capacity. George  Miller (1956) argued that we can process between five and nine items, or bits (such as digits, letters, or words), in short-term memory simultaneously, or in his words, the magical number 7 plus or minus 2. More recently, psychologists have set the limit as about four items (Cowan, 2000). Memory capacities vary from individual to individual, and memory tasks appear prominently in standardized tests of intelligence, discussed in Chapter 10. People who enjoy larger than average short-term memory capacity excel at a number of cognitive tasks, including reading (Baddeley, Logie, Nimmo-Smith, & Brereton, 1985). Because our short-term memory holds limited amounts of information for only a short time, smartphone apps that keep track of things we need to remember to do are popular. You might be wondering how short-term memory could be useful, given these limitations. These limitations actually make short-term memory an ideal solution for the tasks that we ask it to complete. The brief duration of short-term memories ensures that room is freed regularly for incoming information. Most tasks for which we use short-term memory require us to search its contents to find the right information. If short-term memory were capable of holding dozens of pieces of information instead of nine or fewer, this search process would be lengthy and difficult. It is also convenient to have a mechanism that allows you to use information and then discard it. You may not wish to devote precious room in your memory to the telephone number of a plumber you need only once or twice. Short-term memory allows us to use information without overburdening our storage capacities. Nonetheless, it is often desirable to expand our capacity for information in short-term memory. The best way to accomplish this is to redefine what a bit of data is by chunkingchunkingThe process of grouping similar or meaningful information together            chunking        The process of grouping similar or meaningful information together            , or grouping, similar or meaningful information together (Miller, 1956). If the last four digits of your friend’s phone number are one, five, seven, and nine, she could reduce these four bits to two by saying fifteen seventy-nine. Trying to remember the following sequence of letters—FBIIRSCIAEPA—appears to be an insurmountable task. After all, remembering 12 letters lies outside the capacity of short-term memory. The task is greatly simplified by chunking the letters into meaningful batches of common abbreviations—FBI IRS CIA EPA. Now you have only four meaningful bits to remember rather than 12, which is safely within the capacities of short-term memory. Failure to use chunking as a strategy occurs frequently in people with verbal learning disabilities (Koeda, Seki, Uchiyama, & Sadato, 2011). In the absence of chunking, each item is processed as a single, unrelated bit of information, which rapidly overwhelms the capacity of short-term memory. It is easier to remember FBI, IRS, CIA, and EPA in chunks than to remember FBIIRSCIAEPAThe classic description of short-term memory viewed this stage as a place to store information for immediate use. As investigations into memory advanced, researchers proposed an adaptation of this model called working memoryworking memoryAn extension of the concept of short-term memory that includes the active manipulation of multiple types of information simultaneously            working memory        An extension of the concept of short-term memory that includes the active manipulation of multiple types of information simultaneously            , shown in Figure 9.5 (Baddeley & Hitch, 1974). Short-term memory and working memory differed in two ways. First, short-term memory involves the passive storage of information, while working memory involves an active manipulation of information. Second, short-term memory was viewed as managing a single process at a time, whereas working memory was more complex, allowing multiple processes to occur simultaneously. Alan Baddeley’s model of working memory differs from short-term memory in two important ways: (1) information in working memory can be actively manipulated, whereas short-term memory passively stores memory, and (2) working memory can manage multiple types of information simultaneously, whereas short-term memory cannot. Observations that people could manage two short-term memory tasks at the same time led to modifications of this stage of memory (Baddeley & Hitch, 1974). For example, study participants could read a list of numbers and then read a paragraph. This task should quickly overwhelm the limited capacity of short-term memory because reading the paragraph should displace the earlier list of numbers. However, this outcome was not what the researchers observed. Participants had no difficulty remembering the numbers, suggesting that the numbers were stored separately from the words in the paragraph. After further exploration of the types of information that could be maintained separately in short-term memory, four components were proposed (Baddeley et al., 2009): a phonological loop, a visuospatial sketch pad, a central executive, and an episodic buffer. The phonological loop is the working memory component responsible for verbal and auditory information. As you repeat your friend’s phone number while reaching for your phone, you are using your phonological loop. The visuospatial sketch pad holds visual and spatial information. When you describe the route from your friend’s dorm to your favorite coffee shop where you plan to hold your study session, you use your visuospatial sketch pad to describe the way. The central executive manages the work of the other components by directing attention to particular tasks (Baddeley, 1996). Divided attention, which we discussed in Chapter 5, requires the skills of the central executive. While discussing the route to the coffee shop with your friend (phonological loop), you visualize the route (visuospatial sketch pad), and your central executive parcels out the right amount of attention to allow you to do both tasks well. The episodic buffer provides a mechanism for combining information stored in long-term memory, which we discuss in the Long-Term Memory, with the active processing taking place in working memory. This component helps explain why chunking the string of letters earlier (FBI IRS CIA EPA) is easier than remembering the letters as individual bits of information—FBIIRSCIAEPA. Without information from long-term memory about what FBI and the other abbreviations mean, making these chunks would not provide any advantage. The final stage of memory in the information processing model is long-term memorylong-term memoryThe final stage of the Atkinson–Shiffrin model that is the location of permanent memories.            long-term memory        The final stage of the Atkinson–Shiffrin model that is the location of permanent memories.            . Unlike sensory, short-term, and working memory, long-term memory has few limitations in capacity or duration. We do not run out of room in long-term memory for new data, and information can last a lifetime. The oldest person alive can still recall significant childhood memories and learn new things. Although old memories may become more difficult to retrieve, this process is different from losing them simply because of the passage of time. Chase and Simon (1973) presented images of chess pieces on chessboards for only 5 seconds to chess masters and people who didn’t play chess. When the images were from real games, the chess masters recalled the placement of the pieces better than the nonplayers because they were able to use their knowledge of chess to chunk the images of the pieces’ locations in short-term memory. When the pieces were placed randomly on the boards, however, the chess masters were unable to use chunking and performed no better than the nonplayers. In most cases, information moves from short-term or working memory to long-term memory through rehearsal. After seeing your new friend’s number several times as you text her, you memorize it without trying to do so. Rehearsal can be divided into maintenance rehearsal, which means simple repetition of the material, and elaborative rehearsal, which involves linking the new material to things you already know. Of the two types of rehearsal, elaborative rehearsal is a more effective way to move information into more permanent storage. The benefits of elaborative rehearsal can be explained using the levels of processing theorylevels of processing theoryThe depths (shallow to deep) of processing applied to information that predict its ease of retrieval            levels of processing theory        The depths (shallow to deep) of processing applied to information that predict its ease of retrieval             (Craik & Lockhart, 1972). When we look at written words we want to remember, we can attend to many levels of detail: the visual appearance of the word (font, all caps, and number of letters), the sound of the word, the meaning of the word, and the personal relevance of the word. These characteristics can be placed along a continuum of depth of processing from shallow to deep, with the encoding of the appearance of a word requiring less processing and effort than the encoding of the sound of a word, which in turn requires less processing and effort than the encoding of the meaning or personal relevance of a word. According to the levels of processing theory, words encoded according to meaning would be easier to remember than words encoded according to their visual appearance because encoding meaningfulness produces a deeper level of attention and processing (Craik & Tulving, 1975). In one study designed to test the levels of processing theory, participants recalled more words when their instructions elicited the encoding of word meanings than when they were instructed to determine more surface features of each word, such as whether it appeared in capital letters (see Figure 9.6) (Craik & Tulving, 1975). In another study, deeper levels of processing were accompanied by more subvocal speech (reminiscent of talking to yourself), indicated by measurements of tiny movements in the muscles of speech (Cacioppo & Petty, 1981). However, the processing theory is not very specific about the meaning of deep or shallow processing. How would we apply this approach to evaluate participants’ recall for music, touch, or visual images? Further work on this theory needs to identify what determines depth of processing during encoding. The accumulated knowledge of a long life, such as this Australian aborigine tribal elder’s familiarity with his harsh surroundings, probably meant the difference between life and death for many of our ancestors. There is no evidence that older adults are unable to add new information to their long-term memories or necessarily lose information they have known a long time. Our ATMs usually deliver our cash in $20 bills, but handling these regularly does not mean that we remember exactly what they look like. Which president is pictured on the bill? Whose signature as Secretary of the Treasury appears on the bill? Are the fonts used for all the 20s the same or different? Most people can’t answer these questions from memory. Craik and Tulving (1975) tested the levels of processing theory by investigating their study participants’ recognition of words they had seen only briefly (about one-fifth of a second) following one of three types of questions. The three types of questions focused the participants’ attention on aspects of the word they were about to see. The questions pointed to surface features (Is the word in capital letters?), acoustic features (Does the word rhyme with another word?), or semantic features (Does the word make sense in this sentence?). Theoretically, deciding whether a word is in capital letters takes less processing than deciding whether it rhymes with another word, which in turn takes less processing than thinking about its meaning to answer the sentence question. The results supported the levels of processing theory. Recognition of words preceded by a semantic question was better than recognition of words preceded by a rhyme question, which were recognized better than words preceded by a font question. In addition to not sharing the limitations of duration and capacity found with working memory, long-term memory appears to be unique in other ways. Differences between working and long-term memories can be seen in classic experiments demonstrating the serial position effect. This phenomenon can be observed when people are asked to learn a list of words and recall them in any order they choose. As shown in Figure 9.7, recall of items takes on a U-shaped appearance when retrieval is plotted as a function of an item’s position in a list during presentation (Murdoch, 1962). When people are given a list of words to remember and told they can recall the items in any order, the likelihood that a word on the list will be remembered depends on its position in the list. The primacy effect refers to the superior recall for the first words on the list, and the recency effect refers to the superior recall for the last words on the list. The primacy effect probably occurs because people have had more time to place these items in long-term memory. The recency effect probably occurs because these last words still remain in working memory at the time of retrieval. A delay in retrieval erases the recency effect but not the primacy effect. The superior recall for the first items on a list is known as the primacy effect, which results from the storage of these items in long-term memory. Using a list of 20 words, people would have the most rehearsal time for the first word, less for the second, and so on through the list. Because rehearsal moves information from working to long-term memory, the earlier words, with their greater share of rehearsal, would be more likely to be stored in long-term memory than the later words on the list, which probably were not rehearsed much. The superior recall for the last words on the list is known as the recency effect, which occurs because these items remain in working memory at the time of recall. The recency effect, but not the primacy effect, disappears if recall is delayed by 30 seconds (Glanzer & Cunitz, 1966). After 30 seconds, items in long-term memory are still available for recall, but items in working memory are long gone. According to research on primacy, recency, and website menus, we would predict that more people would click on the first and last news articles listed on the Association for Psychological Science’s webpage. Which articles are least likely to get attention? One of the strongest arguments in favor of the separation of working and long-term memory is the occurrence of clinical cases in which one capacity is damaged while the other remains intact. Henry Molaison (the amnesic patient H. M.), whom we discussed at the beginning of this chapter, was able to remember a small amount of information for a few seconds but experienced enormous difficulties when trying to store new information in his long-term memory. In another case study, a patient with another type of brain damage appeared to have the opposite problem. Patient K. F. had normal long-term memory, as indicated by his ability to form new memories. However, his working memory was seriously impaired (Shallice & Warrington, 1970). When asked to recall a list of digits (a typical working memory task), he could remember only one or two digits, a big deviation from the typical ability to recall five to nine digits. Long-term memory can be divided into several categories (see Figure 9.8). These categories not only help us describe memory more precisely, but also represent activity in different parts of the brain. Long-term memory can be divided into several categories, beginning with a distinction between declarative (also known as explicit or conscious memories) and nondeclarative (also known as implicit or unconscious memories). Declarative memories are further divided into semantic and episodic memories, which are combined when we use autobiographical memories. Examples of nondeclarative memories are procedural memories, classical conditioning, and priming. Long-term memory can be divided into declarative, or conscious, memories and nondeclarative, or unconscious, memories. Declarative memoriesDeclarative memoriesA consciously retrieved memory that is easy to verbalize, including semantic, episodic, and autobiographical information; also known as explicit memory.            Declarative memories        A consciously retrieved memory that is easy to verbalize, including semantic, episodic, and autobiographical information; also known as explicit memory.             are easy to declare, or discuss verbally. Declarative memories are also called explicit memories because they are accessed in a conscious, direct, and effortful manner. In contrast to declarative memories, nondeclarative memoriesnondeclarative memoriesAn unconsciously and effortlessly retrieved memory that is difficult to verbalize, such as a memory for classical conditioning, procedural learning, and priming; also known as implicit memory.            nondeclarative memories        An unconsciously and effortlessly retrieved memory that is difficult to verbalize, such as a memory for classical conditioning, procedural learning, and priming; also known as implicit memory.             are difficult to discuss. For example, classical conditioning, which we examined in Chapter 8, produces nondeclarative memories. We might find it difficult to explain to another person why we get nervous right before an exam or dislike a food we ate once before becoming ill. Nondeclarative memories are also called implicit memories because they affect our behavior in subconscious, indirect, and effortless ways. We are aware of their outcomes (I don’t want to eat that food), but we are usually unaware of the information processing that led to that outcome. Declarative memories are further divided into semantic and episodic memories (Tulving, 1972,  1985,  1995). Semantic memorySemantic memoryA general knowledge memory.            Semantic memory        A general knowledge memory.             contains your store of general knowledge in the form of word meanings and facts. Using your semantic memory, you can answer questions such as Which NFL team won last year’s Super Bowl? or What is a churro? Episodic memoryEpisodic memoryA memory for personal experience.            Episodic memory        A memory for personal experience.             is a more personal account of past experiences. We can distinguish between semantic and episodic memories along four dimensions: the type of information processed, the organization of the information in memory, the source of the information, and the focus of the memory (Williams, Conway, & Cohen, 2008). Semantic memories contain general knowledge about the world, whereas episodic memories include more specific information about events, objects, and people. Semantic memory, as we will see later in this chapter, is organized according to categories. For example, we have a category for birds that contains our semantic knowledge of birds. Episodic memory, in contrast, is organized as a timeline. To answer a question from episodic memory, we often use time as a cue: When I was in the eighth grade, my family took a vacation at the beach. Semantic knowledge originates from others, such as your professors, or from repeated experience: The ocean is colder in California than in Florida because every time I’ve gone to the beach in either location, this is what I have observed. An episodic memory can result from a single, personal experience. Finally, the two types of memories serve different purposes. Semantic memory provides us with an objective understanding of our world, whereas episodic memory provides a reference point for our subjective experience of the self. You might have semantic memories that tell you about the characteristics of Labrador retrievers and episodic memories about the day you chose your first puppy. Your autobiographical memories combine these two elements to give you an account of your life. A semantic element of your autobiographical memory might be that your dog’s parents were champions. The episodic elements of your autobiographical memory for the event might include memories of your puppy’s warmth and the happy way you felt that day. Despite the differences just outlined, semantic and episodic memories often overlap. You could form an episodic memory of where you were when you stored a specific semantic memory. A colleague was introduced to a student’s parents as follows: Mom, Dad, this is Professor Jones. He’s the one I told you about who taught us that rats can’t barf. Not only did the student retain a semantic memory about rat behavior (which incidentally is true and is relevant to understanding the classical conditioning of taste aversion in rats), but the student correctly retained an episodic memory of when and where the fact was learned. We can see that semantic and episodic memories interact dynamically to provide a complete picture of the past. Our semantic knowledge of the relative temperatures of the Pacific and Atlantic Oceans depends on the personal experiences of either hearing the fact in a geology classroom or vacationing on both coasts of the United States. At the same time, we use our semantic knowledge to interpret our episodic memories. Without semantic knowledge of the meanings of the words ocean, temperature, Atlantic, and Pacific, we would be unable to organize our experience into a coherent conclusion—the ocean is colder in California than in Florida. This blending of semantic and episodic memories characterizes autobiographical memoriesautobiographical memoriesSemantic or episodic memories that reference the self.            autobiographical memories        Semantic or episodic memories that reference the self.             (Williams et al., 2008). Autobiographical memories can contain factual, semantic aspects of personal experience without episodic aspects. You might know you were born in Pasadena, California, but you would not have any memory of being born. However, your autobiographical memories of Pasadena might also include episodic memories of attending the Rose Parade on New Year’s Day as a child, complete with images of the sights, sounds, and emotions of that experience. A small number of people have been identified as having a rare condition known as highly superior autobiographical memory (HSAM) (Ally, Hussey, & Donahue, 2013;  LePort et al., 2012;  Parker, Cahill, & McGaugh, 2006). One individual with HSAM showed nearly perfect recollection of dates chosen at random from the time he was 11 years old. His recall was corroborated by entries in his grandmother’s diary, interviews with family members, his medical records, and historical facts about his hometown. Individuals with HSAM show superior recall for public events, as well as personal experience, but otherwise perform about the same as typical controls on other tests of memory. HSAM is associated with physical differences in networks in the brain that are associated with autobiographical memory (LePort et al., 2012). Aurelien Hayman can recall detail from random dates in his past. When asked in 2012, Aurelien accurately recalled that October 1, 2006, was a cloudy day, he listened to When You Were Young by the Killers, asked a girl out and was turned down, wore a blue T-shirt, and experienced a power outage at his home. This rare type of memory, known as highly superior autobiographical memory (HSAM), has been the subject of only 20 published case studies. Earlier, we defined nondeclarative memories as unconscious or implicit memories that are difficult to verbalize (Smith & Grossman, 2008). In other words, nondeclarative memories influence our behavior without our conscious awareness of having used a memory. Using nondeclarative memories, you are able to do something, such as using roller blades for the first time in years, without really knowing how you are doing it. Three types of nondeclarative memories have been studied in detail: classical conditioning, discussed in Chapter 8, procedural memories, and priming. Procedural memoriesProcedural memoriesA nondeclarative or implicit memory for how to carry out skilled movement            Procedural memories        A nondeclarative or implicit memory for how to carry out skilled movement             are also called skill memories because they contain information about how to carry out a skilled movement, such as driving a car. PrimingPrimingA change in a response to a stimulus as a result of exposure to a previous stimulus            Priming        A change in a response to a stimulus as a result of exposure to a previous stimulus             occurs when exposure to a stimulus changes a response to a subsequent stimulus. Procedural memories tell us how to carry out motor skills and procedures and are especially difficult to describe in words. Consider the differences between showing somebody how to use scissors and writing an essay about how to use scissors. Which would be easier? Explaining in words how to use scissors, particularly for a person who had never seen a pair of scissors, would be quite a challenge. In contrast, few of us experience difficulties demonstrating procedures (Squire, 1987). One great advantage of procedural memories is their ability to automate our performance. When a novice driver first learns to operate a car with a manual transmission, significant conscious effort is required to remember the correct sequence—clutch, gas, shift. Once the skill is well learned, the driver is far less aware of this sequence; the person just drives. When procedures become automatic, we are free to direct our limited capacities for divided attention to other aspects of the task. A musician who has mastered the notes in a difficult piece can direct attention to the finer points of expression and phrasing. Unfortunately, if a procedure is learned incorrectly, such as a bad golf swing, considerable effort must be expended to fix the swing, which slows performance. The golfer must put in sufficient practice time to make the new, correct swing automatic. It might have been years since this grandfather last put on a pair of ice skates, but to help his granddaughter learn to skate, he’s willing to get back out on the ice. He might be a little wobbly at first, but procedural memories for skilled movement are persistent. He’ll quickly be skating as if he’d done it every day. Priming, or the change in our response to a stimulus because of pre-exposure to related stimuli, explains many everyday effects of familiarity. People rate advertisements they have seen previously, even if they can’t consciously remember seeing them, more positively than those that they have not seen previously (Perfect & Askew, 1994). We agree that the unconscious way our attitudes can be manipulated is unsettling. The distinction between nondeclarative procedural memories and declarative memories is one reason it is so challenging to be a computer help desk technician who must talk people through a repair procedure over the telephone. It would be easier to demonstrate how to fix the computer, which is why some software companies prefer to have the technician take over the computer remotely and apply the needed fixes as opposed to verbalizing procedures for the caller to undertake. It also explains why few star athletes go on to be good coaches. Performing a task is not the same as talking about it. Priming is often studied using a lexical decision task (see Figure 9.9). In this task, a participant views two rapidly presented stimuli and must decide whether the stimuli are both real words (such as fork-roof) or not (such as mork-loof). Reaction time, in the form of hitting one key for real words and another for nonwords, is the dependent variable. When the two stimuli are related words (e.g., doctor–nurse), reaction time for deciding that nurse is a real word is faster than when the stimuli are unrelated words (e.g., butter–nurse) (Meyer & Schvanevelt, 1971). Priming can be investigated within the lexical decision task, in which participants are asked to judge whether two words appearing together are both real words or not (a). Nonreal words are made by switching one letter from a real word, like plame from flame or lork from fork. Pairs of real words are either related to each other by meaning or not. The participants’ reaction time in this task (b) demonstrates that participants respond faster to related word pairs (bread–butter) than to unrelated word pairs (nurse–butter). These results support the idea that we organize items in long-term memory based on their meaning. Type of memoryMajor featuresSensory memoryLarge capacityBrief durationSeparate channels for different sensory types (acoustic, visual, etc.)Short-term memory or working memoryLimited capacity (5–9 bits)Limited duration (30 sec maximum)Expansion of capacity through chunkingExpansion of duration through rehearsalCentral executive, visuospatial scratchpad, episodic buffer, phonological loopLong-term memoryVery large capacityVery long durationSemantic memoryDeclarative/explicitGeneral knowledgeOrganized by categoryEpisodic memoryDeclarative/explicitPersonal historyOrganized chronologicallyAutobiographical memoryDeclarative/explicitCombines semantic and episodic memoriesReferences selfProcedural memoryNondeclarative/implicitDifficult to verbalizeEasy to demonstrateAutomated skillsLibrarians use coding systems to group books on the shelf. Can we identify the systems that we use to organize our memories? Memories that share characteristics are more closely linked in long-term memory than memories that show little overlap among their various features. Connectionism views the mind as a network made up of simpler units or concepts. Connectionist models of memory suggest that thinking about one concept automatically leads to thinking about related concepts and their properties. A spreading activation modelspreading activation modelA connectionist theory proposing that people organize general knowledge based on their individual experiences            spreading activation model        A connectionist theory proposing that people organize general knowledge based on their individual experiences             (Collins & Loftus, 1975) recognizes that people form their own organizations in memory based on their personal experiences (see Figure 9.10). For example, if you ask people to report the first words that come to mind when they see the word red, you will get many different answers. According to the spreading activation theory, thinking about red will activate nearby concepts (orange, green, and fire) faster than more distant concepts (sunsets and roses). This network suggests that a person would answer the question Is a bus a vehicle? faster than the question Is an ambulance a vehicle? The spreading activation model also suggests that concepts differ in the strength of their connections. For example, even though avocados and oranges are both examples of the concept fruit, most people have a closer link in their memories between orange and fruit than between avocado and fruit. If asked whether an avocado or an orange is a fruit, reaction time to the second statement would be faster. The spreading activation model does an excellent job of accounting for the results of the lexical decision experiments described earlier. Using the spreading activation model, the first word activates a concept. This activation spreads to connected concepts and properties. For closely related concepts such as doctor and nurse, activating the doctor concept would activate the nurse concept even before the person sees the word nurse, allowing a quick decision to be made as soon as the word appears. In contrast, with unrelated words like butter and nurse, the nurse concept would not be activated until the word actually appears, resulting in a relatively slower decision. Frederic Bartlett (1932/1967) observed that memory does not work like a video recording of events. He read long, involved stories to study participants and then asked them to recall the stories 20 hours later. Not too surprisingly, the recalled stories were shorter and had less detail than the original story. Somewhat more surprisingly, participants added features to the recalled stories that had not appeared in the original. These additions were not random. In most cases, the details added by participants fit the theme or meaning of the story. We are more likely to remember details that are consistent with our schemas than those that are not. We will remember books in the professor’s office and brushes and canvases in the artist’s studio. Bartlett concluded that memory storage does not occur in a vacuum. When we encounter new information, we attempt to fit the new information into an existing schemaschemaA set of expectations about objects and situations.            schema        A set of expectations about objects and situations.            , or set of expectations about objects and situations. Details that are consistent with our schemas are more likely to be retained, whereas inconsistent details are more likely to be left out. Details may be added in memory if they make a story more consistent and coherent. For example, you are more likely to recall having seen books in a photograph of a professor’s office than in a photograph of a farmer working in the fields. Even if no books appeared in the professor’s office, you might recall seeing some, because most professors have offices filled with books. In Chapter 11, we will explore the development of schemas and the formation of concepts during childhood. Frederic Bartlett observed that using schemas to frame our memories can lead us to add details that improve a memory’s consistency and coherence. In other words, we can remember things that did not occur because they fit our schemas. We can demonstrate this fill in the blank tendency in memory by asking you to memorize some word lists. Read through both lists of words in order, and try to remember as many words as you can. List 1sheets, pillow, mattress, blanket, comfortable, room, dream, lay, chair, rest, tired, night, dark, timeList 2door, tree, eye, song, pillow, juice, orange, radio, rain, car, sleep, cat, dream, eatWithout looking back at the list, write down as many words as possible from List 1 in any order. Check your list of recalled words for any that did not appear in List 1. Pillow and dream appear in both lists, but sleep appears in List 2 only. Many people insert sleep into their List 1 responses (a false memory) because so many of the words on List 1 fit the sleep schema. It is unlikely that you will insert words into your recalled list that are not related to the schema of sleep. See whether you can construct some lists on your own that produce other false memories. We return to the issue of false memories and retrieval later in this chapter. Schemas play important roles in memory storage, as demonstrated by a clever experiment in which participants were asked to read the following passage:The procedure is actually quite simple. First you arrange things into different groups depending on their makeup. Of course, one pile may be sufficient depending on how much there is to do. If you have to go somewhere else due to lack of facilities that is the next step, otherwise you are pretty well set. It is important not to overdo any particular endeavor. That is, it is better to do too few things at once than too many. In the short run this may not seem important, but complications from doing too many can easily arise. A mistake can be expensive as well. The manipulation of the appropriate mechanisms should be self-explanatory, and we need not dwell on it here. At first the whole procedure will seem complicated. Soon, however, it will become just another facet of life. It is difficult to foresee any end to the necessity for this task in the immediate future, but then one never can tell. (Bransford & Johnson, 1972, p. 722)The self is one of the most important schemas we have for organizing our thinking. If you can think about how the material you study is reflected in your experience, it will be easier to remember. At this point, you are probably scratching your head in confusion. Reading this passage is bad enough, and remembering much of it later seems impossible. However, what if we tell you that the passage is about doing your laundry? With the laundry schema in mind, try rereading the passage. It is likely to make a lot more sense than when you read it the first time, and you will remember more of what you read. Storing information does us little good unless we can locate the information when we need it. Without a system of retrieval, our stored memories would be no more useful to us than a library in which books were placed on shelves at random. You might get lucky and find what you are looking for, but in most cases, the search would take so long that the information would no longer be needed. Imagine that you were told to remember the following letters for a subsequent test: c a f h kDuring the test, you are shown a series of letters one at a time. If the letter you are shown matches one on your list, you pull the yes lever as quickly as possible. If the letter is not on your list, you pull the no lever. This procedure is used to investigate recall from short-term memory (Sternberg, 1966,  1967,  1969). Because of the small number of items held in short-term memory, it is tempting to assume that we can retrieve them all simultaneously. However, this is not the way memory works. When the number of letters on the list was increased, a consistent 38 milliseconds of reaction time was added for each additional item. In other words, if you were asked to say whether h was on your list, you would first consider c, then a, then f, and so on until you reached the target letter. These results suggest that we search through short-term memory one item at a time, rather than retrieving its contents all at once. Retrieval systems help us find the information we need, whether we are searching online, looking for a book, or trying to remember something important. Organized information is always easier to find than disorganized information. The popularity of games like Trivial Pursuit, crossword puzzles, and television game shows highlights an interesting aspect of memory retrieval. It feels good when you can remember something. At the same time, most students are all too familiar with the intense feelings of frustration that accompany the inability to retrieve information. You know the answer, but it isn’t coming to mind. A cuecueA stimulus that aids retrieval.            cue        A stimulus that aids retrieval.             is any stimulus that helps you access target information. Most students find recognition tasks such as true–false or matching exam items relatively easy. These tasks provide complete cues (the correct information is on the page in front of you). All you need to do is make a judgment about how well the presented information matches what is stored in memory. Compared to recognition tasks, recall tasks, such as essay exams, require an additional step. Information must be retrieved from memory and then recognized as correct, a process known as generate–recognize (Guynn et al., 2014). Recall tasks provide far fewer cues than recognition tasks and are typically more difficult as a result. In addition to the amount of information provided, what makes a stimulus an effective cue? The most effective cues are those we generate ourselves, a finding that students might find particularly useful. In one experiment, one group of students wrote down 3 words of their choosing for every 1 of 600 words they were expected to learn, while another group studied the same 600 words accompanied by 3 words for each term selected by another person (Mäntylä & Nilsson, 1988). Although recall in the second group was an excellent 55%, the students who selected their own retrieval cues remembered a remarkable 90% of the words. In a later section on improving your memory, we will emphasize the benefits of incorporating your experience when forming new memories. If you are able to put concepts to be learned in your own words and associate them with personal experiences, they will be easier to remember. The popularity of memory games, such as Trivial Pursuit, probably results from the rewarding feeling we get when we retrieve a sought-after memory. Following the Misidentification of five innocent African-American men by a white eyewitness in 1971, William Haythorn was inspired to ask whether cross-racial identifications were as accurate as same-race identifications (Meissner & Brigham, 2001). This led to decades of investigations into the own-race bias (ORB) in memory for human faces, also known as the cross-race effect or other-race effect. Meta-analyses have supported significant ORB effects that are consistent across a number of racial and ethnic groups (Meissner & Brigham, 2001). Researchers still differ, however, in how they explain the effect. One school of thought suggests that the ORB results from perception (Rossion & Michel, 2011). Because people are more familiar with their own race than others, they might initiate different types of attention and perceptual processes when encoding information about faces representing different races (see Chapter 5). Other researchers focus on more social and cognitive factors, such as perceived in-group versus out-group membership (Hugenberg, Young, Bernstein, & Sacco, 2010). According to this approach, a member of your in-group might seem more important to encode, leading to the use of different perceptual strategies. Most of the ORB studies make use of an either-or approach to racial identity. What happens when the stimuli are more ambiguous? Given the large number of people with multiracial identities today, this seems like a logical and important question. When ambiguous racial group faces are used as stimuli, participants’ memory for them depends on whether the ambiguous faces are viewed as in-group members or not (Pauker et al., 2009). When perceivers associated ambiguous faces with their in-group, memory was better, but when the ambiguous faces were not included in the in-group, memory was once again poor. What happens if the perceiver is bicultural? Which faces would seem to fit the own-race category for these individuals? Latino-Americans were primed to focus on either their Latino or American cultural selves (Marsh, Pezdek, & Ozery, 2016). When primed to focus on their Latino cultural selves, the participants demonstrated better memory for Latino faces than for white faces. However, when primed to focus on their American cultural selves, memory for white faces was better than for Latino faces. These results emphasize the importance of considering social and cognitive factors in addition to perceptual and learned aspects in efforts to understand the basis of the own-race bias. Cues might work because of a process known as encoding specificityencoding specificityThe transformation of information from one form to another.            encoding specificity        The transformation of information from one form to another.             (Flexser, 1978;  Tulving, 1983;  Tulving & Thomson, 1973). Each time you form a long-term memory, target information is encoded along with other important bits present at the same time. As a result, each memory is processed in a unique and specific way because this exact combination of bits is unlikely to occur again. Any stimulus that was present and noticed during this encoding process could serve as a cue for retrieving the target memory. Participants typically show an improved memory for faces of people from their own race, a phenomenon known as the own-race bias (ORB). Researchers do not agree on the causes of the ORB, but some argue in favor of the importance of an in-group versus out-group hypothesis. When ambiguous faces are presented, participants’ memories for the faces depend on whether they are primed to think of the ambiguous faces as members of their own in-group or not. It may be tempting to blame context-dependent memory for the experience of alcohol-related blackouts, but these are more likely to result from alcohol’s active interference with the formation of long-term memories (Lisman, 1974). There is no evidence that getting drunk again will make it easier to recall what happened the last time a person got drunk. Among the bits of information that get encoded along with target memories are features of the surrounding environment, leading to context-dependent memory. You have probably been advised to study in a well-lit, quiet, professional environment to perform your best on exams. Duplicating your testing situation when you study should provide the greatest number of retrieval cues. When study participants were asked to learn lists of words in one of two distinctive rooms while either standing or sitting, recall was best when participants were tested in the same room and position as when they learned the information (Greenspoon & Ranyard, 1957). As shown in Figure 9.13, scuba divers who learned words either on land or underwater retrieved the most words when their encoding and testing circumstances were the same (Godden & Baddeley, 1975). Although these effects are small, it is still a good idea to study in a quiet, classroomlike environment, which might explain why studying in the library has remained popular. Mood and other internal states can serve as encoding cues. In one creative study, people with bipolar disorder (described in Chapter 14), whose moods can swing from mania to depression, learned words in one state (mania or depression) and tried to retrieve them in either the same state, mania–mania or depression–depression, or the opposite one, mania–depression or depression–mania (Weingartner, Miller, & Murphy, 1977). The participants were most successful when learning and retrieving occurred in the same state, whether that was mania or depression. Retrieval is not an all-or-none phenomenon. Instead, retrieval proceeds step by step, with each new step bringing you closer to the target. This gradual retrieval is best illustrated by the tip-of-the-tongue (TOT) phenomenon. TOT is probably a familiar experience for you. While trying to remember a word or name, you might retrieve the first letter of the item, but the complete item remains elusive. Features of our environment get encoded along with target memories. Study participants learning lists of words either on land or while underwater recalled more words when tested in the same context compared to when they were tested in the opposite context. This diver might find it more difficult to retrieve information about types of fish if tested on dry land instead of underwater. In a classic series of experiments, more than 200 TOT experiences were induced in research participants by presenting definitions of relatively rare English words (Brown & McNeill, 1966). For example, participants were asked to supply a word for a navigational instrument used in measuring angular distances, especially the altitude of the sun, moon and the stars at sea (Brown & McNeill, 1966, p. 333). You may be picturing the object right now, or thinking about a movie of an old salt using this instrument—it starts with an s—but most of you will have difficulty retrieving the word sextant. Participants showed considerable evidence of partial recall during their TOT experiences. They were able to identify words that they recognized instantly, unlike words they did not know. Many were able to identify the first letter and the number of syllables in the target word. Incorrect words that were retrieved frequently sounded like the target, although their meanings were usually quite different. In some cases, retrieving the incorrect word blocked the retrieval of the correct item, but in other cases, the incorrect word was an additional cue. When retrieved, information to be used flows from long-term memory back into working memory. The mind engages in reconstructionreconstructionRebuilding a memory out of stored elements            reconstruction        Rebuilding a memory out of stored elements            , or the building of a memory out of the stored bits by blending retrieved information with new content present in working memory (Bartlett, 1932/1967). When you retrieve the target information, you are reconstructing something sensible to fit the occasion, as opposed to simply reproducing some memory trace. If the memories are rather fresh, such as when you are describing an automobile accident to a police officer, this process might produce updates in the original memory that will then be reconsolidated into long-term memory. Changes you make in the original memory will be stored as the new truth. However, if you are recalling a memory from a more remote time, less updating is likely to occur (Gräff et al., 2014). Drugs, including the caffeine in coffee, can produce strong context-dependent effects on memory. If you study while drinking coffee, taking a test without coffee is likely to make retrieval more difficult. Storytellers often discover that certain aspects of their stories provoke more of a reaction from the audience. These aspects are emphasized and perhaps exaggerated for even greater effect, and the new, more exciting story replaces the original version in long-term memory. Fish become larger, vacations become more exciting, and heroes become more heroic. When participants in a study were asked to repeat a complicated story on several occasions, they tended to simplify the story, highlight some aspects more than others, and adjust the story to fit their worldviews (Bartlett, 1932/1967). Such alterations probably form the basis of mythology. In the retelling of the adventures of Odysseus or King Arthur, accounts of the original true events are lost or hopelessly distorted. Most of us believe that our memories, especially for important life events, are relatively accurate. Elizabeth Loftus set out to evaluate the reliability of eyewitness testimony in courtroom settings and discovered that memories are rather flexible. In one experiment, participants watched a video of an automobile accident and answered a number of questions about what they had seen (Loftus & Palmer, 1974). One group heard the question How fast was the white sports car going while traveling along the country road? while the other group heard the same question with a slight addition—How fast was the white sports car going when it passed the barn while traveling along the country road? There was no barn in the video, but when participants were asked 1 week later whether they had seen a barn, 20% of those who had heard the barn question answered yes, while fewer than 5% of the other participants did so. One must assume that skilled attorneys are quite aware of this feature of memory and could use such leading questions to the advantage of their clients (see Figure 9.12). Tip-of-the-tongue experiences were elicited in volunteers by describing rare words in English. One of the items was the name of the weird-looking instrument this naval cadet is learning to use. Study participants were often able to retrieve the first letter of the word (s) and the number of syllables (two) without necessarily retrieving the whole word (sextant), which demonstrates retrieval is not all or none. After study participants viewed a short video of an automobile accident, Loftus and Palmer (1974) asked one group, About how fast were the cars going when they hit each other? while a second group was asked, About how fast were the cars going when they smashed each other? One week later, both groups were asked if they recalled seeing glass on the road after the accident. There was no glass on the road in the video, so the correct answer was no. Hearing the word smashed instead of hit increased the likelihood that a participant would remember glass on the road and answer yes. If you began this psychology course believing, like many people do, that memory works like a video of life, it might surprise you to learn that this is not the case. It may be quite unsettling to realize that memories are open to change and revision and that those distinct and confident childhood memories we cherish may be somewhat inaccurate or even flat out wrong. However, it also doesn’t make sense to think that we would evolve a system of memory that was usually wrong. Instead, fuzzy trace theory suggests that we use precious resources to form different types of memories based on our needs, ranging from verbatim, or exact accounts, to gist, which means we retain the general idea of events (Reyna, 2008). We use gist when a relatively vague level of information is sufficient because this is a more efficient use of resources. We use the more energy-intensive verbatim memories for situations that require detailed, accurate recall, such as remembering the periodic table of the elements in your chemistry class. This system works well for us most of the time, but later in this chapter, we will see how relying on gist instead of retrieving verbatim information can lead to false memories (see Figure 9.13). Older children, with their improved language skills, use gist more effectively than younger children (Odegard, Cooper, Lampinen, Reyna, & Brainerd, 2009). When children attended birthday parties with a theme (e.g., Harry Potter or SpongeBob SquarePants), older children appeared to be able to use the theme of the party to provide gist, leading to their successful recall of more theme-related events, such as having magic potions at Hermione’s party. Younger children, however, could not remember theme-related events better than generic birthday party events such as blowing out candles on a cake, suggesting that forming a theme gist was not helpful. People often report especially vivid episodic memories about where they were and what they were doing when they first heard news that evoked a strong emotional response, such as sadness at hearing about the death of a beloved celebrity, such as Carrie Fisher of Star Wars fame. However, research evidence suggests that these memories are not always as accurate as we think they are. A checkpoint for the accuracy of our memories results from source monitoring (Johnson, Hashtroudi, & Lindsay, 1993). Under normal circumstances, we do a good job of distinguishing between external and internal sources of information, as in I said that or I thought about saying that. Again, this system works well for us most of the time, but it can produce false memories when we attribute a memory to the wrong source. For example, you might think you told your roommate you would be coming in late, but you may have only mentally reminded yourself to do so. You have mistaken an internal source of information (I thought that) with an external source (That happened). Take a moment and write down the most important five events in your life last year. Do these events have anything in common with one another? We’re willing to guess that each of the events on your list is associated with strong emotions. From an evolutionary perspective, this makes good sense. In Chapter 7, we argued that emotions provide quick guidance for approach-or-avoidance decisions. Many of our emotional experiences, while not life threatening, have significance for us, and forming strong memories of these events will help us respond effectively to similar situations in the future. Emotions, particularly negative emotions, do not have a simple relationship with memory retrieval. In some cases, we have difficulty remembering negative events, which we discuss in a later section on motivated forgetting. For example, some individuals report that they can recall few or no details of having been sexually molested as children. In other cases, memories for negative events seem even more vivid and intrusive than other types of memories. Many adults in the United States formed a flashbulb memoryflashbulb memoryAn especially vivid and detailed memory of an emotional event            flashbulb memory        An especially vivid and detailed memory of an emotional event             of the terrorist attacks of September 11, 2001, or an especially vivid memory including details of where they were and what they were doing when they first heard the news. Individuals diagnosed with posttraumatic stress disorder (PTSD) often experience intrusive flashbacks of the events that originally traumatized them. How do we reconcile these differences in retrieval for emotional events? As information moves through the stages described by the information processing model, it remains relatively fragile and subject to modification. Stress and strong negative emotions are accompanied by the release of hormones and by patterns of brain activity that can either enhance or impair memory processing, depending on the timing of the emotional response relative to target learning (Joëls, 2006). If stress and learning happen at the same time, an enhanced memory such as a flashbulb memory might be formed (Diamond, Campbell, Park, Halonen, & Zoladz, 2007). Stress occurring either before or after learning impairs memory formation (Joëls, 2006). Impairment of memory following an important life event might protect the fragile memories of that event from interference until they are fully consolidated. So far, we have been considering isolated events that elicit strong negative emotions and stress. As we will see in Chapter 16, chronic stress produces its own set of challenges for memory by producing a number of important changes in the parts of the brain associated with memory formation. For example, chronic stress is associated with a loss of volume in the hippocampus that is likely to have profound influences on the formation of new memories (Roozendaal, McEwen, & Chattarji, 2009). In Chapter 14, we explore a condition known as posttraumatic stress disorder (PTSD) that results when some people experience a traumatic event. Although trauma of many kinds can induce PTSD, one of the most reliable sources of PTSD is combat exposure. Although about 7 to 8% of Americans will experience PTSD at some point in their lives, between 10 and 13% of U. S. soldiers who served in Iraq or Afghanistan and approximately 30% of soldiers who served in Vietnam meet the diagnostic criteria for the disorder (Gradus, 2016). Among the symptoms of PTSD is the experience of vivid, intrusive flashbacks of the traumatic event. These flashbacks can be triggered by environmental stimuli, such as the odor of diesel fumes, according to the principles of classical conditioning we discussed in Chapter 8. Individuals with PTSD would like to eliminate these distressing responses. The current mode of therapy is exposure therapy, which we also discussed in Chapter 8, but many people with PTSD are resistant to extinction learning (Giustino, Fitzgerald, & Maren, 2016). What if we could interfere with memories for traumatic events? If provided along with behavioral therapy soon after a traumatic event, a drug called propranolol can prevent or reduce the later development of PTSD (Giustino et al., 2016). As shown in Figure 9.14, propranolol, which is often prescribed for cardiovascular conditions, affects emotional memories by blocking norepinephrine in the amygdala (see Chapter 4). This in turn prevents people from forming strong memories of the emotions associated with an event, although the facts of the event are processed normally. Propranolol has little effect on older, established fear memories that have already been consolidated. Researchers are making progress on methods for intervening with these older memories. After a single conditioning trial of tone followed by shock (left), rodents learn to freeze in response to the tone. The following day, rodents are given one tone followed by an injection of either a placebo or propranolol (center). Subsequently, animals injected with propranolol show less freezing in response to the tone (right). Propranolol probably achieves this outcome by interfering with the action of norepinephrine, a neurochemical associated with vigilance, in the amygdala. In our discussion of the information processing model, we noted that memories that are retrieved mingle with ongoing material in short-term or working memory. Subsequently, these memories undergo a process of reconsolidation. If you are thinking about something you just read in the chapter, those memories are more open to modification, but memories for the more distant past are less changeable. What if we could get the brain to treat memories from the distant past more like recent memories? Long-term memories involve structural changes in neurons and their synapses that result from changes in gene expression, which we discussed in Chapter 3. In mice, the state of a chemical called histone deacetylase 2 (HDAC2) correlates with the open period of reconsolidation in which memories are modifiable as opposed to the closed period characteristic of more remote memories (Gräff et al., 2014). Application of HDAC2 inhibitors essentially returns remote memories to the same modifiable state that characterizes fresh memories. Extinction of fear responses during this artificial reconsolidation state should produce more effective reductions in traumatic fear. We can’t imagine that anyone would wish continued suffering on people with PTSD, but not all therapists are comfortable with the concept of erasing traumatic memories. As one psychiatrist wrote, There is pain in life, and it needs to be dealt with in a human way …. Our suffering, trauma included, is not a brain problem, but a human problem (Berezin, 2014, para. 3). Reducing emotional distress might make us less disturbed than we should be about events in our environments (Lavazza, 2015). In spite of the cost to individuals, we might ask if we want a community that cannot remember the emotional trauma of war and assault. Our Legal System relies heavily on the testimony of eyewitnesses, especially those who have nothing to gain by telling a lie. Given the flexible nature of human memory as discussed in this chapter, is the trust we place in eyewitness accounts reasonable? Carefully controlled research by Elizabeth Loftus into the use of eyewitness testimony (Loftus, 1979;  Loftus & Palmer, 1974), along with the development of forensic deoxyribonucleic acid (DNA) testing in the 1990s, seriously compromised trust in eyewitness testimony. Out of all cases in which an innocent person has been cleared of a crime because of DNA evidence, about 75% involved mistaken identification of the perpetrator by an eyewitness (Wells, Memon, & Penrod, 2006). Psychologists have used research on eyewitness behavior to make scientifically based recommendations to law enforcement officials. For example, the manner in which photograph lineups of possible suspects are shown to witnesses affects the likelihood of mistaken identification. In the typical procedure, witnesses view lineup photographs simultaneously, which allows them to compare all the people and choose the person who looks most similar to their memories of the perpetrator. Unfortunately, this procedure makes mistaken identification more likely if the real suspect does not appear in the lineup. The witness simply chooses the person who looks most like the remembered perpetrator. If a sequential procedure is used, in which the witness must respond yes or no to a picture before moving to the next one, mistaken identifications occur less frequently (Steblay, Dysart, Fulero, & Lindsay, 2001). Perhaps juries could evaluate eyewitness testimony more accurately if they took the witness’s apparent confidence into account. In other words, a person who seems confident about identifying a suspect might be expected to be more accurate than a witness with less confidence. Unfortunately, witnesses testifying in court who express a 95% confidence in their judgment (expecting to be wrong only 5% of the time) are correct only 70% to 75% of the time (Brewer, Keast, & Rishworth, 2002). However, witness confidence at the time of identification appears to be strongly related to accuracy (Wixted, Mickes, Clark, Gronlund, & Roediger, 2015). If courts paid more attention to this early confidence rather than confidence in the courtroom months or even years later, fewer innocent people should be convicted. The traditional lineup used in the criminal justice system is likely to produce a mistaken identification when the real perpetrator is not included. The witness simply picks the most similar person. Psychologists have shown that giving yes or no answers to one photo at a time reduces the risk of a mistaken identification. Special consideration must be given to cases in which the eyewitness is a child. An understanding of children’s memory development is critical for evaluating the child’s ability to serve as a witness to a crime. Some data indicate that children’s memories for significant events, such as a trip to an emergency room, are quite reliable as long as 4 to 5 years later (Peterson & Whalen, 2001). However, young children are accustomed to pleasing adults with their answers and are more suggestible than adolescents and adults. Fortunately, understanding the strengths and limitations of children’s memory systems has allowed experts to develop methods for obtaining the most accurate reports possible from child witnesses (Bruck & Ceci, 2009). Further improvements should accompany the development of new, more reliable measures of recognition, such as brain imaging, reaction time, rapid presentation of faces, and analyses of witness eye movements (Wells et al., 2006). Retrieval PhenomenonMajor featuresCuesRecognition versus recallSelf-generated cuesEncoding specificityContext-dependent memoryTip-of-the-TongueRetrieval is not all-or-none, but gradualIncorrect retrievals can block correct retrievals or serve as cuesReconstructionNewer memories are more susceptible to reconstructionImplications for eyewitness testimonyFuzzy trace theorySource monitoringEffect of EmotionsFlashbulb memoriesImportance of timingEffects of chronic stressNow that we understand the processes involved with the formation, storage, and retrieval of memories, we can turn our attention to the troublesome topic of forgetting. For students, whose job description involves committing large amounts of information to memory, an understanding of forgetting is the source of practical advice for improving memory and avoiding memory failure. We define forgettingforgettingA decrease in the ability to remember a previously formed memory            forgetting        A decrease in the ability to remember a previously formed memory             as a decrease in the ability to remember a previously formed memory. The key here is that to forget a memory, it has to have been formed in the first place. This definition excludes a number of instances that we have discussed previously. For example, many students maintain that they forgot information needed for an exam but instead were daydreaming during the lecture covering the material and never learned it. This example is better understood in terms of lack of attention and encoding failure than as an example of forgetting. When forgetting is the result of brain injury or disease, we usually refer to the loss of information as amnesia. If this woman was never able to forget where she parked all the previous times she used this lot, finding her car today would be extremely difficult. Understanding forgetting is complicated because we measure memory indirectly by looking at performance. As most students are all too aware, actual memory for a topic can be quite different from performance on an exam. It would be handy for both students and instructors if some sort of modern imaging technology would allow us to see whether introductory psychology had been adequately stored in the brain, but alas, this is not currently possible. Stress, illness, time pressure, and distractions can temporarily reduce our ability to recall information. When we discuss true forgetting, we are not considering the effects of these temporary difficulties. Although forgetting can be frustrating, it also has its adaptive benefits. Forgetting provides a way to prioritize the things we should remember. For example, we are often asked to change our computer passwords to maintain security. At first, this can lead to annoying competition in memory between the old and the new passwords. Over time, however, the strength of an old password weakens. Functional magnetic resonance imaging (fMRI) studies have shown that prefrontal areas of the brain actively suppress memories that are used less frequently (Kuhl, Dudukovic, Kahn, & Wagner, 2007). By suppressing these lower priority memories, we can avoid confusion and reduce the amount of work we have to do to recall higher priority memories. DecayDecayA reduction in ability to retrieve rarely used information over time.            Decay        A reduction in ability to retrieve rarely used information over time.             occurs when our ability to retrieve information that we do not use fades over time. Imagine taking last term’s final exams today. How would you do? You might think that the material you learned last term is gone forever, but just because you can’t retrieve something doesn’t mean that the memories are lost. A classic method of measuring the retention of material in long-term memory over time is the method of savings. This method compares the rate of learning material the first time to the rate of learning the same material a second time. It might take you 50 practice trials to learn the periodic table of elements for your first chemistry class. In a subsequent course, you again need to memorize the table. This time, it only takes you 20 trials. The greater speed of learning the second time indicates that you retained or saved some prior memories of the table. Using this technique, we can demonstrate that people who studied high school Spanish but never used it later in life retained most of their memories for Spanish vocabulary words 50 years later (Bahrick, 1984). Instead of a large amount of forgetting because of the passage of time, most of the material we learn is retained nearly indefinitely. People attending their 70th high school reunions might have forgotten the names of some of their classmates whom they hadn’t seen in decades. Although the idea of decay fits our everyday experience of forgetting quite well, most contemporary psychologists believe that the simple passage of time does not do a good job of predicting memories that are easy or difficult to retrieve (Berman, 2009). It is likely that forgetting occurs because of a combination of factors, which may or may not include decay. InterferenceInterferenceCompetition between newer and older information in memory            Interference        Competition between newer and older information in memory             is the competition between newer and older information in the memory system. The brain requires a measurable amount of time to produce a physical representation of a memory. In the window of time in which memories are being processed but are not yet fully consolidated, they may be subject to distortion, loss, or replacement by interference from other bits of information. How long is this window? The physical changes related to memory that occur at the level of the synapse might take minutes or hours. Memory loss usually occurs when this consolidation is interrupted. Individuals who experience unconsciousness as a result of a head injury rarely remember much about the immediate circumstances leading to the injury. Procedures such as general anesthesia or electroconvulsive therapy (ECT), described in Chapter 15, often produce slight memory deficits spanning a period of hours or possibly a day or two before and after treatment. In contrast, storage of memories in the cerebral cortex might take years, during which time information can be lost or distorted (Dudai, 2004). Interference can be demonstrated by comparing performance in a list-learning task. The more lists someone must learn, the more difficult it becomes to remember words on the first list (Tulving & Psotka, 1971). In other words, learning new lists of words interfered with memory for the first list. Does this mean that the first list is erased from memory by the incoming information? To test this hypothesis, researchers gave participants in one experiment a little help in the form of memory cues. The lists all contained categories of items, such as types of buildings (e.g., house, barn, garage, and hut). If the experimenters provided their participants with a cue in the form of the category (types of buildings), the effects of having learned additional lists were quite small. It appears that the words on the first list were maintained in memory, but learning additional lists made them hard to retrieve. To make matters worse, interference can work in two directions (Underwood, 1957) (see Figure 9.15). Let’s assume that your foreign language class is assigned one list of vocabulary words to study each night. You have procrastinated on your homework, and to catch up for a quiz the next day, you now have three lists of vocabulary words to study instead of the usual single list. Our interest will be in how well you can remember the second of the three lists. If we compare your memories for the second list to those of students who studied the first list when it was assigned, we find that your performance is relatively poor. In other words, learning the first list on the same night as the second list produces proactive interference for the second list. Proactive interference refers to reduced memory for target information as a result of earlier learning. If we measure recall of a target list of words, we find that it is worse both when preceded by learning another list (proactive interference) and when followed by learning another list (retroactive interference). At the same time, we can compare your memories for the second list to the performance of your classmates who studied the third list the night after they studied the second list. Again, your performance is likely to be worse. Reduced memory for target information because of subsequent learning is known as retroactive interference. This type of interference was demonstrated in the multiple lists study we discussed previously, which you may recall unless too much retroactive interference has occurred. Elizabeth  Loftus (2003) demonstrated that it was relatively easy to implant a false memory in study participants of having taken a hot air balloon ride during childhood. The Internal Revenue Service reports that far more people who owe money fail to sign their tax returns than do those who are due a refund. Assuming that the failure to sign the return is not a conscious act of defiance, how can we account for this lapse in memory? Theories of motivated forgettingmotivated forgettingFailure to retrieve negative memories            motivated forgetting        Failure to retrieve negative memories            , or the failure to remember or retrieve unpleasant or threatening information, suggest that the nonsigners are protecting themselves from further unpleasantness by forgetting to sign their tax forms. Memory is a servant to our overarching goals. Retrieval, for better or worse, is often influenced by our motivations, and our motivations can distort the memories we retrieve. While not exactly forgetting in the sense of our earlier definition, motivated distortions of memory can be so extreme that the original information is essentially lost during the process. In one example of the influence of motivation on recall, study participants were presented with a list of choices, such as between two internships, roommates, or cars for sale, with equal numbers of corresponding positive and negative features (high resale value or some rust in the case of the cars). Subsequently, they remembered the positive features associated with their ultimate choices better than the negative features (Henkel & Mather, 2007). When they were deceived into thinking they had chosen the other option instead (because of a friendly reminder from the experimenter), they continued to remember the false choice more positively. In related research, participants conveniently demonstrated less recall for ethical rules after they had been given an opportunity to cheat (Shu, Gino, & Bazerman, 2011). In Chapter 13, we will explore how these types of discrepancies between behavior and attitudes can change the attitudes, not just the memories for them. Identifying the presence of motivated forgetting can have serious practical implications. Beginning in the 1970s, largely because of greater public recognition that incest was more common than previously believed, many adults began to report having been a victim of sexual abuse during childhood. These cases represented a range of possible motivated forgetting from suppression, in which the individual consciously remembered the incidents but had not reported them to parents or other authorities, to repression, in which the individual reported no conscious memory of the incidents until the memories were suddenly recovered during therapy or while reading a news report of a child molestation case. A number of psychologists studying memory suspected that not all reports of recovered memories of child abuse were true, and some might represent confabulation or confusion between imagined and true memories. As we mentioned earlier in this chapter, our source monitoring abilities usually prevent us from mistaking false for true memories, but the system does not perform perfectly. Under the right set of circumstances, it is relatively easy for people to believe strongly in a memory that is simply not true. We demonstrated in an earlier section on schemas that false recall for verbal stimuli can be produced by presenting words that are associated by meaningfulness (e.g., bed, rest, and awake). In this case, most study participants formed a false memory for the presentation of the word sleep (Deese, 1959). Perhaps you are thinking that memorizing strings of words in a laboratory has little relevance to the experience of traumatized victims of child abuse. Loftus, whom we met earlier in our discussion of memory reconstruction, addressed that concern by demonstrating that more complex false memories were rather easy to implant in study participants.  Loftus (2003) described how imagining an event had happened or even reading the testimonials of witnesses could increase a person’s confidence that a false event had occurred. Most persuasive is the use of photographs. When a real family photo was superimposed on a hot air balloon, 50% of participants remembered taking a ride, including details about how old they had been at the time and that the photo was taken by a particular person. Until we understand more about the nature of confabulation, a cautious approach to repressed memories is probably the best course of action. We can neither prove nor disprove these memories without additional evidence, so any therapy should be aimed at relieving distressing symptoms without reference to their source (American Psychological Association, 2014). Cognitive neuroscientists have made considerable progress in discovering the biological correlates of memory processing. In this section, we first zoom in for a look at how memory is managed at the cellular and biochemical levels. Next, we zoom out again to explore patterns of brain activation that are associated with certain types of memory processing. Forming new memories requires changes in the connections neurons make with one another at the synapse, or synaptic consolidation. You might find it strange to think that such a process is going on in your brain as you read this chapter. Eric Kandel and his colleagues have demonstrated persistent changes in the strength of synapses responsible for several types of learning in the sea slug, including classical conditioning (Antonov, Antonova, Kandel, & Hawkins, 2003;  Brunelli, Castellucci, & Kandel, 1976;  Carew & Kandel, 1973). In addition to changes in synaptic strength, it appears that learning stimulates a cascade of gene expression, which in turn produces the long-term structural changes in neurons that represent memories. The number of axon terminals increases following sensitization and decreases following habituation (Bailey & Chen, 1983; see Chapter 8). These observations are consistent with the behavior observed in each case—lower levels of responses to stimuli in habituation and higher levels of responses to stimuli in sensitization (see Figure 9.16). Neurons have smaller numbers of axon terminals following habituation but larger numbers following sensitization. One of the major processes responsible for change at the synaptic level during learning is long-term potentiation (LTP)long-term potentiation (LTP)The enhancement of communication between two neurons resulting from their synchronous activation            long-term potentiation (LTP)        The enhancement of communication between two neurons resulting from their synchronous activation            , which enhances communication between two neurons. This phenomenon can be demonstrated experimentally by applying a rapid series of electric pulses to one area of the nervous system and observing the increased reactions of cells receiving input from that area (Bliss & Lømo, 1973; see Figure 9.17). Results from demonstrations of LTP suggest that the relatively simultaneous activation of a neuron sending information and the neuron receiving this information produces changes that make the synapse between them more efficient. LTP shares many features with memory, which makes it an attractive candidate for being one of the processes underlying memory phenomena. LTP lasts a long time, possibly indefinitely, which is similar to our thinking about long-term memories. Second, both memories and LTP can be formed after only brief exposure to stimuli. LTP can be demonstrated by applying a series of electrical pulses (center) and observing the increased reactions of cells receiving input (right) compared to their previous baseline (left). LTP shares many features with memory, such as being long-lasting and formed after a brief exposure to stimuli. Scientists have also made progress in their search for brain activity that correlates with working memory, although they continue to debate how the executive processes of working memory are organized (Nee et al., 2013). Studies of people with brain damage suggest that several executive functions are managed by different parts of the frontal lobes but that a single central executive probably does not exist (Stuss, 2011). Working memory does not occur in a separate, isolated part of the brain. Instead, the phonological loop and visuospatial sketch pad use the same posterior parts of the brain that are used in verbal and visual perception (Bledowski, Kaiser, & Rahm, 2010). Top-down influences originating in the prefrontal and parietal cortex provide the attention necessary to maintain stimulus information in working memory (Nee et al., 2013). Through the careful observation of people with brain damage, along with brain imaging studies in healthy participants, scientists have discovered correlations between activity in parts of the brain and specific components of long-term memory. These discoveries support the distinctions made by cognitive psychologists between declarative and nondeclarative memories based on observations of behavior. In Chapter 4, we described the important role played by the hippocampus in memory. The hippocampus clearly participates in the consolidation of semantic and location information into long-term memory. The hippocampus might also be involved with the re-experiencing of episodic memories throughout the lifespan (Moscovitch, Nadel, Winocur, Gilboa, & Rosenbaum, 2006). Now that we are familiar with some distinctions between declarative and nondeclarative memories, we can examine the case study of Henry Molaison (the amnesic patient H. M.) in more detail. In follow-up observations of Molaison, Brenda Milner discovered that not all of his memories were equally affected by the surgery that damaged his hippocampus (Milner, 1966,  2005). Molaison retained most of his memory for events leading up to his surgery, but his ability to form new memories was profoundly reduced. The inability to form new memories is known as anterograde amnesia. Much to Milner’s surprise, Molaison learned a new procedural task, mirror tracing, as well as typical control participants did. In one of these tasks, Molaison was asked to draw the shape of a star while looking at a sample star and his hand in a mirror. After 3 days, Molaison mastered the task. However, if asked, he would deny ever having performed the task. His procedural memories were intact, but his declarative memories for the details of the task were nonexistent (see Figure 9.18). The mirror-tracing task requires a participant to trace a five-pointed star, which is mounted on a wooden board that blocks the participant’s view of the star and his or her hand. The participant must view the star and his or her hand in a mirror. This task is especially challenging because the mirror reverses the image, so if you want the pencil to trace around the star away from your body, you have to move your pencil toward your body instead. Brenda Milner was surprised to observe that Henry Molaison learned the mirror-tracing task at a normal rate, even though he didn’t remember the details of the task. This outcome suggested to Milner that nondeclarative, procedural memories such as the mirror-tracing task were not managed by the brain the same way as declarative memories. Semantic memories appear to be widely distributed across the cerebral cortex (see Figure 9.19). Using brain imaging, researchers can observe which parts of the cerebral cortex are active when a person is thinking about particular types of memories (Binder, Desai, Graves, & Conant, 2009). Different areas are activated when a person is accessing knowledge of actions, items that can be manipulated, concrete concepts, and abstract concepts. For example, naming animals is associated with activity in the occipital lobes, suggesting that visualizing an animal’s appearance might be helpful in this task (Martin, Wiggs, Ungerleider, & Haxby, 1996). Naming tools activates areas of the frontal and parietal lobes normally associated with movements and action words. To name a hammer, for example, we might consider the hand movements associated with using hammers and words such as pound or hit. Different patterns of activity in the cerebral cortex are correlated with various types of semantic memories. Naming animals (a) is associated with activity in the visual cortex of the occipital lobe, suggesting that we think about what an animal looks like to name it. Naming tools (b) activates areas associated with hand movements, suggesting that we think about how we would use a hammer or saw to name one. In spite of the overlapping characteristics of semantic and episodic memory, they involve distinctive processing in the brain. Patients in the early stages of Alzheimer’s disease showed much more dramatic episodic memory deficits than semantic memory deficits (Perry, Watson, & Hodges, 2000). The default mode network (DMN; see Chapter 4) is associated with thinking about the self, so it is not surprising to note that structures in this network are also implicated in episodic memory processing (Greicius, Srivastava, Reiss, & Menon, 2004). Areas of the temporal lobe and insula seem particularly important for remembering emotional personal experiences (Fink et al., 1996;  Sheldon, Farb, Palombo, & Levine, 2016). Episodic memories are also affected by damage to the prefrontal cortex. Damage in this area can produce a condition known as source amnesia. People with source amnesia maintain their semantic knowledge but do not recall how they acquired it. A man who experienced damage to his prefrontal cortex as the result of a traffic accident retained his semantic and procedural knowledge of the game of chess, but he could not remember how old he was when he learned or who taught him to play the game (Tulving, 1989). Procedural memories are correlated with activation of the basal ganglia, forebrain structures that are part of the brain’s motor systems (see Chapter 4). People with Huntington’s disease and Parkinson’s disease, both of which produce degeneration in the basal ganglia, typically have trouble learning new procedures (Knowlton et al., 1996;  Krebs, Hogan, Hening, Adamovich, & Poizner, 2001). In contrast, their declarative memories remain relatively intact. Recall that Henry Molaison experienced the opposite outcome. His procedural memory abilities were intact, but his declarative memory abilities were severely impaired. Procedural memories quickly become automatic. When you learn to drive, you must attend to each step, but after learning the process, you just drive. Throughout this chapter, we have emphasized that memory is not a single thing but rather a flexible, multistage process. It should not be surprising, therefore, that we do not have a simple biochemical account for memory. Acetylcholine (ACh), discussed in Chapter 4, affects the encoding of new information (see Figure 9.20). Drugs that inhibit systems using ACh as a major neurotransmitter interfere with memory formation (Atri et al., 2004). People with Alzheimer’s disease, which is characterized by severe memory deficits, show degeneration of neural circuits that use ACh. Medications prescribed to reduce the symptoms of Alzheimer’s disease boost ACh activity (Holzgrabe, Kapkova, Alptuzun, Scheiber, & Kugelmann, 2007). At the same time, high ACh levels might impair memory consolidation and retrieval (Micheau & Marighetto, 2011). Relatively low levels of ACh, characteristic of sleep, improve the transfer of information from temporary to more permanent storage (Diekelmann & Born, 2010). Not only do drugs promoting ACh initiate changes in neural structure in honeybees (Weinberger, 2006), but so does caffeine. Bees rewarded with caffeine were 3 times as likely to remember a floral scent as bees rewarded with sucrose (Wright et al., 2013). Researchers are also interested in the role of the neurotransmitter glutamate in memory formation. One type of glutamate receptor, known as the N-methyl-d-aspartate (NMDA) receptor, is a prime candidate for learning-related changes such as those observed in LTP (Qiu & Knopfel, 2007). Not too surprisingly, chemicals that enhance the activity of glutamate receptors have been shown to boost memory formation in rats (Balschuna, Zuschrattera, & Wetzel, 2006). Similar compounds are being tested for possible use in treating Alzheimer’s disease. Individual differences in working memory capacity are correlated with activity in systems using GABA, the major inhibitory neurochemical in the brain, especially in a part of the frontal lobes known as the dorsolateral prefrontal cortex (Yoon, Grandelis, & Maddock, 2016). Individual differences in working memory play important roles in overall intelligence (see Chapter 10). At the same time, impairments in working memory are characteristic of conditions from schizophrenia to dementia. Most college students by definition have good memory skills—this is an essential component of academic success, and those who lack these skills generally do not end up in higher education. However, we can always improve, and the observations made by psychologists studying memory provide many practical suggestions. We have already discussed several lines of research that have practical implications for improved memory. The structure of long-term memory implies that organized material is easier to remember than disorganized material. Elaborative rehearsal, especially when you connect material to personal experience, anchors new material in your existing memory stores and makes it easier to retrieve. The effects of state, mood, and context on retrieval suggest that studying in circumstances that are most similar to those in which you will retrieve your memories will give you the best outcome. In addition to these basic suggestions, we would like to offer a few additional tips. Psychology professors will never give up trying to convince students that cramming is a terrible memory strategy. Persistent faith in cramming is surprising, given that we all know concert pianists and basketball players are better off practicing 1 hour a day each day for a week than practicing 6 hours straight the night before a performance. The mind works in similar ways whether it learns to play basketball or it learns the periodic table of elements, so the same learning strategies should work in either case. Most of us realize that the best way to improve our music or athletic skills is to practice every day (distributed practice). We would think it odd if an athlete or musician crammed practice in the night before a game or performance (massed practice). The same advantage of distributed over massed practice holds for academic work, but unfortunately, that fact does not deter some students from cramming for exams. Nearly all forms of learning show evidence of an advantage of distributed practice (practice spread out over time) as opposed to massed practice (practice condensed to a short period;  Russo & Mammarella, 2002). In other words, spacing the input of information to the brain over time produces better memory than cramming. Whether we are discussing the learning of classically conditioned responses by sea slugs or the learning of complex semantic information by college students, the advantage of distributing learning over time is a constant. By giving the brain more time to consolidate each memory, less is likely to be lost to interference. We usually think about tests as measuring a student’s ability to retrieve memories, but test taking is a powerful tool for forming memories, too (Roediger & Butler, 2011). Research demonstrates that taking a test produces superior long-term memory when compared with repeated studying of material. In addition, taking tests improved participants’ ability to think about learned material with greater flexibility and to apply material to new situations. We are not advocating that you abandon reviewing your textbook and lecture notes, but we hope you will take advantage of the online testing opportunities that accompany this textbook. It comes as no surprise to students to learn that stress impairs memory retrieval. All of us have had the experience of leaving a classroom after a test only to remember all the things we forgot as we make our way home. How can we avoid such frustrating experiences? The way we study might have an effect on how well our memories hold up under stress. Many of us study by going over material multiple times, or restudying. Research suggests that a more efficient method of studying includes retrieval practice, or the taking of practice tests. These two methods were compared to each other in their ability to withstand stress (Smith, Floerke, & Thomas, 2016). The Question: Which study method (restudying versus retrieval practice) results in better retrieval during a stressful situation? Two groups of 60 participants were given the task of learning either 30 concrete nouns or 30 images of nouns, presented one at a time. Following this first presentation, the restudying group restudied the items, while the retrieval practice group recalled as many of the items as they could remember. No feedback was given to the retrieval practice group. On the next day, half of each group was stressed by being asked to give an unprepared speech and solve math problems in front of judges and peers. While they were doing these activities, the other half was given a comparable but not stressful task. Five minutes into their respective tasks (stressful or not), and again 20 minutes later, the participants were asked to recall the items they studied the previous day. The method of stressing participants used in this study has appeared in many other experiments without report of adverse effects. However, the possibility of being assigned to the stressful condition should be noted in the informed consent form, along with referrals to appropriate professionals who can help manage stress. The groups showed no differences in recall during the first test, which occurred five minutes into their stress or no-stress control experience, but their performance was significantly different at the second, delayed test. For the restudying group, stress produced a significant decline in the items recalled. However, for the retrieval practice group, stress made no difference in the number of items recalled. Regardless of stress levels and time of testing, the retrieval practice groups outperformed the restudying groups (see Figure 9.21). A restudying group went over the material to be learned four times, while a retrieval group was asked to recall the items. The following day, half of both groups were stressed while the other half was given a nonstressful task to complete. When asked again to recall the items, stress had a significant, negative effect on the restudying participants. The retrieval group not only remembered more items than even the nonstressed restudying participants, but they were also unaffected by stress. Students prone to test anxiety might wish to incorporate the retrieval method of preparation into their study regimen. If you have been in the habit of relying on restudying as your major or only strategy for succeeding in your classes, you might want to rethink your methods in light of this experiment. Retrieval practice appears to buffer performance from stress effects much better than restudying. It is likely that retrieval practice accomplishes this feat by producing multiple routes for accessing a memory. Each time you attempt to retrieve information, you also think about associations and the context of the information in slightly different ways. This provides you with multiple pathways back to the information you seek. When you are stressed, physiological correlates of stress (see Chapter 16) might interfere with your use of some but not all of these pathways. The more pathways you have, the more likely you are to find the information in memory. Physical exercise, especially vigorous exercise such as running, increases adult neurogenesis, or the birth of new neurons, in the hippocampus, at least in mice (Bolz, Heigele, & Bischofberger, 2015;  Moon et al., 2016). Not only did exercising mice experience more neurogenesis, but their memory performance improved compared to mice who did not exercise. We will not guarantee that taking up jogging will improve your grades, but it will certainly benefit your health and mood (Chen et al., 2016). Initially, many psychologists believed that the positive role of sleep in memory formation resulted from a lack of interference. If you learned something right before going to sleep, no further information would enter the system to cause interference. More sophisticated research, however, has demonstrated that sleep plays an active role in the consolidation of memories. Learning during waking might strengthen new connections, but sleep-related processing might reorganize existing memories to accommodate new information (Stickgold & Walker, 2007). Most types of memories appear stronger after a period of sleep (Boyce, Glasgow, Williams, & Adamantidis, 2016;  Genzel, Kroes, Dresler, & Battaglia, 2014). We can also say with confidence that students who pull all-nighters are not doing their memory systems a favor (Havekes et al., 2016). In one experiment, staying up all night produced poor memory for a previous task, and two additional nights of adequate sleep did not compensate for the original deprivation (Stickgold, James, & Hobson, 2000). Most students recognize that a certain number of rehearsals of reading and lecture notes are required for success on exams. However, we can mistake the ease with which we cover familiar material for actually knowing the material. Just because you can read something easily doesn’t mean that you know it. A somewhat more efficient method is recitation, or the verbalizing of the material to be learned in your own words. Recitation takes advantage of a general superiority for self-referential information. People who processed words in reference to themselves (e.g., Does the word ‘honest’ describe you?) are more likely to remember the word honest than are people who processed the definition of the words (e.g., Does ‘honest’ mean the same thing as ‘trustworthy’?;  Rogers, Kuiper, & Kirker, 1977). This result has obvious relevance for students wishing to improve their memories. If you can think about the information you are trying to learn in self-referential ways (This example of episodic memories in my textbook reminds me of something that happened to me the other day), your memory for the information will be enhanced. By putting information in your own words, you make it more relevant to yourself. Putting information you need to remember in your own words is an effective memory strategy. Recitation takes advantage of our tendency to remember things better when they’re associated with the self. If they are your words, you will remember them. Recitation is still used as a classroom technique in small classes, but you can duplicate this process on your own. After you have a reasonable grasp of the material, try talking about it. A significant benefit of recitation is that you quickly realize what you do not understand when you try to explain the material to someone else. You can think that you understand something, but when you try to explain it, it doesn’t come out right. This is a signal that more work on the topic is needed. The early Greeks devised a number of methods, known as mnemonicsmnemonicsMemory aids that link new information to well-known information.            mnemonics        Memory aids that link new information to well-known information.            , for improving memory. Mnemonic devices expand memory capacity by linking the material to be remembered to information that is relatively effortless to retrieve. The first-letter approach takes advantage of chunking. You condense a large amount of information into an acronym. For example, in Chapter 12, we’ll use the acronym OCEAN to help remember the five major personality traits: openness, conscientiousness, extraversion, agreeableness, and neuroticism. One of the classic Greek techniques was the method of loci, or places. This technique is particularly handy when you are trying to memorize a list of items in order, such as the planets in our solar system or the cranial nerves. The method takes advantage of the fact that we form excellent representations of visual images in memory. You begin by imagining a familiar place, perhaps your childhood home. As you imagine yourself walking through your home, you visualize each item in a particular location. If you wish to remember your grocery list (although writing the items down is probably easier), you might imagine a carton of eggs on the little table in your entry, a loaf of bread on the sofa, a box of cereal on the television, and so on. To recall your list, all you need to do is to take another imaginary walk through your house, recalling the items you placed as you go. If all goes well, you should remember all your items in the correct order. This technique may sound like a lot of work, but it can be effective. One of the authors of this textbook had a colleague in graduate school who performed so perfectly on her neuroanatomy exams that her professors accused her of cheating. She related to them how she had been taught the method of loci as a childhood game and had practiced the technique throughout her academic career. After they posed several difficult lists to her, all of which she recalled perfectly, they were convinced of her honesty. The ancient Greek mnemonic device, the method of loci, takes advantage of our superior memory for visual images of familiar places. Although the method involves consciously imagining things in a particular place, we often use location as a memory aid less consciously. You are probably familiar with the layout of your favorite grocery store and use that mental image to guide your memories for the food you need to purchase. If the store reorganizes its layout between trips, you might forget something. Shared memories are a characteristic of close relationships. You may know people in close relationships who seem to know intuitively what the other is thinking, perhaps even finishing the partner’s sentences. According to a theory of transactive memory, couples in long-term relationships also develop a division of labor in regard to memory, in which each partner knows certain things but also knows what information can be retrieved from the partner if needed (Wegner, 1986;  Wegner, Giuliano, & Hertel, 1985). For example, one partner might not keep track of where candles are stored in the house but knows that the other partner knows where the candles are and can be called upon to provide that knowledge in the event of an emergency. How do couples develop systems like this? Three major strategies have been identified (Wegner, Erber, & Raymond, 1991). First, one partner can explicitly agree to take on an area of expertise, such as managing the household finances. Second, as people get to know each other better through self-disclosure, they also learn about each other’s relative areas of knowledge and expertise. One partner might have an interest in computer science, while the other thinks that computers work by magic. If something goes wrong with a household computer, the second person will turn to the first. Finally, couples learn about their partner’s access to information. If you know that your partner discussed holiday plans with your families, you are likely to assume that your partner knows more about your holiday options than you do. People in close relationships form transactive memories, or a division of labor for remembering certain things. This woman might remember how to do certain home repair tasks, and her partner might remember others. Together, they have access to far more information than either individual could manage separately. From an evolutionary standpoint, what are the advantages of working out this division of memory labor? One major advantage of this type of transactive memory is that a couple working well together has access to far more knowledge than either individual could manage separately. The convenience of these systems, in contrast to managing knowledge individually, would contribute to further bonding. Transactive memory is negotiated over long periods between each couple in ways that are unique and not interchangeable with others. The concept of transactive memory has been extended from intimate couples to larger groups (Peltokorpi, 2008). In the context of larger groups, transactive memory contributes to group cognition or information processing that differs from individual cognition. Transactive memory is critical to understanding the behavior of teams in organizations (Argote & Guo, 2016;  Lee, Bachrach, & Lewis, 2014). As in the case of intimate couples, transactive memory contributes to the group’s ability to manage more information than any individual could be expected to do in an efficient manner based on the relevant specialties of the individuals making up the group. Transactive memory contributes to the establishment, maintenance, and adaptation of organizational routines, or ways to solve familiar problems (Miller, Choi, & Pentland, 2014). Whether transactive memory takes place at the couple or the organizational level, it takes time to develop. People beginning a new relationship can expect some miscommunications and misunderstandings (and overdue bills and lost candles) until their transactive memory system begins to take shape. Earlier in this chapter, we gave you the opportunity to explore the formation of false memories. After reading two lists, one of which contained many words associated with sleep, most participants mistakenly believe that the word sleep occurred in the sleep-related list (it did not). This type of experiment is known as the Deese–Roediger–McDermott or DRM paradigm, after Roediger and McDermott (1995), who updated the work of  Deese (1959). Using the DRM paradigm, researchers reliably elicit false memories of words that do not actually appear on lists. How might this relate to cyberbullying? Not too surprisingly, aggressive people often form different schemas about how the world works. This leads them to interpret ambiguous information in hostile ways (Dodge, 1980). What happens when we use the DRM paradigm to assess the effects of aggressive schemas on memory? A person’s tendency toward aggressiveness was associated with more false memories of aggressive words in an otherwise ambiguous list (Takarangi, Polaschek, Hignett, & Garry, 2008). Can we link these aggressive tendencies more closely to actual risk of cyberbullying? When adolescents were exposed to a modified DRM paradigm containing a list of ambiguously hostile words, a list of insults, and three lists of neutral, control words, participants who reported having engaged in cyberbullying responded differently than their less aggressive peers (Vannucci, Nocentini, Mazzoni, & Menesini, 2012; see Figure 9.22). Cyberbullies showed more aggressive false memories for the ambiguously hostile words and more verbal/aggressive false memories for insults. For both boys and girls, cyberbullying scores were significantly and positively correlated with false memories for violent and insulting terms but not for neutral terms. These results suggest that cyberbullying is associated with the likelihood of making hostile distortions in memory. As with all correlations, we cannot presume causality. It is possible that thinking in hostile ways promotes cyberbullying, that engaging in cyberbullying promotes hostile thinking, that cognitions and bullying mutually affect each other, or that some third variable promotes both hostile thinking and cyberbullying. These results can be understood within a general aggression model (GAM) (Anderson & Bushman, 2002). The GAM sees aggression as a possible outcome of three stages: person and situation inputs, present internal states (including cognitions and emotions), and appraisals and decision-making processes. The experience of hostile false memories could reinforce hostile schemas, which in turn affect the appraisals of other people’s behaviors as hostile (Takarangi et al., 2008). Seeing innocent or ambiguous behaviors as intentionally hostile might lead an aggressive person to retaliate. As we observed in Chapter 7, the main motive of cyberbullying appears to be reactive or responding to a perceived threat or insult. A tendency to remember the other person’s behavior as more threatening than it really was could lead to an escalation of aggression. Type of forgettingMajor featureAssessmentDecayReduced ability to retrieve infrequently used material over time. Savings technique. InterferenceCompetition between newer and older information in the memory system. Comparison of performance in a list-learning task. Motivated forgettingFailure to retrieve negative information. Comparison of recall for memories associated with positive or negative emotions.  `;\n","export const text = `Observing brain activity while a person watches a movie is helping filmmakers produce more effective movie trailers. Daily Life is full of decisions—some big and some small. How does the mind reach a decision? Psychologists have learned a great deal about the processes involved in reaching a decision and have identified many steps we can take to improve our decision making. Zooming in, we can use imaging technologies to observe how the brain reacts to a number of choices. For example, do you really want to see that movie based on its trailer? Film producers are using brain imaging when editing movie trailers to elicit the reactions they want from an audience (Randall, 2011). They have only a few seconds to convince the audience to see their film. Assuming that the film’s ability to elicit emotion drives the decision to see it, filmmakers remove the parts of a trailer that produce neutral emotional responses and splice together the most dramatic parts. Is this practice scientifically valid? It may be useful in some cases. In Chapter 7, we discussed how emotions range along a continuum. Some emotions, such as fear, are associated with precise physical responses, while others, such as pride, are associated with more ambiguous physical responses. You could probably develop a movie trailer that would evoke fear in viewers, but eliciting more complex emotions might be more difficult. Zooming out to the moviegoers’ decision to see a movie, what factors aside from seeing an emotional trailer might influence their choice? Individual differences play key roles. One person might enjoy horror films, while another does not. Using the social perspective to zoom out still farther, we can see that people respond to horror films based on their beliefs about the expectations of others. You might think that males are predisposed to like horror films and females to like chick flicks, but psychology is rarely that simple. Instead, a person’s reaction to horror films is strongly determined by social attitudes and beliefs about how same- and opposite-sex peers would respond to the film (Mundorf, Weaver, & Zillmann, 1989). The mind uses symbols or mental representations to signify information, just icons represent some of your favorite social networks, such as Facebook, Twitter, Instagram, and Snapchat. In this chapter, we will explore how we use the tools of language and intelligence to think, which in turn guides our behavior. Beginning with the parts of the brain that process information, we will zoom out to the individual differences of personality and experience and farther still to the social and larger cultural contexts that shape the way we think. The word cognitioncognitionInternal mental processes including information processing, thinking, reasoning, and problem solving.            cognition        Internal mental processes including information processing, thinking, reasoning, and problem solving.            , as in the term cognitive psychology, is derived from the Latin cogito, which literally means to think. Thinking allows us to manipulate information internally to construct models of the world, plan our interactions with that world, and regulate ourselves to meet our goals. As you will see, cognitive psychology covers a broad area of topics. In addition to perception, learning, and memory, which we discussed in previous chapters, cognitive psychology addresses questions of thinking, language, and intelligence. We think about what we know. In the broadest sense of the word, knowledge is the entire body of information acquired through study, investigation, observation, and experience. To manage this large collection of knowledge, the mind uses symbols, or mental representations, to signify information. Using mental representations is similar to having an icon on your computer desktop that represents Microsoft Word and a profile photograph on Facebook that represents your best friend. The most familiar uses of mental representations are found in language, both written and spoken. Depending on the language you choose, you could represent the animal coming toward you on the sidewalk as a hund (German), chien (French), gau (Chinese), perro (Spanish), or dog (English). Representations can be visual too. For example, you could snap a photo or sketch a picture of the dog. Despite the differences in these two visual representations, the dog that each portrays is the same. We can view the representation of the dog (the sound or the appearance of the word used, the patterns of light in your photo or drawing) as a vehicle for carrying information about the content of our knowledge of this dog (it seems to be a friendly, young golden retriever). In this chapter, we explore the ability of minds to form mental representations and manipulate them to make sense of the world. What forms can representations take? What are the relationships between the representations and the content they symbolize? Temple Grandin, a college professor with autism spectrum disorder, describes her way of thinking as thinking in pictures (Grandin, 2010). Albert Einstein is also widely quoted as relying on mental visualization in the early stages of his thinking and only later putting his ideas into words (Einstein, 1945). To what extent do the rest of us think in mental images? By mental image, we are referring to a representation of any sensory experience that is stored in memory and can be retrieved for use later. For example, you can call up a visual image of your first car, picture the letters of your name, or silently hum the first bars of Happy Birthday to yourself quite easily. People treat mental images much like they would a real object (Kosslyn, 1978,  1980,  1994). We can turn visual mental images around in our minds, zoom in or out, and identify their features (see Figure 10.1). If you were to think about a map of the United States (assuming you have a good grasp of geography), it would take you longer to mentally count the major cities between Los Angeles and New York than those between Chicago and New York, just as it would if you were looking at a real map. A person familiar with New York City could follow a mental route among these buildings, zoom in or out, and identify important features like landmarks and a favorite place to have coffee. Children are particularly likely to use visual images in their thinking. In one study, between 2 and 15% of elementary school children experienced long-lasting and detailed visual images of a complex picture they viewed for a short interval (Haber & Haber, 1964). The children described the scene using the present tense, suggesting that they were scanning a mental replica. Except for rare cases like Grandin, few adults can do this. It is possible that language becomes an increasingly important way to organize thinking during childhood and might begin to overwrite or interfere with the ability to directly access visual images. Temple Grandin, a college professor with autism spectrum disorder, is shown here with actress Claire Danes, who portrayed her in a 2010 made-for-television movie. Grandin has described her cognitive experience as thinking in pictures. It is likely that young children depend on a similar approach to thinking, but learning language provides new dimension to our ability to think. Regardless of the exact form taken by mental representations, our knowledge would be useless unless we imposed some type of organization on all the bits we know. To supply this organization, we extract special, organizing ideas known as conceptsconceptsAn organizing principle derived from experience.            concepts        An organizing principle derived from experience.             from the specific instances and occurrences we experience. Concept formation is not unique to humans. Nonhuman animals as diverse as pigeons and monkeys demonstrate concept formation (Herrnstein, 1979). In one study, pigeons learned to peck at projected images of water with fish to obtain food rewards but to withhold pecking at images of water without fish (Herrnstein & de Villiers, 1980). Subsequently, a new set of slides was presented. The pigeons successfully distinguished between the fish and the nonfish slides, even though they had not seen these particular images during their prior training. The pigeons appeared to have extracted a fish concept from their experience. What does it mean to have a fish or any other type of concept? How are we (and pigeons) able to construct a new concept? Because of the memory processes discussed in Chapter 9, you already know what a dog is. If you were asked to feed your neighbor’s dog while your neighbor goes on vacation, you could locate the correct animal, provide dog food instead of cat food, and exercise appropriate caution if your neighbor’s dog is unfriendly. This activity sounds simple, but what if you recruited an alien from another planet to help you with your task? What information about dogs would the alien need to know to succeed? Essentially, what you must convey to your alien friend is the concept of dog (see Table 10.1). ApproachApplication to dog categoryDisadvantagesDictionary definitionDescribe a dog as a domesticated carnivorous mammal (Canis familiaris) related to the foxes and wolves and raised in a wide variety of breeds. Gives more information than needed to classify a dog correctly. Common featuresDescribe dogs as furry animals with four legs that bark and wag their tails. Results in too many exceptions (e.g., Chihuahuas are not furry). Overlapping featuresCompare features of a suspected dog with features of the dog concept. Lacks precise boundaries for most categories (e.g., tastes good might be a category feature for dogs in some cultures but not in others). PrototypeAverage all members of the dog category. Does not provide a good way to think about a category’s variability. ExemplarUse a specific instance of a category a model. Excludes some category members that are too dissimilar to model. You might start the discussion with a dictionary definition of dog. In the dictionary, you find that a dog is a domesticated carnivorous mammal (Canis familiaris) related to the foxes and wolves and raised in a wide variety of breeds. This definition seems rather remote from what we know to be a dog. You don’t need to know what dogs eat and who their nearest relatives are to identify one correctly. London-based artist Stephen Wiltshire, who like Temple Grandin has autism spectrum disorder, has been described as a human camera. Wiltshire can fly over a major city such as New York in a helicopter for about 1 hour and then recreate highly accurate and detailed drawings of the city. His talent is an extreme and unusual form of the use of visual imagery. Perhaps you could find some common features of dogs that would allow you to distinguish between dogs and nondogs. You could tell your alien friend that dogs are furry animals with four legs that bark and wag their tails. Unfortunately, this system has a logical flaw (Wittgenstein, 1953). No matter how careful your definition, even for the simplest concepts, somebody will think of an exception. Although we can agree that all dogs are animals, this feature alone is insufficient for distinguishing dogs from cats and raccoons. Having fur does not describe some chihuahuas, and basenjis don’t bark. A dog that has lost a leg in an accident no longer has four legs. Most dogs we know wag their tails, but there may be an abused dog that keeps its tail perpetually between its legs, as well as dogs that have tails too stubby to wag. To resolve this dilemma, you can make your definition more flexible. Instead of being rigidly defined by a checklist of features, a concept could describe a group of instances that share overlapping features. This approach is similar to a feature detection model (Smith, Shoben, & Rips, 1974). According to this type of model, people determine the truthfulness of statements like a canary is a bird by considering overlapping features (wings, beak, flies, and chirps). Using this approach, the alien could compare the features of an animal suspected of being a dog with a checklist of dog features. This approach has problems, too. Some categories are clear, such as triangles, but others do not have precise enough boundaries for the checklist approach to work. People in some cultures would list taste good as a feature of dogs, but Americans probably would not. In addition, the feature checklist approach doesn’t seem to match personal experience with thinking. When you think about a dog, do you access a mental checklist of dog features? An alternate approach to thinking about concepts is to consider some type of standard dog, or a prototypeprototypeA representation of a category formed by averaging all members of the category.            prototype        A representation of a category formed by averaging all members of the category.             that represents your entire category (Rosch, 1973,  1983). A prototype results from an averaging of all members of your category, and it may not even resemble a real instance (Posner & Keele, 1970). For example, the prototypical dog has an average size, an average tail, average ears, average coloring, and so on. When thinking about a category, you might also retrieve a specific instance of a concept or an exemplarexemplarA specific member of a category used to represent the category            exemplar        A specific member of a category used to represent the category            . This could be the dog that you raised during your childhood or a dog featured in your favorite movie. Pigeons appear to be able to form concepts, such as people and nonpeople. If their pecking is reinforced with food in the presence of an image containing people but not in the presence of images without people, they will learn to peck only when the people images are present. The experimenters used hundreds of images that showed people but varied in number, distance from the camera, age, ethnicity, and other features, but the pigeons still responded appropriately. The processes in which prototypes and exemplars are used are quite similar. Whichever standard you choose, you compare new instances to it to get a sense of fit. The more similar the new objects are to the standard, the more likely you are to include them in your concept. Both approaches are also able to account for the tendency of people to rate apples as more typical fruits than avocados or robins as more typical birds than penguins (Malt & Smith, 1984). Apples are not only closer to the average fruit than avocados are, but they are also more likely to be chosen as an exemplar fruit because of the extensive experience most people have with apples, at least in the United States. Despite the similarities between the prototype and the exemplar approaches, exemplars have some advantages. Compared to prototypes, exemplars provide a better way of thinking about the variability of a category (Rips & Collins, 1993). Averages, which characterize prototypes, do not provide much information about the range of features found in a category. For example, imagine that you just met Joe Kovacs, who tells you that he is an athlete. Judging from Kovac’s imposing physical appearance (he weighs 295 pounds), he seems more similar to your prototypical NFL football player than what he is—a star track-and-field athlete. In contrast, while your personal exemplar for a track-and-field athlete might be a sprinter or distance runner instead of a shot-putter like Kovacs, the individual instances that you know can help you comprehend the variability found within the concept. Although correct, this dictionary definition of a dog might not help you figure out whether an animal standing in front of you is a dog or not. Just when you are feeling comfortable about using prototypes and exemplars to solve your alien’s dog problem, you run into an additional dilemma. The use of prototypes and exemplars rests on similarity. Similar objects will be included in a concept, and dissimilar objects will be excluded. What if your new friend comes across a lifelike dog robot? In many ways, the robot looks and acts like your prototype and exemplar dogs, yet you know it is not a dog. What if you find an unfortunate dog that has been killed on the highway? It hardly retains much similarity to the prototype and exemplar dogs, yet nobody, except perhaps the alien, would question its identity as a dog. Clearly, you need more than prototypes and exemplars to solve your problem. Concept formation can be a type of theory building. In Chapter 2, we defined theories as sets of facts and relationships between facts that can be used to explain and predict phenomena. This definition can also apply to a concept. Like theories, concepts can guide our thinking and be continually tested for accuracy against new, incoming information. Also like theories, concepts do not exist in isolation. As we mentioned in Chapter 9, concepts can be viewed as part of a vast, interconnected network of memories. Thinking about concepts as theories provides insight into the problem of judging category membership, such as deciding whether an avocado is a fruit. Prototypes and exemplars provide a useful starting place for judging category membership. We test our theory that the new item (avocado) fits the category by comparing it to the prototypes (average fruit) and exemplars (apple) of a concept (fruit). We might make the occasional mistake, such as including bats or excluding ostriches in a concept of bird, but most of the time, we will be successful and fast. Using common features to identify dogs, such as has hair or has four legs, would exclude these two: a dog with three legs and an American hairless terrier. This approach also has the advantage of being consistent with observations of how children acquire new concepts, which we explore in Chapter 11. A child might start with a prototype or exemplar dog based on the family pet, possibly overapplying the concept to other four-legged animals at the zoo and failing to apply it to dogs of different breeds. With the help of feedback from others (No, that’s a tiger, not a dog or Yes, a Chihuahua is a dog), the concept becomes more refined. Your alien friend is likely to need the same type of feedback from you when attempting to apply the newly learned concept of dog to a variety of animals. Our dog concept is embedded in a rich, complex set of beliefs and expectations about dogs, animals, nature, and personal experience known as a schema, which we discussed in Chapter 9. This type of schema not only shapes memory storage and retrieval of information relevant to dogs but also allows us to predict new facts about them. If our schema includes likes to play ball, we know that playing ball with a friend’s new dog is likely to be successful. Under what circumstances do people apply a schema to new information? Again, comparisons to typical members of a category appear to play a role. People who are told a new fact about a typical instance of a category are more willing to extend the new fact to all members of a category, whereas a new fact about an atypical category member is less likely to be widely applied (Rips, 1975). For example, if you learn about a new fruit disease found in apples (an exemplar fruit for many people), you are more likely to think other fruits are vulnerable than if you learn that the disease targets a less typical member of the fruit category, such as olives or avocados. Apples are more likely than avocados to be a person’s exemplar fruit because of the extensive experience we have with apples in the United States. Can we find brain activity that correlates with thinking about a particular concept? In Chapter 9, we described imaging studies that identified different patterns of brain activity when people thought about animals or tools (Martin, Wiggs, Ungerleider, & Haxby, 1996). When asked to name animals, the participants showed activation in the visual cortex, suggesting that they would need to think about what a zebra looks like to name it correctly. Naming tools, in contrast, was accompanied by activation in frontal and parietal lobe areas associated with movement, implying that it is helpful to think about what to do with a screwdriver when attempting to name it correctly. You might mistakenly put Olympic shot-putter Joe Kovacs in your category of NFL football player because of his appearance, which is closer to the prototypical football player than it is to the prototypical track-and-field athlete. Using exemplars instead of prototypes to guide the use of concepts helps us think about the variability of a category. This specialization in the brain for processing different types of concepts is supported by observations of patients with brain damage. Some patients have specific difficulties naming pictures of animals, although they can successfully name other living things, such as fruits and vegetables, and nonliving things, such as tools and furniture (Mahon & Caramazza, 2009). Not only do these people have difficulty naming animals, but they also struggle to answer questions about animals, such as Does a whale have legs? Similar questions about nonliving things do not cause difficulty. Others demonstrate the opposite pattern. They are able to name living things but struggle to name nonliving things (Mahon & Caramazza, 2011; see Figure 10.2). These observations suggest that our brains are predisposed to distinguish between living and nonliving things. If forming concepts helps to organize appropriate responses, our ancestors’ ability to form some concepts quickly might have conferred a significant survival advantage. Some individuals with brain damage have difficulty processing certain types of semantic information. The pattern of these deficits varies depending on the location of the damage. For example, patient CW can name living things better than nonliving things, while patient RC names nonliving things better than living things. However, the categories that are affected in this way are relatively few: animals, fruits and vegetables, nonliving things, and members of our own species. This suggests that our brains have been shaped through natural selection to form certain types of categories. Even when a prototype is well understood, such as the signs and symptoms of a disease, evidence suggests that exemplars might work better. Physicians diagnosing cancer who had recently seen similar cases (exemplars) made more accurate diagnoses than those who had not (Brooks, 1990; Brooks, Norman, & Allen, 1991). Amyotrophic lateral sclerosis (ALS, or Lou Gehrig’s disease) is primarily a movement disorder, but the condition also affects a person’s ability to form new concepts. Performance by ALS patients on a concept formation task was correlated with the extent of gray matter loss they had experienced in the left prefrontal and parietal cortices (Libon et al., 2012). Although individual cases vary widely, current diagnostic criteria for ALS have been updated to incorporate features of frontotemporal dementia, a collection of symptoms associated with neural loss in the frontal and temporal lobes that includes deficits in cognition, language, and memory (Strong et al., 2016). Prototypes and exemplars depend on similarity, which can result in similar items, such as this robotic dog, being included in our dog category. It is unlikely that this toddler has vocabulary words for many of the animals seen for the first time in a zoo. Theories of what kind of animal this is will be shaped with the help of the child’s parents. Thinking helps us deal with the many types of problems we face daily. Whether you are figuring out how to manage a busy schedule, help a friend who is feeling blue, or complete your calculus homework, a problemproblemA situation in which a current state is separated from an ideal state by obstacles.            problem        A situation in which a current state is separated from an ideal state by obstacles.             exists whenever there is a difference between where you are and where you would like to be (Newell & Simon, 1972). For example, when you step on the bathroom scale after the holidays, the weight shown on the scale might be different from the ideal weight for your health and appearance; you now have a problem. In addition, referring to a situation as a problem implies that obstacles exist. Returning to your preholiday weight can be challenging. Problems can be well defined or ill defined. Well-defined problems have a solution that can be verified as correct or incorrect, like an algebra problem, whereas ill-defined problems have solutions that are evaluated subjectively (Le, Loll, & Pinkwart, 2013). Our weight-loss problem is well defined because it is easy to verify whether you lose the newly gained weight or not, but many problems in everyday life are ill defined. Thinking that you need to improve your appearance would be an example of an ill-defined problem. People might not agree about what makes another person more attractive. Problem solvingProblem solvingSee also Decision-making            Problem solving        See also Decision-making             is defined as the use of information to meet a specific goal (Lovett, 2002). As shown in Figure 10.3, problem solving begins with recognizing a problem exists (you now weigh too much) followed by the development and use of strategies that solve it (you go on a diet) and the evaluation of the success of those strategies (you have either lost the holiday weight or not). Ill-defined problems require additional thought about what constitutes an acceptable solution and how the solution should be justified. Reconfiguring an ill-defined problem into a set of well-defined problems can be helpful. If your ill-defined problem is to improve your appearance, you could divide this into well-defined problems like returning to your preholiday weight, eating five servings of fruits and vegetables per day, and exercising at least 30 minutes each day. Effective problem solving can be achieved by following a system proposed by  Polya (1957). If you reach step 4 and the problem is solved, you can exit the system. If the problem is not solved, you return to step 1 and try again. Because good problem-solving skills are important to psychological well-being, we will examine the process in detail using four steps (Polya, 1957): Understand the problem. Make a plan. Carry out the plan. Look back. Given the close relationship between cognitive psychology and computer science that we described in Chapter 1, we should not be too surprised to see problem-solving as a type of looping computer program. If we reach step 4 and our solution has taken us to where we want to go, we stop. If we reach step 4 and our problem still exists, we return to earlier steps to try another solution or generate new solutions. This looping of steps, reminiscent of looping statements in computer programs, continues until the problem is resolved. Problems that are more complex than our weight-loss example might require a systems engineer. Systems engineering is a Gestalt type of process for problem solving that considers the whole problem to be distinct from the parts (Ramo, 2005). Systems engineers use a looping program that differs from our four-step process in that evaluation takes place each step of the way (see Figure 10.4). Unlike our simple four-step problem-solving model, systems engineering evaluates progress along every step of the way, not just at the end of one attempted solution. Although carefully formulating a problem takes time, this is generally time well spent. All relevant data should be collected, analyzed, and organized. What do we know? What information do we need that is missing? How does all this information fit together? An important part of this step is to represent or frame the problem in a useful way (Lovett, 2002). Earlier in this chapter, we talked about forming mental representations of experiences that subsequently can be manipulated by the mind. In the case of problem solving, the mental representations we form relate to how we see the problem. For example, you might represent your weight problem as the result of your holiday splurge or as the result of a broken bathroom scale. These two representations clearly lead to different solutions, and in many cases, your representation will determine how successful you will be at solving the problem. Psychologists have identified several helpful suggestions for making the most useful representations of problems and for avoiding some common mistakes that frequently lead to failure. For example, big, long-term problems are easier to solve if they are approached through components or intermediate goals. Your primary goal in college is to obtain a degree, but it is useful to consider how you plan to meet intermediate goals such as completing your general education courses, your major courses, and electives. Like ill-defined problems, big complex problems, such as world hunger, can be tackled best by breaking the problem into smaller, intermediate problems. Personal biases can interfere with good problem solving. In our weight example, a reluctance to reduce treats might lead you to conclude that the bathroom scale is at fault instead of your behavior. Your amount of self-efficacy or belief in your abilities to succeed might also influence your approach to a problem (Bandura, 2001). You might believe that dieting doesn’t work, so there is no point in even trying. By recognizing these biases, you can approach your problem and its potential solutions with a more open mind. Another barrier to forming useful representations of a problem is functional fixednessfunctional fixednessA possible barrier to successful problem solving in which a concept is considered only in its most typical form.            functional fixedness        A possible barrier to successful problem solving in which a concept is considered only in its most typical form.            , or a person’s tendency to think about a concept in its most typical form and no others. In an emergency, a belt or a computer cord can be used to tie a door shut, and a pot of hot coffee can be a weapon. In an emergency, people need to use whatever they have in hand to solve problems. Functional fixedness might lead you to believe that a belt can only hold up your pants, but it can also make it harder for someone to come through a door. A plan to solve your problem involves generating possible solutions and then choosing the best one to implement. Generating possible solutions involves creativity, but most importantly, it requires time. People who spend the most time on a problem typically generate the most diverse solutions. Generating the most solutions raises your chances of finding one that will work. Several factors can interfere with finding appropriate solutions. You might be so upset about the numbers on the bathroom scale that you refuse to consider efforts to lose weight. Some people dislike risk so intensely that they fail to consider a full range of alternative solutions. To escape the chaos and the ambiguity involved with facing a problem, people can rush to find a solution, overlooking more appropriate courses of action. Judging and discarding solutions too early in the process also leads to failure. Generating impractical solutions requiring resources we don’t have is not useful. An expensive stay at a weight reduction spa might work, but it probably doesn’t fit a student budget. Understanding that we have these weaknesses in our approach to problem solving should help us avoid them in the future. Some types of problems lend themselves to precise, step-by-step rules for reaching a particular solution, known as algorithmsalgorithmsA precise, step-by-step set of rules that will reliably generate a solution to a problem.            algorithms        A precise, step-by-step set of rules that will reliably generate a solution to a problem.            . Algorithms have the advantage of producing an accurate solution reliably. Let’s assume you have a three-number combination lock on your bicycle, but you have forgotten the combination. You can start with using 001 for the first input, then 002, and so on, until you reach the correct solution for unlocking your bike. Algorithms are efficient when run by a computer, but their cost when used by the human brain can be high. You could be very late to your next class by the time you unlock your bike. Because of the time needed to use algorithms, we often substitute rules of thumb, or shortcuts to problem solving, known as heuristicsheuristicsA shortcut to problem solving; also known as a rule of thumb.            heuristics        A shortcut to problem solving; also known as a rule of thumb.            . Because heuristics do not go through the exhaustive evaluation of solutions required by algorithms, they are faster. Another advantage of heuristics is that they typically require far less information than algorithms do. However, unlike algorithms, heuristics do not guarantee a solution. When aggressive children are asked how to solve social problems, such as persuading another child to share a desirable toy, they generate fewer solutions than do more prosocial children. Eventually, most children will generate an aggressive solution (grab the doll and run), but perceiving additional options (trading a piece of candy for the toy or arguing that sharing is nice) might make it less likely that the prosocial child will use the aggressive solution. Recipes are examples of algorithms. If you follow the recipe step by step, you should be able to produce the cookies you want every time. Not only can heuristics fail to find a solution, they can lead us in the wrong direction. Amos Tversky and Daniel Kahneman identified several heuristics that can produce faulty decisions (Tversky & Kahneman, 1973,  1974). The first type, the availability heuristicavailability heuristicA rule of thumb in which the frequency of an event’s occurrence is predicted by the ease with which the event is brought to mind            availability heuristic        A rule of thumb in which the frequency of an event’s occurrence is predicted by the ease with which the event is brought to mind            , is used when people predict that events that are easy to think about will be more frequent. For example, dramatic media reports make people think shark attacks are more common than they are. If we asked which is more common, being killed by a shark or being killed by falling airplane parts, which would you choose (see Figure 10.5)? You might be surprised to learn that you have 30 times the risk of being killed by falling airplane parts than by sharks (Plous, 1993). In the aftermath of 9/11, with its vivid imagery of airplanes crashing, more Americans chose to drive rather than fly. The extra traffic that resulted from fear of flying led to an estimated 9% increase in automobile fatalities, compared to typical rates, in the 3 months following the attacks (University of Michigan Transportation Research Institute, 2004). Which is more likely? Being hit by an airplane part or being attacked by a shark? The availability heuristic leads us to believe that events that are easy to think about are more frequent. Because of the frequent media coverage of shark attacks, such as Rodney Fox’s close call with a great white in Australia, people think shark attacks are common. You might be surprised to learn that you are 30 times more likely to be killed by falling airplane parts, like this turbine engine that fell from the sky onto a car in Brazil, than by a shark. As we mentioned earlier in this chapter, people often form prototypes or average examples when thinking about a category. These prototypes then represent the category when we are deciding which new examples might also fit the category. Tversky and Kahneman’s second type of heuristic, the representativeness heuristicrepresentativeness heuristicA rule of thumb in which stimuli similar to a prototype are believed to be more likely than stimuli that are dissimilar to a prototype.            representativeness heuristic        A rule of thumb in which stimuli similar to a prototype are believed to be more likely than stimuli that are dissimilar to a prototype.            , leads people to estimate that stimuli similar to a prototype are more likely to fit the category than are stimuli different from the prototype. To illustrate this heuristic, Tversky and Kahneman asked research participants whether Thomas, who is short, slim, and loves poetry, is more likely to be an Ivy League classics professor or a truck driver. Most people assumed that Thomas would be a classics professor because his description is closer to that of a prototypical professor than a prototypical truck driver. However, this conclusion is statistically quite unlikely. There are few Ivy League classics professors, and even if most fit Thomas’s description, the numbers would still be quite low. In contrast, there are thousands of truck drivers in the United States. Even if a small proportion of them shared Thomas’s characteristics, the numbers would be large. Consequently, the odds are that Thomas is a truck driver. Heuristics are most likely to fail when a correct solution requires a sophisticated understanding of probability (Fenton & Neil, 2012). For example, a classic American television show originating in the 1960s called Let’s Make a Deal usually ended with a contestant facing three doors (Behind door number one…) and knowing that a huge prize, such as a car, was behind one door, and worthless prizes were behind the other two. After the contestant selects a door, but before its contents are known, the host reveals that one of the unchosen doors has a worthless prize and asks whether the contestant wishes to switch. Most people erroneously stick with their first door in the mistaken belief that they now have a 50% chance of winning. In fact, their chance of winning with their chosen door remains one-third, but they have a two-thirds chance of winning if they switch. Here’s why. When the contestants first choose, their chance of winning is one-third. When the host reveals another door, he must reveal one of the losing doors no matter what the contestant chose first. If the contestants initially guessed correctly and stay with their original decision, their chance of winning remains one-third. However, if the contestants guessed incorrectly when first asked, switching now guarantees a win, making their chance of winning two-thirds if they switch. The trick here is that all the options seem equal (there is a one-third chance of winning, the host reveals one-third of the unknown, and the contestant can switch to the other one-third option), yet they are not truly so (Fenton & Neil, 2012). Although some of these examples make it sound like heuristics lead to bad decisions, the ability to use heuristics to make quick, effective, and efficient decisions was a significant adaptive advantage for our ancestors (Haselton et al., 2009). An example of an effective heuristic is the recognition heuristicrecognition heuristicA rule of thumb in which a higher value is placed on the more easily recognized alternative            recognition heuristic        A rule of thumb in which a higher value is placed on the more easily recognized alternative             (see Figure 10.6), which predicts that people will place a higher value on the more easily recognized alternative (Gigerenzer, 2008). In one study, American college students were asked which of two cities was larger. In making size decisions about German cities, such as Hamburg and Munich, students tended to choose the city they recognized most (Goldstein & Gigerenzer, 2002). They were correct about 73% of the time (Hamburg has about 1.8 million residents compared to Munich’s 1.3 million). The recognition heuristic has been shown to beat more conventional means of selecting stocks. Companies whose names were recognized by the most people also had stocks that performed better than the overall markets (Reips, 2006). The recognition heuristic predicts that if you think the film Finding Dory sounds more familiar than the film Keeping Up with the Joneses, you will correctly judge Finding Dory as generating more international revenue for the producers than Keeping Up with the Joneses. Once we have generated a set of solutions, we must decide which one to try first. How is this decision reached? The affect heuristicaffect heuristicA rule of thumb in which we choose between alternatives based on emotional or gut reactions to stimuli.            affect heuristic        A rule of thumb in which we choose between alternatives based on emotional or gut reactions to stimuli.             is particularly relevant to the way people make important choices (Slovic, Peters, Finucane, & MacGregor, 2005). According to this approach, we use our emotional responses to each choice to guide our decisions. Based on our past experience with similar choices, we develop a gut reaction to our options, pushing us toward alternatives we expect to produce desirable outcomes and away from alternatives we expect to produce undesirable outcomes. Our decision making is also affected by the need to avoid the emotion of regret (Connolly & Zeelenberg, 2002). By considering ways to justify the reasons for their decisions in advance, people can live more comfortably with the decisions they have made. An alternate approach to decision making, the utility theory, is also popular in economics. According to this theory, we compute the expected outcomes of our choices and select the best likely one. The expected outcome is computed by multiplying measures of the usefulness of the outcome by its expected probability. For example, to choose an elective course for your next term, you might consider a course that provides positive outcomes, such as being interesting and satisfying a graduation requirement. At the same time, you wish to avoid negative outcomes, such as getting a poor grade or spending a lot of money on materials. If you weight each outcome according to its probability and add up the results, you will end up with your own mini cost–benefit analysis. The representativeness heuristic makes us believe that stimuli similar to a prototype are more likely than stimuli that are dissimilar. Katrina Hodge, 2009 Miss England, may not fit your prototype of a Lance Corporal and Iraq War veteran. However, there are many more soldiers than beauty queens, so the likelihood that a beautiful woman is a soldier is higher than the likelihood that she is a beauty queen. You might find that applying utility theory is useful when faced with an important decision, such as whether to accept a job offer with modest pay in your hometown or one with higher pay in another state. It is clear, however, that we rarely make decisions by solving equations. Even when utility is held constant, people show a preference for one solution or the other based on how the solutions are framed. People rate basketball players who are described as making 75% of their free throws higher than those who are described as missing 25% of the time, even though the math is identical in both cases. Medical treatments that are described as effective 80% of the time are viewed more positively than those that fail 20% of the time. Even though we might carefully weigh the pros and cons of buying one laptop over another, the affect heuristic suggests that we use our emotional or gut responses to make decisions too. Tversky and Kahneman (Tversky & Kahneman, 1987) illustrated the importance of framing in decision making using an Asian disease problem. Even though the outcome in both situations is that 400 people will die, 72% of respondents preferred a more secure solution when the problem was framed using the word saved. In contrast, when the problem was framed using the word die, the secure solution was preferred by only 22% (see Figure 10.7). As we mentioned earlier, the way a problem is represented or framed can have a large effect on whether it will be solved. Framing can affect the choice of a solution through its interaction with a person’s willingness to select risky solutions. When a problem is framed in terms of losses, as in the will die framing of the Asian disease problem, people are more comfortable choosing the risky solution. In contrast, when a problem is framed in terms of gains, as in the saved frame of the Asian disease problem, people generally become more cautious. If we have already saved 200 people, we don’t want to risk saving fewer. Amos Tversky and Daniel Kahneman’s classic Asian disease problem asks people to choose one of two solutions to the following problem: If a disease was coming to the United States, and 600 people were expected to die, what should the president do? Both solutions resulted in the deaths of the same number of people (400), but one was phrased positively (200 will be saved) and the other was phrased negatively (400 will die). When the question was framed negatively, people were more likely to choose the riskier alternative. Good decision making is a skill that can be taught (Baron & Brown, 1991), and those who have received formal training in decision making make better real-world choices (Larrick, Nisbett, & Morgan, 1993). Psychologists have identified skills that predict decision-making competence, such as a good understanding of probability (Parker & Fischhoff, 2005). How well do these decision-making skills predict behavior in the real world? When participants’ performance on the identified skills was compared with real-life decision making, such as buying new clothes or shoes they never wore, strong correlations were observed between their abilities on the identified skills and the quality of their real-world decision making (Bruine de Bruin, Parker, & Fischhoff, 2007). Your belief that you can implement a particular solution will determine whether you decide to pursue that solution or look for another solution. Once you have decided on a solution, it is time to try it out. Measures of decision-making skills, such as a person’s understanding of probability, are negatively correlated with poor real-world decision making, such as the number of times you buy things you never use. At this point, one of the most critical elements in successfully implementing your solution is planning. Solutions that might otherwise succeed can fail when people do not anticipate the time and the resources needed for implementation. It is one thing to decide that you need to exercise more to lose your holiday weight, but without further planning, this solution might not work. How much exercise will you need? What activities should you do? When? Where? Is your solution practical? If you are a couch potato, thinking you will implement your exercise solution by training for next month’s marathon might not be realistic. If time management is not your strongest skill, you might consider tools like an online calendar or a bullet journal. Many techniques for helping you manage your time are available, including online calendaring and bullet journals, such as the one shown in this image. As you implement your solution, you need to know whether it is bringing you closer to your goal, which usually means that you need some measureme of success. In the example of holiday weight gain, the logical way to assess progress is to get back on the scale at regular intervals. At the same time, solutions usually require time to work, and a failure to make immediate progress should not be the basis for discarding a promising solution. Evaluation is an ongoing process, but it is especially important once a solution has been fully implemented. You might have predicted that 6 months of exercise should return you to your goal weight. At the 6-month point, we return to the original problem. Does it still exist? If not, the problem has been solved, and you can move on to other issues, although keeping up your exercise program is always a good idea. If the problem still exists, you return to the make-a-plan step and try again. Maybe this time you need to combine exercise with counting calories. People have a reasonably effective commonsense approach to making decisions, but again, careful psychological research can point out where we are vulnerable to poor decision making. If you find yourself unhappy with outcomes of some of your previous decisions, following the steps in this section on future occasions might help you find better solutions. You might recall hearing about the epic battle between world chess champion Garry Kasparov and IBM’s Deep Blue that ended in defeat for Kasparov in 1997. More recently, AlphaGo defeated several champion Go players (Silver et al., 2016). What makes this a remarkable achievement for AlphaGo’s programmers is the possibility of possible moves in the typical game of Go, dwarfing the already huge number of options in the typical game of chess. This remarkable achievement parallels the four problem-solving steps we reviewed in this section. The programmers began by helping their AlphaGo neural network to understand the problem by training it on 30 million board positions taken from real-life games on a Go database. The neural networkmade plans by estimating the likelihood that a particular move will lead to a win. In the next step, these plans werecarried out repeatedly. Most importantly, the program contained alook back strategy the programmers refer to as deep reinforcement learning while playing itself (see Figure 10.8). Now, instead of evaluating moves from the real-life games, AlphaGo generates its own moves and estimates the likelihood that each move will lead to a win, leading to an ongoing tsunami of improvement in the program’s capability. The AlphaGo program recently beat one of the world’s best human Go players, a feat considered nearly impossible with current technology due to the number of possible moves that characterize the game. The neural network program combined policies, or the selection of moves, with values, which predict whether a move will produce a win. Does this mean that our brains are destined to be outmatched by the machines? Not quite yet. Recall that even AlphaGo needed input from real-life games played by real-life people to get started. What processes can we observe in the brain during decision making? As we have seen on previous occasions in this textbook, observing the effects of brain damage on a behavior can illuminate the contributions of particular parts of the brain to that behavior. Damage to the frontal lobes of the brain, as in the case of the unfortunate Phineas Gage described in Chapter 4, reliably produces impulsivity and poor judgment. Patients with this type of damage often begin to gamble obsessively, turn their personal lives upside down, and become unemployable. Sometimes we need to make fast decisions, perhaps at the expense of accuracy. Different levels of activity in the subthalamic nucleus, part of the basal ganglia (see Chapter 4), are associated with fast and more cautious decision making (Herz et al., 2017). At other times, you might feel like you are of two minds when faced with a decision, and this is not so far from the truth. In reality, three major brain circuits interact during decision making (Goschke, 2014). One circuit assigns value to situations along the lines of pleasure or pain and involves the ventromedial prefrontal cortex, the orbitofrontal cortex, the nucleus accumbens, and the amygdala. A second circuit is an impulse control network that controls unwanted responses and includes the lateral prefrontal cortex and the parietal cortex. Finally, an attentional circuit monitors significant stimuli and involves the insula, the anterior cingulate cortex, and the amygdala. This circuit also seems to be involved with more complex social decision making, such as deciding whom to trust (Rilling & Sanfey, 2011). The value circuit, which is impulsive, gradually comes under the control of the impulse control circuit as we mature and are exposed to the social rules of our community (Bechara, 2005). As college students, you are a prime example of this maturity. Spending long hours studying and writing papers produces immediate pain but also promises the future pleasure of obtaining an interesting job and making enough money to support yourself and perhaps a family. Making simple decisions about what to keep and what to throw away can be difficult for some individuals. Hoarding behavior is correlated with unusually low levels of activity in the anterior cingulate cortex (Saxena et al., 2004). Drug addiction provides an interesting example of what happens when these normal decision-making circuits are disrupted. In many ways, drug addicts behave similarly to people who have brain damage in circuits related to decision making. Through their experience with drugs and possibly because of some biochemical consequences of addictive substances, the impulsive system gains control over the impulse control system. The immediate pleasure promised by the drug experience overwhelms the impact of negative future events such as lost jobs, disrupted family life, and even prison. Addicts begin to experience exaggerated responses to the potential reward of drug cues while becoming less responsive to other types of reward (Bechara, 2005). Hyperactivity in this impulsive reward system seems to progressively weaken impulse control. As you will see in Chapter 11, the prefrontal cortex is one of the last parts of the brain to mature (development continues into the early 20s), which might account for some of the impulsive, risky decisions made by adolescents (Blakemore & Robbins, 2012). At the same time that their impulse control system is relatively weak, adolescents might be hypersensitive to reward. Developmental changes in brain structure and function also account for the effects of peers on adolescent risk taking. Teens and young adults make about the same number of risky decisions while alone, but teens are more likely than young adults to make risky decisions in the presence of their peers. As shown in Figure 10.9, parts of the reward circuit were especially active in adolescents making a risky decision in the presence of peers.(a) The stoplight driving game is a simulation in which participants are told to reach the end of a track as quickly as possible. (b) Adolescents driving alone do not make more risky decisions than young adults or older adults, but they make many more risky decisions in the presence of a peer. (c) The ventral striatum, in orange, is part of the brain’s reward pathways. It is also crucial to addiction. (d) This graph shows the amount of change in the ventral striatum for the three age groups in the alone and peer-present conditions. Many variables influence an individual’s approach to decision making. The instrument in Table 10.2 was designed to identify which type of style you use (Scott & Bruce, 1995). Listed below are statements describing how individuals go about making important decisions. Please indicate whether you agree or disagree with each statement. Strongly DisagreeSomewhat DisagreeNeither Agree nor DisagreeSomewhat AgreeStrongly Agree1. I double-check my information sources to be sure I have the right facts before making a decision.123452. When making a decision, I rely upon my instincts.123453. I often need the assistance of other people when making important decisions.123454. I avoid making important decisions until the pressure is on.123455. I generally make snap decisions.123456. I make decisions in a logical and systematic way.123457. When I make decisions, I tend to rely on my intuition.123458. I rarely make important decisions without consulting other people.123459. I postpone decision making whenever possible.1234510. I often make decisions on the spur of the moment.1234511. My decision making requires careful thought.1234512. I generally make decisions that feel right to me.1234513. If I have the support of others, it is easier for me to make important decisions.1234514. I often procrastinate when it comes to making important decisions.1234515. I make quick decisions.1234516. When making a decision, I consider various options in terms of a specific goal.1234517. When I make a decision, it is more important for me to feel the decision is right than to have a rational reason for it.1234518. I use the advice of other people in making my important decisions.1234519. I generally make important decisions at the last minute.1234520. I often make impulsive decisions.1234521. I explore all my options before making a decision.1234522. When I make a decision, I trust my inner feelings and reactions.1234523. I like to have someone to steer me in the right direction when I am faced with important decisions.1234524. I put off making many decisions because thinking about them makes me uneasy.1234525. When making decisions, I do what seems natural at the moment.12345For each item, you will choose a score between 1 (strongly disagree) and 5 (strongly agree). When you’re done, add up your scores for the following groupings: Group R: 1, 6, 11, 16, 21Group I: 2, 7, 12, 17, 22Group D: 3, 8, 13, 18, 23Group A: 4, 9, 14, 19, 24Group S: 5, 10, 15, 20, 25The question groups are relevant to the decision styles in this order: Rational, intuitive, dependent, avoidant, and spontaneous. While you may have a dominant style, these scores are not mutually exclusive. People use multiple styles depending on the amount of information they have at hand and the timing of the decision. Rational decision-makers use reasoning and logic to arrive at solutions, while intuitive decision-makers rely on emotion and feelings. Dependent decision-makers rely on other people to point them in the right direction. Avoidant decision-makers postpone making any decision while hoping a situation will resolve without any actions on their part. Finally, spontaneous decision-makers focus on the immediate needs of a situation. Do your results fit with your experience? Are you generally satisfied with the decisions you make? In this chapter, we argue that decision-making ability is something you can learn to improve, so if you are not pleased with many of your past decisions, it is possible to change. Type of heuristicDefinitionExampleAvailabilityEvents that are easy to think about are more likely. Despite news and missing child reports that make kidnappings appear to be common, fewer than 100 children per year are abducted by strangers in the United States. RepresentativenessStimuli that are similar to a prototype are more likely than stimuli that are different from the prototype. A big, muscular student at Ohio State University is probably a football player (even though fewer than 100 of Ohio State’s 60,000-plus students are on the football team). RecognitionA more recognizable stimulus has a higher value. Wine labeled as California wine is rated higher than the same wine labeled North Dakota wine. AffectWe use an emotional response (gut feeling) to choose one alternative over another. Eating dessert instead of staying on your diet is an emotional choice. LanguageLanguageA system for communicating thoughts and feelings using arbitrary signals.            Language        A system for communicating thoughts and feelings using arbitrary signals.             is a system for communicating thoughts and feelings using arbitrary signals, such as voice sounds, gestures, or written symbols. As we discussed previously, language provides us with powerful tools for organizing and manipulating our thinking, problem solving, and decision making. Although thoughts can be represented visually as well as verbally, language extends our thinking abilities to abstract concepts such as truth and beauty that would be difficult to visualize. Above all, language connects us with others. Not only can we communicate with people in our immediate vicinity, but language spans time and distance, allowing us to share the thoughts of people living long ago and in distant places. Because of language, the thoughts we record today might reach into the future to influence the thinking of people not yet born. Language both reflects and shapes thought. Benjamin Lee Whorf’s hypothesis of linguistic relativity examines the effect of having a rich vocabulary on a person’s ability to think about a topic (Whorf, 1956). According to Whorf, a skier who can name and identify powder, slush, and other variations of snow thinks differently about snow than does a person born and raised in a tropical climate. Whorf’s theory correctly predicted that the use of gender-free words such as server instead of waiter or waitress or flight attendant instead of stewardess would reduce gender stereotyping and discrimination (Sczesny, Formanowicz, & Moser, 2016). Further efforts to address the binary nature of English personal pronouns (he or she) to better reflect a continuum of gender identity have led to the use of completely gender-neutral pronouns (Miltersen, 2016). According to Benjamin Lee Whorf, a skier who knows what early sintering means, is able to think differently about snow than a person who is unfamiliar with the term. Sintering occurs when snow crystals break down and fuse into larger crystals. To fix the emergence of human languages in time, we can assume that certain complex social behaviors would be difficult to conduct without the ability to speak. Elaborate tool use and cooperation would be easier to accomplish using language. Because our version of the human species, Homo sapiens, appeared between 100,000 and 200,000 years ago, most anthropologists are convinced that human language existed at that time, if not earlier. A critical gene mutation in the FOXP-2 gene occurring around 100,000 years ago possibly marked the start of modern language (Corballis, 2004). Regardless of timing, the enormous advantages of language to human culture and cooperation would ensure its continuity. Much evidence points to Africa as the source of the first human languages. Some anthropologists suggest that it would be difficult for early humans to migrate successfully to other continents without the cooperation provided by language. Taking a different approach, an analysis of more than 500 contemporary languages has demonstrated that the number of speech sounds in a language decreases systematically with the culture’s distance from Africa along migration routes (Atkinson, 2011). Many African languages feature more than 100 speech sounds, compared to 45 in English and 13 in Hawaiian (see Figure 10.10). The number of phonemes or speech sounds per language decreases with the distance of the culture using the language (indicated by shades of red) along migration routes from the western coast of Africa. Many African languages feature more than 100 speech sounds, compared to 45 in English and 13 in Hawaiian. Language is intimately connected to cultural values. For example, the English language contains many words that signify the passage of time. In contrast, the Hopi Native American tribes have only two words relevant to time, loosely translated as sooner and later (Le Lionnais, 1960). What does this difference tell you about the two cultures? Languages are living entities, under constant pressure to change. Nearly half the world’s spoken languages may soon be lost. Following assimilation into larger cultures, speakers stop using their native languages or fail to transmit them to their children (Malone, 2006). With each language lost goes an opportunity for scientists to understand the unique local knowledge of a culture, along with the historical and cognitive implications of the language. Language works like a set of building blocks, beginning with basic speech sounds (phonemes), which are combined into meaningful units (morphemes) and then combined in meaningful strings according to the rules of grammar. Humans produce more than 500 phonemesphonemesA speech sound.            phonemes        A speech sound.             or speech sounds, such as the c in cat. The number and types of phonemes are limited only by the physical features of the human vocal apparatus. Not all phonemes appear in all languages. Individual world languages vary from as few as 11 phonemes to more than 140 (Holt, Lotto, & Kluender, 1998). English features about 45 phonemes, which we represent visually using the 26 letters of the alphabet either singly (s) or in combination (sh). Most written languages attempt to match visual symbols to phonemes, although many new readers of English struggle with the different pronunciations represented by the same letters, as in the ough in through, rough, trough, slough, or thought. Phonemes are combined into morphemesmorphemesThe smallest component of speech that carries meaning.            morphemes        The smallest component of speech that carries meaning.            , which are the smallest components of speech that carry meaning. Most morphemes are words, but word prefixes (as in preschool and suburban) and word endings (as in dogs and walked) also qualify as morphemes because they change the meaning of the root word. English contains approximately 100,000 morphemes, which can be used to produce more than a million words. The average 20-year-old native English speaker knows about 42,000 words, in addition to thousands of proper nouns (names) and other versions of the main words (Brysbaert, Stevens, Mandera, & Keuleers, 2016). Education and age increase this number to a high of nearly 60,000 words (see Figure 10.11). Click languages include sounds made by clicking the tongue and might be among the earliest forms of language used by humans. Two trends characterize the growth of vocabulary: age and education. A gradual increase in vocabulary takes place over the adult lifespan. People’s level of education is also predictive of the number of words they know. Morphemes are combined into phrases and sentences according to rules of grammar. For example, different languages have their own rules about the meaningful ordering of nouns and adjectives. In English, you say the brown dog but not the dog brown, while the reverse is true in Spanish. This type of natural grammar is usually learned early in childhood by interacting with other speakers, in contrast to the formal grammar instruction featured in schools. As we discussed in Chapter 4, language is typically managed by the left hemisphere of the brain. Further insights into the localization of language processing result from investigations of brain damage and efforts to teach language to nonhuman animals. We can learn about the biology of language by surveying problems that arise because of brain damage resulting from head injury or stroke. AphasiaAphasiaThe loss of the ability to speak or understand language            Aphasia        The loss of the ability to speak or understand language             is the loss of the ability to speak or understand language. The closely related processes of reading and writing are often, but not always affected by aphasia. In 1861, Paul Broca conducted a case study of a 51-year-old man named Louis Leborgne, who had been institutionalized for more than 20 years (Domanski, 2013). Leborgne came to be called Tan, because when questioned, tan was one of a few syllables he could produce. Leborgne entered the hospital after several months of being unable to speak. He apparently understood much of what was said to him and retained his ability to answer numerical questions by raising an appropriate number of fingers on his left hand (Herrnstein & Boring, 1965). After Leborgne died, Broca performed an autopsy on his patient’s brain and found significant damage to the patient’s left frontal lobe, possibly because of Leborgne’s long history of epilepsy. The damaged area is now called Broca’s area in honor of Broca’s discovery (see Figure 10.12). Today, Leborgne would be diagnosed with Broca’s aphasia. This condition is characterized by difficulty producing speech. The speech that the patient manages to produce is slow and effortful, but it generally makes sense. In some cases, patients retain the ability to curse, as in one of Broca’s other patient’s Sacre nom de Dieu! (In the name of God!), which he uttered in frustration when unable to make himself understood with gestures. A contemporary patient with Broca’s aphasia described his condition by saying, Speech … can’t say … talk, you see (Gardner, 1976, p. 61). About 13 years after Broca presented his revolutionary work on Leborgne, Carl Wernicke published his observations on another type of language deficit (Wernicke, 1874). In honor of his contributions, this syndrome is now called Wernicke’s aphasia, and the affected area of the brain is known as Wernicke’s area. Wernicke’s area is located near the primary auditory cortex, located in the temporal lobe. The symptoms of Wernicke’s aphasia could hardly be more different from those of Broca’s aphasia. Where Broca’s aphasia affects the production of speech, Wernicke’s aphasia affects its comprehension. In Broca’s aphasia, speech is slow and laborious but generally meaningful. In Wernicke’s aphasia, the opposite is true. Speech is rapid and fluent but virtually meaningless. A contemporary patient with Wernicke’s aphasia said,Oh sure, go ahead, any old think you want. If I could I would. Oh, I’m taking the word the wrong way to say, all of the barbers here whenever they stop you it’s going around and around, if you known what I mean, that is typing and tying for repucer, repuceration, well we were trying the best that we could while another time it was with the beds over the same thing. (Gardner, 1976, p. 68)If you don’t pay attention to the meaning, the speech of patients with Wernicke’s aphasia sounds normal, if slightly fast. Grammar is generally correct. On further examination, we find that meaningfulness is lacking. These patients are locked into a world without social connection, yet they do not seem aware of their circumstances or in apparent distress. It is simplistic to assume that the brain has only two main language centers, Broca’s and Wernicke’s areas. Instead, contemporary research has identified complex pathways for processing language that connect Broca’s and Wernicke’s areas to other cortical areas involved in cognition (Dronkers, Pinker, & Damasio, 2000;  Friederici, 2011). Later in this chapter, we discuss several approaches to learning language. The evolutionary approach suggests that language results from some innate capacity shaped by natural selection. Animals and human infants demonstrate similar perceptual grouping, or the linking together of sounds with certain characteristics like the tick-tock of a clock (Toro, 2016). This evolutionary view implies that we should find precursors of human language in the behavior of other animals. Many animals communicate with each other, often in complex ways. All forms of communication, however, do not involve the flexibility and the creativity of language. We can identify three major patterns of animal communication (Dronkers et al., 2000). One is an inflexible group of calls for signaling danger and identifying territories. A second contains signals that communicate magnitude, as in the case of bee dances. Finally, animals communicate through sequences of behavior, as in the case of birdsong. We don’t know to what extent these animal forms of communication may have served as precursors for human language. The most logical place to look for precursors to human language is the great apes. Chimpanzees, bonobos, and gorillas have a part of the brain analogous to the human Broca’s area. In both humans and apes, this area shows a difference in size between the right and the left hemispheres that might be correlated with language ability (Cantalupo & Hopkins, 2001). Many researchers have attempted to teach human languages to apes. In 1931, Winthrop N. Kellogg and his wife adopted a baby chimpanzee, Gua. Because of the limitations of the chimpanzee vocal apparatus, efforts to teach Gua to talk were doomed to failure (Kellogg & Kellogg, 1933). Allen and Beatrice  Gardner (1969) taught sign language to a chimp named Washoe. After 4 years of work, Washoe had mastered 132 signs. The Gardners’ efforts were followed by Francine  Patterson (1978), who trained a gorilla named Koko to use signs. Sue Savage-Rumbaugh and her colleagues (1998) have successfully taught a bonobo chimpanzee named Kanzi to associate geometric symbols with words. Even before his training began, Kanzi learned 10 symbols simply by observing his mother’s training sessions, although his mother never quite mastered the task. Kanzi also seems to be able to understand some human speech. When given 660 verbal requests, such as Put the collar in the water, Kanzi behaved correctly 72% of the time. In addition to investigations of language behavior in apes and monkeys, Irene  Pepperberg (2014) makes a strong case for her African grey parrots. While we use the phrase to parrot to indicate mindless mimicry, Pepperberg argues that her parrots demonstrate complex cognitions related to language. Others have proposed that dolphins, whales, dogs, and prairie dogs possess language capabilities (Bloom, 2004;  Caldwell, 1976; Janik, 2006;  Lilly, 1967;  Slobodchikoff, Perla, & Verdoli, 2009). The soundtracks of horror films, adventure films, dramas, and war films use sound features that characterize animal distress calls to heighten viewer arousal. It is likely that these sounds add to the fear produced by the visual images (Blumstein, Davitian, & Kaye, 2010). Whether animals demonstrate real language abilities is the subject of ongoing debate. Although children build vocabularies spontaneously, critics point out that ape language must be taught laboriously (Terrace, 1979). Word order does not seem to matter to apes, although it has an essential role in human language. Trainers of the apes might be making biased observations. When Washoe signed waterbird while observing a swan, the Gardners concluded that she was making a new, creative observation, but Washoe may simply have noticed a bird sitting on the water. Human infants use cognitive strategies for language learning that are not found in other animals (Toro, 2016). For example, infants pay more attention to consonants than to vowels by the end of their first year. Consonants are often more important sources of information for identifying words than vowels, as you probably already know from the ease with which you interpret txt mssgs. Whether we believe that animals have true language or not, we retain an enormous respect for both the complexity and the intelligence of animal behavior and the remarkable sophistication of human language. Learning language occurs differently than many types of learning. No specific instruction is needed, as it is for the related skills of reading and writing. A typical child exposed to language will learn language. Language learning begins very early. Korean children adopted by Dutch parents showed evidence of having retained information about the Korean language from prenatal exposure (Choi, Cutler, & Broersma, 2017). Early in life, between 6 and 12 months of age, children make an important perceptual shift that impacts their subsequent language learning. They become less successful in perceiving the differences between sounds that are not characteristic of their native language while becoming more successful at perceiving differences between sounds that do occur regularly in their native language (Kuhl, 2011). Underlying this transition is a type of computational or statistical learning, in which the infant pays attention to the frequencies of each type of sound they hear (see Figure 10.13). Efforts to teach sign language to chimpanzees have been more successful than efforts to teach them to speak. Here, the human instructor is teaching the chimpanzee to sign eye. In the second half of their first year, infants use social interaction as a gate for conducting computational evaluations of how often they hear certain speech sounds. As a result, they become less sensitive to sounds that are not in their native languages and more sensitive to those that are. These graphs show an idealized distribution of the frequencies of the /r/ and /l/ sounds in English and the Japanese /r/. Not just any type of exposure to language will engage the infant’s computational learning. Social interaction is an essential component of this process, and two-way interaction in the form of conversation produces maximum results. Because language evolved to allow communication with others, social interaction may serve as a gate for initiating the infant’s computational learning system (Kuhl, 2011). Conversing with children produces greater language competence than reading to them (Zimmerman et al., 2009). Even when exposure is one-way, such as when a child is listening to a speaker, more language learning occurs when a real person is speaking with the child face to face than when a child is listening to the same speaker on television (Kuhl, 2007;  Kuhl & Meltzoff, 1982). This finding implies that passive television or computer videos are unlikely to improve a child’s language learning. Irene Pepperberg’s African grey parrots have demonstrated sophisticated languagelike behaviors. For example, when shown an object, Alex could correctly state its color, shape, and material. After only six repetitions, he correctly referred to himself as gray when asked his color. Observations of the speed and ease with which very young infants can learn complicated language tasks have led to the assumption by many, including linguists Noam Chomsky (Chomsky, 1957) and Steven Pinker (Pinker, 1994), that humans have an inborn capacity for learning language. No human culture on Earth exists without language. Language acquisition follows a common course regardless of the native language being learned. Whether a child is exposed to English or Cantonese, similar language structures appear around the same point in development. For example, children all over the world go through a stage in which they overapply language rules, as in the case of a child saying goed instead of went. Eventually, the child switches to the correct forms, long before formal instruction. In addition, human brain structure is correlated with language. As we discussed in Chapter 4, most people process language in the left hemisphere of the brain. Damage to these left hemisphere language areas because of a stroke or other brain injury produces specific types of language deficits. Two-way interaction in the form of conversation produces the fastest language learning, reinforcing the social nature of this behavior. It is not necessary to assume, however, that we have a unique language module in the brain to account for the development of human language. Many linguists believe that spoken language was built on existing structures that allow primates to produce gestures (Corballis, 2009). Simultaneous vocalizations and gestures (pointing and saying look) could have been followed by a greater reliance on vocalizations, which is the same progression we see in children. Vocalization requires less energy than gesturing, as any sign language interpreter can tell you. It also allows communication to occur at night or when hidden, and it frees the hands for tool use and hand-to-hand combat. Assuming an inborn capacity for learning language further implies that language has a genetic basis. Several genetic conditions selectively affect individuals’ language-learning abilities. A family known as the KE family has members who have severe difficulties with the production of language accompanied by a mutation in the FOXP2 gene (Lai, Fisher, Hurst, Vargha-Khadem, & Monaco, 2001). This is the same gene suspected of mutating about 100,000 years ago, making modern human language possible. The FOXP-2 gene appears to be critical for learning to produce vocalizations (White, 2013). Socioeconomic differences impact language acquisition. By the age of 2 years, English-learning children from low-socioeconomic status Caucasian families were already 6 months behind children from wealthier homes in both vocabulary and language understanding (Fernald & Weisleder, 2015). Hart and  Risley (1995) proposed a language gap, characterized by the exposure to as many as 30 million more words to children in wealthier families than in poor families by the age of 3 years. This report, widely cited in the popular press, was consistent with a cultural deficit model, in which the poor performance of children from disadvantaged families resulted from low levels of cognitive stimulation. The cultural deficit model has been widely criticized as blaming parents for their children’s poor school performance and being insensitive to cultural differences in the ways parents interact with children. Quantity and quality of speech directed at young children are highly correlated, and Hart and Risley noted that quantity of words was just one indicator of possible quality of language exposure. However, the importance of the quantity of exposure remains hotly debated (Baugh, 2017). Many researchers argue that specific aspects related to the quality of the language input to the child, more so than its quantity, predicts later language skills (Ramírez-Esparza, García-Sierra, & Kuhl, 2014). For example, using parentese, or language deliberately simplified for use in communicating with children, was associated with better child language outcomes. The use of rich vocabulary, complex ideas, and back-and-forth conversation promotes language growth in children (Fernald & Weisleder, 2015). Efforts to build parents’ skills in nourishing their children’s language development can bring benefits to all, regardless of socioeconomic status. In most cases, individuals with intellectual disability experience difficulties with language, but individuals with Williams syndrome are fluent speakers with large vocabularies. Williams syndrome and another genetic disorder, Down syndrome, produce a similar level of intellectual disability, but language use in these two groups is quite different (Bellugi, Wang, & Jernigan, 1994). When asked to name all the animals he knew, a child with Down syndrome replied, Horsie, dog, ice cream. A child with Williams syndrome of the same age and with the same IQ score answered, Weasel, newt, salamander, ibex, yak. At the same time, it is not unusual for a person of normal to high intelligence to have significant difficulties learning language (Tallal, Ross, & Curtiss, 1989). Accomplished people who have been diagnosed with verbal learning disabilities or are suspected of having learning disabilities include such notables as Einstein, Sir Winston Churchill, Leonardo da Vinci, Thomas Edison, and psychology’s own William James. Language learning and use can vary widely, and these variations provide us with a richer understanding of how humans can communicate. We examine three variations of particular interest here. Why do some children have such a hard time learning to read? What happens when people learn more than one language? How does American Sign Language (ASL), which uses movement instead of sound, compare with more conventional spoken languages? Individuals with dyslexia experience difficulties in learning to read despite typical intelligence and exposure to adequate teaching methods (Shaywitz, Morris, & Shaywitz, 2008). The first description of a dyslexic patient, Percy F., was published by W. Pringle Morgan in 1896 (Shaywitz, 1996). Percy F. was described by Morgan as quick at games, and in no way inferior to others of his age. His great difficulty has been—and is now—his inability to learn to read (Shaywitz, 1996, p. 98). Carol Greider, a Nobel Prize–winning professor of molecular biology and genetics, struggled with dyslexia from elementary school through graduate school. Although the condition was challenging, Greider states, Perhaps my ability to pull more information out of context and to put together different ideas may have been affected by what I learned to do from dyslexia. Dyslexia is strongly influenced by genetic factors, which in turn result in differences in the symmetry of the cerebral hemispheres (Vanderauwera et al., 2016) and, in particular, the organization of fiber pathways in each hemisphere (Zhao, de Schotten, Altarelli, Dubois, & Ramus, 2016). People with dyslexia are more likely to be left-handed or ambidextrous than people without dyslexia (Eglinton & Annett, 1994). In addition to these differences in brain structure, people with dyslexia have difficulty distinguishing between similar-sounding phonemes or basic speech sounds, such as m or n (Merzenich et al., 1996). Compared to typical readers, their brains show different patterns of activation during rhyming tasks, which are based on the sound of words (Frith & Frith, 1996). Activity in the right prefrontal cortex during a rhyming task, which requires the ability to discriminate among phonemes, accurately predicted gains in reading skills over the next few years by children with dyslexia. Because reading most languages depends on matching phonemes to the letters that represent them, people with dyslexia show evidence of using a workaround when they read. Brain activity has been recorded while university students with and without dyslexia were reading (Shaywitz et al., 1998). Compared to typical readers, readers with dyslexia showed less activity in a pathway connecting visual cortex in the occipital lobe to Wernicke’s area in the temporal lobe. Instead, the readers with dyslexia showed greater activation of Broca’s area, which participates in speech production. Relying on Broca’s area while reading, as people with dyslexia do, is like reading out loud to yourself, which slows reading speed. If you have ever read a bedtime story to a child, you are aware of how much slower reading out loud is compared to reading silently. More than half the world’s population is bilingualbilingualProficient in two languages.            bilingual        Proficient in two languages.             or proficient in at least two languages (Chertkow et al., 2010; Kavé, Eyal, Shorek, & Cohen-Mansfield, 2008; see Figure 10.14). The 19th-century Italian cardinal Giuseppe Caspar Mezzofanti allegedly spoke 50 languages (Della Rosa et al., 2013). Bilingualism is associated with several benefits. Children with several types of bilingualism (Chinese–English, French–English, and Spanish–English) outperformed monolingual English speakers on tests of executive control (Antoniou, Grohmann, Kambanaros, & Katsos, 2016). Some researchers have reported that older adults who were bilingual experienced a delayed onset of symptoms of Alzheimer’s disease compared to monolingual older adults (Bialystok, Craik, & Luk, 2012). Being bilingual possibly contributes to cognitive reserve or protection against cognitive decline. However, a country-by-country analysis of multilingualism and cognitive decline yielded mixed results, and more research is needed on this topic (Klein, Christie, & Parkvall, 2016). Eleven languages are recognized in South Africa, and each province recognizes at least two languages. The most commonly spoken language in homes (23.8%) is Zulu. Many South Africans speak two or more languages. Brain areas involved with multiple languages appear to overlap (Pearce, 2005). The amount of overlap depends on the timing of learning and proficiency in each language. When a person learns two languages early in childhood, the extent of overlap in the brain is greater than when the second language is learned in adulthood (Kim, Relkin, Lee, & Hirsch, 1998). When a person is nearly equally skilled in two languages, more overlap will occur as well (Perani et al., 1998). ASL, used by people with hearing impairments and people who wish to communicate with them, provides further insight into the processing of language. ASL constitutes a distinct language, although it uses spatial cues of sight and movement instead of sound (Klima & Bellugi, 1979). ASL provides an interesting contrast between language functions, generally found in the left hemisphere of the brain, and spatial functions, generally managed by the right hemisphere. American Sign Language (ASL) is a distinct language based on sight and movement rather than sound. Both clinical observation and imaging studies support the notion that the brain manages ASL in the same way it manages other languages despite its obvious spatial characteristics. Prior to brain surgery, surgeons identify which functions are processed by each hemisphere in an individual patient’s brain by anesthetizing one hemisphere at a time. When the left hemisphere of an English-speaking patient proficient in ASL was anesthetized, she made errors in both spoken English and ASL signing. Neither anesthetizing the right hemisphere nor performing subsequent surgery on her right hemisphere produced deficits in either language (Damasio, Bellugi, Damasio, Poizner, & Gilder, 1986). Despite the spatial nature of ASL, this case suggests that a language is still a language, and the left hemisphere is the likely place for that language to be processed. These clinical observations have been confirmed by imaging studies. The same areas of the brain are activated during language tasks regardless of whether the subject uses spoken English or ASL (Neville et al., 1998). Type of aphasiaLocation of damageSymptomsBroca’s aphasiaBroca’s area (left frontal lobe in most people). Slow, effortful speaking combined with good comprehension. Wernicke’s aphasiaWernicke’s area (left temporal lobe in most people). Fluent, meaningless speech without comprehension. IntelligenceIntelligenceThe ability to understand complex ideas, adapt effectively to the environment, learn from experience, engage in reasoning, and overcome obstacles.            Intelligence        The ability to understand complex ideas, adapt effectively to the environment, learn from experience, engage in reasoning, and overcome obstacles.             is an individual’s ability to understand complex ideas, to adapt effectively to the environment, to learn from experience, to engage in various forms of reasoning, and to overcome obstacles (Neisser et al., 1996, p. 77). Along with Théodore Simon, Alfred Binet devised a test aimed at identifying schoolchildren with intellectual disability. Binet and Simon’s test laid the groundwork for today’s intelligence testing. The assessment, or testing, of intelligence represents an attempt to assign a number to an individual’s abilities, allowing that person to be compared with others. Formal intelligence testing began in 1904, when Alfred Binet was instructed by the French government to devise an objective assessment of schoolchildren. Binet and his colleague, Théodore Simon, assumed that relatively bright children behaved cognitively like older children, while less intelligent children behaved like younger children. They devised items that they believed would indicate children’s mental age relative to their peers. In this system, children who successfully completed an item at an earlier age than most of their peers would have a higher mental age, while those who were not able to complete an item until they were older would have a lower mental age. Stanford University professor Lewis Terman adapted Binet’s test for use in the United States and named his revised version the Stanford–Binet Intelligence Scales (Terman, 1916). Terman began using the intelligence quotient (IQ)intelligence quotient (IQ)A measure of individual intelligence relative to a statistically normal curve.            intelligence quotient (IQ)        A measure of individual intelligence relative to a statistically normal curve.            , which is computed by dividing children’s mental age by their chronological age and, for convenience, multiplying by 100. For example, a bright child with a mental age of 15 and a chronological age of 12 would have an IQ score of. The most frequently used intelligence tests today include the Stanford–Binet Intelligence Scales and the Wechsler Adult Intelligence Scale (WAIS). These tests no longer use the concept of mental age. Instead, contemporary tests place individual performance on a statistically normal curve, described in Chapter 2. Contemporary tests also incorporate new features designed to more accurately assess people of extremely low or high intellect, the elderly, and individuals with language difficulties. As we discuss in Chapters 2 and 12, constructing a good test can be challenging. A good test must demonstrate both reliability and validity. In short, the test should provide consistent results that correlate with other measures of the same construct. Although no test is perfect, most IQ tests show relatively good reliability and validity (Kaplan & Saccuzzo, 2001). It is important to keep in mind, however, that the intent of IQ tests is to predict school performance as opposed to the complex construct of intelligence described at the beginning of this section. Consequently, it is not surprising to find that correlations between IQ score and performance in mathematics and verbal skills, which are essential for success in contemporary education, are relatively high but correlations between IQ score and performance in art and design are lower (Deary, Strand, Smith, & Fernandes, 2007). Performance on the verbal components of many commonly used IQ tests are influenced by socioeconomic status, which further complicates the interpretations of the scores (Chapman, Fiscella, Duberstein, Kawachi, & Muennig, 2014). Improvements in scores over the last 60 years on tests that are supposedly culture free suggest that it may be impossible to separate test performance from environmental factors (Fox & Mitchum, 2013;  Nisbett et al., 2012; see Figure 10.15). Matrix reasoning tests are believed to be examples of culture-free and nonverbal intelligence tests. This category of testing is showing the largest increases in performance worldwide, suggesting that our modern experience is making us better at extracting abstract concepts. Which of the items labeled 1 through 8 fits in the lower right corner of the matrix? The correct answer to this item is 4. In discussions of intelligence, some psychologists focus on an individual’s overall abilities, while others focus on particular types of abilities. Using the statistical technique of factor analysis, which he helped develop, Charles Spearman distinguished between a general intelligence (g)general intelligence (g)A measure of an individual’s overall intelligence as opposed to specific abilities.            general intelligence (g)        A measure of an individual’s overall intelligence as opposed to specific abilities.             factor and specific factors that apply to single tasks (Spearman, 1904). For example, Spearman argued that g was important for learning Latin but not for distinguishing between two musical tones. General intelligence can be divided into fluid intelligence and crystallized intelligence (Cattell, 1971). Fluid intelligenceFluid intelligenceThe ability to think logically without the need to use learned knowledge            Fluid intelligence        The ability to think logically without the need to use learned knowledge             refers to the ability to think logically without needing previously learned knowledge, such as seeing patterns in a visual stimulus, while crystallized intelligencecrystallized intelligenceThe ability to think logically using specific learned knowledge            crystallized intelligence        The ability to think logically using specific learned knowledge             requires specific, learned knowledge, such as vocabulary or the multiplication tables. A person needs pattern recognition, a type of fluid intelligence, to play chess or Sudoku well, but understanding the rules of the game requires crystallized intelligence. Fluid intelligence peaks in young adulthood and then declines, although it declines more slowly in individuals who continue to use fluid intelligence regularly, while crystallized intelligence remains more stable through adulthood (see Figure 10.16). Fluid intelligence, like a reaction time measured with a stopwatch, includes abilities that do not require acquired knowledge. Fluid intelligence peaks in young adulthood and then gradually declines over the remaining life span. In contrast, crystallized intelligence, which does require acquired knowledge, remains fairly stable throughout adulthood. Composer John Williams has used his crystallized intelligence of music theory to contribute memorable soundtracks (for films including Jaws, Star Wars, and the Harry Potter series) in a career spanning more than six decades. General intelligence theories do not provide explanations for splinter skills or islands of ability that occur against an overall background of lower functioning. Gloria Lenhoff, who has Williams syndrome and an IQ of about 55, cannot make change for a dollar or subtract 3 from 5, but she has performed as a lyric soprano with the San Diego Master Chorale and as an accordionist with Aerosmith (Maher, 2001). Leslie Lemke, who is both intellectually disabled and blind, listened one time to Pyotr Ilyich Tchaikovsky’s Piano Concerto No. 1. Hours later, Lemke sat down at the piano, which he had never studied, and played the entire composition without error (Treffert & Wallace, 2002). Professional musicians are unlikely to be able to duplicate this feat. The separability of intelligences in these exceptional cases suggests that intelligence is a combination of factors rather than a single thing, such as the processing speed of your computer. Howard Gardner (1983, 1999) interprets these and similar findings to mean that we have multiple, independent types of intelligence. Robert Sternberg proposed a triarchic theory of intelligence, in which a combination of analytical, creative, and practical abilities allows people to achieve success (Sternberg, 1985;  Sternberg & Salter, 1982). He argues that it is possible to be gifted in one aspect without being gifted in others. Both general and specific approaches to intelligence can be useful. We can assume that people have separate strengths and abilities, such as those outlined by Gardner and Sternberg. Among these diverse abilities, purely cognitive abilities, such as verbal, mathematical, spatial, and logical skills, show strong positive correlations with one another, supporting an argument for general intelligence, or g. At the same time, abilities involving sensory, motor, and personality factors are less likely to show strong correlations, supporting an argument for multiple intelligences in these domains (Visser, Ashton, & Vernon, 2006). Traditional views of intelligence focused on education-related skills, but emotional and social skills are important to successful adaptation too. Building on arguments in favor of multiple intelligences, and in recognition that previous work on intelligence seemed to neglect social skills, models of emotional and social intelligence were proposed (Goleman, 2006;  Mayer & Salovey, 1993). Social and emotional intelligence allow people to manage emotions and reason about the mental states of others. Clinical cases involving individuals with brain damage, psychological disorders, and intellectual disability provide evidence for social and emotional abilities that are separate from cognitive abilities. In Chapter 7, we discussed roles for the prefrontal cortex, amygdala, and insula in emotion. Patients with damage to connections between these structures performed very poorly on a standard measure of emotional and social intelligence, even though they have a normal IQ score and show no evidence of other psychological disorders (Bar-On, Tranel, Denburg, & Bechara, 2003). Individuals with autism spectrum disorder often display cognitive abilities that are superior to their social abilities. Individuals with Down syndrome, which produces moderate intellectual disability, display relatively strong social skills (Fidler, Most, Booth-LaForce, & Kelly, 2008). Individuals with Williams syndrome provide examples of how general intellect can diverge from other abilities and talents. Gloria Lenhoff (right), has a tested IQ score of 55 but has performed as a lyric soprano and has played the accordion with Aerosmith. Even if emotional and social intelligence can be separated from traditional measures of intelligence, these factors can still exert a strong influence on a person’s academic and professional outcomes. In global assessments of math and science proficiency, students from East Asian countries (China, South Korea, and Japan) consistently outscore students from the United States and the European Union, who score around the mean (Guglielmi & Brekke, 2017). Further analysis suggests that the advantage shown by East Asian students is largely predicted by environmental factors, including parents’ educational goals for their children, students’ own educational goals, work ethic, and self-perception. Student self-perception includes a sense of academic efficacy (I can do this) and self-concept (I’m good at this). A not very intuitive finding is that self-concept related to math and science achievement is lowest in the very nations where performance is best! Overestimating one’s abilities and achievement can result in poorer performance. As described in more detail in the Hub Science feature in this chapter, a student’s mindset regarding improvement can also influence performance. The East Asian cultural norms of viewing the self as worthy of improvement increases students’ willingness to accept negative feedback and make corrections (Guglielmi & Brekke, 2017). Some psychologists view social and emotional intelligence as different from school-based intelligence and equally if not more important for success. In a classic study, children were given a single marshmallow with instructions to not eat it when an experimenter left the room. They were told that if they did not eat it, they would receive two marshmallows when the experimenter returned. When the children were tracked down 12 years later, the two-thirds who had been able to delay gratification (they didn’t eat the marshmallow) were less stressed and more confident than the one-third who had not been able to delay gratification (Mischel, Ebbesen, & Raskoff Zeiss, 1972). Because the mind is the outcome of the brain, associations between its structure and activity and an individual’s intelligence have been investigated. For the purposes of this type of research, standardized tests are used as the measure of intelligence. Brain imaging studies demonstrate that standard measures of intelligence positively correlate with overall brain volume (Deary, Penke, & Johnson, 2010). More precisely, intelligence measures are positively correlated with the thickness of the cerebral cortex, particularly in the prefrontal cortex and temporal lobes. Because intelligence is such a broad concept, it is unlikely that we have intelligence centers in the brain. Instead, scientists have suggested that intelligent brains enjoy quick, efficient communication of information from one area to another along axon pathways (see Figure 10.17). This is particularly true of people with high levels of verbal intelligence, which is more closely related to the crystallized intelligence discussed earlier (Khundrakpam et al., 2017). Studies of people with brain damage indicate that fluid intelligence, but not crystallized intelligence, is negatively affected by damage to the frontal lobes. These observations are consistent with studies showing high levels of frontal lobe activity during diverse tests believed to demonstrate fluid intelligence (Gray & Thompson, 2004). Fluid intelligence abilities are highly correlated to working memory capacities (Khundrakpam et al., 2017). The relationships between brain structure and IQ scores are complex. The individual depicted with the blue line had the highest IQ score out of 100 participants, and in each area of the brain believed to correlate with IQ results, he showed much higher than average gray matter volume. Person 2 and Person 3 had identical scores on an IQ test, but their patterns of gray matter volume were quite different from each other. Further research might help psychologists better understand the relationships between observed differences in brain structure and intelligence. In response to research showing that students from the poorest schools in Shanghai outscored the children of doctors and lawyers in the United Kingdom on math tests, the British government flew 60 English-speaking Chinese teachers to London to provide advice about how to improve British math achievement. Chinese teachers recommended teaching to the brightest students in the class, while also ensuring the children with moderate or lower abilities were also served. Chinese teachers and students commonly express a belief that hard work is more important than natural ability in math achievement. According to Carol Dweck (Dweck, 2000), people believe one of two theories about their own intelligence. Some people believe in an entity theory that states that people have a certain fixed, unmodifiable amount of intelligence. Others believe in an incremental theory that states that intelligence is something you can improve through learning. This second approach does not deny that learning is easier for some people than for others but simply suggests that everyone can improve through effort. Students who believe in entity theory feel best when they complete easy, low-effort tasks and outperform other students. Challenges in the form of difficult tasks or higher performing peers threaten the self-esteem of the entity theorists. Dweck believes that the lavish praise heaped on today’s students promotes belief in the entity theory, resulting in anxiety about looking smart, avoidance of challenges, and an inability to cope with the inevitable educational setbacks. In contrast, incremental theorists like to stretch their skills and put their knowledge to use. They view easy tasks as a waste of time. These students are not as affected by setbacks and worries about looking smart as their entity theorist peers. Dweck’s two theories about intelligence parallel cultural differences regarding proficiency in math. In the United States, we tend to hold an entity theory of math ability—you have it or you don’t. People feel comfortable admitting that they are bad at math and view this as a good reason to stop taking math courses or working to improve. In East Asia, the incremental theory is dominant. Math is viewed as a difficult subject for most people that you must work to master. When Japanese and American students were randomly assigned to succeed or fail on a task by giving them either an easy or difficult task, the two groups responded very differently (Guglielmi & Brekke, 2017). The Americans who succeeded were more likely to persist on a follow-up task, view the tests as accurate, and rate the skills assessed by the task as important. This pattern was reversed for the Japanese students. The students who failed initially showed the same response as the Americans who had succeeded. As mentioned previously in this chapter, this difference in self-concept probably contributes to the superior achievement in math and science seen in East Asian students relative to their peers with different ethnicities. According to Dweck, educators who view self-esteem (see Chapter 12) as something you give to a student through the application of easy tasks, lavish praise, and avoidance of failure are making a big mistake. Instead, self-esteem could be viewed as something students give themselves. Educators can better promote student self-esteem by teaching students to value learning over the appearance of being smart, promoting the love of challenge and effort, and helping students view mistakes as stepping stones to mastery. Intelligence also seems to be related to the timing of brain development. As children approach puberty, the gray matter of the brain experiences a period of growth that slows down again later in adolescence. Children with average IQ scores showed a peak thickening of cortical gray matter around the age of 8, but children with superior IQ scores experienced a later peak, thickening around age 13 (Shaw et al., 2006). As we observed in Chapter 3, simple either–or thinking about nature and nurture is rarely correct when considering human behavior, including intelligence. The influences of heredity (nature) interact intimately with experience and other environmental factors (nurture) to produce a result. For example, infants who are breastfed have higher IQ scores than those who are not. However, the experience of being breastfed increases IQ results only in children with one variation of a gene; it has no effect on IQ results in children with another variation of the same gene (Caspi et al., 2007). A comparison of the amount of gray matter in the brains of identical and fraternal twins showed that the identical twins were more like each other than were fraternal twins, as indicated by red and yellow coloring, especially in areas where gray matter volume is correlated with IQ test results. (F is for frontal lobe and W indicates Wernicke’s area.)To recap the discussion in Chapter 3, heritability estimates how much of the variability in a characteristic observed in a population, such as variations in adult height, is because of genes. If genes play no role in this variation, such as the likelihood that people have tattoos, heritability is 0. When a condition is completely genetic, such as Huntington’s disease, heritability is 1.0. The heritability of adult intelligence as measured by IQ tests is usually reported to be about .75 (Neisser et al., 1996). When we say that the heritability of intelligence is .75, we are saying that 75% of the variance in intelligence observed in the population can be attributed to genetics, not that 75% of an individual’s intelligence is because of that person’s genes. As we observed in an earlier section, both overall brain size and proportion of gray matter are correlated with intelligence, and both are approximately 85% heritable, based on comparisons of identical and fraternal twins (Gray & Thompson, 2004). Although many genes are known to influence brain development, none have been conclusively linked to high IQ test performance. It is likely that a very large number of genes, each having a small impact and interacting with the individual’s environment, contribute to the development of intelligence. The fact that a trait demonstrates high heritability does not imply that change or improvement is impossible. A case in point is the highly heritable condition of phenylketonuria (PKU), in which a person cannot properly metabolize a particular amino acid found in many foods. Left unchecked, the condition leads to intellectual disability, discussed in a later section of this chapter. However, if a person with PKU is given an appropriate diet in which problem foods are avoided, intellectual development proceeds normally. The presence of data showing that IQ results are heritable does not mean we give up on anybody’s intellectual development. Worldwide increases in IQ scores of about 3 points per decade over the last 100 years illustrate the potential for change (Dickens & Flynn, 2006;  Dickens & Flynn, 2001). This increase in IQ scores, known as the Flynn effect, has occurred far too quickly to represent genetic changes (see Figure 10.18). Improvements in nutrition and other health factors probably account for some of the change. Using information from the World Health Organization, researchers have identified strong correlations between a nation’s freedom from serious infectious diseases and its citizens’ average IQ scores (Eppig, Fincher, & Thornhill, 2010). As nations become wealthier and more capable of battling disease, their citizens’ IQ scores increase. Surprisingly, the test score gains are most pronounced in supposed culture-free tests such as the Raven’s Progressive Matrices (Fox & Mitchum, 2013). Participants born after 1990 scored far better on these tests than did participants born in 1940. This change might reflect an improvement in the ability to manage dissimilar items that accompanies living in a modern society. IQ test results have been gradually increasing, although the exact causes for these changes are not completely understood. Improvements in nutrition, health care, and education are likely reasons for much of the increase. Matrix reasoning tests, containing items like the one in Figure 10.15, show the largest gains. Psychologists have produced long lists of environmental factors that influence intellectual development, including nutrition and exposure to mentally stimulating activities. Most of these environmental advantages are not cheap. Consequently, it is not surprising to find that socioeconomic status, a measure of family income, education, and other class factors, is positively correlated with IQ score (Turkheimer, Haley, Waldron, D’Onofrio, & Gottesman, 2003). This correlation does not tell us whether more intelligent people enjoy higher socioeconomic status, whether higher socioeconomic status provides means to optimize intelligence, whether some unknown third factor produces both high intelligence and high socioeconomic status, or whether some combination of these factors occurs. Nonetheless, poverty continues to be a significant risk for a low IQ score. We have been discussing the concept of a general intelligence that characterizes an individual, but is it possible for the same sort of measure to describe the abilities of groups of people? Such a measure might be quite useful to organizations using teams. The Question: Do groups of people have a collective intelligence that predicts their performance, and if so, what factors contribute to a group’s collective intelligence? Nearly 700 people completed individual intelligence tests and then were randomly assigned to work groups of two to five individuals (Woolley, Chabris, Pentland, Hashmi, & Malone, 2010). Each group was assigned several tasks to complete that varied in the amount of coordinated activity required for success, including solving visual puzzles, brainstorming, making collective moral judgments, and negotiating the allocation of limited resources. Some groups finished their session by playing checkers against a computerized opponent, while others completed an architectural design task. Additional measures were taken of factors believed to be related to group performance, including motivation, satisfaction, group cohesion, social sensitivity, and distribution of speaking turns. This research does not pose any unusual ethical dilemmas. Routine protections of informed consent and confidentiality should be sufficient. Evidence for a group or collective intelligence did emerge. For each group, performance across the different tasks was highly and positively correlated. Further statistical analyses ruled out the average individual intelligence of the group or the maximum individual intelligence of individuals in the group as significant predictors of group performance, leaving collective intelligence as a single, strong predictor of group performance (see Figure 10.19). A group’s performance on a variety of tasks, from playing video games to constructing a complex architectural design, was predicted better by the group’s collective intelligence than by the average intelligence of the group’s members or by the highest IQ score among the group’s members. Surprisingly, typical group features such as motivation, satisfaction, or group cohesion failed to predict group performance. Individual features, such as the individual intelligence of the members, also failed to predict group performance. Instead, the members’ scores on the social sensitivity instrument, called the Reading the Mind in the Eyes test, and how evenly speaking turns were distributed had a large impact on the group’s collective intelligence. You might have had opportunities to observe collective intelligence while working on group projects. This research shows that bringing together the most intelligent people might not guarantee the best results. Instead, groups of people who are more empathic and do not dominate the conversation might have the greatest success. Assuming that IQ scores are normally distributed, with an average of 100 and a standard deviation of 15, about 68% of the population should fall within one standard deviation (85–115), and 95% of the population should fall within two standard deviations (70–130). The remaining 5% is divided between the two tails of the distribution, either above 130 or below 70 (see Figure 10.20). IQ test results form an approximately normal distribution, which means that about 68% of the population falls between IQ scores of 85 and 115. Ninety-five percent of the population falls between IQ scores of 70 and 130, with the remaining 5% divided between the two tails. According to the Diagnostic and Statistical Manual of Mental Disorders (DSM-5; APA, 2013; see Chapter 14), intellectual disabilityintellectual disabilityA condition diagnosed in individuals with IQ scores below 70 and poor adaptive behaviors; also known as mental retardation.            intellectual disability        A condition diagnosed in individuals with IQ scores below 70 and poor adaptive behaviors; also known as mental retardation.             or intellectual developmental disorder is diagnosed in individuals who show deficits in intellectual functioning beginning early in childhood, accompanied by deficits in independent living skills. Although individually administered standardized IQ tests are often used as a starting point when evaluating an individual who might have an intellectual disability, the clinician’s impressions can be used to make the decision, too. Approximately 1 to 3% of the population will score 70 ± 5 or below on a standardized IQ test (Szymanski & King, 1999). In addition to an IQ score in this range, individuals with intellectual disability demonstrate problems with adaptive behaviors or life skills. These behaviors might include the ability to balance a checkbook, receive correct change from a cashier, read a bus schedule, buy groceries, maintain personal hygiene, and other skills required for independent living. Intellectual disability is divided into categories of mild, moderate, severe, and profound based on IQ scores and level of adaptive functioning (APA, 2013). Mild intellectual disability is typically accompanied by IQ scores of 55 to 70, or between two and three standard deviations from the mean of 100. With proper intervention, individuals in this group are able to master a sixth-grade curriculum, although it will take them until age 18 to do so. Most individuals in this group can live independently, and many work, marry, and have families. Mild intellectual disability is typically not the result of genetic or medical problems (Zigler, 1967;  Zigler & Hodapp, 1986). As a result, mild intellectual disability is frequently called familial retardation, recognizing that it usually results from preventable, environmental causes. Poverty is an obvious risk factor for this condition because poor parents may be unable to obtain the health care, diet, and other environmental benefits needed to produce healthy children (see Figure 10.21). Mild forms of intellectual disability, leading to IQ scores between 55 and 70, are also known as familial retardation because these cases are not due to medical or genetic problems. Poverty is a large risk factor for this level of intellectual disability. A little more than 5 in 1,000 children living above the poverty line have intellectual disability, but this number jumps to 9 in 1,000 among children living below the poverty line. IQ scores between 40 and 55, or between three and four standard deviations from the mean, indicate moderate intellectual disability. Unlike mild cases, moderate intellectual disability typically results from genetic or medical conditions rather than familial factors. Genetic conditions such as Down syndrome are likely to produce disability in this range. The academic attainment of most individuals in this category is limited to the second-grade curriculum, and again, most do not achieve this level until about the age of 18 years. Most of these individuals need some type of assisted living, although the type and extent of needed services vary widely among affected individuals. Severe intellectual disability is diagnosed in individuals with IQ scores between 25 and 40, or four to five standard deviations below the mean, while profound intellectual disability is diagnosed in individuals with IQ scores below 25. Both severe and profound intellectual disabilities generally result from serious medical conditions and are identified at birth or early in infancy. Individuals with severe intellectual disability can learn a few high-priority words, such as stop or hot. Individuals with profound intellectual disability can learn some basic self-care skills, such as feeding themselves or washing their hands. Depending on the nature of the medical problems, these individuals can require significant assistance and supervision. Individuals with intellectual disability face several common problems. Many children with intellectual disability form insecure attachments with their caregivers, described in detail in Chapter 11 (Clegg & Sheard, 2002). Ideally, children form a secure attachment characterized by demonstrating a clear preference for a caregiver over strangers, protesting when the caregiver leaves, and indicating a willingness to explore their environment (Ainsworth & Bowlby, 1965). Australian model and fashion designer Madeline Stuart has Down syndrome. Language skills are affected depending on the severity of the intellectual disability (Chapman, 1995). Children with intellectual disability have problems expressing themselves clearly, although they may understand a great deal that is said to them. In conjunction with typically poor social skills, these language deficits may result in peer rejection. Consequently, successful intervention programs address language and social skills, in addition to academic and life skill work. Between 10 and 40% of individuals with intellectual disability experience some type of emotional or behavioral disorder (McGaw, Shaw, & Beckley, 2007). Anxiety, impulsiveness, and mood disorders are the most common disorders seen in this population. When intellectual disability is more severe, individuals may engage in pica, or the eating of nonedibles, or self-injurious behaviors, such as banging the head against the floor or wall. Individuals with intellectual giftedness, who make up approximately 1 to 3% of the population, score two or more standard deviations above the mean, or more than 130 on a standardized IQ test. Genius combines high intelligence with creativity and achievement (Renzulli & Delcourt, 1986). Having a high IQ score provides advantages in many aspects of life. One of the longest running studies in psychology is a longitudinal examination of gifted children initiated in 1921 (Terman, 1925;  Terman & Oden, 1959). The original participants were children selected because of their very high IQ scores, averaging 150. The gifted participants not only maintained their high IQ scores but also enjoyed better physical health, emotional stability, occupational attainment, and social satisfaction throughout their adult lives. Despite advice from the American Academy of Pediatrics telling parents that children under the age of 2 should have zero screen time (television, tablets, phones, or computers), the market for electronic media designed to make babies smarter is booming (Committee on Public Education, 2001). Sporting names like Baby Einstein, Brainy Baby, and Baby Genius, these materials make their intent clear to consumers. The popularity of these programs indicates that today’s parents take the need for nurture seriously. Little scientific research has been done on the use of these products with very young children. One study found that watching baby videos actually had a detrimental effect on young children’s language acquisition. Infants younger than 16 months and exposed to the videos understood from six to eight fewer words for each hour of exposure compared to nonexposed infants (Frederick, Dimitri, & Andrew, 2007). This should not be surprising, given other research that emphasizes the social nature of language learning discussed in this chapter, such as the study showing that young children learn language from a speaker in the room, but not from the same speaker being televised. Rather than investing in baby videos, we hope that parents can sit back, relax, play with, and enjoy their children, who grow up all too quickly. Despite advice from the American Academy of Pediatrics to avoid screen time for children under the age of 2 years, there is a booming market for apps and DVDs advertising their ability to make children smarter. High intellect, particularly as measured by an IQ score, does not capture completely what we mean by genius. Despite their successes, the gifted participants in the longitudinal study just cited did not produce genius work. Nor do all remarkable thinkers possess particularly high IQ scores. Nobel laureate Richard Feynman was disappointed to learn that his IQ score was only 124 (Gleick, 1992). These teens, standing with former astronaut and NASA administrator Charles Bolden, are not satisfied by finding the right answer to teachers’ questions. Instead, they tackled tough problems to compete in the White House Science Fair. Like individuals on the lower extreme of IQ scores, children with high IQ scores benefit from educational opportunities tailored to their abilities. Unfortunately, the structure of public education in the United States, with its emphasis on grade levels based on age, rarely provides optimum learning for gifted children. A report by the U. S. Department of Education noted that children with IQ scores of 140 or above typically know at least half of a grade’s curriculum before the school year even begins, and those with IQ scores of 170 or above know virtually all of it (U. S. Department of Education Office of Educational Research and Improvement, 1993). Their time in school is subsequently wasted, and the material is so easy that it encourages poor learning habits ill-suited to later educational challenges. Educators are returning to ability groupings and acceleration to resolve these problems. These practices benefit not only the more gifted students, but they also provide benefits for average and below-average students, although equitable assignment to groups remains a concern (Steenbergen-Hu, Makel, & Olszewski-Kubilius, 2016). Classic research suggests that some people are maximizers, who strive to reach the best outcome. Satisficers are more willing to choose outcomes that are merely acceptable (Simon, 1957). Maximizers are perfectionists who evaluate all choices before settling on one, so they do not make quick decisions. Because maximizers feel more regret when a decision doesn’t work out well, they tend to avoid making decisions. Although they may experience better outcomes than satisficers on many occasions, maximizers might still feel worse. Satisficers are more likely to see their glass of life as half full, whereas maximizers are more likely to see their glass as half empty. What does this distinction mean for interpersonal relationships? Finding a romantic partner and making a long-term commitment to that person involve some important decisions. Romance is consistently ranked among the top regrets in life for most people (Roese & Summerville, 2005). These facts suggest that maximizers might experience obstacles within the realm of relationships. Either they will let opportunities pass by because they are still looking for that perfect person, or they will avoid making a commitment for fear of regretting it later. Their perfectionist standards might make it impossible for them to appreciate a partner who is a good match, perhaps because that person does not look like a supermodel or squeezes the toothpaste tube the wrong way. Faced with a large array of choices, satisficers will pick one that works, but maximizers will evaluate all the options to find the perfect choice. We’re guessing that most of these candies taste good. Does this mean maximizers are doomed in relationships? Not at all. Recognizing that our automatic ways of behaving will not always lead us to our goals can help us identify areas where we need to take conscious, systematic control. Many maximizers blame their partners for a failed romance (the partner just didn’t measure up) instead of taking a hard look at their own contributions to the relationship. If you know your maximizing tendencies are leading you in the wrong directions, you can use the steps to effective problem solving outlined earlier in this chapter to supply alternatives that might work better. If Psychologists are to be able to inform policy makers about ways to discourage cyberbullying, it would be helpful to be able to predict the individuals who are most likely to engage in this behavior. Looking at the category of cyberbullies, we can see how using exemplars and prototypes might help us predict the features of cyberbullies. Perhaps you have a personal acquaintance who is known to be a cyberbully. This might be your exemplar. You might also consider a prototypical or average cyberbully based on your personal experience and what you have heard from others or read. Take a moment and write down a list of what you think might be the key features that define the category of cyberbully. Now let’s see what psychological science has to say, and you’ll be able to see where the features of your category agree or disagree with the scientific literature. Did you make a conclusion about gender? Prior research has shown that girls are less likely to participate in traditional bullying, but their role in cyberbullying remains unclear. Researchers have reported that slightly more males than females engage in cyberbullying (Aricak et al., 2008;  Li, 2006;  Slonje & Smith, 2008), more females than males engage in cyberbullying (Kowalski & Limber, 2013), or that there are no significant gender differences in perpetrators of cyberbullying (Hinduja & Patchin, 2008). Given these mixed results, we might assume that gender is not a very helpful feature in establishing a cyberbully profile. Age might be a useful dimension for our concept of cyberbully. Is there a particular age range where this behavior is more common? If you guessed that cyberbullying peaks around age 14, then you are correct (Cook, Williams, Guerra, & Tuthill, 2007). Knowing this could be very helpful. You would not want to plan an educational program to be administered to students who are past this peak age group, thereby missing the main targets of your interventions. Some other characteristics of cyberbullies are intuitive. Cyberbullies are more expert in their use of the internet than their noncyberbullying peers, and they experience less parental supervision (Vandebosch & Van Cleemput, 2009). As we have seen in other situations, however, intuitive conclusions are not always consistent with scientific facts. You might have a prototype or exemplar cyberbully in mind who is socially awkward and more likely to be a victim of traditional bullying than to be a perpetrator. Facts show that cyberbullies are actually more socially competent than their peers, not less, and they are about equally likely to engage in traditional bullying as they are in cyberbullying (Vandebosch & Van Cleemput, 2009). Widely held stereotypes about who is more likely to cyberbully are not always supported by scientific research. How did you do? Did you find a need to adjust the characteristics of your cyberbully concept in light of the scientific evidence? If so, that is part of education. We are constantly revising our concepts in light of new information. Type of intelligenceDescriptionExampleGeneral intelligence (g)A single measure of an individual’s intellectual ability that predicts most of that person’s intellectual performance. Verbal, mathematical, spatial, and logical skills showing high positive correlations within an individual. Fluid intelligenceA type of general intelligence that allows logical thinking without needi.ng learned knowledgeRecognizing relationships between geometric shapes. Crystallized intelligenceA type of general intelligence that requires learned knowledge. Using your knowledge of the multiplication tables to figure out interest on a new car loan. Multiple intelligencesSingle skills that tend to show relatively low correlations with one another and with general intelligence. Strong musical abilities in people with Williams syndrome, which results in a lower than normal IQ score. Emotional and social intelligenceThe ability to manage emotions and reason about other people’s states of mind. Managing anger appropriately and showing empathy for others. Collective intelligenceA quality of group process independent of individual group members’ intelligence that predicts group performance. A team featuring good social sensitivity and conversation sharing that completes a task successfully. `;\n","export const text = `Around the age of 3 years, fiber pathways connecting the frontal lobes with the rest of the brain are developing rapidly, making changes in decision making and memory possible. What is your earliest memory? A birthday party? A new puppy? We are guessing that your earliest autobiographical memories, which we discussed in Chapter 9, probably date to when you were between 3 and 5 years old. Why this age period and not another? Zooming in, we can see that one of the factors driving memory abilities is the development of the brain during childhood. As you will see in this chapter, changes taking place in the hippocampus and frontal lobes during the third year of life give children a boost in their abilities to form long-term memories, particularly about the sequences of events that make up our autobiographical memories. But there are other parts to this story. To form an autobiographical memory, children must have a sense of self. As we discussed in Chapter 6, babies as young as 18 months can recognize themselves in a mirror. In this chapter, you will learn about the further development of self-awareness as children gradually separate from their parents and family and move into adult roles in the community. Zooming out still farther, we find that cultures influence the formation and timing of autobiographical memories. When white American adults are compared with Chinese adults, the white Americans access more autobiographical memories from earlier ages and provide more episodic details of these memories (Wang, 2008). Wang believes that this difference observed in adults originates earlier in childhood, when white American children show more emotion knowledge, or an understanding of how situations can lead to emotions, than do Chinese children (see Figure 11.1). Emotion knowledge is assessed by asking children such questions as, What makes people happy? Cultures influence emotion knowledge development by focusing on information relevant to the self versus more general aspects of information. White American adults recall more childhood autobiographical memories in greater detail than do Chinese adults. These differences may have their roots in earlier cultural influences on emotional knowledge or an understanding of how a situation can lead to emotions. When asked to describe situations that would make a person happy, sad, fearful, or angry, white American children provided more information than Chinese or Chinese immigrant children. Emotional knowledge increases when a person focuses on information relevant to the self rather than more general information. To understand the development of autobiographical memory, psychologists must explore the biological, individual, developmental, social, and cultural contributions to this ability. At the same time, the development of an autobiographical memory capability forms reciprocal influences with these other aspects of the individual’s behavior. In this chapter, we will explore many examples of how the normal changes that occur across the life span interact with our behavior and mental processing. A major milestone in children’s development of a sense of self is the ability to recognize themselves in a mirror by the age of 18 months. In Chapter 8, we defined learning as a relatively permanent change in behavior due to experience. In this chapter, we will explore an additional force for changes in behavior—changes that unfold with the passage of time. The study of human development considers changes in behavior that correlate with growth or maturation. As we explore the normal changes that occur at different stages of life, we will concentrate on three interwoven threads: physical development, cognitive development, and social/emotional development. Although we discuss each separately, they are constantly interacting. For example, moving independently by crawling or walking produces a more rapidly changing and interesting environment, which spurs an infant’s cognitive growth. At the same time, a mobile infant is likely to elicit different reactions from parents or caretakers than one who is stationary, stimulating further social and emotional development. In addition to documenting these three types of change over time, the study of development highlights several major questions that stretch across the entire life span. How does nature interact with nurture? Is development gradual and continuous or abrupt and discontinuous? How much of human development is universal across the human species or dependent upon a person’s culture? Although we will be discussing the genetic heritage of individuals, as well as environmental influences on their development, we will be emphasizing the interactions between nature and nurture that predict developmental outcomes. Significant progress has been made in the understanding of how genes and the environment interact. As we discussed in Chapter 3, we now understand that the underlying deoxyribonucleic acid (DNA) that makes up our genes is turned on or off by the surrounding chemical tags (the epigenome) that accumulate through life. Among the external environmental factors that can chemically tag our DNA are diet, nurture, and stress. Early developmental psychologists described normal age-related changes up through childhood to puberty but not beyond. Today, we know that the ages represented in these four generations bring their own set of characteristic behaviors. Epigenetic influences accumulate over the life span. Because younger children have had less time for epigenetic interactions between their genes and their experience to take place, they have more in common with one another at any particular age than do older adults, whose decades of experience have caused their epigenetic paths to lead them in many directions. A group of 8-year-olds is more consistent in its physical characteristics and behaviors than a group of 20-year-olds, who still have more in common with one another than any group of 65-year-olds. This contrast between children and adults might have led earlier psychologists to the mistaken belief that development stopped at some point in childhood or adolescence, when all they were seeing was a transition point where internal influences were becoming overshadowed by external ones. Another major question asks whether development proceeds gradually and smoothly over time (continuity) or changes more abruptly from one stage to the next (discontinuity). The gradual approach might be visualized as a rainbow, with one color (stage) gradually merging into the next, with no bold line separating the two. In contrast, the discontinuity approach views development as a staircase to be climbed. The behavior on one step is qualitatively different from that on the step that preceded it, and the change from one step to the other is abrupt (see Figure 11.2). Development can appear as a gradual, continuous process, such as the acorn growing into a mighty oak tree, or as a sudden, discontinuous process, such as the butterfly emerging from a chrysalis. As children develop physically, they are able to move out into the world. Mobile children face new problems that stretch their cognitive abilities and new social and emotional challenges as they learn self-regulation and experience control from their parents. As in the case of nature and nurture, the continuity–discontinuity debate does not have a right answer. Some features appear to be more continuous in their developmental course, such as the gradual development of infant temperament into later personality. Other features seem to develop more abruptly. Parents are often surprised when their child becomes suddenly more mobile, such as rolling across the bed or walking, although the underlying physical development is probably quite gradual. Psychologists taking the universal approach to development look for age-related behaviors that are found across the entire human species, whereas psychologists taking an ecological view ask questions about the impact of culture and environment on development. Which approach is correct? Both universal and ecological approaches improve our understanding. Although we will see universals of development in this chapter, such as the age at which most children begin walking unassisted, we will also see that children growing up in cultures where early walking is encouraged walk at slightly younger ages than children growing up in cultures where restricting infant movement is the norm. Although a consideration of human universals is useful, most contemporary psychologists would be unwilling to consider many behaviors outside their context of social relationships and culture. Epigenetic influences accumulate over the life span. Groups of 8-year-olds have more in common than groups of 20-year-olds, who have more in common than groups of older adults. The contrast between the degree of similarity seen in children and that seen in adults might have led early developmental psychologists to the mistaken conclusion that development stops. The 9 months between conception and birth are full of growth and activity. For the first 2 gestational weeks, the developing organism is known as a zygotezygoteThe term used to describe a developing organism immediately following conception            zygote        The term used to describe a developing organism immediately following conception            . During the second gestational week, the zygote completes its journey through the mother’s fallopian tube to the uterus, where it implants in the lining. During gestational weeks 3 through 8, we use the term embryoembryoThe term used to describe a developing organism between the zygote and the fetus stages, or between 3 and 8 weeks following conception in humans            embryo        The term used to describe a developing organism between the zygote and the fetus stages, or between 3 and 8 weeks following conception in humans            , and for the remainder of the pregnancy, we use the term fetusfetusThe term used to describe a developing organism between the embryo stage and birth, or between 8 and approximately 40 weeks following conception in humans            fetus        The term used to describe a developing organism between the embryo stage and birth, or between 8 and approximately 40 weeks following conception in humans            . The universal approach sees development as common to all people, whereas the ecological approach emphasizes the influences of environment and culture on development. In the United States, we tend to look at child soldiers, some as young as 10 years old, as being a product of the poverty and lack of opportunity in their environments. However, some universal aspects of being a preteen or young teen, such as an immature sense of morality and greater obedience to adults, might apply equally to very young violent gang members in the United States. During the first gestational week, the zygote differentiates into three germ layers: ectoderm, mesoderm, and endoderm (see Figure 11.3). The ectoderm develops into nerve tissue and skin, while the mesoderm gives rise to muscle and bone. The endoderm is the source of the body’s soft tissue, such as the organs of the digestive tract. By gestational week four, the central nervous system has differentiated into forebrain, midbrain, hindbrain, and spinal cord. By gestational week seven, cells that will form the cerebral cortex begin a carefully orchestrated journey from the lining of the neural tube (which will ultimately develop into the ventricles of the brain) to their ultimate destinations.(a) For the first 2 gestational weeks, the developing organism is known as a zygote. (b) Cell division continues at a rapid pace as the zygote travels down the mother’s fallopian tube, reaching the uterus during its second week. (c) The embryo stage lasts from gestational week 3 through gestational week 8. (d) During the zygote stage, cells begin to differentiate into three types: The ectoderm, which is the source of cells making up the nervous system; the mesoderm, which develops into muscle and skeleton; and the endoderm, which will form internal organs. Other systems, while perhaps not as dramatic as the nervous system, also begin to differentiate and develop during the embryonic stage. The heart, stomach, liver, and other organs are formed during this period. During the sixth gestational week, expression of a gene on the Y chromosome initiates the differentiation of the generic gonads into testes in males, while alternate genes guide the development of the gonads into ovaries in females. The remainder of the pregnancy involves continued growth and maturation of the systems that are now in place (see Figure 11.4). During gestational month three, generic internal reproductive organs differentiate into the uterus, fallopian tubes, and upper portion of the vagina in females and into the seminal vesicles, vas deferens, and prostate in males. New neurons are born in large numbers and begin the process of forming connections with one another. Myelination of the nervous system begins at gestational month six (Rivkin et al., 1995). As you may recall from Chapter 4, myelin allows neurons to communicate faster and more efficiently. By gestational month seven, most of the brain’s cells have been formed (Rakic, 2000). During the fetal stage, from gestational week 8 to approximately gestational week 40, considerable growth and maturation of organs and tissues prepare the fetus for birth. You may be under the impression that the fetus resides in a rather isolated, self-contained environment in the womb, but this is not the case. The fetus’s ability to hear noises outside the mother’s body is quite good during the last few months of pregnancy (Birnholz & Benacerraf, 1983). Newborn babies show an immediate preference for their mother’s voice (DeCasper & Fifer, 1980). When pregnant mothers read the Dr. Seuss classic The Cat in the Hat out loud twice a day, their infants later preferred to listen to tapes of their mothers, but not others, reading this story (DeCasper & Spence, 1986). This preference has obvious advantages in the attachment and bonding that occurs between mother and newborn. Most pregnancies last about 40 weeks, and babies born between gestational weeks 37 and 42 are considered typical or term births. In the United States, just over 12% of babies are born prematurely or before 37 weeks of pregnancy (Witt et al., 2014), and 2% are born before 32 weeks of pregnancy (Cole et al., 2011). Premature infants are at greater risk than term infants for a number of conditions that can affect lifetime health and development, including disorders of the lungs, vision, and development of the brain (Cole et al., 2011). Compared to a typical baby born around gestational week 40, who is usually about 20 inches long and weighs about 7 pounds, little Baby Emilia was only about 8 inches long and weighed about 1/2 pound, about the size of a bell pepper. Nine months after birth, Emilia had grown to the size of the typical newborn. In most cases, prenatal development proceeds according to plan. As we observed in Chapter 3, genetic abnormalities that have adverse effects are rarely passed along in large numbers. The more common genetic abnormalities seen in children are those that become more likely when their parents are older. Errors in the development of eggs and sperm are more likely in older parents. The final division of a woman’s immature egg, formed during her own prenatal period, into a mature ovum does not occur until just before ovulation. Before they make this last crucial division, a 40-year-old woman’s eggs have been exposed to 40 years of potentially harmful environmental influences that raise the probability that something will go wrong as they divide. In contrast, it takes a little over 2 months for a man to produce a sperm cell, which reduces the chances of the cell’s exposure to harmful influences. Down syndrome, or trisomy 21, results when the child receives a third full or partial copy of the 21st chromosome along with the usual two, usually because of faulty cell division (see Figure 11.5). Down syndrome results in intellectual disability (see Chapter 10), a pattern of mild physical abnormalities, and a probable life span of only 40 to 50 years. A woman’s chances of giving birth to a child with Down syndrome are only about 1 in 2,000 at age 20 but rise to 1 in 30 at age 45. Many genetic disorders, such as Down syndrome, are more common as the age of the mother increases. Errors in the final cell division forming the mother’s egg immediately before ovulation become more likely over time as a woman is exposed to more potentially harmful environmental influences, such as radiation. According to one expert in medical ethics, today’s technology makes it possible to screen embryos for every known disease-causing mutation in the human genome (Leroi, 2006). What would be the benefits and costs of this practice? During pregnancy, the developing fetus is protected from exposure to toxins and disease-causing agents by the placenta, an organ attached to the wall of the mother’s uterus that provides the fetus with nutrients and oxygen. This protection, however, is not perfect. In some cases, substances that are relatively safe for the mother can have devastating effects on the health and development of her fetus. Any agent that can produce harmful effects in the zygote, embryo, or fetus is known as a teratogenteratogenA chemical agent that can harm the zygote, embryo, or fetus            teratogen        A chemical agent that can harm the zygote, embryo, or fetus             (see Figure 11.6). Table 11.1 lists some common teratogens and their effects. Exposure to commonly used antidepressant medications has been linked to higher rates of premature birth (Kendall-Tackett & Hale, 2010) and autism spectrum disorder (Harrington, Lee, Crum, Zimmerman, & Hertz-Picciotto, 2014;  Rai, Lee, Dalman, Newschaffer, Lewis, & Magnusson, 2017). Use of acetaminophen (Tylenol and Anacin) during pregnancy is associated with higher risk for attention deficit hyperactivity disorder in children (Liew, Ritz, Rebordosa, Lee, & Olsen, 2014). Use of antifungal medications by a pregnant woman reduces masculinization of her male offspring (Mogensen et al., 2017). Because even the safest and most routine substances are capable of acting as teratogens, pregnant women are well advised to consult with health care providers before taking any type of prescription or over-the-counter medication. The orange bars indicate the overall time during which a system is developing. The yellow bars indicate when particular systems are most vulnerable to disrupted development because of teratogens. TeratogenTypePossible effects on the childAlcoholRecreational drugSmall statureFacial abnormalitiesHyperactivityIntellectual disabilityPhenytoin (Dilantin)Prescription medication for epilepsyHeart defectsIntellectual disabilityCleft palateSlow growthSelective serotonin reuptake inhibitors (Prozac, Zoloft, etc.)Prescription antidepressantsPremature birthAutism spectrum disorderVaricella (chickenpox)Viral infectionScarsSmall head sizeBlindnessSeizuresMalformed and paralyzed limbsZikaViral infectionMicroencephalyAspirinOver-the-counter pain medicationHeart defectsIntellectual disabilitySlow growthAcetaminophenOver-the-counter pain medicationHigher risk for ADHDRecreational drugs consumed by the mother are the most common and preventable sources of adverse effects in the developing fetus. Infants born to women who use tobacco are at risk for premature birth and being underweight, circumstances that are major risk factors for numerous physical and psychological problems later on in the child’s life (CDC, 2010). The intricate programming of nervous system development in the fetus appears to be especially vulnerable to interruption by alcohol consumed by the mother during pregnancy (Haycock, 2009). Alcohol consumption by a pregnant woman can result in a condition known as fetal alcohol syndrome (FAS)fetal alcohol syndrome (FAS)A condition resulting from alcohol consumption by the mother during pregnancy that produces physical abnormalities            fetal alcohol syndrome (FAS)        A condition resulting from alcohol consumption by the mother during pregnancy that produces physical abnormalities            . As shown in Figure 11.7, FAS produces a number of physical abnormalities, including growth retardation, skin folds at the corners of the eyes, nose, and mouth abnormalities, and a small head circumference, as well as cognitive and behavioral problems, including reduced IQ, attention problems, and poor impulse control (Streissguth, 1991). No known amount of alcohol consumption is considered safe during pregnancy, yet about 8% of pregnant women in the United States drinks some alcohol, and about 1.4% binge drink or have five or more drinks at one time (CDC, 2012). Drinking during pregnancy can result in FAS, which produces (a) characteristic facial features, (b) reduced brain volume compared to (c) a typically developing fetus, and attentional and other behavioral problems in children. A mother’s use of alcohol, tobacco, marijuana, or cocaine during pregnancy significantly reduces the volume of gray matter of her child’s brain, but combinations of two or more of these drugs produce even more dramatic reductions. As we mentioned in Chapter 4, gray matter refers to areas populated by neural cell bodies, and the thickness of cortical gray matter is correlated with measures of intelligence. These are not short-term effects because children exposed prenatally to multiple drugs show reduced gray matter thickness and head circumference at 10 to 13 years of age (Rivkin et al., 2008). A pregnant woman can be exposed to viruses that can trigger conditions that occur immediately or later in the child’s life (Cannon, 2003). HIV can be passed from mother to child during pregnancy, birth, or breast-feeding. Prenatal exposure to the mosquito-borne Zika virus, first identified in Brazil in 2015, produces abnormally small brains (microcephaly) and other serious brain abnormalities in the fetus (Rasmussen, Jamieson, Honein, & Petersen, 2016). Newborn babies grasp anything placed in their hand, including a parent’s finger. Human development is characterized by a longer period of dependency than in other primates, including chimpanzees and gorillas. Physical abilities present at birth in other primates, such as independent movement, require about 1 year to develop in human infants (Walker & Shipman, 1996). Human adults house, feed, clothe, protect, and educate their young for the better part of two decades, if not longer. However, even though human newborns are less mature than primate infants of other species, they come into the world with a number of useful capacities. The newborn stage of life begins officially at birth and lasts 28 days. Newborns have a number of reflexive behaviors that begin to operate immediately (see Table 11.2). Babies immediately turn their head to the source of a touch, open their mouth, and search for the mother’s nipple. This rooting reflex assists the newborn with the essential process of feeding. If an object is placed in the mouth, babies begin to suck reflexively, which also leads to effective feeding. Very young babies reflexively grasp any object placed in the hand, which leads to the later voluntary grasping of objects. If parents hold newborns upright with their feet touching a surface, babies show a stepping reflex, which possibly serves as a basis for later walking (Zelazo, 1998). Reflex nameActionPossible purposeBabinskiStroking the baby’s foot causes toes to spread out. Unknown; disappears as nervous system matures. BlinkEye closes in response to strong stimuli. Protects eyes. MoroIf the baby’s head falls backward, the arms first spread out and then hug. A possible evolutionary remnant allowing primates to cling to an adult. PalmarPlacing an object in the hand produces reflexive grasping. A possible evolutionary remnant allowing primates to cling to an adult’s fur. RootingStroking a baby’s cheek results in the baby turning toward the touch and opening the mouth. Helps the baby nurse. SteppingPlacing the baby’s feet on a flat surface initiates stepping. Possible precursor to walking. SuckingThe baby sucks anything that touches the roof of the mouth. Helps the baby nurse. Newborns spend up to 16 to 18 hours per day sleeping. As we mentioned in Chapter 6, a large proportion of this time is spent in REM sleep, which might participate in the wiring of the newborn’s brain. A child’s sex chromosomes, XX or XY, typically begin a trajectory towards adult maleness and femaleness but not always. Intersex refers to the presence of male and female features in the same individual. Model Hanne Gaby Odiele recently disclosed that she has androgen insensitivity syndrome (AIS). People with AIS are genetic males (XY) who do not respond to circulating male hormones in utero. They are born with female-appearing external genitalia and typically maintain female gender identities. Because the female reproductive organs are suppressed during fetal development, these individuals are likely to be infertile. During times of wakefulness, the baby alternates between periods of alert looking about and periods of physical movement of the arms and legs. Unfortunately for parents, most newborns also spend about 2 to 3 hours per day either crying or being close to tears. As upsetting as this crying may be, it represents the infant’s first efforts at communicating with the parents. Babies generally cry for a reason, such as hunger or pain, and most parents are stimulated by the crying to find ways to make their infant more comfortable. We refer to physiological characteristics, such as having an XX or XY genotype as sex, while psychosocial aspects of the maleness—femaleness continuum are referred to as gender. The receipt of either an XX or XY genotype at conception (there are variations, though) begins a cascade of hormonal, structural, and behavioral events that lead to mature sex and gender. The appearance of the newborn’s external genitalia remains an important marker of sex and gender. The appearance of the external genitalia is usually, but not always, consistent with genetic sex. In cases of androgen insensitivity syndrome (AIS), an individual with male genetic sex (XY) cannot respond to circulating male hormones, leading to female external appearance. A very rare condition known as 5-alpha-reductase syndrome results in ambiguous genitalia at birth with later masculinization at puberty. Congenital adrenal hyperplasia (CAH) exposes the fetus to excess male hormones, which has little effect on male sexual development but might masculinize the external genitalia of genetic females (Meyer-Bahlburg et al., 2016). Newborns have significant, although immature, sensory capacities. Because babies can’t talk, researchers have had to be creative in assessing their sensory capacities (see Figure 11.8). Measures of heart rate, facial expression, and head movements correlate with distinctions among stimuli. Researchers also take advantage of habituation, the type of learning in which we reduce responding to repeated stimuli (see Chapter 8). Because of habituation, babies gaze longer at a new stimulus than at one they have seen previously. When babies spend equal time looking at two stimuli, we can assume they cannot tell the difference between them. By studying the amount of time infants look at a new stimulus after habituation, such as the face of a person, Ross Flom and Lorraine Bahrick were able to demonstrate that infants as young as 4 months could correctly perceive emotional expression if they both saw and heard the same expression simultaneously. The ability to perceive emotion using voice alone appeared around the age of 5 months, and the ability to perceive emotion from facial expression alone appeared around 7 months. Newborns show considerable sensitivity to smells and respond differently to pleasant smells such as chocolate or honey than to unpleasant smells such as rotten eggs (Maurer & Maurer, 1988). They are also capable of recognizing their mother by smell, which has obvious survival advantages. Babies even recognize their mother’s favorite brand of perfume (Porter, 1991). Infants are quite sensitive to taste and can respond differentially to the major taste qualities of sweet, sour, bitter, and salty. They apparently have a sweet tooth and nurse longer when their breast-feeding mother has consumed sweet-tasting foods (Mennella & Beauchamp, 1996). The fetus can hear quite well beginning in gestational month seven, but improvement in hearing continues after birth. We might tiptoe around a sleeping newborn, but older children and adults have superior hearing for very quiet sounds. The infant’s ability to hear is best for sounds found in the range of frequencies that normally occur in human speech. This ability allows infants to learn language. Infant vision can be tested by measuring the amount of time the child spends viewing a pattern rather than a uniform screen. Using this technique, we can demonstrate that young infants do not see detail at a distance as well as adults do. In addition, infants need more contrast than adults to see well. As we discussed in Chapter 5, contrast refers to differences in intensity between adjacent stimuli. The black letters on this white page are an example of high contrast, whereas dark gray letters on a black background would be an example of low contrast. These features of infant vision probably explain babies’ preference for large, high- contrast, colorful objects. Newborns demonstrate an innate preference for looking at faces (Otsuka, 2014) (see Figure 11.9). This capacity sets the stage for further social behavior and language learning. We know that infants who refuse to make eye contact often develop social and language impairments later (Knickmeyer, Baron-Cohen, Raggatt, & Taylor, 2005;  Mundy et al., 2007). Other primates show similar preferences for faces, which may be related to the importance of social relationships in primate species, including our own (Thierry, 1994). Newborns looked at the stimulus on the left for about 16 seconds longer than the less facelike stimulus on the right. Looking time has been used as one measure of preference for infants. The emerging picture of the newborn infant is one of immature yet formidable capacities. The remainder of this chapter will explore the unfolding of these capabilities as the individual matures. StageTimingHighlightsZygoteGestational weeks 1–2Differentiation into germ layersEmbryoGestational weeks 3–8Nervous system and organs formSex organs differentiate into male and femaleFetusGestational week 9 through remainder of pregnancyGrowth and maturation of existing organsREM sleep from seventh month of pregnancyGood hearing from seventh month of pregnancyNewbornFirst 28 days of lifeReflexive movementLarge amounts of sleepImproving sensory capacitiesPreference for facesPhysical development in infancy and childhood features rapid growth, particularly during the first year of life, when children triple their birth weight (see Figure 11.10). Infants grow rapidly, tripling their birth weight during the first year alone. As they grow, their proportions also change significantly. The head takes up approximately one-quarter of a newborn’s height, versus about one-seventh to one-eighth of an adult’s height. In contrast, the ratio of the trunk to the overall height, about three-eighths, remains the same over the life span. In the latter months of pregnancy and for the first 18 months of life, the human brain shows rapid growth in gray matter or collections of neural cell bodies, which we discussed in Chapter 4. Following this burst of brain growth, cells and connections that are not useful are systematically deleted. In other words, we produce more neurons and synapses than we will eventually need and then keep only those that are working well for us (see Figure 11.11). During development, we produce more synapses than necessary. Synapses that are not useful are deleted in a process known as synaptic pruning. Pruning takes place at different rates in different systems, as illustrated by this comparison of pruning in the visual and prefrontal cortices. This process emphasizes the importance of experiencing enriched environments during development. If a child’s rate of growth during the first year of life continued, a 10-year-old would be about the size of a jumbo jet (McCall, 1979). What does it mean to have neurons and synapses that are working well? The nervous system develops according to a use it or lose it principle, which emphasizes the important role of experience in wiring the brain. Children who are raised in a stimulating and enriched environment, with many things to explore, are likely to have the best outcomes. Children living in intellectually impoverished circumstances may retain too few connections, which may lead to mild forms of intellectual disability (see Chapter 10). Myelination, the growth in white matter that begins around gestational month six, continues to develop at a regular rate throughout childhood and adolescence (see Figure 11.12) (Houston, Herting, & Sowell, 2013). An interesting spurt in myelination occurs between the ages of 6 and 13 years in parts of the brain associated with language and spatial relations (Thompson, 2000). The rather sudden end of this white matter growth coincides with the end of a sensitive period for language development. Bilingual individuals who learn language after the age of 10 or so process their second languages differently than do native-language speakers (van Hell & Tokowicz, 2010). Unfortunately, most students in the United States begin foreign language study after this age. Imaging technologies have captured the progress of myelination (white matter growth) in childhood. Red and yellow areas show the largest changes. Myelinated systems are faster and more efficient than unmyelinated systems. As we mentioned earlier, newborn human infants are relatively helpless compared to their chimpanzee and gorilla counterparts. Newborn humans’ mobility is handicapped by their proportions. One quarter of their length is made up by newborns’ heads. In contrast, adults’ heads make up approximately one-seventh to one-eighth of their height. Motor development, or change in our ability to move and perform physical skills, is largely driven by our human genetic blueprint. Identical twins typically reach motor milestones such as walking with more similar timing than that of fraternal twins or nontwin siblings (Fox, Hershberger, & Bouchard, 1996). Because identical twins share more genes in common compared with fraternal twins, reaching milestones at the same time indicates an important role for genetics in early motor development. Motor development in childhood proceeds simultaneously in two directions. First, we see development in the head-to-toe direction. Controlling the muscles of the neck and shoulders allows 2-month-old infants to raise their head to look around. This achievement is followed by development of the muscles of the torso around 3 months of age, which are necessary for rolling and sitting. Between 6 and 9 months, babies begin to crawl, although some happily skip this stage. Finally, around the first birthday, the muscles of the legs are developed sufficiently to support the weight of a child when standing or walking. Well into middle childhood, children continue to improve their movement and coordination, demonstrated by the abilities to skip rope, hop on one foot, and combine movements, such as throwing a ball while running (see Figure 11.13). These motor milestones emerge at predictable times over the course of childhood but can range significantly from child to child. The ages in this chart represent the average time at which children reach a milestone. Parents in most cultures attempt to teach toileting practices when children are about 2 years old. Not coincidentally, the motor systems required to achieve sphincter control are generally myelinated somewhere around 21 months of age (Largo, Molinari, von Siebenthal, & Wolfensberger, 1996). The second direction of motor development begins at the midline, an imaginary line dividing the human body in equal halves, and proceeds outward. Infants can bat at toys suspended above their cribs (3 months) before they are able to grasp objects (5 months). In other words, accurately directing the arms precedes the ability to direct the hands and fingers. Incidentally, infants learn to grasp objects before they learn to release them, a fact parents may wish to consider before wearing long hair or dangling earrings. Well into elementary school, children are continuing to master control over the fine muscle movements of the hands and fingers required for writing. Perhaps you recall the rather large-lined paper you used in first grade and the challenges of coloring within the lines. Kenyan parents often encourage early walking in their children. Compared to American children, whose average age of walking unassisted is about 1 year, Kenyan children observed by  Super (1976) walked between the ages of 7 and 11 months. In contrast, practices such as swaddling young infants are associated with slightly later attainment of motor milestones. Although early motor milestones are driven largely by biology, they respond to experience. It is possible to slow down motor milestones by restricting movement. Some cultures, out of safety concerns, restrict the movement opportunities of young children, which slightly delays motor milestones such as walking alone (Kaplan, 1987). In other cases, motor milestones may be reached slightly earlier by providing specific practice in sitting, standing, and walking (Super, 1976). These changes in the developmental timeline are usually slight, because myelination of motor nerves usually just precedes the achievement of a new skill. Once the relevant parts of the nervous system are sufficiently mature, new skills develop rapidly with little practice. Like most other types of human behavior, gender identity, or the sense of one’s own place on a scale from maleness to femaleness, develops in response to both biological and environmental factors. Children begin to prefer sex-typed toys between the ages of 12 and 18 months, although at this age, they are not yet able to match sex-typed toys (vehicles or dolls) with male or female faces and voices. This suggests that they have not yet been socialized to think of particular toys as male or female (Serbin, Poulin-Dubois, Colburne, Sen, & Eichstedt, 2001). The fact that young vervet monkeys make similar sex-typed toy choices as young humans suggests that socialization has relatively little to do with these early differences in behavior (Alexander & Hines, 2002). Cases of girls with CAH, who experience unusually high levels of prenatal exposure to male hormones also provide insight into toy choice. In spite of the fact that many parents of CAH girls are determined to socialize them in a feminine direction to make them normal, the girls show much greater interest than typically developing girls do in masculine toys (see Figure 11.14) (Hines, 2010). This is not to say that parental and societal socialization has no impact on gender identity, but rather to remind us that biology and experience interact to produce an outcome. Congenital adrenal hyperplasia (CAH) results in higher prenatal exposure to male hormones in both females and males. Girls with CAH show less interest in girls’ toys and more interest in boys’ toys than is shown by healthy females. Boys with CAH show the same interest in both types of toys as do healthy boys. Very young vervet monkeys show the same toy preferences as young human children, suggesting that biology plays a role in the choice of sex-typical toys. By the age of 3 years, children begin using gender labels consistently for themselves, although we don’t know for certain whether this represents an awareness of their assigned gender or an identification with their assigned gender (Meyer-Bahlburg et al., 2016). A small number of children with unambiguous biological sex develop incongruent gender identity. Complex interactions among genetic, hormonal, cognitive, and psychosocial factors contribute to the development of a transgender identity. Follow-up studies suggest that earlier social transition to the experienced gender is the most reliable predictor of persistence with that gender in adulthood (Meyer-Bahlburg, 2013). The concept of gender has been transformed from a binary, categorical system (male or female) to more of a bimodal continuum (see Chapter 2). This transition is evident in the legal codes of many nations, where official government documents such as passports now provide options other than male or female, and in the social media giant Facebook’s 58 choices for personal identifiers, plus unique identifiers that users choose for themselves. Health care providers find no easy solutions within this environment when faced with cases of individuals with ambiguous physical features resulting from chromosome and hormonal influences. Gender assignment using the more traditional binary choices, in spite of passports and Facebook, remains a common goal within the medical community. One of the most contentious areas of gender assignment involves individuals born with ovotesticular syndrome (Meyer-Bahlburg et al., 2016). These individuals used to be referred to as true hermaphrodites, as they possess a gonad that contains both testicular tubular structures and ovarian follicles. Typically, about half of the newborns in this category are assigned to the female gender and the other half to the male gender, but clinicians do not necessarily agree on the criteria for making this judgment. Some are more influenced by the surgical challenges presented by an individual case while others consider fertility. As a result, many individuals with ovotesticular syndrome later develop gender dysphoria, a sense of mismatch between their assigned gender and their identities, and/or patient-initiated gender change. Perhaps the fluidity of the concept of gender will make gender assignment decisions on the part of the medical community simply unnecessary. In the meantime, further research supporting evidence-based decisions (see Chapter 15) that include the goals of the patient would be very helpful (Consortium on the Management of Disorders of Sex Development, 2006). Advocates for individuals with intersex are encouraging medical professionals to postpone surgical and hormonal treatments until a child is old enough to participate in decisions about their sex and gender. Interacting with brain development and the child’s ability to move and explore the environment are changes in the way children process information and solve problems. Even when children are exposed to the same information as adults, their thinking often leads them to different conclusions. When asked why clouds move across the sky, a 4-year-old might suggest that the clouds are pulled by airplanes, but an adult is unlikely to come to this conclusion. Both child and adult have observed clouds and airplanes moving across the sky, but viewing these occurrences as related is something that only a small child would do. To understand these differences, we need to examine how cognition changes as a function of age and experience. A preschool child might believe that airplanes pull clouds across the sky, but an adult is unlikely to come to the same conclusion, even though they are watching the same events. One of the classic theories about the development of cognition was proposed by Swiss psychologist Jean Piaget (1896–1980). While working with Théodore Simon, who along with Alfred Binet developed some of the first intelligence tests (see Chapter 10), Piaget became more interested in the errors made by the children than in their correct responses. He believed that these errors were representative of growth in the child’s ability to reason. Later, Piaget’s views were challenged and revised by psychologists who believed that he underestimated the capabilities of children at particular ages. We will explore these contemporary approaches after we look more carefully at Piaget. According to Piaget’s theory, cognitive abilities develop through regular stages, making Piaget’s work a classic example of the discontinuity approach discussed earlier in this chapter. Cognition matures as the child increasingly uses concepts and organizing schemas to think. We discussed concepts and schemas in Chapters 9 and 10. Two types of adjustments can be made to a schema, depending on the nature of discrepancies between the new information and the existing category. Assume that a child has a rather well-developed schema for birds, which includes features such as has wings, has feathers, and can fly. If the child then learns about a new species of bird, such as a hawk, the child can assimilate the new information into the existing bird schema: Hawks have wings, feathers, and the ability to fly. In assimilationassimilationThe incorporation of new learning into an existing schema without the need to revise the schema.            assimilation        The incorporation of new learning into an existing schema without the need to revise the schema.            , no changes to the existing schema are required to add the new instance. However, let’s now assume that the child meets a kiwi, a small, flightless bird from New Zealand. The kiwi doesn’t fit neatly into the existing schema, which assumes that all birds can fly. The schema must be adapted to fit the new information, a process Piaget called accommodationaccommodationThe incorporation of new learning into an existing schema that requires revision of the schema.            accommodation        The incorporation of new learning into an existing schema that requires revision of the schema.            . Through accommodation, the child now understands that although most birds can fly, some exceptions occur. Jean Piaget (1896–1980). If children can learn about robins without making changes to their schema of bird, assimilation will occur. In contrast, learning about flightless kiwis might require changes in the bird schema, and accommodation will occur. In addition to his interest in the development and refinement of schemas, Piaget made many other observations about cognition at different ages. We will now explore the particular characteristics found in each of Piaget’s four stages (see Table 11.3). StageApproximate ageHighlightsSensorimotor stageBirth to age 2Here and now rather than past and futureExploration through moving and sensingObject permanencePreoperational stage2–6 yearsLanguage acquisitionEgocentrismIllogical reasoningConcrete operational stage6–12 yearsLogical reasoningMastery of conservation problemsLearning by doingFormal operational stage12 years and olderAbstract reasoningIdealismImproved problem solvingPiaget’s first stage, the sensorimotor stagesensorimotor stageJean Piaget’s stage of development beginning at birth and ending at the age of 2 years and characterized by active exploration of the environment.            sensorimotor stage        Jean Piaget’s stage of development beginning at birth and ending at the age of 2 years and characterized by active exploration of the environment.            , begins at birth and lasts until the child’s second birthday. The name of this stage reflects the infant’s focus on the here and now rather than the past and the future. Sensations immediately evoke motor responses. As soon as they can crawl, infants work their way (usually quite rapidly) through the environment, exploring new objects by interacting with them physically, particularly by putting them in their mouths. Infants do not seem particularly deterred by features such as taste and are very likely to sample the drain cleaner under the sink unless parents take appropriate precautions to babyproof their home. One of the major milestones that occur during the sensorimotor stage is the achievement of object permanenceobject permanenceThe ability to form mental representations of objects that are no longer present.            object permanence        The ability to form mental representations of objects that are no longer present.            . Infants under the age of 8 months or so do not seem to be able to form clear memories or mental representations for objects once they are removed from the immediate present. If Piaget covered his gold pocket watch with his beret while an infant watched, the infant would not show signs of searching for the now missing watch. Around the age of 8 months or so, infants become more persistent. If he hid the pocket watch, 8-month-olds would look for it. Object permanence coincides with achieving sufficient growth in the prefrontal cortex (Diamond & Goldman-Rakic, 1989). During the sensorimotor period, the child’s language abilities are developing rapidly. By the age of 18 months, children usually have a working vocabulary of 10 to 50 words, and by the age of 2 years, they are beginning to combine words into short but meaningful sentences, such as Want cookie or Go bye-bye. These new ways of thinking symbolically lead the child into the subsequent preoperational stage. Piaget’s preoperational stagepreoperational stageJean Piaget’s stage of development beginning at the age of 2 years and ending at the age of 6 years and characterized by use of symbols, egocentrism, and limits on the ability to reason logically.            preoperational stage        Jean Piaget’s stage of development beginning at the age of 2 years and ending at the age of 6 years and characterized by use of symbols, egocentrism, and limits on the ability to reason logically.             lasts from approximately age 2 to age 6. The stage gets its name from the notion that children are still incapable of engaging in internal mental operations or manipulations, such as following the transformation of objects from one form to another. Piaget used a variety of creative tasks to demonstrate the characteristics of the reasoning of the preoperational child. ConservationConservationThe ability to understand that changing the form or appearance of an object does not change its quantity.            Conservation        The ability to understand that changing the form or appearance of an object does not change its quantity.             tasks require the child to recognize that changing the form or appearance of an object does not change its quantity. One of the observations that led Piaget to consider cognitive development as occurring in stages is the rather abrupt way in which conservation tasks are often solved. Children who could not solve the problem a few weeks earlier suddenly look at the adult experimenter with pity, as if to say, This may be the world’s stupidest grown-up. Preoperational children are unlikely to recognize that pouring liquid from one glass to another of a different size and shape does not change the amount of liquid. They usually believe that the glass that is higher holds more. The abruptness with which children master conservation tasks led Jean Piaget to suggest that cognitive development proceeds in stages, but other psychologists believe changes occur more gradually. In addition to difficulty with transitions, preoperational children have limited abilities to understand points of view other than their own, a characteristic described by Piaget as egocentrismegocentrismLimitations on the ability to understand the point of view of other people.            egocentrism        Limitations on the ability to understand the point of view of other people.            . One kindergarten teacher asked his class where the sun went at night and received an enthusiastic response of Yuba City (which is located in northern California) from one of the students. As it turned out, the young student had moved recently from Yuba City to his current home in San Luis Obispo. Because these were the only two places on earth the child knew, if he was in one place and the sun wasn’t there, there was only one other logical place for the sun to be. You guessed it—Yuba City. Preoperational thinking is also limited by beliefs that appearances are real. While older children understand that movies feature real people dressed up and acting like superheroes, the younger child might believe that not only are superheroes real, but also that they can really fly. The young child’s inability to distinguish fantasy from reality has led to a lively debate about managing entertainment materials for children. Is it appropriate to let young children believe that fairy tales are real? Piaget referred to the stage between the ages of 6 and 12 as the concrete operational stageconcrete operational stageJean Piaget’s stage of development beginning at the age of 6 years and ending at the age of 12 years and characterized by logical but not abstract reasoning.            concrete operational stage        Jean Piaget’s stage of development beginning at the age of 6 years and ending at the age of 12 years and characterized by logical but not abstract reasoning.            . Rather suddenly, problems of conservation are easily solved, and thinking becomes more logical. The only remaining limitation is an inability to handle abstract concepts, which will not be resolved until the next and final stage. Piaget used the term concrete operations for this stage because he observed that children reasoned best when allowed to engage in hands-on learning. This observation of Piaget’s has had an enormous impact on the way early childhood education is conducted in many nations. Math instruction now features colorful boxes of manipulatives, which allow children to count and sort real things. Children’s museums, petting zoos, interactive websites, and other opportunities for experiential learning have become commonplace. For Piaget, cognitive development matures in the formal operational stageformal operational stageJean Piaget’s stage of development beginning at age 12 and extending through adulthood and characterized by mature reasoning capabilities            formal operational stage        Jean Piaget’s stage of development beginning at age 12 and extending through adulthood and characterized by mature reasoning capabilities            , which begins around the age of 12. The final piece to be added to the child’s cognitive skill set is the ability to handle abstract concepts. Abstract concepts usually involve what if types of questions and form the heart of scientific inquiry. If you were to ask a group of sixth graders what would happen if everyone on Earth became blind tomorrow, they would likely struggle to come up with a reasonable answer. A group of high school students, however, would readily reply that people who were already blind would become instant leaders because they would have existing skills for coping with blindness. Accompanying the ability to think abstractly is a big improvement in problem solving. Younger children tend to approach problems by trying things out and seeing what happens, a rather inefficient trial-and-error approach. Teens are more likely to think through several alternatives more strategically. Abstract thinking ability also appears to stimulate a burst of idealism. What if we could solve world hunger? These are the types of questions teens begin to ask, and they are not always content with the more practical and less idealistic responses of older adults. Piaget’s classic theory set the standard for his time, but more contemporary developmental psychologists argue that some aspects of his view of cognitive development need updating and revising. One of Jean Piaget’s legacies to early childhood education is his emphasis on hands-on learning. Colorful objects or manipulatives have replaced endless worksheets used to teach math concepts such as sorting to older generations of students. Piaget’s strong interest in biology led him to look at human development from a species perspective, which in turn reduced his interest in individual differences. Other psychologists point out that individual cognitive development can be quite variable. Some adults fail to achieve the ability to reason abstractly or do so only on some problems and not on others. This variability makes cognitive development look more gradual and continuous than stagelike, as suggested by Piaget. Piaget did not specify mechanisms responsible for moving from one stage to the next. Today, using brain imaging methods, we can explain some of these changes, such as the correlation between the achievement of object permanence and frontal lobe development. Other transitions, such as the relatively sudden mastery of conservation problems, are not so easily explained. Many psychologists believe that Piaget underestimated the capabilities of young children. At the same time, he might have overestimated the cognitive abilities of adolescents. As we discussed in Chapter 4, the human brain is not fully mature until a person’s early 20s, so we would not expect fully adult cognition in an adolescent. Finally, Piaget has been criticized for viewing developing children in relative isolation from their family, community, and culture. These criticisms have formed the basis for extensions to Piaget’s approach, described in Alternative Approaches to Cognitive Development. In the next sections, we will explore alternatives to Piaget’s view of cognitive development. An alternative to Piaget’s approach was proposed by Soviet psychologist Lev Vygotsky (1896–1934). Piaget and Vygotsky illustrate the theme of universal versus ecological development introduced at the beginning of the chapter. Unlike Piaget, whose interest viewed human development as universal across cultures, Vygotsky (1934/1962) stressed the role of culture and cultural differences in the development of the child. For Vygotsky, cultures teach children not only what to think but also how to think. A burst of idealism, leading to a desire to make the world a better place, often accompanies the onset of formal operational thinking in adolescence. Piaget’s developing individual learns to understand the world by actively exploring it, like a miniature scientist, whereas Vygotsky’s individual gains knowledge of the world by interacting socially and collaboratively with parents, teachers, and other members of the community (see Figure 11.15). Language was a particularly important aspect of cognitive development for Vygotsky. Children first use language to initiate social contact and opportunities to learn. Later, self-directed talk, such as I need to finish my game before dinner, aids in problem solving and, in the older child, forms inner speech. Unlike Jean Piaget, Lev Vygotsky viewed development as taking place within the social and cultural environments. His zone of proximal development included tasks that the child could learn to do with the assistance of parents, teachers, or other adults or peers. Vygotsky’s advice is to provide children with activities that are within reach but neither too easy nor too hard. Information processing models of cognitive development provide important extensions to Piaget’s theory (Kail & Bisanz, 1991). These models describe specific changes in the child’s ability to reason. The overall rate of processing information, analogous to your computer’s CPU speed, increases during childhood (Cerella, 1994). In tasks that require judgment, such as deciding which of two numbers is larger, adults are three times faster than 4- and 5-year-olds and twice as fast as 8- and 9-year-olds (Kail & Bisanz, 1992). Surprisingly, by the time children enter elementary school, their performance on tasks requiring focused attention and disregard of distracters reaches adult levels (McKay, Halperin, Schwartz, & Sharma, 1994). In contrast, the ability to sustain attention over time remains limited until the age of 11 years. Around this age, attention span begins to lengthen significantly each year until adulthood. Children’s memory abilities change dramatically following their second year of life (Bauer, Larkina, & Deocampo, 2011;  Bauer & Lukowski, 2010). These changes are probably due to maturation of brain structures related to memory. The amygdala and most of the hippocampus are relatively mature at a very early age, around 6 months, but changes in the frontal lobe and hippocampus during the third year of life allow children to form more long-term memories, particularly for sequences of events. Improved memory and a growing sense of self combine to produce some of the first autobiographical memories between the ages of 3 and 5 years (Wang, 2008). Piaget suggested that children’s abilities to understand objects in their world develop slowly over time. Contemporary developmental psychologists believe that this assertion was wrong. Instead, very young children seem to understand a great deal about objects and how they work, even before they have had much experience interacting with them (Baillargeon, Li, Gertner, & Wu, 2011). For example, infants as young as 2-1/2 months old looked longer at an impossible situation (a toy mouse that should have been seen moving between two blocks did not appear), as shown in Figure 11.16. Many contemporary psychologists believe that Jean Piaget underestimated the reasoning abilities of young children. Children as young as 2-1/2 months of age looked longer at this impossible situation (the mouse toy, which is moved from left to right behind the blocks, should have been visible as it passed between the two blocks). Piaget used the concept of egocentrism to describe the young child’s relative lack of awareness of the viewpoints of others. An elaboration of this concept, theory of mind (TOM)theory of mind (TOM)The understanding that others have thoughts that are different from one’s own.            theory of mind (TOM)        The understanding that others have thoughts that are different from one’s own.            , has emerged as an important tool in tracking typical and abnormal development. TOM occurs when people understand that others have beliefs, desires, and intentions that are different from their own (Premack & Woodruff, 1978). While Premack argued that TOM in humans was an extension of similar abilities in apes, others believed this was a uniquely human ability. Recent research making use of eye-tracking technology, however, has provided stronger evidence that Premack’s observations about apes were correct (Krupenye, Kano, Hirata, Call, & Tomasello, 2016). The classic procedure for demonstrating TOM is the false belief task, often called the Sally–Anne task (Wimmer & Perner, 1983). Imagine that one doll, Sally, places her ball in a basket and then leaves the room. Another doll, Anne, enters the room, moves the ball from the basket to a box, and then leaves. If Sally comes back, where will she look for her ball? Children who have developed a TOM understand that their personal knowledge of where the ball is located is different from the knowledge of Sally, who was absent when the ball was moved. They will correctly decide that Sally will look in the basket (see Figure 11.17). Children who have achieved TOM recognize that their personal knowledge of the whereabouts of Sally’s ball is different from Sally’s knowledge, and they will predict correctly that she will look in the basket. Developmental psychologists generally agree that TOM emerges in children around the age of 3 to 4 years, somewhat earlier than the end of Piaget’s preoperational period. We can identify behaviors in younger children that seem to act as building blocks leading to the achievement of TOM. Joint attention, which includes behaviors such as following another person’s gaze with one’s own and using pointing to direct another person’s attention to something in the environment, emerges in the first year of life (Barresi & Moore, 1996). The ability of young children to distinguish between living and nonliving objects is another important step in the development of TOM. Finally, young children appear to make distinctions between intentional and unintentional behaviors, because 3-year-olds will imitate the former but not the latter (Williamson, Jaswal, & Meltzoff, 2010). TOM appears critical to further social development, the topic of How Do Social and Emotional Behaviors Change During Infancy and Childhood?. A failure to develop a typical TOM has been linked to the development of autism spectrum disorder, a condition characterized by extreme social difficulties that we discuss in more detail in Chapter 14 (Gopnik, Capps, & Meltzoff, 2000). The physical and cognitive development we have witnessed so far does not occur in a vacuum. Children mature within complex environments affected by their own intrinsic tendencies, the responses to those tendencies on the part of others, and the ways in which they are nurtured. Children’s temperaments reflect their prevailing patterns of mood, activity, and emotional responsiveness. Differences along these initial dimensions predict adult personality. Mary  Rothbart (2007) divided temperament into three categories: Surgency or extroversion (happy, active, vocal, and social), negative affect or mood (angry, fearful, shy, and frustrated), and effortful control (the ability to pay attention and inhibit behavior). While temperament may predispose a child to interact with the environment in certain ways and affect the behavior elicited from caregivers and others, the environment in which children find themselves also has an impact. For example, fearful children with gentle parents develop a stronger internal sense of right and wrong than do fearful children with punitive parents (Rothbart, 2007). Similar interactions between temperament and parenting have also been reported in rhesus monkeys, emphasizing the possible biological and evolutionary contributions to these outcomes (Suomi, 2006). Rhesus infants with a particular genotype were more aggressive, but only if they were raised by their mothers rather than by peers (see Figure 11.18). In rhesus monkeys, variations in a gene called monoamine oxidase A (MAOA) are related to aggressive behavior. Monkeys with the low-activity variant are typically more aggressive and attain higher rankings within the monkeys’ dominance hierarchy than do monkeys with the high-activity variant of the gene. This relationship is seen when the monkeys are raised normally with their mothers. However, when monkeys are raised in peer groups without contact with their mothers, the relationship is reversed: The monkeys with the high-activity version are more aggressive. This research supports the goodness-of-fit interactions between child temperament and parenting style proposed by Alexander Thomas and Stella Chess and by Mary Rothbart. Temperament interacts not only with the socialization provided by parents but also with the larger-scale socialization provided by culture. In the United States, but not in China, children with higher effortful control showed lower levels of negative affect, such as anger. In China, but not in the United States, higher effortful control was correlated with less surgency or extroversion (Ahadi, Rothbart, & Ye, 1993). Because of the physical dependency of the human infant on the adult caregiver, maintaining closeness is a high priority. Infants engage in a number of behaviors, such as smiling, cooing, and crying, that are effective in ensuring adult attention. There is a big difference between a newly hatched goose following its mother because of imprinting and the two-way interactions that promote attachment between human mothers and their newborns, including body contact, mutual gaze, and exaggerated facial expressions. Are these more sophisticated interactions exclusively human? Research evidence suggests that the answer to this question is no. Rhesus monkeys also show complex mutual behaviors between mothers and their newborns (Ferrari, Paukner, Ionica, & Suomi, 2009). The Question: Do rhesus monkeys have behaviors that are the equivalent of human mother–infant behaviors that promote bonding? Fourteen mother–infant rhesus monkey pairs were observed while they were living among social groups housed in indoor–outdoor enclosures. Six additional pairs were observed in a more naturalistic setting. For the first 2 months of the infants’ lives, the researchers filmed and recorded the frequencies of mutual gaze (infants and mothers making eye contact) and lip smacking (an important rhesus social behavior). All ethical principles regarding animal research outlined in Chapter 1 would apply here. The observational method used in this study poses little risk of violating these principles. The rhesus infant–mother pairs engaged in frequent mutual gaze and lip-smacking behaviors. The infants spent more time gazing at their mothers than at other individuals, and this time increased with the age of the infant. Lip-smacking behavior was also a frequently observed method for mothers and infants to interact with each other. This behavior is never observed among adults, so it is exclusive to interactions with infants. Mutual gaze and lip smacking both appear to be sensitive to the age of the infant. By the time the infant is in its second month of life, it is spending considerable time outside the mother’s vicinity, much as a human child might do when it attends day care or preschool. As the infant physically separates more frequently from the mother, the frequency of the mutual gaze and lip-smacking behaviors also decreases. The behaviors observed in the enclosed area were confirmed by observations in the more naturalistic setting. Rhesus monkeys show some of the same types of mother–infant interactions, believed to enhance bonding, that have been observed in humans. This finding suggests that behaviors that set the stage for social interactions in humans are not unique to our species but rather have their roots in the evolutionary past. Rhesus monkey mothers and infants show many of the same types of behavior that enhance bonding in human mothers and infants, including mutual gaze. AttachmentAttachmentEmotional bond linking an infant to a parent or caregiver.            Attachment        Emotional bond linking an infant to a parent or caregiver.             between child and parent is not unique to humans. To identify behaviors leading to attachment between infants and their mothers, Harry Harlow (1905–1981) investigated attachment in rhesus monkeys (Harlow, 1958). The Freudian theories still popular in Harlow’s day suggested that the pleasure obtained through feeding formed the basis of infants’ bond with their mothers. Behavioral approaches suggested that the infant was positively reinforced with food for staying near the mother. Harlow was not convinced by either of these arguments and set out to contrast the mother’s ability to provide food with her ability to provide comfort, security, and safety. Harry Harlow (1905–1981) studied infant attachment by removing baby rhesus monkeys from their mothers at birth and providing them with a wire mother, such as the one shown here, and a similar mother covered with carpet. Regardless of which mother provided food, the infant monkeys spent more time cuddling with the cloth mother than the wire one. Harlow concluded that contact comfort provided by the mother was more important to attachment than the food she provided. After removing newborn rhesus monkeys from their biological mothers, Harlow presented the infants with two surrogate mothers, one made of wire and the other of cloth. Either surrogate could be rigged with a milk bottle, allowing Harlow to investigate all combinations of comfort and feeding. Regardless of which mother provided food, infant monkeys spent most of their time clinging to the cloth mother and ran to her immediately when threatened by a novel toy. The wire mothers were visited only if they provided food, and then for only as long as the infant was feeding. Based on his observations, Harlow concluded that the Freudian and behavioral emphasis on feeding was wrong. Instead, the mother’s ability to provide contact comfort was critical in forming a strong attachment on the part of her infant. A key factor in predicting the timing of attachment appears to be mobility. Young birds that are mobile upon hatching need to form immediate attachments to a protective adult. Human infants, who are not particularly mobile until the second half of their first year of life, have more time to bond with a caregiver. About the same time infants begin to crawl, usually between 6 and 8 months, they also begin to show evidence of having bonded with particular people in the form of separation anxiety (Bowlby, 1969/1982, 1973/1999a, 1973/1999b). Before this stage, infants tolerate being handed around to admiring relatives and babysitters without much protest. Once children demonstrate stranger anxiety, however, they respond to unfamiliar people with crying and distress. Although stranger anxiety might be a nuisance for parents hiring a new sitter or an insult to visiting relatives, it is an important step forward in infants’ social development. The children recognize who does and, more importantly, who does not belong in their social world. Several patterns of attachment between infant and caregiver were demonstrated in classic research by Mary Ainsworth and her colleagues (Ainsworth, Blehar, Waters, & Wall, 1978). Young children in a laboratory playroom were observed through a two-way mirror as their mothers or a friendly stranger left or entered the room. Based on the children’s reactions to these separations and reunions, their attachment to their mothers was classified as either secure or insecure. Children who demonstrated secure attachmentsecure attachmentA pattern of infant–caregiver bonding in which children explore confidently and return to the parent or caregiver for reassurance            secure attachment        A pattern of infant–caregiver bonding in which children explore confidently and return to the parent or caregiver for reassurance             played happily and interacted positively with the stranger as long as their mothers were present. However, when their mother left, the children responded by searching for her, crying, and showing other signs of distress. Efforts by a stranger to comfort the children were rejected. When their mother returned, she was greeted warmly, and the children returned to their play. Secure attachment results from responsive parenting that teaches children that the parent is there for them. Unresponsive parenting teaches children that their needs may or may not be met, so their best strategy is to make regular, frequent, and persistent demands for the parent’s attention. The remaining children demonstrated different patterns of insecure attachmentinsecure attachmentA pattern of infant–caregiver bonding that can take several forms but is generally characterized as less desirable for the child’s outcomes than secure attachment.            insecure attachment        A pattern of infant–caregiver bonding that can take several forms but is generally characterized as less desirable for the child’s outcomes than secure attachment.            . Two of these, avoidant and anxious–ambivalent attachment, were identified by Ainsworth, and a fourth, disorganized attachment, was identified by other researchers (Main & Solomon, 1986). Children with avoidant attachment did not react to their mother’s leaving with distress and allowed themselves to be comforted by the stranger. When the mother returned, the child with an avoidant attachment did not immediately approach her. Children who showed anxious–ambivalent attachment never seemed comfortable, even when their mother was present. Her leaving was greeted with great distress, and the child was alternately clingy and rejecting when she returned. Children with disorganized attachment seemed confused and not well attached. Such children showed contradictory behavior, such as approaching the mother walking backward. Cultural influences on parenting behaviors influence attachment. Psychologists have compared the frequencies of the attachment types in the United States to frequencies in Japan and Germany (Cole, 1999). The Japanese sample had about the same proportion of securely attached infants as reported by Ainsworth but had higher proportions of anxious–ambivalent attachments and zero cases of avoidant attachment. In Germany, the number of anxious–ambivalent infants was about the same as in the United States, but many more had avoidant attachments. The exact features of each culture’s norms responsible for these differences remain to be explored. What types of parenting behavior might foster secure attachment? When you are in public places, such as parks and malls, take a minute to watch parents interact with their children. Some parents are sensitive and responsive to their children’s needs (Cox, Owen, Henderson, & Margand, 1992). The responsive parents are teaching their children that the world is a predictable place in which their parents are there for them when needed. Consequently, these children feel free to explore their environment, knowing that if an emergency arises, they will be able to quickly and reliably obtain assistance from their parents. Small securely attached children have a relatively short radius of exploration, which lengthens gradually as they get older. In contrast, unresponsive or inconsistent parents are teaching their children that the world is an unpredictable place and that the parent may or may not be there for them when needed. In response to such insecure circumstances, children do not venture far from the parent. As suggested by the results of partial reinforcement experiments discussed in Chapter 8, children with unresponsive or inconsistent parents make regular, frequent, and persistent demands for attention because they know that only a small subset of these demands will be met. It might seem counterintuitive, but ignoring children results in clinginess and constant demands for attention, whereas meeting children’s needs promptly and reliably fosters independence. Mary Ainsworth used the strange situation to assess the quality of children’s attachment to their mothers. While watched through a two-way mirror, the child’s mother first leaves and then returns. Using a careful grading scale, researchers rate children as having a secure attachment or one of several variations of insecure attachments. Although American psychologists view a secure attachment to be ideal, attachment might be better viewed with an ecological approach than with a universal approach. Secure attachment is a good model for relatively safe societies (Belsky, 1999). In a safe society, parents have the time and resources to be attentive to children, and exploration is unlikely to put the children in danger. In contrast, secure attachments in dangerous societies might be maladaptive. If the environment is dangerous, parents have less opportunity to be responsive to their children, and exploration might put the children in great danger. The message that The world is an unpredictable place in which we may or may not be able to help you, so stay close might be more realistic under these circumstances. Not all parents approach parenting the same way. Parenting styles can be divided into four categories that vary along two dimensions: Parental support and behavioral regulation (Baumrind, 1975;  Maccoby & Martin, 1983). Parental support takes the form of empathy and recognition of the child’s perspective. Behavioral regulation involves supervision of the child’s behavior accompanied by consistent discipline and clear expectations (Luyckx et al., 2011). If we look in the upper left-hand box of Table 11.4, we find the authoritative parenting style, a cross between high parental support and high behavioral regulation. This is the ideal style for parents, as evidenced by the superior outcomes among children raised by parents using this style (Milevsky, Schlechter, Netter, & Keehn, 2007;  Simons & Conger, 2007). Limits are appropriate for the age and stage of the child’s development, and consequences are educational, not punitive. Authoritative parents are consistent and firm but also warm and reasonable. They communicate their standards and invite feedback, but there is no question as to who is running the show. High parental supportLow parental supportHigh behavioral regulationAuthoritativeAuthoritarianLow behavioral regulationIndulgentUninvolvedIn her 2011 book Battle Hymn of the Tiger Mother, Amy Chua described her initial efforts to raise her daughters in a strict, Chinese fashion instead of a Western style emphasizing self-esteem over achievement, followed by adjustments she made in response to her older daughter’s rebelliousness at age 13. Contrary to fears expressed at the time the book was published, Chua’s daughters seem to be doing just fine. Sophia graduated from Harvard and is studying law at Yale as a second lieutenant in the U. S. Army. Lulu is studying art history at Harvard. How do Chua’s experiences fit with our discussion of parenting styles? The upper right-hand box contains the cross between low parental support and high behavioral regulation, or the authoritarian parenting style. The high behavioral regulation provided by these parents prepares children for the limits they inevitably meet in the community from other authority figures such as teachers and law enforcement personnel, but this is considered a less-than-ideal approach in most contemporary environments because of lower levels of warmth and support (Luyckx et al., 2011). Authoritarian parents have a greater tendency than authoritative parents to use harsh punishments, including physical punishment. This behavior can prompt some rebelliousness in the child that is not seen as frequently in response to authoritative parents. In the lower left-hand box, we find the indulgent parenting style, produced by crossing high parental support with low behavioral regulation. These parents are warm and loving, but they simply do not want to be the ones who tell their children the dreadful word no. They are more comfortable letting others, such as teachers, neighbors, and law enforcement, establish rules. Children of indulgent parents are monitored less than children of authoritative and authoritarian parents and show higher levels of antisocial behavior than children of authoritative and authoritarian parents (Luyckx et al., 2011). The lower right-hand box represents the cross between low parental support and low behavioral regulation, or the uninvolved parenting style. This style generally does not occur unless there is something seriously wrong with the family situation. Illness, marital discord, psychopathology, or substance abuse can prevent parents from carrying out their duties in regard to children. By the 12th grade, children of uninvolved parents drink and smoke nearly twice as much as children of authoritarian or authoritative parents, and sons of uninvolved parents are at a very high risk of antisocial behavior (Luyckx et al., 2011) (see Figure 11.19). Authoritative and authoritarian parenting styles are associated with lower rates of antisocial behavior in children than the indulgent or uninvolved parenting styles. However, children with authoritarian parents experience more depression and withdrawal than do children of authoritative parents. Does a parent consistently stay in the same box? The answer to that question is no. Life circumstances can radically change a parent’s style. A competent parent can be thrown off balance by loss of a job, poor health, divorce, or other disruptions. Once the challenging circumstances are resolved, we would expect that parent to get back on track. What if a person’s parents each use a different style? Although this does happen occasionally, most couples share a common style of parenting (Milevsky et al., 2007). If the parents do have differing styles, the presence of one authoritative parent can protect the children from the negative outcomes associated with the other styles (Milevsky et al., 2007). Aspect of developmentHighlightsPhysical developmentHeight and weight increase rapidly. Both gray and white matter in the brain increase. Motor control improves from head to toe and from the midline outward to the periphery of the body. Children select sex-typed toys and use gender pronouns to refer to themselves. Cognitive developmentChildren demonstrate naïve theories. Children learn language rapidly. Assimilation and accommodation of information build better concepts. Thought becomes more logical and abstract. Social and emotional developmentTemperament shapes initial responses to the environment. Children form secure and insecure attachments to caregivers. Parenting styles interact with temperament. AdolescenceAdolescenceA period of development beginning at puberty and ending at young adulthood.            Adolescence        A period of development beginning at puberty and ending at young adulthood.             is an artificial, arbitrary period of development that is a relatively recent phenomenon in human experience. In hunter–gatherer societies, sexual maturity at puberty signaled the young person’s entry into adult roles, privileges, and expectations. Our word puberty comes from the Latin word for adult. Although adolescence is assumed to begin with puberty, its endpoint is less clear. Lifestyle factors associated with adulthood, such as financial independence from parents, separate living arrangements, marriage, and parenthood, are far more descriptive of the end of adolescence than age. Two factors have resulted in an extended period of adolescence in modern, industrial societies, such that it starts earlier and ends later than in the past. One is the dramatic decrease in the age at which pubertypubertyA period of physical changes leading to sexual maturity.            puberty        A period of physical changes leading to sexual maturity.             occurs. Over the last 100 years, the average age of puberty in the United States has dropped from about age 16 to around age 12 (Frisch, 1983) (see Figure 11.20). A second factor contributing to the length of modern adolescence is the extended period of education and training needed in technological societies. Youth at puberty are simply not prepared to assume adult working roles in most modern cultures. Since 1840, the average age of first menstruation, a major indicator of puberty in females, has dropped from nearly 16 years to its current average of 12.6 years. Physical changes in adolescence provide individuals with their final preparation for taking on adult roles. Among the most striking physical changes in this period are those related to sexual maturity and brain development. The onset of puberty is marked by a cascade of hormone release, culminating in the maturity of reproductive organs and the development of secondary sex characteristicssecondary sex characteristicsPhysical changes occurring at puberty associated with sexual maturity.            secondary sex characteristics        Physical changes occurring at puberty associated with sexual maturity.            . Males experience muscular development, maturity of the external genitalia, growth of facial hair, and enlargement of the larynx, which leads to a deeper voice. Females experience menarche (the first menstrual cycle), breast growth, maturity of the external genitalia, maturity of the uterus, and changes in fat distribution and quantity. The obvious evolutionary purpose of puberty is to prepare an individual for sex, so not too surprisingly, teens respond to the physical changes of puberty with a dramatic increase in sex drive. The physical growth that accompanies the onset of adolescence does not happen to everyone at the same time. Middle school students show a range of physical maturity. Sexual maturation during the teen years parallels further development in gender identity, the assumption of sex-role behaviors, and sexual orientation. Gender identity refers to a person’s sense of being male or female. Sex-role behavior refers to a pattern of traditionally male or female behavior. As we discussed in Chapter 7, sexual orientation refers to a stable pattern of attraction to people of a certain sex, independent of gender identity, sex-role behavior, and sexual experiences. Many complex variations in combinations of these variables occur. People with a homosexual orientation typically demonstrate a gender identity consistent with their biological sex, while showing the same range of variation in sex-role behaviors, from traditionally masculine to traditionally feminine, as people with a heterosexual orientation. Individuals with AIS, mentioned earlier in the chapter, typically develop female gender identity and sex-role behaviors in spite of their genetic male sex (XY). Girls with CAH, who are exposed to much higher levels of prenatal male hormones than typically developing girls, have higher rates of bisexual and lesbian sexual orientations. However, it’s important to note that the vast majority of bisexual and lesbian women do NOT have CAH, and that a substantial majority of women with CAH are heterosexual. Early developmental psychologists, including Piaget, argued that cognition is rather mature at puberty, which in turn implies that the brain is mature. In some aspects, this conclusion is correct. By the teens, areas of the brain that process language, spatial relations, hearing, and other sensory processes appear complete. However, much additional work remains to be done before we can consider the brain to be fully adult. Far from considering the brain complete at puberty, today’s psychologists are viewing the early teen years as being a second critical period of brain growth, the first being prenatally up to age 18 months (Kuhn, 2008). The onset of puberty is accompanied by substantial gray matter growth, which peaks between the ages of 11 and 12 years (Semple, Blomgren, Gimlin, Ferriero, & Noble-Haeusslein, 2013). Following this burst of growth, the gray matter normally thins somewhat over the remainder of the teen years (see Figure 11.21). Abnormalities in this process may be associated with the onset of schizophrenia, described in Chapter 14. Youth with early-onset forms of this disorder show four times more loss of gray matter in the frontal lobes than is usually seen in teens of the same age (Rapoport et al., 1999). White matter, or myelin, continues to mature through the teen years and into young adulthood. Myelination of the frontal lobes is greater in adults 23 to 30 years old than in youth between the ages of 12 and 16. Unfortunately, an adolescent’s white matter is more susceptible than an adult’s to damage, such as that caused by binge drinking (McQueeny et al., 2009). In contrast to early developmental theories such as Jean Piaget’s theory that viewed the mind as mature at the outset of adolescence, gray matter growth continues into early adulthood, although the pace slows, and the cortex thins in some areas. Areas shown in red and yellow are experiencing the greatest rates of growth. In this chapter, we explore the impact of brain development on risk-taking. In general, we find that behaving in risky ways is more typical of teens, in spite of the fact that they perceive risk similarly to adults. How do psychologists know this? We can use instruments like the Domain-Specific Risk-Taking (DOSPERT) Scale (Blais & Weber, 2006). To complete this scale, rate each of the 30 items twice: Once for how likely you would be to do the behavior (your risk-taking score) and then for how risky you think the behavior is (your risk-perception score). Once you have your responses, use the codes at the end of the box to divide your scores into the five domains: Ethical, financial, health/safety, recreational, and social. Add your scores for risk-taking and risk-perception for each domain. You might find that your behavior and perceptions are quite different from one domain to the next. Risk-taking Scale:1234567ExtremelyNot SureExtremelyUnlikelyLikelyRisk Perception Scale:1234567Not at allExtremelyRiskyRiskyAdmitting that your tastes are different from those of a friend. Going camping in the wilderness. Betting a day’s income at the horse races. Investing 10% of your annual income in a moderate growth mutual fund. Drinking heavily at a social function. Taking some questionable deductions on your income tax return. Disagreeing with an authority figure on a major issue. Betting a day’s income at a high-stake poker game. Having an affair with a married man/woman. Passing off somebody else’s work as your own. Going down a ski run that is beyond your ability. Investing 5% of your annual income in a very speculative stock. Going whitewater rafting at high water in the spring. Betting a day’s income on the outcome of a sporting eventEngaging in unprotected sex. Revealing a friend’s secret to someone else. Driving a car without wearing a seat belt. Investing 10% of your annual income in a new business venture. Taking a skydiving class. Riding a motorcycle without a helmet. Choosing a career that you truly enjoy over a more secure one. Speaking your mind about an unpopular issue in a meeting at work. Sunbathing without sunscreen. Bungee jumping off a tall bridge. Piloting a small plane. Walking home alone at night in an unsafe area of town. Moving to a city far away from your extended family. Starting a new career in your mid-30s. Leaving your young children alone at home while running an errand. Not returning a wallet you found that contains $200. Ethical: 6, 9, 10, 16, 29, 30Financial: 3, 4, 8, 12, 14, 18Health/Safety: 5, 15, 17, 20, 23, 26Recreational: 2, 11, 13, 19, 24, 25Social: 1, 7, 21, 22, 27, 28Risk taking and risk perception are negatively correlated in individuals (see Chapter 2). As the perception of riskiness increases, people are less likely to take a risk. Structural differences between the brains of teens and those of adults affect their interpretations of the emotions of other people (Thomas, De Bellis, Graham, & LaBar, 2007; Yurgelun-Todd, 2007). Adults identify expressed emotions accurately, but teens frequently misunderstand the emotions being displayed. Brain imaging demonstrated that the amygdala, which provides a quick, subconscious assessment of emotion, showed the same pattern of activity in adults and teens. However, the adults showed more activity in the frontal lobes than the teens did, which might account for the adults’ superior judgment of emotional expression. The earlier maturation of the more emotional parts of the brain, including the amygdala, relative to the logical frontal lobes might account for some risky behaviors that characterize the teen years, such as reckless and drunk driving, driving without seatbelts, experimenting with drugs, and having unprotected sex. Risky behavior does not result from the teen’s inability to judge what is risky or not (Fischhoff, de Bruin, Parker, Millstein, & Halpern-Felsher, 2010). Instead, teens typically believe that they are somehow immune from the consequences of risky behavior—for example, that drunk driving accidents happen to other people (Reyna & Farley, 2006). The teen brain also responds more vigorously to pleasure than does the adult brain, which means that the immediate pleasurable consequences of a risky behavior can overwhelm the teen’s better judgment (Galvan, Hare, Voss, Glover, & Casey, 2007). The continuing maturity of the nervous system, interacting with the expanding physical and social environment of the adolescent, leads to more adult ways of thinking and reasoning. Adolescence is when youths apply their new executive strategies to problems and build their knowledge base for specific subjects. High school senior Ronit Abramson won an Intel International Science and Engineering Fair prize for her science project on marine biology. During adolescence, both working memory and reaction time reach adult levels. Adolescents begin to think more logically or analytically (Kokis, Macpherson, Toplak, West, & Stanovich, 2002). For example, children often use their personal experience as a heuristic (the teacher didn’t catch my friend cheating on the test, so it’s safe for me to cheat too) instead of the more logical, statistical reasoning of the teen (my friend might have had a lucky break, but this teacher is extra good at catching cheaters, so my odds of getting caught remain high despite my friend’s experience). Teens are hard at work acquiring data, and as they build up their knowledge bases, their ability to use executive processes such as strategies improves (Peverly, Brobst, & Morris, 2002). As we mentioned in Chapter 9, the more you know about a subject, the easier it becomes to learn new things about it. You acquire skills specific to a subject, such as how to solve statistics problems or prepare for a final covering thousands of pages of reading in your literature course. Our human history and literature are filled with heroes and villains, and we are fascinated by their development. How does one child grow to be Mother Teresa and another to be Adolf Hitler? Lawrence Kohlberg, a student of Piaget’s, attempted to extend Piaget’s theory of cognition to explain the development of moral reasoning (Kohlberg, 1981,  1984). To assess changes in moral reasoning, Kohlberg presented children, adolescents, and adults with a number of ethical dilemmas. He was not as interested in the choice participants would make in response to the dilemma as he was in the reasoning participants provided their choice. Based on the participants’ responses to his dilemmas, Kohlberg identified three major stages in moral reasoning, shown in Table 11.5. Children and young adolescents are typically in the preconventional moralitypreconventional moralityLawrence Kohlberg’s stage at which moral choices are made according to expectations of reward or punishment.            preconventional morality        Lawrence Kohlberg’s stage at which moral choices are made according to expectations of reward or punishment.             stage. In this stage, children make moral choices based on their expectations of reward and punishment. Some individuals never progress beyond this stage, even in adulthood, and believe that stealing or cheating is okay as long as they don’t get caught. StageFeaturesPreconventionalUses probability of rewards and punishments to guide behavior. ConventionalMaintains reputation and follows the law. PostconventionalFollows self-chosen ethical principles. Kohlberg believed that during adolescence most of the population moves to the next stage, conventional moralityconventional moralityLawrence Kohlberg’s stage of moral development in which moral choices are made according to law or public opinion.            conventional morality        Lawrence Kohlberg’s stage of moral development in which moral choices are made according to law or public opinion.            , and stays there throughout adulthood. In this stage, rules are seen as governing moral behavior and are, therefore, to be followed. Conventionally moral people are also sensitive to public opinion. They want to do the right thing so that others will approve of their behavior. Although conventional morality adds to the stability of a society, it runs the risk of being arbitrary and rigid. In addition, history is full of situations in which local public opinion was supportive of morally abhorrent activities, such as genocide or slavery. The final step of moral development for Kohlberg was postconventional moralitypostconventional moralityLawrence Kohlberg’s stage at which moral choices are made according to personal standards and reason            postconventional morality        Lawrence Kohlberg’s stage at which moral choices are made according to personal standards and reason            . Relatively few people, according to Kohlberg, attain this stage. In conjunction with the abstract reasoning of formal operations, the individual now recognizes that rules are made by humans and can, therefore, be flawed. Consequently, personal standards are used as reference points. If everybody used postconventional reasoning to select a driving speed on the highway, chaos would ensue. However, the postconventional thinker is likely to evaluate laws and rules critically before complying with them. Kohlberg is not without his critics. Postconventional reasoning has been criticized as characteristic of males in Europe and the United States, rather than representing a universal stage of moral development (Murphy, Gilligan, & Puka, 1994). Children and adults living in the United States give responses to Kohlberg’s dilemmas consistent with the culture’s emphasis on personal justice and individual rights. Other cultures place a higher priority on interpersonal factors, such as duty and responsibility to others, and these values can lead to different responses to ethical dilemmas (Miller & Bersoff, 1992). As the teen prepares to transition to adulthood, questions emerge about what kind of adult to be. The focus of social activity begins to shift from the family to the peer group, which is where most adult interaction will be taking place. We pick up social development in adolescence by introducing Erik Erikson, a student of Sigmund Freud who outlined stages of social development beginning in infancy, shown in Table 11.6. Erikson made significant contributions to our understanding of identityidentityA consistent, unified sense of self.            identity        A consistent, unified sense of self.            , or a unified, consistent sense of self (Erikson, 1968). For Erikson, social development proceeds in stages, with each stage characterized by a fork in the road between a positive outcome and a less than desirable outcome. Adolescence challenges teens to develop a sense of identity (the positive outcome). If they fail, the result is role confusion (obviously, the less desirable outcome). StageChallengeDescriptionBirth to 18 monthsTrust versus mistrustChildren view the world as a safe, dependable place.18 months to 3 yearsAutonomy versus shame and doubtChildren begin to explore.3–6 yearsInitiative versus guiltChildren begin to act on the world.6–12 yearsIndustry versus inferiorityChildren develop self-confidence. AdolescenceIdentity versus role confusionTeens begin to form an identity by asking Who am I? Failure to achieve a stable identity leads to role confusion and problems with subsequent stages. Young adulthoodIntimacy versus isolationYoung adults with clear identities form stable, intimate relationships, while others experience feelings of loneliness and isolation. MidlifeGenerativity versus stagnationMidlife adults who find value in their lives, even if they have not met all their earlier goals, experience generativity. They are likely to put back energy into family, work, and community. Late adulthoodIntegrity versus despairToward the end of life, adults who feel that they have lived fully experience a sense of integrity and calm. Many teens explore questions of identity by trying on different roles and affiliating with different groups. Teens begin the process of identity formation by asking such questions as Who am I? and What kind of person do I want to be? These are profound questions, made possible by the newly developed abstract reasoning skills of the teen. The answers are not immediately apparent. Considerable exploration may be necessary before the teen discovers a direction. Many teens try on several identities, from jock to nerd to goth, before finding one that works. Because developing an identity can be somewhat unpleasant, teens may be tempted to short-circuit the process. One technique for prematurely adopting an identity is to assume the identity of a group (Erikson, 1968). By affiliating with a particular club, clique, or gang, teens take on a preformed identity and avoid the challenges of finding their individual identities. A second approach is the adoption of ready-made identities provided by parents or other mentors. Although parents may have valuable advice regarding choice of college major and career, these are highly individual choices with significant implications. For many adolescents, ethnic identity, or how an individual feels about being a member of a particular ethnic or racial group, is a major part of their overall identity (see Figure 11.23). Having an ethnic identity boosts the overall self-esteem of adolescents and young adults (Erol & Orth, 2011). A growing number of individuals identify with more than one ethnic or racial group. Psychologists are investigating this multiethnic experience to see what changes might be required in their theories of ethnic identity development (Shih & Sanchez, 2009). Identifying with an ethnic group appears to boost the overall self-esteem of African American and Hispanic teens during adolescence and young adulthood. Running parallel to the search for identity is a tendency on the part of teens to spend more time with peers and less with family. A healthy balance between family and peer influences appears to produce the best outcomes. Teens who continue to interact regularly with their parents, such as having regular dinners together, are more likely than teens who don’t interact much with parents to avoid pitfalls of substance abuse (see Figure 11.24) and promiscuity (Fulkerson et al., 2006). Adolescence is a time characterized by increasing focus on relationships with peers, but strong relationships with family members remain important. Teens who report lower quality relationships with their parents are more likely to use marijuana, alcohol, and tobacco. The importance of the peer group to adolescents can be demonstrated by their heightened sensitivity to peer exclusion. Compared to adults over the age of 22, adolescents reported significantly worse mood following an experimental procedure designed to instill feelings of social exclusion (Blakemore & Mills, 2014). Teens are also more susceptible than adults to negative peer influence. Compared to young adults and older adults, adolescents made many more risky decisions when in the presence of peers than while alone (Chein, Albert, O’Brien, Uckert, & Steinberg, 2011; Simons-Morton et al., 2014). In contrast to popular press accounts of the stormy teen years, most teens (and their parents) navigate their passage to adulthood with no more than a few bumps and bruises (Steinberg, 1997). Generation gaps may occur, but teens typically do not deviate too far from their parents’ values. Research indicates that most teens love their parents, feel loved by their parents in return, share many of their parents’ values, and turn to them regularly for advice. As we mentioned previously, defining the exact point when a person becomes an adult is not easy. Numerous roles are considered as marking this transition, including the completion of education, full-time employment, independent living, marriage, and parenthood. People do not usually assume all these roles simultaneously, leading to a gradual emergence of adulthood over a period of several years. The young adult has reached the pinnacle of physical development. If you are in this age group, you are as tall, strong, and fit as you’re ever likely to be. Your brain is mature, and your senses and reaction time are in excellent shape. It is not too early for young adults to consider how to best maintain their physical health. Through childhood and adolescence, we have seen that physical development often follows a blueprint that is strongly genetic. In adulthood, however, lifestyle factors begin to play a greater role in a person’s outcomes. In a longitudinal study of more than 20,000 adults, following four simple steps added 14 years to life expectancy: Eating fruits and vegetables, not smoking, drinking alcohol moderately if at all (fewer than two drinks per day for men and fewer than one drink daily for women), and exercising (defined in the study as 1 hour per week;  Khaw et al., 2008). Becoming a parent is no longer restricted to young adulthood. In 1970, the average age of first-time mothers was 21, and few women became mothers for the first time after the age of 35. Today, many women are postponing parenting until midlife. In your college experience, you might have seen a professor become annoyed by a student’s inquiry about the right answer. The search for a single answer, however, is characteristic of formal operations as described by Piaget. At this stage of cognitive development, the teen can follow a logical course of steps to solve a problem (Guignard & Lubart, 2006;  Wu & Chiou, 2008). Lack of agreement with the solution is viewed by the formal operational thinker as a sign that the solution is incorrect. Although many questions we face in college have right answers, such as an element’s atomic weight or whether a hypothesis was shown to be incorrect, many more are open to debate. The professor’s frustration with your fellow student’s interest in only one right answer reflects a difference in adolescent versus adult thinking, or postformal thought. Postformal thought recognizes that the right answer is often it depends and that many important questions are complex and ambiguous (King & Kitchener, 2002). Adolescents are more likely to think in absolutes and defer to authorities, such as professors (Perry, 1970). Efforts to move adolescents toward independent, postformal thought are often described as critical thinking exercises, described in Chapter 2. Why is it important to embrace postformal ways of thinking? In Chapter 2, we discussed the importance of generating and testing new hypotheses to move science forward. Using formal operations, a student can evaluate a hypothesis and conclude whether it needs to be rejected. However, postformal thinking is an integral part of creativity, in science or in any other field (Guignard & Lubart, 2006). Scientists who are unable to look beyond the right answers already existing in their fields are unlikely to make creative contributions that stretch that knowledge. As teens enter young adulthood, Erickson suggests that they confront a new challenge: Intimacy versus isolation. Those who fail to find the level of intimacy they seek might experience feelings of loneliness, which in turn can elicit behaviors that are more likely to alienate others instead of promoting intimacy (Cacioppo, Cacioppo, & Boomsma, 2014). One key to successful intimacy, according to Erikson, is to have established a solid identity in adolescence. If you still don’t know who you are, it is unlikely that you will make good judgments about the type of person who will make you happy. Current trends in the United States include diverse approaches to intimacy and family. In addition to forming traditional families that include two biological parents and their children, large numbers of people are choosing to remain single, live together without being married, be single parents, have children without being married, divorce, and remarry. Along with these changes, public attitudes regarding the question What is a family? continue to change. In the United States, 80% of the public consider an unmarried couple with children to be a family (see Figure 11.26), although only 43% consider an unmarried couple without children to constitute a family (Pew Research Center, 2010). American attitudes about what constitutes a family, a term once restricted to two parents living with their biological children, continue to change and expand to recognize diverse approaches to intimacy. Today’s emerging adults have been classified as members of the millennial and Gen-Y generation, or those born after 1980. Since the time of Aristotle, it has been almost a tradition to despair over the traits of young people. The millennials have been described alternately as the newest greatest generation because of their information-seeking and multitasking abilities but also as generation whine, whose overprotective parents have raised fragile offspring with no tolerance for adversity (Dannar, 2013). Cohorts like the millennials, and the baby boomers and Gen Xers that preceded them, are shaped by their experience. Social trends have the greatest influence on the young (Twenge, 2013). What effects have the last decades had on emerging adults? Jean  Twenge (2013) argues that despite the technology that connects us around the clock, the U. S. culture is becoming more individualistic and self-oriented. Television shows are more likely to focus on fame, books use more individualistic language than previously, and song lyrics are more narcissistic and antisocial. In response to these cultural trends, Twenge argues that millennials show higher rates of narcissism, or an unrealistically positive, inflated view of the self. Narcissistic personality disorder is quite rare, but scores on scales that measure narcissistic traits are increasing (see Figure 11.25). Scores on the Narcissistic Personality Inventory (NPI) have risen since 1990. Increased narcissism has been accompanied by less empathy and more self-oriented behavior, such as increases in elective plastic surgery, cheating in school, and the desire to have authority over others. Narcissism is linked to significant interpersonal problems, including reduced commitment to romantic relationships and increased aggression within those relationships (Keller et al., 2014). Whether you think Twenge is being fair, is there something we could do to reduce narcissism? Twenge argues that efforts to artificially boost self-esteem (see Chapter 12), such as participation trophies and telling everyone they are special, should be abandoned. Instead, she advocates emphasis on building empathy and becoming involved in larger social issues. For those who do choose to have children, the experience of being parents often dominates much of their adult lives. In spite of some stereotypes of miserable parents, parents report feeling happier than nonparents (Nelson, Kushlev, Dunn, & Lyubomirsky, 2014). As is the case with all correlations, however, we cannot conclude that being a parent makes someone happy. Perhaps happier people are more likely to become parents or a third factor might be responsible for both a higher likelihood of becoming a parent and being happy. Although most parents are quite happy with their choice to take on this role, many young couples experience a drop in marital satisfaction as they adjust to new responsibilities (Goldscheider & Waite, 1993). After all, it’s rather difficult to feel attractive for your partner with peanut butter and jelly in your hair. Marital satisfaction decreases when children are quite young and rebounds when children become more independent. Couples who describe their relationships as successful experience milder impacts from having children. Couples’ relationship quality acts as a significant buffer against depression in mothers of children with autism spectrum disorder (see Figure 11.27) (Weitlauf, Vehorn, Taylor, & Warren, 2014). Parenting is tough under any circumstances, but parenting children with special needs puts parents under considerable strain. Mothers of children with autism spectrum disorder are at higher risk for major depressive disorder, but less so when they have strong relationships with their partners. Just as our criteria for leaving adolescence for young adulthood were somewhat vague, we do not have a set of reliable characteristics that signal the transition between young and middle adulthood. Some people view a specific age, such as 40, as a starting point for midlife. Psychologists often point to physical changes, such as graying hair or menopause in women, as signposts for this new transition. Others suggest that a person’s growing sense of mortality, possibly boosted by the death of a parent, leads to a feeling of being closer to death than to birth (Jaques, 1965). As with the urban myths about the storm of adolescence, you have probably heard popular accounts assuming that we all have some midlife crisis (Levinson, Darrow, Klein, Levinson, & McKee, 1976). For most people, this is simply not true (Rosenberg, Rosenberg, & Farrell, 1999). Middle adulthood is not without its challenges because many adults find themselves simultaneously caring for children and aging parents, working to meet their career and financial goals, and dealing with physical and possibly marital changes. However, there appears to be no evidence for a large-scale crisis associated with any one age. The major theme of physical and cognitive development in midlife is stability. Assuming that people maintain good health, changes in the physical and cognitive domains tend to be mild and gradual. The most significant physical change in midlife for women is the end of their ability to reproduce. MenopauseMenopauseThe complete cessation of a woman’s menstrual cycles.            Menopause        The complete cessation of a woman’s menstrual cycles.             is the point where menstruation stops. For most women, menopause is complete in their early 50s, but loss of fertility is a gradual process beginning many years earlier. Menstrual cycles may become irregular in a woman’s 40s as her sex hormone levels that regulate the cycles begin to drop. During the period of transition between the start of irregular periods and the final disappearance of menstruation, women can experience a number of physical and psychological symptoms. Hot flashes, night sweats, headaches, joint pain, mood swings, and sleep disturbances are common complaints, and the severity of these issues can range from barely noticeable to nearly disabling. Hormone treatments to offset these symptoms are used quite cautiously because they can also increase a woman’s risk of stroke, heart attack, breast cancer, and later dementia (Banks & Canfell, 2009). Men experience more gradual changes in their reproductive status through the remainder of their lives. Sperm quantity may be reduced, but men in their 80s remain half as fertile as men who are 25. Most men experience little to no decrease in testosterone over the course of the midlife years, although other conditions, such as diabetes, can interfere with the maintenance of an active sex life. For both men and women in midlife, the availability of a partner is more predictive of sexual activity than any health issues (Karraker, DeLamater, & Schwartz, 2011). In contrast to the relative stability in the physical and cognitive domains during midlife, this period of life is characterized by significant changes in social and work roles. Children grow up and move out. If you are the youngest child in your family, you may have noticed that your parents are beginning to feel somewhat more relaxed. For many parents, the empty nest period following their last child leaving home is accompanied by an improvement in marital satisfaction (Birchler, 1992). Your parents no doubt love you, but they’re also relieved to leave many of the responsibilities of parenting behind them. However, as Figure 11.28 shows, the likelihood that negative emotions will accompany an empty nest varies from culture to culture (Mitchell & Lovegreen, 2009). Although most Americans report positive reactions to their last child leaving home, reactions vary across cultures. Among Canadians of different ethnic groups, being an empty-nest parent has different meanings. Parents from Chinese and Southern European cultures are less likely to report negative responses to their last child leaving home than are parents with a British background. Parents from India experienced the most negative response of all groups studied. With increasing life expectancies on one side and later parenthood on the other, many contemporary midlife adults find themselves sandwiched or caring for their aging parents and growing children at the same time. Adults in midlife continue to pursue their careers, becoming more expert, or they might initiate a new career that was not previously compatible with the financial and social responsibilities of raising a family. Recent economic and social changes have made midlife work less stable than it has been for previous generations. Today’s midlife workers express intent to stay employed longer than previous cohorts (at least until age 65), but some find themselves taking jobs below their levels of skill and experience or withdrawing from employment (Elman, 2011). During midlife, Erikson suggests that adults experience either generativity or stagnation. Many people, particularly during adolescence, set a number of goals for themselves. You may want to win a Nobel Prize or the Olympics or simply help others have a better life. Whatever your goals, by the time you reach midlife, you will have a good idea about whether these goals will be met. In many cases, people find that progress has been made toward their goals, although they perhaps didn’t meet them. Wherever an adult finds himself or herself at midlife, the question posed by Erikson is whether value exists. If people feel that their lives have had value, even if they fall short of their goals, they experience generativity. Adults experiencing stagnation, in contrast, have not found much satisfaction in their lives. As we have seen in our discussion of previous stages of development, the starting point of late adulthood is vague. You might think of age 65, a common retirement age, as a starting point, but many of today’s older adults are living and working longer than ever before and so are thinking about old age differently than in the past. For example, the baby boomers, or individuals born between 1945 and 1964, are more likely than other age groups to choose 72 as the start of old age (Cohn & Taylor, 2010). Current life expectancy at birth in the United States is 78.7 years (Murphy, Xu, & Kochanek, 2013). Assuming good health, the effects of aging on an adult’s physical status are gradual and rather mild. Nonetheless, some physical change is inevitable, and understanding the nature of these changes provides guidance for coping with them. The brain reaches maturity around age 25, and few further changes occur until about the age of 45. At that time, the weight of the brain begins to decrease, leading to about a 5% decrease by the age of 80 (Anderton, 1997). However, this decrease is not accompanied by dramatic cognitive and behavioral changes. Results from the Baltimore Longitudinal Study of Aging (2000) show that healthy aging is accompanied by mild changes in the speed of learning and problem solving, and most of these observable changes occur very late in life. As with other physical changes, age-related changes in sensory abilities are gradual, and their effects can be offset by the use of technologies, such as eyeglasses and hearing aids. Intelligence remains relatively stable during adulthood, as long as general health is good. Although many people erroneously believe that aging is inevitably accompanied by loss of intellect, only about 8.8% of the population over the age of 65 experiences dementia, such as Alzheimer’s disease (Langa et al., 2017). Rates of dementia have been dropping over the last three decades (Satizabal et al., 2016) and nearly 25% between 2000 and 2012 alone (Langa et al., 2017). Improved control of cardiovascular disease and higher levels of education might account for some of these improvements. Fauja Singh, born in 1911, competed in marathons worldwide until his retirement in 2013 as part of the Sikhs in the City team, a group of four runners whose combined age is more than 330 years. Running marathons isn’t necessary to prevent cognitive decline, but people who participate in 20 minutes or more per day of physical activity have the lowest rates of dementia. Certain types of intelligence change more because of aging than others. As we discussed in Chapter 10, crystallized intelligence, or the use of accumulated knowledge, changes less than fluid intelligence, or a person’s basic information processing skills (Horn & Hofer, 1992). Fluid intelligence correlates with speed of processing, whereas crystallized intelligence might be viewed as wisdom, or the ability to draw on past knowledge and experience to solve problems. Early studies of scientific, scholarly, and artistic accomplishments as a function of age demonstrated a peak in a person’s 40s (Dennis, 1966). However, more recent studies show a relatively constant level of professional output. Among professors, research output was far more related to pressure to publish than to a professor’s rank or age (Stroebe, 2010). The average age of Nobel Laureates between 2011 and 2016 was 66 years (Average age for Nobel Laureates, 2017). In later adulthood, Erikson suggests that people experience either integrity or despair. Older people who are happy with their life’s experiences generally experience integrity (Sneed, Whitbourne, & Culang, 2006;  Vaillant & Koury, 1993). They met their goals for the most part and had a fun time doing so. In contrast, people who reach late adulthood feeling that life passed them by are likely to experience a sense of despair. People turning 65 years of age in 1950 could expect to live about 13.8 more years, or until age 78.8, whereas people turning 65 in 2015 are likely to live 5.5 years longer, or until about age 84.3 (Gatz, Smyer, & DiGilio, 2016). Quantity, of course, does not guarantee quality, so psychologists have investigated ways to improve the well-being of older adults. These investigations have produced valuable results in five domains: Knowledge about healthy cognitive aging, understanding decision-making, ways to encourage good health habits, understanding of stereotypes and bias, and improved adaptive technologies. Not only has psychological research identified neural correlates of healthy and unhealthy cognitive aging, but psychologists have collaborated with legal experts regarding judgments of diminished capacity. As a result, older Americans with cognitive deficits enjoy better protection from abuse, neglect, and financial exploitation (Gatz et al., 2016). Psychology’s contributions to improved understanding of decision-making, particularly the effects of aging on decision making, can assist older adults and those helping them make better decisions related to retirement and medical care. In Chapter 8, we explored basic principles of behavior change related to classical and operant conditioning. Many habits, which respond to behavioral methods, have critical influence on healthy lifestyles, regardless of age. Developing healthy habits and reducing unhealthy behaviors can have significant effects on well-being. In an upcoming chapter, Chapter 13, you will learn more about how stereotyping and biases can have negative influences on the targets’ outcomes. Psychology provides tools for tackling bias among health care providers, such as the willingness of physicians to rate suicidal ideation in 78-year-olds as normal and not needing treatment (Uncapher & Areán, 2000). New technologies, such as social media, have transformed the ability of all age groups to interact with others and seek information. Technologies have the potential to help older adults remain in their homes, but this will require access to digital devices and the knowledge of how to use them. Overall, psychology has already made significant contributions to policy decisions and allocations of resources related to the aging population, and is likely to continue doing so in the future. Grown children and grandchildren play an important part in the social life of many older adults. Unlike other primates, humans live long past their reproductive years, suggesting that grandparents may have historically contributed to the survival of their extended families. You might hold a common stereotype about older adults being sad and lonely. If so, you will probably be surprised to learn that psychological research has found that depression is higher in young people than in healthy, older adults (see Figure 11.29) and that older adults report higher subjective well-being overall than young adults (Carstensen, Isaacowitz, & Charles, 1999). Given the frequent challenges of aging, including physical and cognitive changes, what could possibly account for this increase in emotional well-being? Part of the explanation lies in the way people look at the time they have remaining in life (Carstensen, Fung, & Charles, 2003;  Carstensen et al., 2010). When people see a long stretch of time remaining in their lives, they pursue achievement, possibly at the expense of meeting their social and emotional needs. In contrast, when older adults are looking at a shorter period of remaining life, their goals are refocused toward seeking positive emotional experiences. In contrast to stereotypes about sad, older adults, depression rates are highest in late adolescence and young adulthood and decrease gradually over the life span. One thing that doesn’t change during our journey through the life span is the need for social connection. Older adults show significant benefits from their friendships, but they have fewer of them than younger adults and are less likely than younger adults to form new ones (Rawlins, 2004). Relationships with siblings can be especially meaningful for older adults (Connidis, 1992). Relationships with grown children and grandchildren also form a significant part of the social life of older adults. Historically, many parents entered the role of grandparent during midlife, but with more young adults postponing parenting, the grandparent role is occurring later as well. Humans are somewhat unusual among primates in their length of life span beyond their reproductive years, a phenomenon known as the grandmother effect. A healthy woman might live 40 or 50 years past her age at menopause. It is possible that her contributions to the care of her grandchildren, freeing her children for other tasks, might have provided an adaptive advantage over the course of evolution (Herndon, 2010). Relationships continue to play important roles in the social lives of seniors. Marriage is related to continued good health in seniors. Marriage in late adulthood makes an important contribution to continued good health. In married older adults, interacting with the spouse contributes to happiness by buffering the individual from stress (Waldinger & Schulz, 2010). Seniors who have remained married to the same person for decades have seen many changes in their relationships over the years. We observed earlier that parents of young children experience a drop in marital satisfaction, which improves as children leave the home as young adults. Long-term marriage partners show more similarities with each other because of a lifetime spent in each other’s company, along with a characteristic lack of attention to past negative events and personality traits (O’Rourke & Cappeliez, 2005). In other words, they believe their relationship has been more positive than it was. These couples also show an interesting pattern of conflict avoidance, which contributes to higher levels of marital satisfaction. Getting married and being married can mean different things at different stages in life. One of the more interesting age-related changes in marriage is the impact of similar personalities on a couple’s marital satisfaction throughout the life span (Shiota & Levenson, 2007). Overall, the saying Birds of a feather flock together is quite true, so it is surprising that some similarities in personality are predictive of reduced marital satisfaction and are more so at some life stages than at others. Early in marriage, personality similarities can promote bonding and intimacy. At midlife, however, the focus of the couple often turns to more practical than romantic tasks, such as raising children, advancing careers, and running a household. In this new environment, similarities in the personality trait of conscientiousness are associated with less marital satisfaction (Shiota & Levenson, 2007). The interaction between the highly conscientious person’s drive to finish tasks and the midlife tendency to be a bit less flexible in routine suggests that putting two such individuals under the same roof is likely to produce substantial conflict. We might also speculate that two nonconscientious partners might be overwhelmed by the midlife workload, and the stress resulting from constantly playing catch-up on tasks might take a toll on the relationship. Fortunately, as the family workload diminishes later in life, the married couple’s focus returns to intimacy, and the negative effects of similar personality recede again. We usually think of cyberbullying as something children and teens do, not necessarily something that older adults would do. As a result, we know relatively little about the prevalence and characteristics of cyberbullying by adults. Christopher Bartlett and his colleagues (Barlett & Chamberlin, 2017) surveyed 167 youth (average age of 13.76) and 552 adults (average age of 36.20) to attempt to fill this gap in our understanding of cyberbullying. Results indicated that cyberbullying does indeed occur in adulthood. In general, these researchers found that cyberbullying behavior increased into young adulthood and then decreased with further increases in age. The authors suggest several hypotheses for these findings. First, as we observed in Chapter 10, cyberbullying is most prevalent among individuals who are relatively sophisticated in their use of the Internet and social media. Although the adult sample for this study was recruited using Amazon Mechanical Turk, which implies a reasonable level of Internet competence among the participants, the age groups might have varied substantially in their online sophistication. Second, national law enforcement data cited by Bartlett et al. (2017) demonstrate that antisocial and aggressive crimes are more common in youth than in adults. We discussed several factors in this chapter, such as maturity of the parts of the brain that participate in decision-making, that might account for this finding. As cyberbullying qualifies as an aggressive behavior, we might expect it to follow the pattern of the crime statistics. Many further questions about adult cyberbullying remain. Are the characteristics of the perpetrators the same or different across age groups? Are the motives and objectives similar or different? To what extent do workplace and relationship issues shape cyberbullying by adults? We usually think of cyberbullying as something that happens to teens, but the extent of cyberbullying among adults, especially in the workplace, remains poorly understood. Aspect of developmentHighlightsPhysical changeAdolescents mature sexually and reach adult height and weight. The brain continues to grow through adolescence and into young adulthood. Menopause is a major transition for women in midlife. Among healthy older adults, physical changes are gradual. CognitionWorking memory and reaction time reach adult levels during adolescence. Thinking becomes more logical during adolescence and more independent in young adulthood. Moral reasoning in adults is usually conventional or postconventional. Intelligence remains relatively stable in healthy adults. Social and emotional lifeAdolescents seek to develop identity. Relationships might lead to marriage and parenting in young adulthood. Midlife adults may experience changes in family and work domains. Older adults show significant benefits from social connections. Be sure that you can define these terms and use them correctly..`;\n","export const text = `Different versions of the serotonin transporter gene are not equally distributed throughout the world. Although you might be comfortable with the idea that a trait like a moth’s color can make it better adapted to one environment over another, how do you feel about your personality as an adaptation? Are there some types of personalities that thrive in some circumstances better than others? Can a culture shape personality? Can the personalities of a group of people shape their culture? In Chapter 3, we introduced you to the serotonin transporter gene, which can come in two versions or alleles: Short (S) and long (L). Children with two short alleles (SS) were more likely than children with either a combination of alleles (SL) or two long alleles (LL) to react negatively to the experience of being bullied frequently (Sugden et al., 2010). Zooming out to the map of the world, you can see that these genotypes are not equally distributed worldwide. Yellow areas on the map have low frequencies of the S allele, and orange and red areas have higher frequencies. South Africa has a relatively small number of people with the S allele (28%) compared to the United States, Australia, and Great Britain (about 40 to 45%), which in turn have many fewer than China (about 80%). Besides having different distributions of the serotonin transporter gene alleles, what else do you know about these cultures? If you thought about these cultures as differing in individualism versus collectivism, you are correct. Although some cultures with lower frequencies of the S allele have more collectivistic cultures than the United States, there is a positive correlation between the percentage of S allele carriers and a culture’s rating on a scale from individualistic to collectivistic (Chiao & Blizinsky, 2010). Why would this be the case? Like all correlations, multiple relationships among genetic profiles and culture ratings are possible, and we cannot make conclusions about causes for these relationships. A population with a certain genetic mix might be more likely to form a particular type of culture, a culture might shape the reproductive success of its members, some outside variable could influence both the genetic mix and the form of the culture, or some combination of these factors could influence one another simultaneously (see Figure 12.1). Asians have a much higher probability of having the S allele of the serotonin transporter gene than do people living in many other parts of the world. The percentage of people carrying the S allele varies according to a culture’s position on the continuum from individualism to collectivism. Only about 40% of people in highly individualistic cultures such as the United States, Australia, and the United Kingdom carry the S allele, compared to nearly 80% in highly collectivistic cultures such as China, Japan, and Korea. Zooming in to the level of the individual, we find that people carrying the S allele are especially attentive to negative information, such as words and pictures that produce negative emotions (Beevers, Gibb, McGeary, & Miller, 2007;  Osinsky et al., 2008). This focus on negativity might help people in collectivistic environments because it could lead to a heightened sensitivity to another person’s anger or fear. Early recognition of impending negative interactions might give people a chance to smooth things over before they escalate. The L allele, however, is associated with reduced responses to psychological and social sources of stress (Nardi et al., 2013), which might enhance coping in the individualistic environment. The person carrying the S allele might be too cautious for the individualistic cultural environment, whereas the person carrying the L allele might act like a bull in a china cabinet in the collectivistic environment (see Figure 12.2). The serotonin transporter gene comes in two versions or alleles. People with a copy of the short (S) allele tend to be more anxious and cautious, whereas people with a copy of the long (L) allele are prone to greater risk taking and creativity. Although the S allele is a risk factor for depression (see Chapter 14), China has a lower rate of depression than the United States despite the higher frequency of the S allele in the Chinese population (Chiao & Blizinsky, 2010). The collectivistic environment might buffer an individual with the S allele from the stress that can lead to depression. As you will see in this chapter, individual differences are an important aspect of behavior and mental processing, but they must be viewed within the larger contexts of the social and cultural environment. Suppose that you were given the following feedback about your personality (Forer, 1949, p. 120):You have a need for other people to like and admire you, and yet you tend to be critical of yourself. Although you have some weaknesses, you are generally able to compensate for them. You have considerable unused capacity that you have not turned to your advantage, however. Disciplined and self-controlled on the outside, you tend to be worrisome and insecure on the inside. At times, you have serious doubts as to whether you have made the right decision or done the right thing. You prefer a certain amount of change and variety and become dissatisfied when hemmed in by restrictions and limitations. You also pride yourself as an independent thinker and do not accept other’s statements without satisfactory proof. But you have found it unwise to be too frank in revealing yourself to others. How well does this description fit your beliefs about your personality? You might be impressed by how this paragraph presents such specific and personalized information about you. Ironically, you probably relate to this description because it applies to nearly everyone. So, the question is: If our personalities are so unique, how can we all be so easily described in a few brief sentences? The science of personality explores characteristic patterns of thinking, feeling, and behaving. To what extent is a person’s personality truly unique, and to what extent is personality based on general characteristics that describe nearly everyone? How consistent is our behavior across time and across situations? How much of a role does biology or experience play in developing our personalities? Is the basic nature of humans good or bad? Personality is our characteristic way of thinking, feeling, and behaving. How unique are we? How much do our personalities overlap with the personalities of other people? Personality theories represent far more than simple curiosity about human behavior. These theories are intimately bound to our views of normal and abnormal behavior, which we will discuss in Chapter 14. Many of the psychologists who proposed personality theories also contributed influential ideas to the treatment of psychological disorders. The study of personality for the first half of the 20th century was dominated by the psychodynamic approaches of Sigmund Freud and his followers. Behaviorists such as B. F. Skinner attempted to apply principles of learning to personality. The humanistic psychologists, including Carl Rogers, actively rebelled against Freud’s psychodynamic approach. While few contemporary psychologists adhere to these historical approaches to personality, these theories stimulated interest in individual differences. The term psychodynamicpsychodynamicA theory put forward by Sigmund Freud in which psychic energy moves among the compartments of the personality: Id, ego, and superego.            psychodynamic        A theory put forward by Sigmund Freud in which psychic energy moves among the compartments of the personality: Id, ego, and superego.             was first used by Ernst von Brücke in 1874 (Hall, 1954). Von Brücke used the term as an extension of the first law of thermodynamics, which states that energy in a closed system can change from one form to another but cannot be created or destroyed. You may not have heard of von Brücke before, but you are likely familiar with one of his students, Freud, who made psychodynamics the foundation of his theory of personality. As a young physician, Freud became famous for his success in treating patients with hysteria, a condition characterized by physical symptoms without medical cause that is known today as somatic symptom disorder (see Chapter 14). Freud sought to incorporate his observations of his patients into a new, unifying theory of psychology. His treatment approach, which we cover in Chapter 15, was known as psychoanalysispsychoanalysisSigmund Freud’s treatment approach based on his psychodynamic theory            psychoanalysis        Sigmund Freud’s treatment approach based on his psychodynamic theory            . Personality was viewed by Freud as a closed energy system, in which psychic or instinctive energy moves among three compartments: Id, ego, and superego (see Figure 12.3). Freud proposed that the ididThe component of Sigmund Freud’s personality theory containing primitive drives present at birth            id        The component of Sigmund Freud’s personality theory containing primitive drives present at birth            , which literally means it in Latin, is present at birth and contains the primitive drives that serve as a source of energy for the personality, such as hunger, thirst, and sex. Freud believed that the id operates according to the pleasure principle, seeking immediate gratification and relief. As a child begins to interact with parents and other social influences, the ego and the superego begin to control the id. The egoegoThe component of Sigmund Freud’s personality theory that is the self that others see            ego        The component of Sigmund Freud’s personality theory that is the self that others see             is the component of the personality that is readily seen by others, so it acts as the person’s self. Its task is to coordinate the needs of the id with reality. The superegosuperegoThe component of Sigmund Freud’s personality theory that internalizes society’s rules for right and wrong, or the conscience            superego        The component of Sigmund Freud’s personality theory that internalizes society’s rules for right and wrong, or the conscience             develops when a child begins to internalize society’s rules for right and wrong, forming what we normally refer to as a conscience. As shown in Figure 12.4, we are not conscious of the operations of the id, although we might become consciously aware of the operations of the ego and superego. Sigmund Freud viewed the mind as a closed system in which energy flows from one compartment to the next. It is the job of the ego, or self, to balance the pleasure-seeking impulses of the id with the morality imposed by the superego. Imbalances can be the source of unhealthy behavior. An overly dominant id can lead to antisocial behavior, and an overactive superego might produce too much guilt. According to Freud, we are consciously aware of only a small percentage of the mind’s activities. Some information is located in the preconscious mind and can become conscious at any moment. Information in the unconscious mind can affect our behavior without our awareness. Threats to the balance of the id, ego, and superego can result in anxiety or the sense that danger is around the corner. When the ego is faced with anxiety, it engages in a variety of protective behaviors called defense mechanismsdefense mechanismsIn Sigmund Freud’s personality theory, a protective behavior that reduces anxiety.            defense mechanisms        In Sigmund Freud’s personality theory, a protective behavior that reduces anxiety.            . According to Freud, each of these mechanisms helps us channel potentially self-destructive or painful psychic energy into more constructive or manageable behaviors. For example, when we use the defense mechanism of sublimation, we redirect negative energy. An aggressive teen might take up martial arts rather than starting fights at school. Freud noted, though, that each mechanism has its limits; if people continually use defense mechanisms, they risk doing damage to the ego or self as well. For Freud, the development of the personality is critical to the remainder of his theories and to the therapeutic process of psychoanalysis. Freud assumed that the origins of his patients’ problems could be found in their developmental past, not in their present circumstances. As the word psychosexual implies, Freud was interested in how the developing personality would deal with the sexual impulses of the id. Freud proposed five psychosexual stagespsychosexual stagesA stage in Sigmund Freud’s theory of the developing personality.            psychosexual stages        A stage in Sigmund Freud’s theory of the developing personality.             of personality development: Oral, anal, phallic, latency, and genital. Personality, according to Freud, is the product of conflict occurring during these psychosexual stages. Personality characteristics emerge as the id, ego, and superego resolve their conflicts and struggle for balance. Should the conflict become so great as to produce a clash between ego and reality, psychosis (including abnormal behaviors characteristic of schizophrenia and other serious disorders) might occur (Freud, 1938). Although there is no question regarding the tremendous influence Freud has had on modern thinking, his legacy today is considered mixed. Freud’s approach to development contrasts sharply with the scientific theories of development we explored in Chapter 11, but his work remains important in that it had such a profound influence in psychology and popular culture. Sigmund Freud has become a mainstay in popular culture. A few Freudian constructs appear to be supported by scientific investigation, even though the theoretical mechanisms he proposed have been refuted. The notion of defense mechanisms, the distinction between conscious and unconscious processes, and the descriptions of some types of adult personalities remain topics of research in psychological science (Kline, 2013). However, contemporary scientists point out that Freud’s ideas rarely lend themselves to scientific investigation. Some of his ideas, such as women’s penis envy, are viewed today as remarkably sexist (Sprengnether, 1990). Additional criticisms center on the questionable effectiveness of psychoanalysis, which we discuss in more detail in Chapter 15. No theory as influential as Freud’s is likely to be left alone without revision. Several of Freud’s followers built on his approach to construct their own psychodynamic theories. We refer to these as neo or new Freudians. A common feature of the neo-Freudianneo-FreudianA theorist who attempted to update and modify Sigmund Freud’s original theory of personality.            neo-Freudian        A theorist who attempted to update and modify Sigmund Freud’s original theory of personality.             theories was a substitution of social competence for sexuality and pleasure as the major motivation for human behavior. These theorists brought the psychodynamic approach closer to contemporary views of humans as social animals, a theme we discuss throughout this textbook. Unfortunately, neo-Freudian theories share one of Freud’s greatest weaknesses—the proposal of constructs that do not lend themselves to scientific investigation. We will highlight a few of the notable contributions of this group. Alfred Adler (1870–1937) contributed the notion of an inferiority complex to psychodynamic thought. According to  Adler (1956), an inferiority complex or overwhelming feelings of inferiority can lead to overcompensation, usually in the form of seeking the appearance of superiority rather than the substance. For example, a person might overcompensate for feelings of inferiority by purchasing a fancy, expensive automobile. Adler was also one of the first psychologists to suggest that siblings, along with parents, influence development, leading to investigations of the effects of birth order. Contemporary research, however, casts serious doubt onto the existence of any birth order effects on personality or intelligence (Damian & Roberts, 2015). Carl Jung (1875–1961), a Swiss psychiatrist, divided the unconscious mindunconscious mindThe part of mental activity that cannot be voluntarily retrieved.            unconscious mind        The part of mental activity that cannot be voluntarily retrieved.             into two components. The personal unconscious remained the same as Freud’s unconscious mind. Jung added a deeper level, the collective unconscious, composed of our common psychological predispositions as humans passed from generation to generation (Jung, 1928). After observing common themes in cultures around the world during his extensive travels, Jung argued that generations of experience with concepts such as darkness, power, death, and parents would lead to characteristic ways of unconsciously thinking about such topics. Jung’s ideas are echoed in today’s evolutionary approach to psychology, which views the mind as having adapted over the course of evolution (Walters, 1994). This Tibetan mandala represents unity formed out of the four elements of earth, wind, water, and fire, a theme that Carl Jung believed was universal in the human collective unconscious. In addition to his expansion of the concept of the unconscious, Jung deviated from Freud by providing more specific information about differences in individual personality. In particular, Jung’s concepts of the personality characteristics of extroversion (outgoing) and introversion (relatively less outgoing) are still very much alive in current discussions of personality. Jung’s personality characteristics form the basis for several popular personality tests, such as the Myers–Briggs Type Indicator (MBTI). Unfortunately, as we will see later, the MBTI does not hold up well to analyses of validity and reliability. Karen Horney (1885–1952) was one of the first female psychiatrists. Horney rejected many of Freud’s ideas to concentrate on aspects of culture that contribute to women’s feelings of inferiority. Simultaneously, while treating male patients, Horney became convinced that men envied women’s ability to become pregnant and be mothers. Her emphasis on the impact of a male-dominated society on women helped set the stage for many subsequent feminist scholars (Horney, 1923–1937/1967, 1950). Karen Horney (1885–1952) was one of the first female psychiatrists, and although she is considered a neo-Freudian, she rejected many of Sigmund Freud’s ideas to concentrate on the sources of women’s feelings of inferiority. Alfred Adler suggested that an inferiority complex can lead to overcompensation in the form of seeking the appearance of superiority. Man heels that add at least 2 inches of height have become increasingly popular and are worn frequently by Hollywood stars. Behaviorist approaches to personality emphasized learning as an important influence on personality development. Because learning is a lifetime process, behaviorists expected to see change in personalities over time. According to B. F. Skinner, the principles of operant conditioning, discussed in Chapter 8, would lead to the development of stable response tendencies that might appear to others as traits. For example, if your parents were strict about enforcing rules at home, such as finishing chores, you might look like a conscientious person who always finishes assigned tasks on time. Compared to stable personality traits, Skinner’s response tendencies were more flexible and subject to change. If people experienced something new, their ways of responding would change to reflect that new interaction with the environment. If you move away from home to college, and you no longer have your parents to reward or punish you based on your completion of tasks, you may not be conscientious about finishing your homework on time. Until the 1960s, American psychology was dominated by two perspectives: Behaviorism and the psychodynamic approach. At that time, some psychologists began to consider other ways of approaching psychology in general and in personality specifically. One alternative approach, initially called third-force psychology (behaviorism and psychodynamic theory were forces one and two), later became known as humanistic psychology. Humanistic psychology differed from behaviorism and the psychodynamic approach in several ways. Humanistic psychologists were convinced that humans are unique, and they argued that the animal research favored by behaviorists was irrelevant. The humanistic psychologists’ belief that human nature is basically good contrasts dramatically with the darker view of human nature put forward by Freud and other psychodynamic theorists. Humanistic psychologists believed that psychodynamic theories placed too much emphasis on abnormal behavior. Instead, they advocated studying exceptional people to see why they succeeded, a tactic continued by contemporary positive psychology approaches, discussed in Chapter 16. One of the leading humanistic psychologists, Abraham Maslow, was interested in motivation, and we discussed his contributions to this area in Chapter 7. He did not attempt a full-fledged theory of personality, although he made some interesting conclusions about the characteristics of exceptional people. After reviewing biographies of people such as Albert Einstein, Jane Addams, William James, and Abraham Lincoln,  Maslow (1950) observed that exceptional people shared a number of common traits. In addition to expected characteristics, such as creativity, realistic thinking, and concern for others, Maslow’s exceptional people had some surprising qualities. They had few friends, well-developed senses of humor, and periodic mystic or peak experiences. Carl Rogers (1951, 1980), another humanistic psychologist, put forward a theory of personality based on his experience as a therapist. We discuss Rogers’s approach to therapy in Chapter 15. Rogers, like Maslow, believed that humans strive toward self-actualization or maximizing their individual potential. For Rogers, people who follow the path laid out by their feelings will lead productive, healthy lives. They will experience congruence, or similarity, between their real selves and their ideal selves. Congruence is most likely to occur in response to unconditional positive regard, or a nonjudgmental appreciation for a person’s true nature. Being treated judgmentally makes people act in ways that are incongruent or inconsistent with their ideal selves, which might lead to disordered behaviors (see Figure 12.5). Rogers, like other humanistic psychologists, believed that people were striving for self-actualization or the ability to maximize their potential. Achieving that goal depended on the response of other people (family, friends, and society) to the self. If people receive unconditional positive regard or nonjudgmental acceptance, they can reach self-actualization. However, if their acceptance is conditional on whether they please others, the result is self-discrepancies that can lead to depression, anxiety, and other negative outcomes. Source: Adapted from D. A.  Bernstein (2011). Essentials of Psychology, 5th ed. Belmont, CA: Cengage. As was also the case in psychodynamic theories, it is difficult to move from the humanistic psychologists’ description of personality to our definition of personality as characteristic ways of thinking, feeling, and behaving. The humanistic psychologists are more interested in the process by which personality develops than in the actual characteristics that emerge. Like the psychodynamic theories they rejected, the humanistic psychologists are subject to criticisms of being unscientific in their methods (Child, 1973). TraitDescriptionSelf-conceptAttributes assigned to the self, as in I am a first-year botany major. Self-schemaCollection of all self-concept attributes, used to organize thinking about the self. Self-awarenessKnowledge of the self. Self-esteemJudgments of the self’s worth. Self-regulationSelf-control or conscious efforts to manage the self. Following a discussion of psychodynamic, behaviorist, and humanistic approaches to personality, the trait approach seems relatively simple. A traittraitA stable personality characteristic.            trait        A stable personality characteristic.             is a stable personality characteristic. Psychologists were hard at work testing personality traits long before they derived coherent theories to explain their findings. They realized intuitively that traits clustered together. An outgoing person might also be expected to be brave and cheerful. However, these approaches lacked the precision required by a true theory of personality. Statistical methods of correlation and factor analysis came to the rescue (see Figure 12.6). As we discussed in Chapter 2, correlations allow us to see whether two variables have some systematic relationship with each other, such as height and weight. Factor analysis allows a researcher to compare large numbers of correlations simultaneously, which makes the technique perfectly suited to the study of clustered personality traits (Spearman, 1904). The statistical technique of factor analysis allows psychologists to identify factors that positively correlate or cluster together, as well as the strengths of those relationships. Raymond Cattell identified 16 personality factors that formed five clusters (extroversion, anxiety, self-control, independence, and receptivity) that are similar to the factors in the Big Five theory proposed later by McCrae and Costa (1985). The darker lines represent stronger statistical relationships. One of the earliest efforts to identify personality traits began with the selection of 4,500 words that described observable traits out of a dictionary (Allport & Odbert, 1936). Some of these terms were defined as central traits, or characteristics that most clearly define and differentiate a person (Allport, 1937). These are the terms you might use when trying to describe someone you know: smart, hardworking, and outgoing. Emerging computer technologies in the 1940s allowed psychologists to refine lists of traits further (Cattell, 1946/1969). Using factor analysis, Allport and Odbert’s original list of 4,500 words was reduced to 16 major personality traits. Each trait took the form of a continuum between opposites. For example, the trait of suspiciousness would range from trusting at one extreme to suspicious at the other. The basic methods and elements for trait theories were now in place. A common theme found in the work of early trait theorists is that relatively few traits are required to describe and predict an individual’s behavior accurately. Based on this work, a Big Five theoryBig Five theoryA trait theory that identifies five main characteristics that account for most individual differences in personality            Big Five theory        A trait theory that identifies five main characteristics that account for most individual differences in personality             identified five core traits: Openness to experience, conscientiousness, surgency (extroversion), agreeableness, and neuroticism (McCrae & Costa, 1985,  1987; Norman, 1963). To remember the Big Five traits, use the acronym OCEAN. The development of a personality test to measure the five traits has made it possible for many investigators to explore the Big Five theory scientifically (see Figure 12.7). The current version of the test is known as the NEO-PI-R Inventory (Costa & McCrae, 2011). You will have an opportunity to take a short version of this test in this chapter. Each of the Big Five traits consists of a continuum from one extreme to the other. We will discuss the five traits in order of the OCEAN acronym. OpennessOpennessA Big Five personality trait characterized by an appreciation for fantasy, feelings, actions, ideas, values, and aesthetics.            Openness        A Big Five personality trait characterized by an appreciation for fantasy, feelings, actions, ideas, values, and aesthetics.             to experience involves fantasy, aesthetics (an appreciation for the arts), feelings, actions, ideas, and values. People high on openness are curious, unconventional, and imaginative. They are interested in exploring aspects of life that are different from their own, whether this means trying new foods, traveling to exotic locations, or studying other religions. People low on openness are more practical, traditional, and conforming. They prefer the familiar over the new, choosing a chain restaurant in a new city rather than experimenting with the local cuisine. ConscientiousnessConscientiousnessA Big Five personality trait characterized by competence, order, dutifulness, achievement striving, self-discipline, and deliberation.            Conscientiousness        A Big Five personality trait characterized by competence, order, dutifulness, achievement striving, self-discipline, and deliberation.             incorporates competence, order, dutifulness, achievement striving, self-discipline, and deliberation. People high on this trait are reliable, work hard, and complete tasks on time. People low on this trait are unreliable, somewhat lazy, and undependable. It is frustrating to work on group projects with people who are low in conscientiousness, but under some circumstances, they make valuable contributions. For example, they might identify unnecessary steps in a process, reducing the time pressure felt by a group, or promote some forms of outside the box thinking and problem solving. ExtroversionExtroversionOne of the Big Five traits characterized by warmth, gregariousness, assertiveness, activity, excitement seeking, and positive emotion; opposite of introversion            Extroversion        One of the Big Five traits characterized by warmth, gregariousness, assertiveness, activity, excitement seeking, and positive emotion; opposite of introversion             is characterized by warmth, gregariousness, assertiveness, activity, excitement seeking, and positive emotion, although extroverts also tend to be insensitive and overbearing. IntroversionIntroversionOne of the Big Five traits characterized by coolness, reserve, passivity, inactivity, caution, and negative emotion; opposite of extroversion            Introversion        One of the Big Five traits characterized by coolness, reserve, passivity, inactivity, caution, and negative emotion; opposite of extroversion             is characterized by coolness, reserve, passivity, and caution, although introverts also tend to be sensitive and reflective. Extroverts seek social activity and find it invigorating, whereas introverts want time to themselves to recharge their batteries. AgreeablenessAgreeablenessA Big Five personality trait characterized by trustworthiness, altruism, trust, compliance, modesty, and tender mindedness            Agreeableness        A Big Five personality trait characterized by trustworthiness, altruism, trust, compliance, modesty, and tender mindedness             includes trust, straightforwardness, altruism, compliance, modesty, and tender mindedness. People low in this trait are cynical, uncooperative, and rude. It is challenging to identify advantages for a trait like disagreeableness. In employment settings, however, disagreeable people are capable of making tough decisions, whereas their more agreeable colleagues find it difficult to supervise others (Rust, 1999). In one study, men who scored below the mean on measures of agreeableness earned 18% more than men who were more agreeable (Judge, Livingston, & Hurst, 2012). People who score high on the trait of openness are curious, unconventional, and imaginative. Finally, neuroticismneuroticismA Big Five personality trait characterized by anxiety, angry hostility, depression, self-consciousness, impulsivity, and vulnerability.            neuroticism        A Big Five personality trait characterized by anxiety, angry hostility, depression, self-consciousness, impulsivity, and vulnerability.             combines anxiety, angry hostility, depression, self-consciousness, impulsivity, and vulnerability. The opposite trait could be described as emotional stability. Although scoring high on neuroticism sounds unpleasant, this trait has survival advantages (Zhang, Chrétien, Meaney, & Gratton, 2005). In threatening and impoverished environments, high neuroticism increases a person’s chances of surviving to reproduce. We observed a similar argument in Chapter 11, in which a child’s insecure attachment to the parents had benefits for survival in a dangerous environment (Belsky, 1999). Whether considering a personality trait or attachment, the anxious, clinging child is more likely to stay near protection, enhancing survival in dangerous situations. Across a range of professional, skilled, and unskilled job descriptions, the conscientiousness variable had the greatest positive correlation with work-related success (Barrick & Mount, 1991). Surprisingly, the same meta-analysis found little relationship between agreeableness and job performance. Big Five traits have provided a framework for investigating job performance (Barrick & Mount, 1991), parenting style (Prinzie, Stams, Deković, Reijntjes, & Belsky, 2009), psychological disorders and substance abuse (Kotov, Gamez, Schmidt, & Watson, 2010), cognitive decline in older adulthood (Curtis, Windsor, & Soubelet, 2014), happiness (DeNeve & Cooper, 1998), academic performance (Komarraju, Karau, & Schmeck, 2009), and many other specific outcomes. Evidence across diverse cultures, including the United States, Japan, South Korea, Germany, Portugal, Israel, and China, suggests that these personality traits represent universal human characteristics (McCrae & Costa, 1997). Out of the Big Five characteristics, conscientiousness shows the highest positive correlation with work-related success, whether a person is employed in a professional, skilled, or unskilled job. The full version of the Big Five Inventory (BFI) contains 44 items. The 10-item version in Table 12.1 has served as a reasonable substitute for the full test for research purposes when time is limited (Rammstedt & John, 2007). If reading about the different Big Five traits has left you wondering about your own personality, this is your chance to get an idea of where you stack up on each of these factors.1. I see myself as someone who is reserved.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly2. I see myself as someone who is generally trusting.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly3. I see myself as someone who tends to be lazy.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly4. I see myself as someone who is relaxed, handles stress well.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly5. I see myself as someone who has few artistic interests.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly6. I see myself as someone who is outgoing, sociable.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly7. I see myself as someone who tends to find fault with others.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly8. I see myself as someone who does a thorough job.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly9. I see myself as someone who gets nervous easily.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree strongly10. I see myself as someone who has an active imagination.1Disagree strongly2Disagree a little3Neither agree nor disagree4Agree a little5Agree stronglyHere is how you should score your results. To ensure that you don’t just pick the same number for each item, half of the questions reverse the scale. Follow the directions below carefully so that you obtain a correct score. Scores can range from 1 to 5 on each trait. Extroversion: Q1R (Reverse) and Q6 Subtract your answer to Q1 from 6 and add to your answer to Q6. Divide by 2. Scores of 1 or 2 are introverted, scores of 4 and 5 are extroverted, and a score of 3 is neutral. Agreeableness: Q2 and Q7R (Reverse) Add your response to Q2 to (6 minus your answer on Q7), and divide the sum by 2. Scores of 1 or 2 are disagreeable, scores of 4 and 5 are agreeable, and a score of 3 is neutral. Conscientiousness: Q3R (Reverse) and Q8 Add (6 minus your answer on Q3) to your answer on Q8, and divide the sum by 2. Scores of 1 or 2 are low conscientiousness, scores of 4 and 5 are high conscientiousness, and a score of 3 is neutral. Neuroticism: Q4R (Reverse) and Q9 Add (6 minus your answer on Q4) to your answer on Q9, and divide the sum by 2. Scores of 1 or 2 are low neuroticism, scores of 4 and 5 are high neuroticism, and a score of 3 is neutral. Openness: Q5R (Reverse) and Q10 Add (6 minus your answer on Q5) to your answer on Q10, and divide the sum by 2. Scores of 1 or 2 are low openness, scores of 4 and 5 are high openness, and a score of 3 is neutral. Circle the number that represents your best answer for each item. At the end, we’ll tell you how to add up your scores. A self-report test like this is only as good as you make it by answering honestly. Wanting to appear socially desirable by downplaying your negative characteristics might make you feel better, but this approach reduces the validity of the test results. At the same time, no paper-and-pencil test is perfect, and if a result does not fit the way you honestly view your personality, feel free to disagree with the results. Not all psychologists agree that personality originated in stable traits that produce consistent patterns of behavior across different situations. Research showing that situations could exert a stronger influence on people’s behavior than any individual predispositions highlighted limits to the general trait approach. We discuss several of these situations in Chapter 13. For example, studies of obedience or conformity might be better understood by evaluating the situations people encounter rather than the predispositions or traits they bring to the laboratory (Asch, 1951;  Milgram, 1963). Kurt  Lewin (1951) argued that a person’s behavior (B) is a function of the person (P) and the person’s environment (E). This led to a vigorous debate regarding which element, personality or the situation, produced the greatest influence on behavior. Historically, many psychologists continued to focus on personality traits. More recent work has demonstrated an increasing respect for the influences of situational variables on behavior. Personality may predict typical behaviors performed by individuals, but situations often provide better insight into cases in which people behave in uncharacteristic ways (Sherman, Rauthmann, Brown, Serfass, & Jones, 2015). As predicted by Lewin, personality and situations usually interact. For example, individual personality traits influence the types of situations we choose to encounter as well as the way we think about those situations, which in turn reinforces the original personality traits (Rauthmann, Sherman, Nave, & Funder, 2015). In other cases, the power of situations can have large and lasting effects on personality. For example, the experience of unemployment produced long-term changes in individual levels of agreeableness, conscientiousness, and openness (Boyce, Wood, Daly, & Sedikides, 2015). Is this man helping these women because of his individual personality traits? Or do situational variables do a better job than personality traits at predicting behavior? Personality is usually studied, not surprisingly, in individuals, but one situation that might have a particularly strong influence on personality is being in a group. At least in the case of fish, individual personality is sometimes suppressed in groups in favor of the collective decisions of the school (McDonald, Rands, Hill, Elder, & Ioannou, 2016). Fish, like humans, show individual differences in boldness, or the willingness to accept reward over risk. The boldness of individual fish tested alone is quite consistent, but the behavior of the same fish within a school of fish is not. This suggests that at least in the area of risk-taking, testing individuals might give psychologists very different results than testing the same people in a group setting. Midway in the 20th century, some behaviorists began to think in more cognitive terms and believed that learning theories of personality had neglected motivations, emotions, and cognitions. These behaviorists recognized the importance of personality in social contexts, where behavior is shaped and rewarded by interactions with other people. The resulting social–cognitive learning theoriessocial–cognitive learning theoriesA theory of personality that features cognition and learning, especially from the social environment, as important sources of individual differences in personality.            social–cognitive learning theories        A theory of personality that features cognition and learning, especially from the social environment, as important sources of individual differences in personality.             are consistent with our observations that single perspectives in psychology do not always capture the whole story. One influential social–cognitive learning theory featured a concept known as locus of controllocus of controlA cognitive expectancy featured in social–cognitive learning theories of personality about the source of individual outcomes; an external locus of control sees outcomes as resulting from luck or chance, while an internal locus of control sees outcomes as the result of individual effort.            locus of control        A cognitive expectancy featured in social–cognitive learning theories of personality about the source of individual outcomes; an external locus of control sees outcomes as resulting from luck or chance, while an internal locus of control sees outcomes as the result of individual effort.             (Rotter, 1966). If you have an external locus of control (locus means place in Latin), you expect that most of your outcomes occur because of chance, luck, opportunity, or other factors beyond your control. However, if you have an internal locus of control, you believe that most of your outcomes are due to your own talent and effort. People with an internal locus of control manage stress more effectively, floss their teeth more regularly, are more likely to seek shelter in response to tornado warnings, use their seat belts more consistently while driving, and are more likely to practice effective birth control (Wallston, 2005). However, people with an internal locus of control tend to be less sympathetic with the plight of others, viewing their troubles as due to their choices and behavior as opposed to outside forces they can’t control (Lane, 2001). People with an internal locus of control believe that most of our outcomes are due to our own efforts, while people with an external locus of control believe that our outcomes are more often due to factors beyond our control. Although people with an internal locus of control experience less stress, they tend to be less sympathetic with the plight of others. The person with an internal locus of control might believe this woman needs to try harder to escape homelessness, while the person with an external locus of control might believe this woman just needs help and better opportunities. Albert Bandura, whom we met in Chapter 8, attempted to capture a balance between external, environmental factors and internal traits in his theory of reciprocal determinismreciprocal determinismA social–cognitive learning theory of personality that features the mutual influence of the person and that of the situation on each other.            reciprocal determinism        A social–cognitive learning theory of personality that features the mutual influence of the person and that of the situation on each other.             (Bandura, 1990,  1993). Like other social–cognitive learning theorists, Bandura rejected the strict behaviorism of Skinner as applied to personality. He agreed with Skinner that the environment can determine behavior but thought that it does so reciprocally, as people influence the environment through both their internal cognitions and their behavior. Bandura believed that observation of others’ behavior played an important role in the development of personality. Although much of his work centered on aggression, Bandura believed that many other characteristics were learned in this same manner. He was particularly interested in self-efficacy (Bandura, 1990,  1993,  1995). This characteristic refers to confidence in your abilities to gain reinforcement. If self-efficacy is high, you have strong expectations that positive outcomes are within reach. If you study hard and get good grades, you expect to obtain a good job in your chosen career. However, if self-efficacy is low, you might doubt your abilities to reach your goals. Self-efficacy can be situation specific. You might be confident in your abilities in science but less so in your abilities in rock climbing. Self-efficacy influences a person’s environment and experiences. A confident job candidate is probably more likely to be hired than an applicant who appears insecure. Self-efficacy is the belief that you can handle a challenging situation and obtain future rewards. Walter Mischel argued that trait theories could not explain the variation in an individual’s behavior across situations (Mischel, 1968). For example, when 51 boys at a summer camp were observed, their extroversion–introversion scores did not predict their extroverted or introverted behaviors across 21 situations (Newcombe, 1929). Instead of focusing on traits alone, Mischel argued that the stability we expect in a person’s behavior arises from that individual’s characteristic way of interpreting situations (Mischel, 2004). To illustrate this process, consider two children who both score high on some measure of aggression. Trait theorists would expect the children to be consistently more aggressive than their low-scoring peers regardless of circumstances, yet these are precisely the results that Mischel found to be lacking in actual studies (Mischel, 1968). Instead, an individual child’s behavior can be predicted by considering if–then relationships between situations and behavior (Mischel & Shoda, 1995). One child may be aggressive when approached by peers but respond passively to an adult’s efforts to apply discipline, while another might show the opposite pattern. In other words, if the child experiences a particular situation (interacting with peers or adults), that child then responds with a unique, stable set of behaviors (aggressive or passive). These if–then patterns are characteristic of an individual, leading to the stable characteristics we think of as personality. Biological theories of personality build bridges between observed traits and their underlying biological correlates. Among the biological variables connected to personality are temperament, genetic predispositions, and brain structure. These variables serve as behavioral phenotypes that can be used to compare and contrast underlying genetic profiles (Skuse, 2000). We introduced the concept of temperamenttemperamentA child’s pattern of mood, activity, or emotional responsiveness linked to later personality.            temperament        A child’s pattern of mood, activity, or emotional responsiveness linked to later personality.            , or a child’s pattern of mood, activity, or emotional responsiveness, in Chapter 11. Contemporary psychologists believe that temperament is biological in origin (Rothbart, 2011) but quickly begins to interact with the social and physical environments. Not only does a bold, fearless child select different activities, but that child is treated differently than a quiet, cautious child by peers, parents, and teachers. As we have observed in this chapter, children demonstrate individual differences along several dimensions of temperament, including positive emotionality, negative emotionality, and effortful control (Shiner & DeYoung, 2013). While we have emphasized a role for biology in temperament, it is clear that environment also plays important roles. Preschool-aged children are in a state of transition as they become less dependent on their parents and begin to venture into a world featuring more peer interaction. The Question: Does increased interaction with peers influence preschoolers’ temperament traits? Two classrooms of 3- and 4-year-olds at a university preschool were observed over the course of one year. Trained observers rated the children for temperament traits and observed their social networks. A total of 15,387 observations were taken and analyzed. Children are subject to enhanced protection when they serve as research participants. In a university preschool, it is likely that parents have signed a global consent for their children to participate in research. To the extent that they can understand, children can withhold assent to participate even when their parents approve their participation. Temperament traits were stable over the year of observation, but some change did occur. Children who were high in negative emotionality were less likely to be selected as a play partner. Children with high levels of positive emotionality formed social relationships with others sharing this trait. Over the year, children’s levels of positive emotionality and effortful control became more like that of their playmates (see Figure 12.8). Children’s temperaments shape their choice of peer relationships, as shown by the clustering of the children with higher levels of positive emotionality. At the same time, longitudinal observations show that children’s temperaments also shape those of their playmates. The results of this study suggest that for preschoolers, positive emotionality and effortful control are contagious. This finding is consistent with research using adults that show people become more similar to others with whom they associate in the expression of positive emotion, such as happiness (Fowler & Christakis, 2008). A married partner who is high in conscientiousness has a positive influence on the other partner’s work outcomes, above what would be predicted by the other partner’s own conscientiousness (Solomon & Jackson, 2014). In contrast, children’s negative emotionality did not seem to be influenced by their peers, perhaps because children with high negative emotionality experienced fewer peer interactions. They would have fewer opportunities to model (see Chapter 8) the more positive behaviors of their peers. Several dimensions of temperament are particularly important for adult personality, including reactivity and self-regulation (Ahadi & Rothbart, 1994;  Rothbart, Ahadi, & Evans, 2000;  Rothbart & Derryberry, 1981). Reactivity describes differences in people’s responses to novel or challenging stimuli. People’s responses may vary in terms of both intensity and timing. Some people respond to such stimuli immediately with highly intense anxiety, others respond more slowly, and some barely notice the stimuli. Self-regulation involves the ability to control attention and inhibit responding to perceived stimuli. These dimensions of temperament appear to be related to the Big Five traits. Higher reactivity is characteristic of introversion and neuroticism (Aron et al., 2010), while self-regulation is associated with later conscientiousness (Rothbart, Sheese, Rueda, & Posner, 2011). Other discussions of temperament separate reactivity into positive emotionality and negative emotionality (Shiner & DeYoung, 2013). Differences in the temperament dimension of reactivity predict children’s responses to novel or challenging stimuli. Children with high reactivity often display high introversion and neuroticism later in life. The early emergence of temperament suggests a genetic origin for personality. Large genomewide association studies (GWAS) have identified six genetic loci that are significantly associated with the Big Five traits (Lo et al., 2017). These loci also are predictive of different types of psychological disorders (see Chapter 14 and Figure 12.9). A genome-wide analysis (GWAS) identified relationships between six genetic loci, personality traits, and several types of psychological disorders. Narrow angles between arrows, such as between ADHD and extroversion indicate stronger correlations. Arrows pointing in opposite directions, such as agreeableness and autism, indicate negative correlations. The effects of genetics on personality often use the twin study method, such as the Minnesota Study of Twins Reared Apart (Bouchard, 1994;  Bouchard, Lykken, McGue, Segal, & Tellegen, 1990). In this ongoing research, Thomas Bouchard and his colleagues have studied 59 pairs of identical twins and 47 pairs of fraternal twins who had been raised in separate families, as well as a larger number of twins raised in the same home. The pairs of identical twins were quite similar, regardless of whether they were raised together. Some characteristics showed stronger positive correlations between identical twins, such as their scores on an occupational interest scale. Other characteristics showed relatively weaker positive correlations, such as nonreligious social attitudes. The critical finding was that the identical twins raised apart and together were similar to each other, whether the overall positive correlation for a particular characteristic was strong or weak (see Figure 12.10). Some characteristics, such as the number of ridges you have in your fingerprints, are more strongly influenced by genetics than other characteristics, such as your nonreligious social attitudes. Regardless of how much genetic influence a trait might have, identical twins show considerable similarity, whether they grow up in the same household or not. The Big Five personality traits show an approximate heritability of 0.50 in humans (Jang, Livesley, & Vernon, 1996;  Loehlin, McCrae, Costa, & John, 1998). As discussed in Chapter 3, heritability refers to populations rather than individuals. Saying that a trait such as shyness has a heritability of 0.40 does not mean that 40% of a person’s shyness is produced by genes and the other 60% is produced by the environment. Instead, a 0.40 heritability ratio suggests that across the human population, the variations we see in shyness from high to low are influenced moderately by both genetics and environmental factors. Adrian and Julian Riester were identical twin friars who died within hours of each other at the age of 92. Their shared experiences of more than 65 years as members of the Franciscan order probably accentuated their genetic similarities. If genetics accounts for about half of the variability seen in the population’s personality, what is the source of the other half? Environmental influences take the form of shared experiences, which affect all members of a family, and nonshared experiences, which affect an individual. For example, you and your siblings might attend the same elementary school and have Thanksgiving dinner with the extended family (shared experiences) while studying different musical instruments, having different friends, or playing different sports (nonshared experiences). Knowing the heritability for a trait like extroversion does not tell us much about the development of an individual’s personality. Heritability estimates are similar to knowing the overall average annual temperature in North America. Although this information could be useful for the study of global climate trends, it doesn’t tell us whether California is likely to have more wildfires or whether the skiing industry in Colorado will have a good year. We know that the population heritability for positive emotionality is between .46 and .52, but this doesn’t tell us how heritability interacts with shared and nonshared environments to produce particular outcomes (Krueger, South, Johnson, & Iacono, 2008). For example, depending on teenagers’ perceived relationship with their parents, the heritability of positive emotionality can vary widely. The positive emotionality of teens who perceive their relationship with their parents as poor is more strongly predicted by their individual, or nonshared, experiences, whereas the positive emotionality of teens who enjoy strong relationships with their parents is more strongly predicted by genetics (Krueger et al., 2008). These fine-tuned analyses of the interactions of nature and nurture are likely to provide psychologists with more accurate views of the sources of personality (see Figure 12.11). Without taking environment into account, genetics explains between 46% and 52% of the variability we see in positive emotionality. However, we gain a better understanding of positive emotions when we also consider environments, such as teens’ relationships with their parents. When teens report a positive relationship with their parents, most of the variation we see in their positive emotionality is genetic, but when their relationship with their parents is poor, their positive emotionality is better predicted by their individual experiences than by their genetics. In spite of their heritability, personality traits are far from being set in stone. We observed earlier that confronting experiences such as unemployment can produce wide-ranging influences on personality. Researchers have even noted that if you want to change a personality trait (for example, by becoming more agreeable), that can actually be done (Hudson & Fraley, 2015). Not only can people change personality traits on purpose, but the change will be reflected in their relevant behaviors. Given the genetic influence on personality and the early emergence of temperament in life, it is reasonable to assume that individual differences in personality are reflected in brain structures. Early efforts to link personality and biology examined the correlations between extroversion and neuroticism traits and the activities of the autonomic nervous system and reticular formation, structures involved with arousal discussed in Chapter 4 (Eysenck, 1967). Brain structure correlates in adults with four of the Big Five personality traits: Extroversion, agreeableness, neuroticism, and conscientiousness (Canli, 2009;  DeYoung et al., 2010). The differences in structure occur in logical places. For example, people who differ in neuroticism show different volumes of areas of the prefrontal cortex, anterior cingulate cortex, and hippocampus, areas that have been implicated in sensitivity to threat and stress. Variations in conscientiousness are associated with different volumes in areas of the prefrontal cortex involved with planning and the voluntary control of behavior (DeYoung et al., 2010). Measures of the Big Five traits of conscientiousness are positively correlated with the volume of areas in the prefrontal cortex involved with planning and the voluntary control of behavior. Activity in brain networks forms the basis of another personality model called reinforcement sensitivity theory (RST). Individual differences in approach, avoidance, and inhibition of behavior reflect the activity of three brain networks: the behavioral approach system (BAS), the fight–flight freeze system (FFFS), and the behavioral inhibition system (BIS) (Gray & McNaughton, 2003;  Kennis, Rademaker, & Geuze, 2013). Using functional magnetic resonance imaging (fMRI), researchers have mapped the networks in which activity successfully predicts individual differences in these three dimensions of behavior (Kennis et al., 2013). Variations in BAS activity predict a person’s response to reward, or impulsivity. The FFFS is active when an organism experiences unlearned punishment or lack of reward, while the BIS is active when an organism senses learned signals for punishment or lack of reward (Gray, 1970;  Gray, 1987). Individual differences in FFFS activity correlate with reactivity, described earlier, while BIS activity is associated with anxiety levels or neuroticism. If we accept the idea that personality characteristics are influenced by genetic factors leading to individual differences in brain structure and function, an interesting question arises: How might differences in personality have evolved? Evolutionary psychologists argue that the Big Five traits are present across a variety of cultures because they influence survival for the entire species (Buss, 1996,  1999). Identifying personality in other species would support this evolutionary approach. Ivan Petrovich Pavlov, discussed in Chapter 8, noticed that individual dogs responded differently to frustration: Some responded cheerfully, others responded with anger, some went to sleep, and still others appeared to have a nervous breakdown (Pavlov, 1906). Using a modified human personality questionnaire to evaluate animals, extroversion and neuroticism have been identified in hyenas, octopuses, guppies, rats, pigs, dogs, cats, donkeys, monkeys, gorillas, and chimpanzees (Gosling, Mollaghan, & Van Lange, 2006). Agreeableness, which might form the basis for cooperative behavior, has been observed to vary in nonhuman primates (Adams, 2011). However, the only species identified as showing variations in conscientiousness were humans and chimpanzees (Gosling et al., 2006). After noticing that staff at the Seattle Aquarium had been naming their octopuses based on their behavior (from shy Emily Dickinson to destructive Lucretia McEvil), Jennifer Mather tested 44 octopuses systematically for their responses to stimuli, such as being touched with a brush. The octopuses seemed to have unique and stable responses, similar to having a personality. Personality is assessed or tested for a variety of reasons, including the refinement of personality theories, diagnosing psychological disorders, selecting employees, or making decisions in courts of law. In everyday life, we assess other people’s personalities by listening to them and watching their behavior. Observation remains one of the most reliable means of making personality judgments about others (Leichtman, 1995; Asendorpf, Banse, & Mücke, 2002). However, observation takes precious time and can be biased, as we will discuss in Chapter 13. In addition to observation, we rely on interviews. Although a skilled interviewer might obtain reliable information, it is also easy for the person being interviewed to conceal some aspects of personality. To avoid the pitfalls of observation and interview, many psychologists turn to standardized tests of personality. In Chapter 2, we noted that good tests require validity and reliability. A valid test literally does the job it is advertised to do. For example, if you constructed a test of creativity, people with high scores on your test should demonstrate more creativity than do people with lower scores. A reliable test returns similar results when it is interpreted by different observers or is taken repeatedly. The results of most standardized personality tests rely on self-report, which introduces errors. Self-reports can be influenced by a person’s need to appear socially appropriate (Cyders & Coskunpinar, 2011). An individual applying for a job in sales is unlikely to admit to being shy. In Chapter 9, we distinguished between conscious or explicit and unconscious or implicit memories. Self-report measures do a good job of tapping into our explicit information about our personalities but may not capture implicit aspects of personality (Asendorpf et al., 2002). Furthermore, answers on a questionnaire may indicate a person’s beliefs but have relatively less to say about how a person would behave. Claiming to be honest on a questionnaire says little about whether a cashier will steal money from the cash register, as we will explore in Chapter 13. Increasingly, analysis of data from cell phones or social media provides personality psychologists with ecologically valid snapshots of real behavior (see Figure 12.12). Alternately, a growing field of personality neuroscience takes another methodological route. Answers on standardized instruments are correlated with observed differences in brain structure and activity (DeYoung et al., 2010). Personality psychologists can take advantage of data from social media platforms such as Facebook to obtain ecologically valid snapshots of real behavior. Not surprisingly, the Big Five trait of extroversion correlates with the number of friends a person has on Facebook but only for those whose numbers of friends are in the top 50% of users. Perhaps avoiding human judgment altogether might give us even more accurate information about personality. Researchers constructed a computer algorithm that assesses a person’s likes on Facebook (Youyou, Kosinski, & Stillwell, 2015). The resulting assessment of a person’s Big Five personality traits turned out to be more accurate than the assessments made by work colleagues, friends, roommates, and family members (see Figure 12.13). Only a person’s spouse could rival the computer. As entertaining as these results might be, they do raise significant privacy concerns. A computer analysis of the Facebook likes of 86,220 people provided a more accurate assessment of their personalities, as measured by responses to a 100-item online questionnaire, than judgments made by their human friends. Personality inventoriesPersonality inventoriesAn objective test, often using numbered scales or multiple choice, used to assess personality            Personality inventories        An objective test, often using numbered scales or multiple choice, used to assess personality             are objective tests, often using numbered scales or multiple choice, such as the short Big Five test that you took earlier in the chapter. These tests have advantages over more subjective measures, such as interviews or observation. Large numbers of people can be assessed quickly and inexpensively. Many of the tests perform quite well in terms of reliability and validity (McCrae, Kurtz, Yamagata, & Terracciano, 2011)We can learn quite a bit about what kind of person we have just met through an interview, but it is also relatively easy for people being interviewed to present an inaccurate view of their personality. A frequently used personality inventory is the Minnesota Multiphasic Personality Inventory (MMPI), published in the early 1940s and most recently revised in 2001. This test compares the responses of typical people with those hospitalized for a psychological disorder to a number of true–false items (see Figure 12.14;  Hathaway & McKinley, 1940,  1943). For example, responses to the statement I usually feel that life is worthwhile and interesting might differentiate between people who either are or are not depressed. People with schizophrenia, but not others, are likely to agree with the statement I seem to hear things that other people can’t hear. Although the MMPI was designed to assist with clinical diagnosis, this is only one of its present-day uses. The publisher’s website suggests that the test can also be used for college and career counseling recommendations, screening of potential public safety employees, marriage and family counseling, and many other purposes (Pearson Assessments, 2005). The MMPI compares a participant’s responses along several dimensions to people with diagnosed psychological conditions. Scores above 65 are cause for concern. In this case, the individual scored normally with the exception of his score on scale 4, which is the psychopathic deviate scale. This scale measures conflict, struggle, anger, and respect for society’s rules. This person might be diagnosed with antisocial personality disorder, which we discuss in Chapter 14. The MMPI has been joined by hundreds of additional objective tests featuring personality characteristics, including the NEO-PI-R, which assesses the Big Five traits. Most of these tests use response scales rather than the true–false options of the MMPI. These numbered scales are often called Likert scales, after Rensis  Likert (1932). For example, you might be asked to select the number that best describes you: Outgoing 7 6 5 4 3 2 1 WithdrawnProjective testsProjective testsA test of personality based on Freudian theory that provides an ambiguous stimulus onto which test takers project their personality.            Projective tests        A test of personality based on Freudian theory that provides an ambiguous stimulus onto which test takers project their personality.             are derived directly from projection, one of Freud’s defense mechanisms. According to the psychodynamic approach, tests can access the unconscious mind by displaying an ambiguous stimulus, onto which the test participants project their personalities as they describe the object. A famous projective test, the Rorschach Inkblot Test, contains 10 different inkblots, some in color and others in black and white (Rorschach, 1921). The job of the participants is to describe what each inkblot looks like to them. These responses are written down by the examiner and scored according to a manual. For example, the examiner might note that a participant talked about a portion of the inkblot instead of the entire image. This response would be scored as indicating the participant’s concrete, rather than abstract, way of thinking. Despite serious questions about its reliability and validity (Wood et al., 2010;  Wood, Nezworski, & Lilienfeld, 2003), the Rorschach Inkblot Test remains more widely used than the MMPI. Out of a randomly selected group of 1,000 therapist members of the American Psychological Association, 82% reported using the Rorschach at least occasionally, and 43% use it frequently or always (Watkins, Campbell, & Nieberding, 1995). Swiss psychologist Hermann Rorschach (1884–1922), the son of an artist, was nicknamed Klecks, or inkblot, by his high school classmates because of his hobby of making inkblot pictures. After becoming a psychiatrist, he became curious about why people saw the inkblots differently, and he began analyzing their responses. Despite poor validity and reliability, the Rorschach Inkblot Test continues to be used widely. One of the most widely used personality tests is the Myers–Briggs Type Indicator (MBTI), which is based on the work of Carl Jung. More than 2 million Americans take the MBTI annually, and most of the top employers in the United States use it for employment purposes (Paul, 2010). After answering several forced-choice questions, the MBTI sorts you into 1 of 16 possible personality types combining four dimensions: Extroversion–introversion, sensing–intuition, thinking–feeling, and judging–perceiving. In our discussion of personality testing, we noted that validity and reliability were essential features of any good test. How does the MBTI hold up on these criteria? Unfortunately, the answer is not well (Druckman & Bjork, 1991). Validity describes how well a test measures what it is supposed to measure. Two approaches to evaluating the validity of the MBTI are using factor analysis and assessing the predictive power of the MBTI (Pittenger, 1993). Factor analysis allows researchers to explore correlations among items on a personality test such as the MBTI. If the MBTI is a valid model, the factor analysis should identify the four clusters, answers to the questions associated with each cluster should be highly correlated with each other, and the questions that fit in a cluster statistically should make sense. In other words, an item stating, I like to be the life of the party should be found in the extroversion– introversion cluster (Pittenger, 1993). Items in the MBTI do not meet these validity criteria well. What about the predictive power of the MBTI? One of the major uses of the test is in career counseling and employment. Unfortunately, there is no evidence that certain MBTI types are more successful than others in an occupation, that certain types are more satisfied with their occupations, or that certain types remain in their occupation longer (Pittenger, 1993). What about the reliability of the MBTI? If your personality as measured by the MBTI is stable, we would expect little variation as you take the test multiple times. However, even with test–retest intervals as short as 5 weeks, as many as 50% of all test takers will be classified into different types on the second try (Pittenger, 1993). This leaves us wondering why tests that do not meet the standard criteria for validity and reliability are still used. Why is the Rorschach Inkblot Test still admissible to American courts of law? Why do employers invest in the MBTI for their employees? One reason is our love of insight, which drives otherwise reasonable people to consult horoscopes, tarot cards, palm readers, and crystal balls. Upon receiving a description of their MBTI profile, many people have an aha moment, but the same could be said of horoscopes. In fairness, however, tests like the MBTI are so well marketed and used by so many reputable institutions that it is natural for the public to feel a level of trust (see Figure 12.15). Not only is the Myers–Briggs Type Indicator questionable in terms of validity and reliability, but this real-world application of the test to an automobile customer’s choice for a hands-free, foot-activated liftgate in her car is not in the best scientific tradition. As a fun example of how this works, let’s analyze Carrie—she’s an ESFP and the target customer for the Ford Escape compact utility vehicleOutgoing and socialExtraversionSpacious seating for five friendsShe lives in the moment and travels to experience lifeSensingHer powerful EcoBoost engine delivers the fuel efficiency to feed her curiousityStays true to her values, beliefs and idealsFeelingIntelligent 4WD system helps the vehicle stay groundedI know marriage and family is in my future. I crave change!PerceivingClass-exclusive hands-free, foot-activated liftgate is perfect for her evolving lifestyleIn Chapter 10, we discussed how decision-making depended on a level of understanding of probability theory that might elude even college graduates, and a similar argument can be made about personality tests. Understanding their strengths and weaknesses requires a sophisticated mastery of statistics that most people simply do not possess. However, what we all can do is use our best critical thinking skills and ask meaningful questions about the validity and reliability of the tests we are asked to take rather than passively assuming the tests are what they’re advertised to be. Personality tests are used by therapists, potential employers, judges, attorneys, educators, and others, raising important ethical concerns. Who gets to see these data? How are they stored? For what purposes may they be used? Is the examiner trained in the administration and evaluation of tests? In many cases, such as in the use of the MMPI, sensitive and private data are generated. In other cases, like the Rorschach Inkblot Test, the results are at best controversial. This widely used test has described normal children as having problems approaching psychosis (Garb, Wood, Lilienfeld, & Nezworski, 2005;  Hamel, Shaffer, & Erdberg, 2000). Using invalid results to make important real-world decisions could be disastrous. The American Psychological Association has wrestled with these questions, incorporating some conclusions regarding the disclosure of test data into the organization’s code of conduct (American Psychological Association, 1996/2007). Should you be asked to take a personality test, it would be prudent to fully educate yourself regarding the test and make an informed decision about whether to proceed. TraitDescriptionOpenness to experienceArtistic, curious, imaginative, insightful, original, wide interests, unusual thought processes, intellectual interestsConscientiousnessEfficient, organized, planful, reliable, thorough, dependable, ethical, productiveExtroversionActive, assertive, energetic, outgoing, talkative, gesturally expressive, gregariousAgreeablenessAppreciative, forgiving, generous, kind, trusting, noncritical, warm, compassionate, considerate, straightforwardNeuroticismAnxious, self-pitying, tense, emotionally unstable, impulsive, vulnerable, touchy, prone to worryOne of the most fundamental psychological distinctions we make is the distinction between our self and others. William James (1890, p. 330) argued in favor of the belief in a distinct principle of selfhood and believed that such a principle could be studied scientifically. Knowing something about the personalities of others makes it easier for us to anticipate what others will feel, think, or do and, therefore, to interact more effectively with them. But what about the patterns of thought, feelings, and actions we see in ourselves? When we think about this personal knowledge, we are thinking about the selfselfPatterns of thought, feelings, and actions that we perceive in our own minds.            self        Patterns of thought, feelings, and actions that we perceive in our own minds.            . A person’s sense of self combines a number of separate factors (Kahn, 2013). Having a self means having a sense of unity, or of being a single human. The self incorporates a history or a sense of continuity. This aspect of self helps us organize our personal knowledge on a timeline, providing us with a past, present, and future. A sense of embodiment means that the self is embedded in the body (as opposed to the sensation of having an out-of-body experience, discussed in Chapter 6). A self has free will, or the feeling we can do whatever we want to do, and self-location, which means we understand our current location. The self uses a first-person perspective. Finally, the self is embedded in a social network and is influenced by the quality of those relationships. Psychologists approach the study of the self by examining the ABCs: Affect, behavior, and cognition. The affective or emotional components of the self include our evaluation of the self or level of self-esteem, discussed in greater detail later in this chapter. Behavior and self-concept form reciprocal relationships. We gain self-knowledge by observing our behavior, and our concepts of the type of people we are influence our behavior. Cognitive processes help us organize information and think about the self. For the moment, set aside your reading and make a list of 10 things that complete the sentence I am. If you are like most college students, this description of your self-conceptself-conceptPeople’s description of their own characteristics.            self-concept        People’s description of their own characteristics.             includes references to demographic factors, such as age, sex, or student status (first year or psychology major); personality features (outgoing or shy); relationships with others (friend, lover, daughter, son, sister, or brother); physical attributes (pretty, tall, or brown eyed); and various other roles (athlete, club member, or Texan). Our self-concept captures the way we see ourselves, which we often portray through our choice of social networking profile pictures. Here are your authors’ current profile pictures on Facebook. What do your profile pictures say to others about you? We use self-schemasself-schemasA cognitive organization that helps us think about the self and process self-relevant information            self-schemas        A cognitive organization that helps us think about the self and process self-relevant information             to think about these many aspects of the self (Markus, 1977). As we discussed in Chapters 10 and 11, schemas provide cognitive structures for organizing information. The structure provided by a self-schema influences not only the way you view yourself but also how you view and interact with others. A major implication of having a self-schema is that we process self-relevant information differently than information about others. For example, we might think about our own brown hair in different ways than we think about our neighbor’s brown hair. This self-reference effect (SRE) was demonstrated when four groups of participants were given lists of adjectives to memorize with different instructions (Rogers, Kuiper, & Kirker, 1977). The first group judged the size of letters making up the word, the second group judged whether the word rhymed with another word, the third group judged whether the word had the same meaning as another word, and the last group judged whether the word described them. The fourth group’s recall for the words was subsequently superior to recall by the other groups (see Figure 12.16). These results implied that the self provided a superordinate schema (Rogers et al., 1977, p. 685). Participants in this experiment were asked whether a word was in larger font (structural), rhymed with another word (phonemic), meant the same thing as another word (semantic), or described the participant (self-reference). As you can see from these results, when participants used their self-schemas to consider a word, later recall for the word was better than when the word was considered structurally, phonemically, or semantically. These results suggest that your memory for material you are studying will be improved by thinking about how it relates to your own life. Questions remain, however, as to the true cause of the observed SRE. Is there something special about self-schemas, or is the SRE simply a result of general memory processes such as depth of processing or elaboration, which we discussed in Chapter 9? For example, memory enhancement also occurred in a group that judged whether words described the study participants’ mothers (Bower & Gilligan, 1979). We may remember self-relevant information simply because we know a lot about the subject—ourselves. When answering the questions posed in the previous section, how do you know what to say? To know yourself requires you to make the self an object to be examined. Just as you try to figure out what kind of people your classmates and instructor are, you have the capacity to turn this analysis inward to the self. Self-awareness, or knowledge of your own internal traits, feelings, roles, and memories, is the result of this self-study. In addition, self-awareness allows us to establish boundaries between the self and other people. When we talk about ourselves, look at ourselves in a mirror, watch a video of ourselves, or stand out from the crowd in some way, we become more self-aware than usual. This state of heightened self-awareness can be quite unpleasant. Many people experience a negative mood when they look in a mirror (Fejfar & Hoyle, 2000). Related to this negative self-awareness is the concept of self-consciousness, our awareness of our own characteristics and the way the self is perceived by others. In many cases, we overestimate how much attention others pay to our behavior, a phenomenon known as the spotlight effect. When student participants wearing an embarrassing T-shirt featuring 1970s pop singer Barry Manilow entered a room full of other students, they expected at least half of their fellow students to notice (Gilovich, Medvec, & Savitsky, 2000). In fact, only 23% of the students paid attention to the T-shirts. The particularly active spotlight effect that accompanies puberty is responsible for substantial pain during adolescence (Elkind, 1967;  Elkind & Bowen, 1979). Teens are convinced that their every flaw and blemish is scrutinized by others. How reassuring it would be to understand that other teens are so worried about the imaginary audience’s reactions to their own flaws that they scarcely have time to notice the flaws of others. Self-consciousness might be unpleasant, but it makes people behave more ethically. When children were instructed by a sign to take only one Halloween candy, they were more likely to comply when they could see their image in a mirror (Beaman, Klentz, Diener, & Svanum, 1979). In a similar study, an honor-system coffee bar featured either a poster showing a pair of eyes or a poster showing a bouquet of flowers (Bateson, Nettle, & Roberts, 2006). People were more likely to put money in the box to pay for their coffee when they saw the poster of the eyes (see Figure 12.17). Use of body cameras by police officers, raising their self-awareness of their actions, has decreased officers’ use of force and number of complaints (Ariel, Farrar, & Sutherland, 2015). The amount of money collected in a college department honor-system coffee bar relative to consumption was measured by tracking milk consumption (this study was conducted in the United Kingdom, where coffee is rarely consumed black). Donations increased when a poster of eyes appeared above the bar and decreased when a poster of flowers appeared above the bar. Self-consciousness, perhaps from feeling watched by the photograph of eyes, typically results in more ethical behavior. Whether we are responding similarly to the large numbers of surveillance cameras in our environments remains to be demonstrated through future research. How do we obtain self-knowledge? An obvious source of self-awareness is the process of introspection, or the direct observation of one’s own thoughts and feelings. After all, who is in a better position to know what goes on inside our heads than we are? Unfortunately, as we discussed in Chapter 1, introspection is not considered reliable. Barry Manilow might have sold millions of records in the 1970s, but students in 2000 thought wearing this shirt would be embarrassing. We tend to overestimate how much attention others pay to us, though, and only 23% of study participants noticed the T-shirt. Perhaps we could bypass mental states and focus instead on our observable behavior. People seem to use their behavior as a clue to their characteristics, just as any other observer does (Bem, 1972). If they see themselves behaving in a kindly way toward others, maybe they are kind people. In support of this approach, people who were gently encouraged to provide flattering descriptions of themselves scored higher on a subsequent test of self-esteem (Jones, Rhodewalt, Berglas, & Skelton, 1981). In other words, if they observe themselves providing a positive self-analysis, it must be true. The people around us also contribute to our self-knowledge. We use the reactions of other people like a mirror to see our own characteristics, developing a looking-glass self. We also incorporate the opinions of significant others into our self-concept. If our parents viewed us as brilliant, it is likely that we would view ourselves as brilliant too. Using the reactions of others works better when defining some characteristics than others. If our parents inaccurately view us as brilliant, we soon learn the truth at school. If they unfairly think we’re lazy, though, it is more difficult to figure out they’re wrong. An obvious component of self-knowledge arises from autobiographical and episodic memories, described in Chapter 9. Who we are today is intimately linked with the self from the past that we remember. While offering valuable information about who we are, episodic and autobiographical memories can provide a partial, incomplete, and occasionally inaccurate picture. As one psychologist noted, The past is remembered as if it were a drama in which the self was the leading player (Greenwald, 1980, p. 604). Not only do we form a self-concept, but we also judge its value. This personal report card is known as self-esteemself-esteemA judgment of the value of the self.            self-esteem        A judgment of the value of the self.            . Self-esteem may be global, based on our overall self-worth, or specific, relating to particular characteristics such as appearance or intelligence. Self-esteem introduces an emotional aspect to our self-concept. Not only do we judge ourselves, but we experience emotional responses to that judgment. Judgments of the self are not necessarily accurate. Most people experience positive illusions, which lead them to judge themselves more positively and less negatively than they judge others (Taylor & Brown, 1988). Despite inaccuracies, people who experience positive illusions enjoy a heightened sense of well-being. People with high and low self-esteem respond differently to failure. The low self-esteem person overgeneralizes from failure (Brown & Mankowski, 1993). In other words, people with low self-esteem who fail one exam might assume that they are going to fail all their classes. This tendency to overgeneralize might explain why people with low self-esteem are more prone to depression (Sowislo & Orth, 2013), discussed further in Chapter 14. In contrast, people with high self-esteem respond to failure in one domain by exaggerating their abilities in other domains (Brown & Smart, 1991). If people with high self-esteem fail an exam, they are more likely to remind others about how great they are in sports, appearance, or dating. Because of the importance of self-esteem to psychological well-being, psychologists have looked at the origins of self-esteem carefully. Many psychologists believe that the process of developing self-esteem begins early in childhood, and stable self-esteem has been observed as early as age 5 (Cvencek, Greenwald, & Meltzoff, 2016). Long before children have a working model of the self, they experience the emotional consequences of acceptance and rejection (Erikson, 1963;  Sroufe, Carlson, & Levy, 2003). As we discussed in Chapter 11, children form secure or insecure attachments with their primary caregivers (Ainsworth, Blehar, Waters, & Wall, 1978; Bowlby, 1969), which predict a child’s self-esteem at the age of 6 years (Cassidy, 1988). Self-esteem does not appear to change much over the life span (Block & Robins, 1993;  Trzesniewski, Donnellan, & Robins, 2003). Early differences in self-esteem seem to be magnified and reinforced over time because having high or low self-esteem influences both the selection of activities and the reactions of others to the self. Self-esteem is also influenced by social comparisons, or the judgments we make of our own worth relative to those around us (Festinger, 1954). The impact of social comparisons on self-esteem was demonstrated in an experiment in which researchers placed an advertisement for a research assistant (Morse & Gergen, 1970). Prospective employees found themselves in a waiting room with another candidate, who was actually an actor working for the experimenters. The actor took the role of either Mr. Clean or Mr. Dirty. Participants waiting with Mr. Clean experienced drops in self-esteem, whereas the self-esteem of participants waiting with Mr. Dirty rose. Dove, a division of Unilever that produces skin and hair care products, lists its social mission as building women’s self-esteem. The company reports that only 4% of women worldwide consider themselves beautiful. To combat the influence of upward social comparisons, in which girls and women judge their attractiveness against the impossibly beautiful images of women in the media, Dove has committed to using diverse, natural looking models in their advertisements. Psychologists have been curious about the effects of social media on social comparisons and self-esteem. Viewing selfies frequently on social media is negatively correlated with self-esteem (Wang, Yang, & Haigh, 2016). People with a high need for popularity responded more dramatically to viewing selfies. The same researchers reported that viewing groupies frequently was positively associated with self-esteem. People with high self-esteem tend to make downward comparisons, in which they compare themselves to those they consider less worthy on a particular dimension. For example, a beautiful woman with high self-esteem is more likely to congratulate herself on how much better looking she is than her friends but is unlikely to compare herself with Hollywood beauties. In contrast, people with low self-esteem are likely to do the opposite. A beautiful woman with low self-esteem will engage in upward comparisons, chastising herself because she doesn’t measure up to the most beautiful women in the world. Regardless of self-esteem, we all seem to engage in a reassuring type of downward comparison when viewing our own pasts (Wilson & Ross, 2000). Most of us look back in time with a sense that we have improved. Our virtues are greater today than they used to be, and our faults are less. Given the influence of social comparisons on self-esteem, we might suspect that belonging to certain social groups would affect an individual’s self-esteem. If you are a member of a privileged group in society, for example, shouldn’t you have higher self-esteem than people who have experienced prejudice and discrimination? Higher rates of depression among women, discussed further in Chapter 14, might suggest that females have lower self-esteem than males. In a very large meta-analysis representing thousands of participants, males had a very small advantage over females in self-esteem (Kling, Hyde, Showers, & Buswell, 1999). This advantage is larger in adolescence and early adulthood but dissipates with age. Similar meta-analyses of self-esteem demonstrated differences based on race and ethnicity in the United States (Erol & Orth, 2011;  Twenge & Crocker, 2002). As we noted in Chapter 11, beginning in adolescence, black Americans score higher than white Americans in measures of self-esteem. However, Americans identifying themselves as Asian or Native American scored below white Americans in measures of self-esteem. Hispanic adolescents had lower self-esteem than white teens but reported higher self-esteem at age 30 than white Americans (Erol & Orth, 2011). Cultural differences among groups might account for some of these results. As we mentioned previously, people in more collectivistic cultures might avoid standing out from the group by giving themselves highly positive ratings (see Figure 12.18). Results from tests that tap into implicit attitudes by asking participants to associate themselves with either positive words (glorious, happy, or laughter) or negative words (agony, horrible, or nasty) suggest that Asian Americans have about the same level of self-esteem as white Americans, but cultural values of modesty usually override their outward expression of that self-esteem (Greenwald & Farnham, 2000). Apparent cultural differences in self-esteem must be evaluated carefully. Using explicit measures of self-esteem, it appears that white Americans have higher self-esteem than Asian Americans. However, when implicit measures of self-esteem are used, this difference disappears, suggesting that the observed difference is probably because of cultural norms related to modesty. Culture plays an additional role in self-esteem by setting the values by which people measure themselves. Adherence to cultural values, more so than to individual values, were shown to predict self-esteem across 20 different cultural settings (Becker et al., 2014). Although individual achievement is an important part of self-esteem, finding meaningfulness through belonging to multiple groups is also a predictor of a person’s self-esteem (Jetten et al., 2015). The group membership effect on self-esteem is not simply a matter of having more friends but more likely from a sense of belonging to something larger than the self. The good news is that most of us feel rather positively about ourselves. The bad news is that we are a wee bit unrealistic. People who perform most poorly are the most likely to engage in self-enhancement by inflating their opinions of themselves. College students who performed in the bottom 12% on tests of logic, grammar, and humor guessed that they performed better than 62% of their peers (Kruger & Dunning, 1999). As student performance increased, the amount of inflation decreased. Some people begin protecting their self-esteem long before a performance takes place. In self-handicapping, people begin to build an excuse in advance, just in case they fail (Berglas & Jones, 1978). Self-handicapping strategies take many forms, including taking drugs, failing to practice or do homework, commenting on poor health or stress, or sandbagging. Sandbagging occurs when you let everyone know how bad you are at something. By lowering expectations of your performance, nobody will notice much when you fail. Although self-handicapping may protect your self-esteem, it does not make you popular with others (Rhodewalt, Sandonmatsu, Tschanz, Feick, & Waller, 1995). Another method self-enhancement is to associate with others you admire. This approach has been labeled the bask-in-reflected-glory (BIRG) effect (Cialdini et al., 1976). On the Monday following a football game, university students are more likely to wear sweatshirts with their school’s name if their team won than if it lost. We are equally quick to distance ourselves from failure. Many an athlete or performer has been surprised at how many friends they have during successful times and how quickly those friends disappear when times are tough. The bask-in-reflected-glory (BIRG) effect predicts that sales of apparel featuring a winning team are likely to be much higher than those for a losing team. Associating with those we admire can produce self-enhancement. Self-enhancement may serve to protect self-esteem, but it can have many unwanted effects. Primary among these negative side effects is being disliked by others. People who engage in self-enhancement typically make good first impressions, but these impressions do not last (Paulhus, 1998). Having high self-esteem provides several benefits both to the individual and to the groups to which the individual with high self-esteem belongs. Among the advantages of high self-esteem for individuals are happiness and persistence (Baumeister, Campbell, Krueger, & Vohs, 2003). These outcomes occur because high self-esteem acts as a buffer. When faced with bad news, people with high self-esteem are more likely to weather the storm than are people with low self-esteem. In addition, high self-esteem enhances social dominance within a peer group (Juvonen, 2005). Self-esteem contributes to belongingness or the maintenance of good social relationships (Baumeister & Leary, 1995;  Leary, 2004;  Leary & Downs, 1995;  Leary, Tambor, Terdal, & Downs, 1995). Because social rejection could have such devastating effects on survival in our evolutionary past, self-esteem could have emerged as a way of estimating the likelihood of rejection. Self-esteem drops following the experience of social exclusion (Leary et al., 1995). A drop in self-esteem serves as an early warning signal that rejection by the group is imminent, and behavior designed to regain the favor of the group should be initiated. Terror management theory suggests that self-esteem serves as a protection against our fear of death. When people are reminded of their own mortality, self-esteem can increase. Ironically, these labels proposed by the government for cigarette packages might have the unintended consequences of enhancing, rather than diminishing, favorable attitudes about smoking. Some smokers equate smoking with improving their image, which in turn improves their self-esteem (Hansen, Winzeler, & Topolinski, 2009). A terror management theory suggests that self-esteem interacts with a cultural worldview to buffer our fear of death (Greenberg, Pyszczynski, & Solomon, 1986;  Pyszczynski, Greenberg, & Solomon, 1991). Cultural worldviews can include shared beliefs that allow us to attain feelings of immortality, whether through producing children, writing books, or believing in an afterlife. By conforming to these cultural worldviews, people enjoy enhanced self-esteem that reduces their anxiety related to death. Having members with high self-esteem is an advantage to groups in several respects. People with high self-esteem are more likely to speak critically when in groups (LePine & Van Dyne, 1998). Groups that do not have members who are willing to question group decisions might fall prey to poor decision-making, discussed further in Chapter 13. People with high self-esteem not only value themselves but also value the groups to which they belong (Aberson, Healy, & Robero, 2000). People with low self-esteem do not show this favoritism. If a group is highly valued by its members, its overall success is likely to be enhanced, which in turn benefits its members. In 1989, the california task force on self-esteem and personal and social responsibility published a report that declared many, if not most, of the major problems plaguing society have roots in the low self-esteem of many of the people who make up society (Mecca, Smelser, & Vasconcellos, 1989, p. 1). Teachers, parents, and therapists express similar beliefs and actively work to increase the self-esteem of others. These approaches mirror a commonsense attitude toward self-esteem. High self-esteem seems more desirable than low self-esteem, and interventions designed to raise self-esteem appear on the surface to be a good thing. How accurate are these views of self-esteem? Self-esteem and school grades are positively correlated, although the effects are not large (Hansford & Hattie, 1982). The important question, however, is whether high self-esteem is a result or a cause of good grades. As we discussed in Chapter 2, it is not possible to use correlational data to determine causal factors. Consider the example of students who achieve very high grades. Does the students’ success reflect the impact of high self-esteem, or does the achievement of high grades make the students feel more positively about themselves? Or like many correlations, is an unidentified third factor such as intelligence or social class responsible for the observed correlation? Although many people believe that raising children’s self-esteem is a good thing, research suggests that self-esteem is more likely to be the outcome rather than the cause of behavior. Providing feel-good messages that are not based in real achievement can backfire (Forsyth & Kerr, 1999). Explicit efforts to identify the causal relationships between self-esteem and school performance have failed to support the commonsense hypothesis that high self-esteem produces good performance. A sample of 1,600 young males in the 10th grade was followed for an additional 8 years (Bachman & O’Malley, 1977). No evidence for a causal role for self-esteem in school performance was discovered. Instead, third factors, including socioeconomic status, IQ, and early school performance, appeared responsible for the correlation between self-esteem and school performance. Among studies that have reported a causal relationship between self-esteem and school performance, the direction of the relationship is not consistent with the commonsense hypothesis. High self-esteem is more likely to be a result of good grades, not vice versa (Rosenberg, Schooler, & Schoenbach, 1989). What do these findings mean for efforts such as the California Task Force, which sought to raise achievement by improving self-esteem? Unfortunately, interventions intended to boost self-esteem can backfire. When students receiving poor grades on a midterm were sent an e-mail message designed to boost their self-worth, they performed significantly worse on the next exams than did students in control groups (Forsyth & Kerr, 1999). This study provides an important lesson: Providing indiscriminate feel-good messages that are not anchored in achievement results in lower achievement. If relationships between school achievement and self-esteem are weaker than many believe, what about the relationships between self-esteem and social problems such as aggression? Again, our commonsense hypotheses fail us. We have heard parents, teachers, and legislators lament that many social problems would dissipate if only we could raise self-esteem. This commonsense notion was challenged by Roy Baumeister and his colleagues, who discovered that aggressive people were more likely to have positive and frequently inflated views of their self-worth (Baumeister et al., 2003;  Baumeister, Smart, Boden, & Baumeister, 1999). Contrary to many stereotypes, bullies typically do not suffer from low self-esteem. They experience less depression, less anxiety, and less loneliness than prosocial youth and are likely to be considered by peers the coolest kids in their classes. (Juvonen, 2005). To meet the expectations we have for ourselves, we engage in self-regulation, also known as self-control or willpower. Self-regulation consists of conscious executive efforts to control our thoughts, motives, feelings, and behaviors (Carver & Scheier, 2000). Failures to regulate behavior frequently lead to some of the most perplexing problems facing society, including drug abuse, domestic violence, and binge eating. In addition, deficits in self-regulation characterize several psychological disorders we discuss in Chapter 14, including schizophrenia, attention deficit hyperactivity disorder (ADHD), and obsessive-compulsive disorder. Sometimes when we try to use our self-regulation abilities to not think of something (Whatever you do, don’t think about white bears), we end up thinking quite a bit about the object we wish to avoid. Individual differences in self-regulation, as we observed earlier in this chapter, are apparent at young ages. While a certain amount of self-regulation is related to training and supervision on the part of adults, individuals show different patterns of brain activation while facing temptation. Increased activity in the nucleus accumbens, which is implicated in processing reward and addiction, is associated with self-control failures, such as eating something we intended to pass by. Simultaneously, higher activity in the prefrontal cortex was associated with an individual’s ability to withstand temptation (Lopez, Hofmann, Wagner, Kelley, & Heatherton, 2014). In self-regulation, we make distinctions between what is good for the current self (eating a yummy treat) and what is good for the future self (having good health). We perceive the future self more like a different individual who stands in judgment of our current choices. Thus, activity in brain areas associated with self-centered (egocentric) thinking is associated with more impulsive and selfish decisions (Soutschek, Ruff, Strombach, Kalenscher, & Tobler, 2016). Self-regulation is made even more difficult because the harder we think about not doing something, the more likely we are to do it (Wegner, 1994). Try, for example, to avoid thinking about a white bear for the next 30 seconds. If you are like most people, your thoughts were suddenly dominated by images of white bears. We can explain this phenomenon as follows. When you try to control your behavior, you simultaneously activate a concern about failing. In an effort to avoid failure, you automatically pay more attention to the item or behavior to be avoided. What exactly is this white bear thing I should avoid? Because of this attention, we end up doing what we hoped to avoid doing. A widely distributed network of brain structures participates in processing the different aspects of self. Self-referential processing, or the processing of information relevant to ourselves but not others, can occur in our emotional, facial, memory, motor, social, spatial, and verbal domains. If the mind is the product of the activity of the nervous system, is it possible to locate the self within these patterns of activity? This question suggests that information about the self is processed in unique and special ways by the nervous system (Gillihan & Farah, 2005). As we have seen so far in this chapter, the concept of self can mean many things. Brain correlates have been found for knowledge of our own bodies—their configuration, appearance, and location—as well as for memories of personal experiences, interpersonal connections, and use of a first-person perspective. Given the large number of different aspects of the self, it should not be surprising that a widely distributed network of brain structures participates in the processing of the self (Cacioppo & Decety, 2011b). Activity in the default mode network or DMN (see Chapter 6) is often associated with thinking about the self (Kahn, 2013). However, during thinking about the self, activity in the DMN is joined by activity in parts of the brain that process external sources of information, such as the awareness of other people and the environment. These observations remind us of our constant use of context when we think about ourselves. An important aspect of the self is the recognition of our own faces. The right hemisphere plays a special role in distinguishing our own faces from those of other people. In one study, one hemisphere of each participant’s brain was made inactive using an anesthetic. When study participants then viewed a photograph combining one half of their face with the half-face of a famous person, they recognized themselves when the left hemisphere was anesthetized but reported seeing the famous person when the right hemisphere was anesthetized (Keenan, Nelson, O’Connor, & Pascual-Leone, 2001; see Figure 12.19). Participants in this experiment viewed blended faces made from one half of their own photograph and one half of a photograph of a famous person. When the participants’ left hemispheres (LHs) were anesthetized, leaving the right hemisphere (RH) active, they believed the photo was their own (a). When their right hemispheres were anesthetized, leaving the left hemisphere active, most believed the photo showed the famous person (b). These results suggest that for many people, the right hemisphere plays a role in recognizing our own faces. In addition to playing an important role in recognizing our own faces, the right hemisphere is necessary for the construction of stable traits and attitudes. The location of brain damage (right hemisphere only, left hemisphere only, or both hemispheres) can be correlated with observed changes in personal characteristics that usually show little change over time, such as political views and religious beliefs (Miller et al., 2001). Only people with right hemisphere damage alone showed changes in these normally stable characteristics. Schizophrenia, which we discuss in Chapter 14, produces distortions in a person’s sense of self (Feinberg, 1978). One common symptom is a delusion of control or the sense that some external agent is either monitoring or influencing a person’s behavior. This delusion is highly relevant to our current discussion, because it represents confusion over the behavior produced by the self or by others. For example, people with this symptom might describe their behavior as controlled by others through implants in their brains. A comparison of brain activity during such delusions and after they recede indicated that the delusions of control were correlated with excess activity in the right hemisphere (Spence et al., 1997). The self we have discussed so far represents the personal self or the attributes that differentiate an individual’s self from all others (Brewer & Gardner, 1996). This self both shapes and is shaped by interactions with others. Just who those others are and how they interact with the self has been the subject of significant psychological inquiry. When we interact with significant others, such as family members, we experience the relational self. When we interact with larger groups, such as other fans supporting the same sports team, such as these Brazilian soccer fans, we experience the collective self. William  James (1890) remarked that an individual has as many different social selves as there are distinct groups of persons about whose opinion he cares (p. 294). In other words, in addition to having a personal self, we experience an interpersonal selfinterpersonal selfThe self we are in the presence of other people            interpersonal self        The self we are in the presence of other people            . In current psychological discussions of the self, influences on the interpersonal self fall into two categories: A person’s significant others and the social groups to which the person belongs. Significant others include family members, friends, coworkers, and others with whom the self interacts or has interacted. As we interact with significant others, we experience the relational self (Brewer & Gardner, 1996;  Chen, Boucher, & Tapias, 2006). The influence of the larger groups to which we belong provides our collective self (Brewer & Gardner, 1996). We have seen how cultural values can influence the experience of self. Regardless of culture, however, all humans share both a need for independence and a need to belong. Is it possible to show that situations can modify a person’s emphasis on individualistic and collectivistic values? To investigate the effects of the situation on cultural values, thinking about independence versus interdependence was manipulated by reading a story about a general choosing a warrior to send to the king (Gardner, Gabriel, & Lee, 1999). The stories differed in the warrior chosen (the best individual for the job versus a member of the family) and in the benefits of the choice (benefits to the general versus benefits to his family). A control group did not hear a story. The effects of the two stories were assessed by asking study participants to complete a values inventory that measures how obligated a person feels to help others. Participants hearing the independence version of the warrior story, regardless of culture, produced more individualistic responses to the values survey than collectivistic responses. Participants hearing the interdependence version of the warrior story, again regardless of culture, produced more collectivistic responses to the values survey than individualistic responses. Although we tend to think of culture influencing the self, this experiment demonstrated that views of the self as independent or interdependent can be changed by a person’s immediate situation (such as hearing a story that emphasizes one aspect or the other). This result supports the ideas that both the need for independence and the need to belong are human universals and that the relative dominance of one over the other is modifiable through experience and situational variables (see Figure 12.20). Listening to a story that emphasized individualistic or collectivistic values elicited congruent responses from study participants, regardless of whether they lived in an individualistic culture (United States) or a collectivistic culture (Hong Kong). These results suggest that both independent and interdependent views of the self are human universals, and the relative dominance of one over the other is modifiable by a person’s experience and situation. The concept of the interpersonal self implies that an individual’s self is more flexible and complex than it would be based on a personal self alone. This flexibility is illustrated by the suggestion that we have a relational self when we are with a particular person, such as the me when I’m with mom (Chen et al., 2006, p. 153). It is likely that a great deal of overlap occurs among the many interpersonal selves we experience, but recognizing that we may not be consistent across all social situations is important. The differences we experience echo the discussion earlier in the chapter about personality traits and situations. Given the interpersonal dimensions of the self described in the previous section, it should come as no surprise that the concept of the self means different things in different cultures. One aspect of cultural differences that is particularly relevant to the study of the self is the distinction between individualism and collectivism (see Figure 12.21). Individualism is characterized by an emphasis on independence and self-reliance. In contrast, core values of collectivism include interdependence, cooperation, and lack of conflict. As we discussed in the introduction to this chapter, these relative weightings of self and others tend to characterize entire nations. The United States, Australia, Great Britain, Canada, and the Netherlands appear to be the most individualistic, while Venezuela, Colombia, Pakistan, Peru, Taiwan, and China appear to be the most collectivistic (Hofstede, 1980). Geert Hofstede compared cultures on five dimensions relevant to individualism and collectivism. Compared to the Chinese, Americans are less tolerant of unequal distributions of power (PDI), more likely to expect self-reliance (IDV), about equally assertive and competitive rather than modest and caring (MAS), more tolerant of ambiguity and lack of structure (UAI), and far less thrifty and persistent (LTO). PDI: Less powerful accept and expect that power is distributed unequally. IDV: Refers to individuals or groups, not the state, and their role in taking care of you. MAS: Higher numbers indicate more assertive and competitive, rather than modest and caring. UAI: Tolerance for ambiguity, unstructured situations. LTO: Thrift, perseveranceTwo major differences occur between highly individualistic cultures and more collectivistic cultures (Markus & Kitayama, 1991). The first difference predicts a person’s source of satisfaction. People in individualistic cultures feel best about themselves when they experience personal achievement, whereas people in collectivistic cultures feel good about belonging to a successful group. For example, a baseball player from an individualistic culture, such as the United States, might take pride in having a terrific batting average even though his team is losing, whereas the player from a more collectivistic culture, such as Japan, might feel better about being a member of the top team regardless of his personal performance. The second difference predicts a person’s sense of being similar to other group members. In individualistic cultures, people are more likely to see themselves as unique. In collectivistic cultures, conformity and appearing like others are more highly valued. White American and Korean study participants were offered the gift of a pen, which they were to choose from a group of three or four pens of the same color and one pen with a unique color. Whereas 74% of the white Americans chose the uniquely colored pen, 76% of the Koreans chose a pen with the more common color (Kim & Markus, 1999). The individualistic–collectivistic nature of a culture is a moving target across time (Hamamura, 2012), and our characterization of particular social groups requires constant updating. For example, a recent meta-analysis of individualism and collectivism among European Americans, African Americans, Latino Americans, and Asian Americans suggest that although minor differences remain, values related to individualism and collectivism are becoming more similar across these groups (Vargas & Kemmelmeier, 2013). Personality provides a perfect opportunity to revisit the birds of a feather flock together theme that we discussed previously in this textbook. Is it true that people with similar personalities are attracted to each other? In Chapter 11, we discussed research that showed that people in long-term relationships who have similar personalities experienced different levels of satisfaction at different stages in their relationships (Shiota & Levenson, 2007). Early in a relationship, similarity was particularly important to promoting bonding and intimacy. But what happens when we take one more step backward in time to look at the effects of similarity when two people first meet each other? In one study, pairs of heterosexual, opposite-sex college students were left in a waiting room while a researcher ran an errand (Cuperman & Ickes, 2009). Without their knowledge, the pairs’ interactions were filmed. As predicted by birds of a feather, pairs of introverts and pairs of extroverts reported having a better conversation and liking their partner more (see Figure 12.22). However, similarity in agreeableness was not so important. Leaving two disagreeable people together did not result in a pleasant conversation. Surprisingly, though, having at least one agreeable person produced the same quality of interaction as having two agreeable people. Apparently, agreeable people know how to produce a good interaction whether or not their partner is equally capable. Introverts conversing with other introverts and extroverts conversing with other extroverts were more likely to believe their partner would want to interact in the future than pairs in which one person was introverted and the other was extroverted. When agreeableness was investigated, one disagreeable person having a conversation with another disagreeable person was not very positive, but as long as one of the partners was agreeable, the conversation went well. You might be wondering how personality similarities compare to other factors when people first meet, and the answer is not well, at least in the context of a speed-dating session for college students (Luo & Zhang, 2009). Under these circumstances, the overwhelming determinant of attraction for both men and women was physical appearance. No similarity in attitudes and Big Five characteristics significantly predicted attraction. This outcome might be one of the reasons we are cautioned to get to know each other better before becoming involved in a serious relationship. If similarity plays an important role in long-term satisfaction, even with the bumps and bruises of middle age described by Shiota and  Levenson (2007), it appears that we need more time than the speed-dating situation provides to make important relationship decisions. It seems intuitive to predict that bullies have lower levels of emotional self-regulation than nonbullies, and this is in fact what the results of scientific studies demonstrate (Garner & Hinton, 2010). Individuals with more self-control in regard to their anger and frustration are less likely to lash out and dominate others. However, as many people have learned the hard way, it’s easier to be impulsively and very publicly aggressive online than in person. Thus, cyberbullying might present unique challenges for emotional self-control compared to face-to-face bullying. Middle school bullies, regardless of whether they engage their victims in person or through digital technology, are able to perceive the emotional responses of others (Baroncelli & Ciucci, 2014). If anything, launching a successful attack demands that the bully have the ability to anticipate the victims’ responses. We can safely say that the bullies are not wreaking havoc by mistake—they understand the pain they are inflicting. What they seem to lack is the ability to inhibit their behaviors using emotional self-control. A difference between types of bullies appears, however, when you ask bullies to rate their own emotional regulation abilities. Face-toface or traditional bullies did not perceive themselves as having a problem with selfregulation, but cyberbullies gave themselves (perhaps more accurately) lower grades on emotional self-regulation (Baroncelli & Ciucci, 2014). This increased awareness of their own deficits might lead the would-be bully to the safer method of cyberbullying. Traditional (face-to-face) bullies differ from cyberbullies in that the former do not perceive themselves as having problems with emotional self-regulation. In contrast, cyberbullies rate themselves as having lower emotional self-regulation. It is possible that this leads them to participate in the safer method of cyberbullying rather than traditional bullying. While we have seen in previous chapters that the same individuals often participate in both traditional bullying and cyberbullying, recognizing differences in the patterns of behavior between these two domains could better inform intervention programs and policies. Addressing the low levels of emotional self-regulation found in both types of bullies is important, but self-efficacy in controlling one’s emotions seems particularly relevant to cyberbullying. Be sure you can define these terms and use them correctly.`;\n","export const text = `Genes on chromosome 6 not only influence a person’s immune system, but also determine the nature of a detectable body odor that affects our choice of a partner. Much of our Success as a Species can be attributed to our social nature. But as we all know from personal experience, the social world can be challenging; consider, if you can, some truly embarrassing memories about social mistakes that you’ve made. Our need to understand our complex social environment was one of the driving forces that shaped the development of our large human brains (Cacioppo & Decety, 2011). Among the social environments with the largest minefields is the connections that we make with others in intimate, romantic relationships. The two people in the photograph at left appear to find each other attractive, and they are engaged in behaviors—both variable and choreographed—designed to further their relationship. Looking at the developing relationship, psychologists can see a number of processes at work, such as first impressions and the effects of perceived similarities that are discussed in detail in this chapter. Zooming in, however, we see a different set of processes at work. Although most of us believe that we choose romantic partners on the basis of perceived attractiveness and compatibility, there is some chemistry at work here too. Overall, humans share more genes in common with spouses (Domingue, Fletcher, Conley, & Boardman, 2014) and friends (Christakis & Fowler, 2014) than they do with strangers. In some cases, however, being genetically different from your partner is desirable. The highlighted genes in the larger image help determine a person’s immune system response. Furthermore, variations of these genes, or alleles, are associated with different detectable body odors (Wedekind & Füri, 1997). As attractive and compatible as these individuals seem to each other, one might reject the other in response to this odor. Our evolutionary history has left us with the ability to detect a genetic compatibility in a partner, as well as more surface compatibilities. Individual humans are not that impressive, but cooperating in groups makes our species more capable. Zooming out from our couple, we see that their romance takes place within a larger social context. What will their families and friends think of their relationship? How do cultural attitudes and biases shape the likelihood of their success? To what extent will they feel comfortable conforming to these cultural expectations or rejecting them? These external factors can help determine the course of their relationship. Humans depend on one another. We sit in houses and offices built by contractors and laborers, work on computers designed and built by armies of engineers and technicians, eat food grown and transported by others, wear clothes made by people living halfway around the globe, and look forward to the next text message from a friend. As far back as the historical record goes, we have evidence that humans lived in groups. What advantages might cooperative living have provided to our early ancestors? As predators or fighters, individual humans have rather modest equipment relative to other species. Our teeth and nails are not particularly frightening, and we are frequently outmatched in speed and strength. It is our human abilities to think and to use tools, to employ and detect deceit, and to communicate, work together, and form alliances that make us such a formidable species. Early evolutionary pressures favored humans who chose to affiliate or associate with others, and by doing so, sculpted much of the social behavior that we observe today (Cacioppo, Cacioppo,  Cole et al., 2015;  Dunbar, 2011). Social interaction is not just a helpful option but is also a basic human need, which can be more powerful than other basic needs such as hunger, which is addressed in Chapter 7 (Cacioppo, Cacioppo, & Boomsma, 2014). A mother’s ability to comfort her infant through touch has a greater impact on the infant’s attachment to her than her ability to provide food (Harlow & Harlow, 1973). Study participants expecting a painful electric shock prefer the company of strangers to facing their fears alone (Schachter, 1959). We depend on today’s mobile technologies that allow us to carry our social networks wherever we go (Finkel, Eastwick, Karney, Reis, & Sprecher, 2012;  Przybylski & Weinstein, 2013). We not only need social contact, but also react negatively when social support is withdrawn (see Figure 13.1). Perceived social isolation shares similar risks of death with obesity and a sedentary lifestyle and is positively correlated with risk of suicide (Cacioppo, Cacioppo, Capitanio, & Cole, 2015;  Holt-Lunstad, Smith, Baker, Harris, & Stephenson, 2015). Disruptions of social connections through divorce, bereavement, separation, or ridicule are known to be among the most stressful experiences people may face (Williams, 2007). Perceived isolation, or loneliness, can initiate a cascade of negative interactions that further separate individuals from the social contact they need and want, interfere with the quality of sleep, increase feelings of negative mood and hostility, and lead to a heightened sensitivity to threat and attack (Cacioppo et al., 2015).  Cacioppo, Hawkley, & Thisted, 2010). The isolated person begins to behave in ways that are self-protective but that paradoxically push people away and promote further rejection and isolation. As a result, we are highly motivated to stay in the good graces of our social groups. In the remainder of this chapter, we will see how this need to maintain our social connectivity shapes many types of behavior, from the attitudes that we hold about brands of cereal to our responses as members of teams and committees. Psychologists have many ways to make study participants feel socially excluded for short periods so that they can evaluate the effects that social exclusion has on everything from brain activity to sharing cookies. Many experiments use this Cyberball game, developed by Kip Williams of Purdue University. The participant, represented by the little hand in the foreground, is initially included in the game of catch with the other two players, who are just part of a computer program. Midway through the game, the other players throw the ball between themselves, excluding the participant, whose little hand is left with nothing to do.1. How often do you feel that you are in tune with the people around you? Never1Rarely2Sometimes3Always42. How often do you feel that you lack companionship? Never1Rarely2Sometimes3Always43. How often do you feel that there is no one you can turn to? Never1Rarely2Sometimes3Always44. How often do you feel alone? Never1Rarely2Sometimes3Always45. How often do you feel part of a group of friends? Never1Rarely2Sometimes3Always46. How often do you feel that you have a lot in common with the people around you? Never1Rarely2Sometimes3Always47. How often do you feel that you are no longer close to anyone? Never1Rarely2Sometimes3Always48. How often do you feel that your interests and ideas are not shared by those around you? Never1Rarely2Sometimes3Always49. How often do you feel outgoing and friendly? Never1Rarely2Sometimes3Always410. How often do you feel close to people? Never1Rarely2Sometimes3Always411. How often do you feel left out? Never1Rarely2Sometimes3Always412. How often do you feel that your relationships with others are not meaningful? Never1Rarely2Sometimes3Always413. How often do you feel that no one really knows you well? Never1Rarely2Sometimes3Always414. How often do you feel isolated from others? Never1Rarely2Sometimes3Always415. How often do you feel that you can find companionship when you want it? Never1Rarely2Sometimes3Always416. How often do you feel that there are people who really understand you? Never1Rarely2Sometimes3Always417. How often do you feel shy? Never1Rarely2Sometimes3Always418. How often do you feel that people are around you, but not with you? Never1Rarely2Sometimes3Always419. How often do you feel that there are people you can talk to? Never1Rarely2Sometimes3Always420. How often do you feel that there are people you can turn to? Never1Rarely2Sometimes3Always4Type of ErrorDefinitionExampleCorrespondence bias Overestimating the dispositional causes of a person’s behaviorMy friend flunked her exam because she’s not very bright. Self-serving bias Attributing personal success to dispositional causes, while attributing personal failure to situational causesI got an A on one test because I’m smart, but I flunked the other test because it was unfair. Group-serving biasAttributing group success to dispositional causes, while attributing group failures to situational causesOur team won its first game because we’re talented and skilled but lost the second because the referees were biased. Just-world belief Assuming that people get what they deserveThat homeless person doesn’t try hard enough to get off the streets. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. The two on the left won; the two on the right lost. Our success in the social world begins with our ability to perceive the characteristics and intentions of others. What was your immediate reaction to your psychology professor on your first day of class? If you’re like most people, you came to some quick conclusions that are likely to last a long time. We form rapid and enduring first impressions of other people. When we say first impressions are formed rapidly, we are not talking about months, weeks, days, or hours, but in some cases, less than a minute. In a meta-analysis of research using thin slices of behavior (i.e., exposure lasting less than 30 seconds), initial impressions did not change much after longer periods of exposure (Ambady & Rosenthal, 1993). Speed-dating provides an interesting environment for observing the accuracy of these thin-slice impressions. Researchers asked men and women to participate in 3-minute social exchanges, followed by their rating each potential partner as date, friend, or no contact (Wilson, Cousins, & Fink, 2006). After only 3 minutes of observation, people accurately predicted their compatibility with a potential partner as measured by a reliable instrument called the compatibility quotient (CQ). If you have experienced pangs of loneliness during your time at college, you are not alone. University students are especially susceptible to loneliness (Cutrona, 1982). Why do we feel loneliness? It might have evolved to provide our ancestors with an early warning system that their social connectivity needed repair (Cacioppo, Cacioppo, Cole, & Capitano, 2015). The UCLA Loneliness Scale (Russell, 1996) is a standardized instrument that assesses a person’s subjective self-report of loneliness. The scale, found in Table 13.1, includes 20 items; after you complete them, we will tell you how to score and interpret your results. Adding up your scores: To obtain your final score, add up your responses to all the nonasterisked items. Then add up all the asterisked items and subtract this total from 45. You will notice that these items are reversals, in which the nonlonely answers are the higher numbers. Questionnaires use this strategy to keep someone from just picking the same number all the way through an instrument. Add the two sums together to find your total score, which can range from 20 to 80. Interpreting your score: Obviously, there are no right or wrong answers on an instrument like this, but higher numbers indicate higher levels of subjective loneliness. For comparison’s sake,  Russell (1996) administered this instrument to college students, nurses, and the elderly. The nurses had the highest scores (mean = 40.14), followed by college students (40.08) and older adults (31.51). It might surprise you that the college students felt more loneliness than the older adults! This is probably because loneliness results from one’s perceived disconnection, not absolute number of relationships. It is possible that college students have higher expectations for connectivity or that the elderly have had more time to establish meaningful relationships with others. If your score seems on the high side, don’t despair. Not only do psychological scientists understand quite a bit about loneliness, but they also have some empirically supported suggestions for alleviating it. One key to reducing feelings of loneliness is to understand how lonely people behave. Loneliness is associated with a number of maladaptive social cognitions, including increased sensitivity to social threats, better memory for negative social information, more negative social expectations, and behavior that confirms negative expectations. In other words, lonely people act in counterproductive ways that further drive them away from the social connectivity they seek (Cacioppo & Hawkley, 2009;  Cacioppo, Norris, Decety, Monteleone, & Nusbaum, 2009). Interventions designed to reverse those maladaptive cognitions were found to reduce loneliness better than other strategies in a large meta-analysis (Masi, Chen, Hawkley, & Cacioppo, 2011). We make quick assessments of others by focusing on a subset of traits and behaviors, rather than trying to sum up an entire person. In particular, people are likely to use faces to form impressions. Halo effects occur when one characteristic or a small number of characteristics have a large impact on overall perception. If people see characteristics that they value or dislike, they tend to make simple thumbs-up or thumbs-down assessments. One person might be attracted to blue eyes, and another might be turned off by tattoos. In one study, students hearing a professor described as warm instead of cold were willing to say after a brief encounter that he was a more effective teacher, less ruthless, and more humorous (Widmeyer & Loy, 1988). Speed dating, in which people have a limited time to decide whether they would like further contact with another person, takes advantage of the rapid assessments of one another that we make. After only 30 seconds of contact, people make enduring evaluations of other people. First impressions are not only fast, but also persistent. Once we have reached a conclusion about a person, even after a brief social exchange in a speed-dating experiment, we tend to stick with that assessment for a long time. Most of us have formed a false first impression of others (either overly positive or overly negative) that breaks down only after extensive further exposure, if at all. However, in the face of certain types of new information, first impressions do change (Brannon & Gawronski, 2016;  Mann & Ferguson, 2017). It would not be very adaptive to hold onto a false impression indefinitely, so it makes sense to think that we also have the ability to update our beliefs. Although the tendency to make first impressions may be characteristic of all humans, our abilities to shape accurate judgments are also influenced by culture. As shown in Figure 13.2, Japanese and American participants successfully used their first impressions to predict which political candidates would win an upcoming election, but their predictions were more accurate for candidates from their own culture (Rule et al., 2010). Can you pick which of these candidates won or lost their elections?* Apparently, people can do this quite well, but they perform better within their own cultures. Based on viewing the photos of candidates’ faces, without recognizing the candidate or even knowing that the person was running for office, Americans did a good job of predicting the outcome of an election for unfamiliar American candidates for the U. S. Senate but did not do as well when predicting the outcome for Japanese candidates for the shugi-in (the Japanese equivalent of the U. S. House of Representatives). Japanese participants predicted the success of Japanese candidates, but most of the American candidates who they expected to win ended up losing.*The two on the left won; the two on the right lost. Group ProcessDefinitionExampleSocial facilitationIndividual performance is influenced by the presence of others. A jogger runs faster with a buddy than when running alone. Social loafing Working in a group decreases individual effort. A student puts less effort into a group project than into an individual term paper. Deindividuation Behavior differs when in a group, as opposed to when alone and identifiable. A normally law-abiding citizen loots a store during a riot. Group polarization Attitudes become more intense following discussion. After a late-night debate with like-minded friends, students hold more extreme views about an issue than they did before the discussion. Groupthink Flawed group decision making fails to consider dissenting views. A group of friends decides to engage in a risky behavior that seems dumb in hindsight. We not only size up other people quickly in terms of their personal characteristics, but also ask why they behave the way they do. Being able to predict the behavior of others has significant survival value. Our answers to questions about why others behave in certain ways are attributionsattributionsA judgment about the cause of a person’s behavior.            attributions        A judgment about the cause of a person’s behavior.            , or judgments about the causes of other people’s behavior. Behavior can be seen as resulting from some combination of dispositional attributiondispositional attributionA judgment assigning the cause of a person’s behavior to personal qualities or characteristics.            dispositional attribution        A judgment assigning the cause of a person’s behavior to personal qualities or characteristics.             (internal) factors and situational attributionsituational attributionA judgment assigning the cause of a person’s behavior to the environment.            situational attribution        A judgment assigning the cause of a person’s behavior to the environment.             (external) factors (Heider, 1958). For example, if your friend just flunked an important exam, you could try to answer the question Why? by considering your friend’s personal characteristics and circumstances. In considering your friend’s disposition, you might recall that she is anxious (being anxious is dispositional), which might have interfered with her performance on a stressful test. In considering your friend’s situation, you remember that she just broke up with her romantic partner of several years (breaking up is situational), which was depressing and distracting for her as she prepared for her test. Thinking that disposition and situation act in an either/or fashion instead of interacting to produce an outcome is no more helpful to our understanding of behavior than using either/or thinking in considering nature and nurture (Funder, 2009;  Johnson, 2009). However, as will be shown in the next section, the weight placed on dispositional and situational variables as explanations for behavior can vary dramatically from one case to the next. Although our attributions typically guide us in the right direction, we do make errors. We are not able to read minds, so we must make a logical leap from what we observe in a person’s behavior to the internal state of mind that produced it. However, sometimes that leap takes us in the wrong direction. When you hear about people flunking a test, you are likely to start making conclusions about their dispositions (she is a lazy student or he is not very smart), even though you are quite aware that the professor giving the test has a reputation for writing brutal exams. This willingness to infer a person’s disposition from observed behavior, even when strong situational factors are obvious, is known as the correspondence biascorrespondence biasThe tendency to view behavior as the result of disposition, even when the behavior can be explained by the situation in which it occurs            correspondence bias        The tendency to view behavior as the result of disposition, even when the behavior can be explained by the situation in which it occurs             (Gilbert & Jones, 1986;  Gilbert & Malone, 1995). In other words, we usually expect people’s behavior to correspond to their dispositions, even when we know for sure that the situation is having a powerful effect. Given the importance of making accurate attributions to our survival and social relationships, why would such a bias ever occur? Psychologists are still debating the reasons for this error. One possibility might be the genuine stability of personality traits (discussed in Chapter 12). Although traits interact with situations to produce behavior, people often believe that the You did it because that is the kind of person you are explanation is accurate. There is some truth to these beliefs, which is why they persist. But at the same time, our discussion of personality has shown that situations make important contributions to behavior. You might consider yourself to be a helpful person, but you might ignore someone with a flat tire when you’re late for class or work. Are the famous Wal-Mart greeters really friendly people? Or are they just doing their jobs? Because of the correspondence bias, we tend to view Wal-Mart greeters as friendly people, even though we fully understand that their jobs require them to act this way. Other factors that contribute to correspondence bias include a lack of awareness of the power of the situation and unrealistic beliefs about how situations should affect people (Gilbert & Malone, 1995). In many everyday instances, we simply do not have enough information about somebody’s circumstances to consider situational variables in our attributions. A grumpy, unenthusiastic professor might be having health or personal problems of which students in the classroom are unaware, but by default, the students believe that the professor is characteristically grumpy and unenthusiastic. In other cases, we seem to misjudge the likelihood that typical people will respond to a situation in a certain way. Later in this chapter, you will learn how a majority of individuals were willing to electrocute another participant simply because they were told to do so by an experimenter. You might think, I would never respond to the situation like that, which might lead to your misjudging the power of the situation. A concept closely related to the correspondence bias is known as the fundamental attribution error (FAE; Ross, 1977). According to this view, people do not underestimate situational variables as proposed by correspondence bias, but instead completely fail to consider situational variables while making attributions. By default, this total neglect of situational influences leads to a reliance on dispositional contributions to explain the resulting observed behavior. Research evidence for such a consistent neglect of observed situational variables is lacking (Gawronski, 2004). For example, in the study mentioned earlier about administering shock, how do you think people view the minority of participants who did not comply? If we globally ignore situational variables, as required by the FAE, we should attribute the behavior of these individuals to their strong moral character (disposition), but this is not what usually happens (Gilbert & Malone, 1995). Instead, most people attribute the minority’s behavior to simply responding to the situation the way people ought to do. The situation still plays a significant role in people’s interpretation of the observed behavior, which is inconsistent with the premises of the FAE. This leaves the correspondence bias as the superior explanation for how people balance their assessments of disposition and situation. The correspondence bias is not the only flaw in our judgment of behavior. When our personal needs and motives enter the picture, additional errors can occur. Classic studies suggested that an actor–observer biasactor–observer biasEmphasizing dispositional attributions to explain the behavior of others while emphasizing situational attributions to explain our own behavior.            actor–observer bias        Emphasizing dispositional attributions to explain the behavior of others while emphasizing situational attributions to explain our own behavior.             occurs when we use situational variables to explain our own behavior while continuing to use dispositional variables to explain the behavior of others (Jones & Nisbett, 1972): If my friend and I both flunked a test, I flunked because I was sick, but my friend flunked because he’s not a very hard worker. Although this suggestion has some intuitive appeal, a more recent meta-analysis suggested that people judge their own behavior similarly to the behavior of others (Malle, 2006). Olympic figure skater Rachael Flatt told a newspaper, I was ultimately responsible for my work, no one else! No excuses! And, that is true in daily training. I am responsible for my own work, no one else. Elite athletes like Flatt rarely indulge in the self-serving bias, in which we attribute our successes to dispositional qualities (I’m a talented athlete) and our failures to situations (the judging wasn’t fair or the ice wasn’t prepared properly). By avoiding this bias, athletes can learn from their mistakes and continue to improve their performances. Because of a self-serving biasself-serving biasAttributing success to dispositional factors while attributing failure to situational factors            self-serving bias        Attributing success to dispositional factors while attributing failure to situational factors            , we view our successes differently than our failures (Miller & Ross, 1975). When we succeed, we are more likely to attribute our success to dispositional variables. For example, if you get an A in this course, you are likely to congratulate yourself for your brains and hard work. In contrast, if you fail the course, it is because of situational factors, such as the professor’s asking unfair questions on the final exam. The self-serving bias can make you feel better, but it also can make it harder to learn from experience. Elite athletes, for example, are less likely than less successful athletes to use situational variables to excuse poor performance, such as blaming the weather when they lose a game (Roesch & Amirkhan, 1997). By considering their own contributions to failure, elite athletes can make the adjustments necessary for future success. It is Erroneous to Assume that large populations, like the people of China, are all alike, yet many cross-cultural psychologists have assumed that Chinese culture is uniformly collectivist. Not only is this not the case, but researchers also have suggested some novel contributing factors to explain the variations in social structure that can be observed. For example, differences in the type of crops that you raise is probably not the first variable that comes to mind when we consider differences in individualism and collectivism, yet this variable seems to play a role (Talhelm et al., 2014). The Question: Does the type of crop farmed predict the tendencies of a culture to be collectivist or individualist? More than 1,000 Han Chinese students in six areas of China (see Figure 13.3) completed measures of cultural thought, implicit individualism, and loyalty/nepotism. None of the measures were self-report because the researchers expressed concerns that the students would feel the need to answer in socially appropriate ways. For example, the cultural thought task asks the participants to decide which two items out of a group of three go together. If we asked you to do this for train, bus, tracks, what would you choose? People from individualistic cultures tend to make abstract pairings, such as train and bus, which belong to the same category. People from collectivistic cultures tend to make more holistic pairings, such as train and tracks, because they share a functional relationship. The responses of the students were analyzed in terms of their location in the wheat-growing north of China or the rice-growing south. Raising rice requires high levels of cooperation (a married couple cannot grow the rice needed to support a family by themselves), but growing wheat requires less, so the authors expected more collectivist responses from southern students than from northern students. The authors quoted a 17th-century Chinese farming guide that says, If one is short of labor, it is best to grow wheat. Han Chinese living in areas that have historically farmed rice scored higher on holistic thinking and were less likely to disrupt social organization through divorce than Han Chinese living in areas that have historically farmed wheat. Chinese protections for research participants are similar to those in the United States, but there are some concerns about how these translate to a different culture. Participants who are expected to put the collective good ahead of their individual interests might participate when they otherwise would prefer not to. The researchers tested their rice explanation against other potential explanations, such as the influence of pathogens or modernization. They found that the rice theory was the only one that consistently fit the cultural data. People from provinces that traditionally farmed rice performed more collectively and holistically, while people from provinces that traditionally farmed wheat performed more individualistically and analytically. The participants were unlikely to have ever engaged in farming, but traditional farming practices had shaped their cultural values nonetheless. A history of having to cooperate to survive might have shaped the local culture for centuries. These results also remind us that simple answers to psychological problems are usually not the best. Rather than thinking of East Asia as a consistent whole, it is more accurate and interesting to investigate the nuances of behavioral differences within cultures. Similar attribution errors occur at the group level (Forsyth & Schlenker, 1977). In a group-serving bias, an organization tends to attribute success to its dispositional characteristics but blames failures on the situation. If a sports team wins, most members attribute the victory to team disposition, such as talent, good preparation, or sheer will. If the team loses, the members are unlikely to blame the loss on team dispositional factors (Sherman & Kim, 2005). By failing to consider dispositional factors that might be remedied in the future, a group might miss opportunities to correct its course of action. In addition to self-serving bias, we frequently engage in other types of defensive reasoning. It is a short step from Your behavior reflects your disposition to causal judgments about the outcomes of behavior, such as You get what you deserve. Through the just-world beliefjust-world beliefThe assumption that good things happen to good people and bad things happen to bad people.            just-world belief        The assumption that good things happen to good people and bad things happen to bad people.            , people assume that good things happen to good people and bad things happen to bad people (Lerner & Miller, 1978). When you see something bad happen to a good person, you might reduce your anxiety by thinking that the person must have done something to deserve the bad outcome and that you can avoid that outcome by behaving differently. However, this reduction in personal anxiety can come at the expense of the victim, who is being blamed unfairly. Strong just-world beliefs have been positively correlated with negative attitudes toward the poor, people with HIV/AIDS, the elderly, and the unemployed (Sutton & Douglas, 2005). A just-world belief leads people to expect good things to happen to good people and bad things to happen to bad people. A strong just-world belief might lead people to blame-the-victim thinking, making them less sympathetic to the poor and the sick. There are subtle differences in attribution between cultures that value individualism and cultures that value collectivism. Individualistic cultures, including the United States and many other Western nations, stress individual achievement and competition. Collectivistic cultures, including those in many Asian nations, traditionally have valued cooperation as a means of attaining family and work group goals. Members of collectivistic cultures tend to place more emphasis on situation than on disposition (Choi, Nisbett, & Norenzayan, 1999;  Ho & Fung, 2011). For example, in the United States, murder is viewed as resulting from the disposition of the murderer, whereas in China, murder is often described in terms of the histories of the people involved and the situation in which the murder occurred (Morris & Peng, 1994). As a result, members of individualistic cultures are more likely to demonstrate the correspondence bias in evaluating crime. These cultural differences in attribution may be rooted in the tendency for people in Eastern cultures to focus on context or situations and relationships, while people in Western cultures are more likely to focus on objects than on situations (Masuda & Nisbett, 2001). As mentioned earlier, we tend to attribute behavior to the factors that we perceive. If being raised in a particular culture leads you to focus on different features of the environment, the attributions that are made vary accordingly (see Figure 13.4). In the individualist U. S. culture, but not in collectivist Singapore, recognition of peer pressure leading to crime (a situational variable) reduces the attribution of the crime to a person’s disposition. An American criminal who commits a crime because of peer pressure is not seen to be as bad as a person who acts without such pressure. As a result, extenuating circumstances usually lead to reduced sentences in the United States. Even though collectivist nations like Singapore place a higher emphasis on situational variables when making attributions overall, they do not view peer pressure as a reason to reduce punishment of an individual. If anything, the individual will be blamed for associating with the wrong peer group. Because of the importance placed on individual achievement in individualistic societies, members of these societies are more likely to use defensive attributions, such as self-serving bias (Leung, Kim, Zhang, Tam, & Chiu, 2011). Members of more collectivistic cultures show an interesting group-serving bias, in which they attribute more success to the group than it deserves and less failure than it deserves (Leung et al., 2011). That is, if you were raised in a collectivistic culture, you would show the same attributional biases about your in-group as individuals raised in an individualistic culture would show about themselves. To survive, all animals must make appropriate approach or avoidance responses to stimuli. The reflexes, fixed action patterns, and unconditioned responses discussed in Chapter 8 provide some guidance in simple situations, such as seeing a predator. To respond to more complex situations, humans form attitudesattitudesA positive or negative evaluation that predisposes behavior toward an object, person, or situation.            attitudes        A positive or negative evaluation that predisposes behavior toward an object, person, or situation.            , or favorable or unfavorable evaluations that predispose behavior (Petty & Cacioppo, 1986a). Attitudes are pervasive. We form positive and negative attitudes about everything from world events to celebrities to brands of peanut butter, and attitudes provide an efficient means of communicating with others. If a friend asks you for a recommendation for a movie, restaurant, or class, a simple attitude statement (I liked my psychology class!) can serve as a quick and efficient summary for a wide set of beliefs, behavioral experiences, and emotions. Decision making is simpler, faster, and less stressful when attitudes exist to serve as a guide. Attitudes can influence behavior, which is why the advertising industry spends billions of dollars each year in an attempt to shape your attitudes toward various products. Because we depend on our attitudes to guide our behavior, we want to make sure that they are correct (Festinger, 1950). Imagine what would happen if you woke up one day to discover that your attitudes about people, objects, and issues were all wrong. Suddenly your closest friends would dislike you, your major would be a poor match for your interests and abilities, and your entire collection of music would sound dreadful. Occasional mistakes happen, but holding accurate attitudes makes it easier for us to maneuver through environments successfully. Attitudes share three basic elements: affect (i.e., emotion), behavior, and cognition (the ABCs). For example, how might your attitudes about your college major be affected by hearing a negative news report about the job market for new college graduates in that field? The affective aspect of an attitude addresses emotional responses to the object. This information about your future job prospects could make you feel quite helpless, frightened, or angry. Behavior reflects the way that people respond to the object. After hearing the news, you might schedule an appointment with your adviser to discuss ways that you can pursue the subjects you love and still have a great career. The cognitive aspect of an attitude includes beliefs about the object. In response to this news, you might believe that you have chosen the wrong major. If you have any doubt whether people are emotional about their attitudes, just ask some friends whether they prefer an Apple (represented by the late Steve Jobs, left) or a PC (represented by Bill Gates, right). Be prepared to duck for cover. We form many attitudes on the basis of our personal experiences. You like a flavor of ice cream after sampling it, and you dislike the parking situation at your school because you can never find a spot near your classes. In other cases, we absorb attitudes from our contacts with peers, parents and other family members, religious leaders, teachers, and the media. We often adopt the attitudes of those around us to strengthen our chances of being socially included. As a result, many people prefer to stay on the winning side by forming attitudes based on how frequently they hear an opinion expressed by others and in the media (Noelle-Neumann, 1984). Later in this chapter, we explore how fear of social rejection also can lead to conformity and strict adherence to social norms or values. The learning principles explored in Chapter 8 play important roles in the development of attitudes. Operant conditioning suggests that approval or disapproval shapes a person’s attitudes. An opinion expressed in class that is followed by nods and claps will be repeated in the future more than one that is met with horror and disgust. Because of classical conditioning, you are likely to form positive attitudes toward stimuli associated with positive outcomes. If a song is popular at a happy time of your life, it is likely that your subsequent attitude toward the song will be positive too. Observational learning also can be a powerful mechanism for learning attitudes. Children who are exposed to particular attitudes in their homes, communities, and media duplicate them, especially if they observe the person expressing the attitude being rewarded. Can attitudes be influenced by genetics? Identical twins who were adopted by different families at birth, making it unlikely that they were exposed to the same attitudes, still show a surprising amount of similarity in their attitudes as adults (Bouchard, Lykken, McGue, Segal, & Tellegen, 1990). You might have noticed that certain social, political, and religious attitudes seem to follow consistent patterns in the people you know. In other words, a person who holds liberal social and political attitudes is unlikely to hold traditional religious attitudes. Twin studies suggest that this range of attitudes can be explained by a single dimension—traditionalism versus nontraditionalism—which appears to be quite heritable (Ludeke, Johnson, & Bouchard, 2013). Some of our attitudes result from personal experience, but others are quickly absorbed from the world around us, especially through peers and the media. Although our attitudes typically guide us well, attitude change is often desirable. After all, we are sometimes flat-out wrong. Addiction, for example, includes attitudes and preferences that are not in the best interests of anybody concerned. The addict by definition cannot easily change these attitudes and preferences, however. Similarly, people who are unable to change their attitudes in the face of new evidence can be quite handicapped. Understanding the circumstances in which attitude change takes place gives us additional insight into the formation of our attitudes and their role in guiding our behavior. Cognitive dissonance helps explain why hazing, or requiring an unpleasant initiation to become a member of a group, increases the value of the group (Aronson & Mills, 1959). If you choose to endure so much to join, the group must be something special. Cognitive dissonanceCognitive dissonanceThe uncomfortable state that occurs when behavior and attitudes do not match and that can be resolved through attitude change            Cognitive dissonance        The uncomfortable state that occurs when behavior and attitudes do not match and that can be resolved through attitude change             provides a powerful tool for producing attitude change (Festinger, 1957). Cognitive dissonance is an uncomfortable state that occurs when our outward behavior doesn’t match our attitudes. For example, you might have been accepted by several colleges and universities, but you can choose to attend only one. Thinking about the positive features of the schools you reject causes dissonance (I like these schools, but I didn’t choose them), which could be resolved either by becoming a superfan of your chosen school or by forming negative attitudes toward the schools that you reject. How could you even consider making another choice? Who would want to go to those other schools? For attitude change to occur as result of cognitive dissonance, we must attribute our actions to our own voluntary, free will. In an early demonstration of cognitive dissonance (Festinger & Carlsmith, 1959), students completed two boring tasks (putting empty spools on a tray, emptying the tray, then filling it again; turning square pegs one quarter-turn at a time). Afterward, they were asked to tell another person that the tasks were fun. This should produce some level of cognitive dissonance (I publicly said the tasks were fun, but I really think they’re boring). The researchers offered to pay $1 (about $8.35 in 2017 dollars) or $20 (about $167.08 in 2017 dollars) to participants for saying that the tasks were fun. In contrast to students who were neither paid nor asked to say that the tasks were fun (the control group), the students paid $1 rated the tasks as much more fun. Interestingly, rating of the tasks’ enjoyability by the students paid $20, who could easily have justified that behavior (I’m being paid well to do this), did not differ from the controls. Brain imaging studies provide substantial support for cognitive dissonance (see Figure 13.5). Many people undergoing functional magnetic brain imaging (fMRI) find being confined in the scanner tube and the loud banging of the magnets to be unpleasant. Still, in one study, participants undergoing fMRI were asked to describe the somewhat unpleasant scanner experience as pleasant (van Veen, Krug, Schooler, & Carter, 2009). Levels of activity in the anterior cingulate cortex (ACC) and insula were higher when a participant made dissonant statements about how pleasant the fMRI experience was compared to neutral statements. Subsequent research has confirmed that the ACC is often engaged during episodes of cognitive conflict, while activity in areas of the insula corresponds to negative arousal (Kitayama, Chua, Tompson, & Han, 2013). Study participants undergoing fMRI were divided into two groups: a dissonance group, which was asked to tell the next participant how pleasant the rather unpleasant scanning experience was; and a control group, which did not discuss pleasantness. Both groups responded to target sentences, such as I feel calm, peaceful in the scanner, or neutral sentences, which did not involve attitudes toward the scanner experience. (a) The dissonance group responded to the target sentences with more activation in several parts of the brain, including the ACC and the insula (located at the junction of the temporal and frontal lobes), compared to the control group. (b) The groups did not show differences in response to the neutral sentences. (c) The yellow–orange areas are more active during the dissonance situation (dissonant group responding to target sentences) than during the other situations (dissonant group responding to neutral sentences or control group responding to target and neutral sentences). Cognitive dissonance, which focuses on conflicts between attitudes and behaviors, is part of a broader phenomenon—our need for cognitive consistencycognitive consistencyA preference for holding congruent attitudes and beliefs.            cognitive consistency        A preference for holding congruent attitudes and beliefs.             (Gawronski & Brannon, 2017). We form many associations between ideas, which in turn lead to expectations. Discomfort caused by conflicts among these ideas might help us recognize errors in our belief systems. You might believe very strongly that being vegan will make people healthier, and your friend wants to be healthy, but she is not vegan. To resolve this inconsistency, one of these ideas needs to change. You might now believe that it’s possible to be healthy without being vegan, your friend really doesn’t want to be healthy, or your friend really is vegan and just doesn’t want her family to know. Aversive feelings that we experience when our ideas are inconsistent might motivate us to think more deeply about our beliefs. Choosing to smoke despite constant warning messages can produce cognitive dissonance. To reduce dissonance, this smoker may start to think that the warnings are not believable and that smoking isn’t that bad for you. Cognitive dissonance can produce attitude change, but it does not occur in response to a direct effort by another person. When we change our attitudes in response to information provided by others, persuasionpersuasionA change in attitude in response to information provided by another person.            persuasion        A change in attitude in response to information provided by another person.             occurs. Knowing how to persuade others and understanding the efforts of others to persuade you are important life skills. The elaboration likelihood model (ELM)elaboration likelihood model (ELM)A model that predicts responses to persuasive messages by distinguishing between the central and the peripheral route to persuasion.            elaboration likelihood model (ELM)        A model that predicts responses to persuasive messages by distinguishing between the central and the peripheral route to persuasion.             organizes and predicts our responses to persuasive messages by recognizing two major pathways leading to changes in attitudes (Petty & Cacioppo, 1981). The first, known as the central route to persuasion, occurs when a person considers persuasive arguments carefully and thoughtfully. The second, known as the peripheral route to persuasion, occurs when a person responds to peripheral cues, as described in the following section, without considering the quality of the argument carefully. Importantly, attitudes achieved through the central route are more enduring, more resistant to counterpersuasion, and more predictive of behavior than are attitudes achieved through the peripheral route (Petty & Cacioppo, 1986b). Why do we need two systems for responding to persuasive messages? Ideally, we would use the central route to consider all messages that we encounter. However, given our limited time, knowledge, and cognitive resources, a thorough evaluation of all the messages that we perceive is not practical. Using the peripheral route allows us to use heuristics, or rules of thumb (as described in Chapter 10), to react promptly to many situations. If you feel overwhelmed by the large number of candidates on your ballot, you might use the heuristic of voting along party lines or voting like your friends. Experts attending a technology trade show are likely to use the central route to persuasion while evaluating the speaker’s message. They will concentrate on the logic of the message and will be relatively unimpressed by peripheral cues, such as the speaker’s attractiveness or use of emotion. What determines whether we use the central or peripheral route? According to the ELM, a person’s motivation influences which route is used to evaluate a particular argument. When students are highly motivated to take time out of their busy schedules to attend a presentation by a guest speaker on campus because they know and care about the topic of the lecture, they are likely to use the central route and evaluate the speaker’s points carefully. People are also likely to use the central route to persuasion when they are well educated about a topic. However, even people who are knowledgeable occasionally take the peripheral rather than the central route. If you know about a topic but do not have the time to think about it or don’t care about it, you might find the shortcuts provided by the peripheral route more useful. In addition, because using the central route requires so many cognitive resources, including attention, we are less likely to process persuasive messages this way when we are distracted by other information or activities. As you concentrate on driving down a busy highway, you do not have the cognitive resources to evaluate the messages on billboards carefully. If a person is using the peripheral route to persuasion, what cues might influence the amount of attitude change that occurs? Peripheral cues usually apply to the persuasive message (e.g., the number of arguments), the manner in which the message is presented (e.g., speed of speech and accent), or the characteristics of the speaker (e.g., credibility or celebrity). Emotional appeals can influence people using the peripheral route. Both positive and negative emotions can enhance persuasion. Students asked to read persuasive messages about topics such as curing cancer and the state of military forces were more likely to agree with the statement they read while happily snacking on Pepsi and peanuts (Dabbs & Janis, 1965). Efforts to persuade using negative emotions are especially common. We hear daily warnings about the dangers of drinking too much, not using our seatbelts, engaging in risky sex practices, riding a bicycle without a helmet, eating trans fats, and so on. Arousing negative emotions, such as fear, can be risky. If people are too frightened by a message, they go into a state of denial. It’s really not that bad, they will say, or That can’t happen here. Denial is most likely to occur if the audience is not given an action to do to reduce their anxiety. It is fine to scare smokers to death about the appearance of their lungs, so long as you include a message that tells them where to get help to quit smoking. Having a sense of control over their response will reduce the fear and make the message more persuasive (Ajzen, 1991). Driving down a busy highway requires your full attention, making it unlikely that you will have the resources you need to evaluate persuasive messages using the central route. Instead, the advertisements that you pass on the road attempt to influence you in a few seconds to think better of the product that they are boosting or the message that they display. A positive response to the speaker can enhance a message’s persuasion. Despite the common-sense warning that beauty is only skin deep, people have a deep-seated stereotype that what is beautiful is good (Dion, Berscheid, & Walster, 1972). Student evaluations of a professor are heavily influenced by the perceived attractiveness of the professor (Hamermesh & Parker, 2005). People are more likely to be persuaded by people they like and with whom they share some similarity (Cialdini, 2001;  Van Knippenberg & Wilke, 1992). Similarities need not be substantial to produce this effect; study participants who shared a first name or birthday with the supposed author of a persuasive essay were more likely to agree with the essay (Silvia, 2005). The less a speaker stands to gain by persuading you, the more credibility the speaker appears to have. We understand that athletes and other celebrities are being paid large amounts of money to advertise particular products, so we tend to discount their opinions (Moore, Mowen, & Reardon, 1994). A testimonial by someone who has nothing to gain by speaking in favor of a product or issue is more persuasive. As a result of these findings, bloggers must disclose whether they are being compensated for writing about a brand. Without this information, we might not evaluate their arguments correctly. A credible speaker is usually more persuasive, especially when listeners are traveling the peripheral route. In turn, credibility is based on the speaker’s perceived expertise and trustworthiness. Expertise is often established by a person’s history and credentials. We suspect that your psychology professors would look different to you if they indicated that their degrees were in other subjects and that they really didn’t know much about psychology. Persuasion is also influenced by the medium used to communicate a message, from the face-to-face presentation of the door-to-door salesperson (a rarity these days) to print media, social media, the Internet, and movies and television. Typically, we find that the more a medium resembles a live interaction with another person, the more effective it is in transmitting a persuasive message. In general, face-to-face contact remains more effective than the use of other media (Bavelas, Hutchinson, Kenwood, & Matheson, 1997). This arrangement is the most natural way for humans to communicate. Not only is face-to-face communication the first medium we learn in childhood, but various technologies allowing communication at a distance developed at much later points in human history. Children are more likely than adults to respond to persuasive messages emotionally rather than logically, making them more susceptible to persuasion. Threatened with lawsuits from public interest groups, several major U. S. food companies agreed to reduce their marketing to children and to make products that appeal to children healthier. Finally, the age and intelligence of an audience can influence the effectiveness of a persuasive message. Young children are more likely to respond to persuasive arguments emotionally rather than logically, which might make them more susceptible to attitude change (and advertising) than adults. Intelligent, well-educated audiences are more difficult to persuade in most situations (Rhodes & Wood, 1992). Again, the influence of cues related to the message, speaker, medium, or audience is greatest when people are using the peripheral route to persuasion. When people are using the central route, their focus remains on the quality of the arguments being presented. The internet has provided new ways for people to communicate, from the use of webcams, video, and live chat to Twitter, Facebook, Instagram, Snapchat, and blogs. Many of these new technologies have advantages in providing immediacy and collaboration. To the extent that they duplicate the advantages of face-to-face communication, we would expect them to provide effective means of persuading others. The Internet not only provides a new medium for communicating persuasive messages, but also challenges the status of credible, expert sources. Digital sources of news currently rank second only to television, and twice as many adults see news online compared to seeing news in print (Matsa & Lu, 2016). Historically, persuasive messages from traditional news outlets such as newspapers, magazines, radio, and television were more of a one-way street. However, in an environment in which everybody with a computer can produce information, receivers of information can be influencers as well by sharing and commenting on messages in blogs and on social networking sites. Research in this area suggests that user-generated content in the form of blogs, uploaded videos, comments, and reviews is considered as credible and accurate as formal news reports prepared by professional journalists (Lee, Park, Lee, & Cameron, 2010). In addition, knowing that content is popular with other users significantly raises a message’s impact (Steyn, Wallstrom, & Pitt, 2010). As more people depend on online sources for their information, psychologists and other professionals have expressed concerns about the quality of that information. So-called fake news is not a new concept, but the ability of the Internet to spread it brings a whole new dimension to its ability to persuade large numbers of people. At risk are people’s collective memories, or our shared understanding of what really happened. Fake news can occur in both traditional and online sources, but in the online domain, the term usually refers to sensational stories that are used to lure readers to a site to gain ad revenue. Among the most widely spread fake news stories of 2016 included Obama signs executive order banning the pledge of allegiance in schools nationwide (with 2,177,000 Facebook shares, comments, and reactions) and Pope Francis shocks world, endorses Donald Trump for President, releases statement (with 961,000 Facebook shares, comments, and reactions). About 16% of Americans have admitted to having shared fake news by mistake (Barthel, Mitchell, & Holcomb, 2016), and an alarming number seem to think satirical sites like The Onion post real news stories. While many people seem to have difficulty distinguishing between fake and real news, the critical thinking steps outlined in Chapter 2 are a good place to start. In addition, it can be helpful to search for information about the source, the author, and the date of the story. Some people actually mistake satire and jokes for real news, so if something sounds especially far-fetched, it might just be a joke. Again, we see the power of the social group in shaping our ideas (see Figure 13.6). Emerging interactions between new social media and credibility are likely to continue changing the landscape of persuasion. Social media, with their ability to make users into spokespeople, raise new issues for our understanding of speaker credibility in persuasion. Different age groups have embraced user-generated content, such as blogs, tweets, and articles shared on Facebook, to different degrees. Younger age groups are as likely to say they would comply with a food recall message posted on their Facebook wall by a friend (user-generated message) as they are to an official notice from the Centers for Disease Control and Prevention (professional message), but older age groups are less likely to say that they would comply with the user-generated messages. Traditional studies of persuasion, many of which are described in this chapter, typically feature the use of questionnaires and scales that measure changes in attitudes and the perceived persuasiveness of messages. Researchers are beginning to explore the underlying brain processes responsible for these changes (Cacioppo, Cacioppo, & Petty, 2017). Examinations of neural correlates of source factors (such as expertise and celebrity) and message factors (such as positivity/negativity) identify areas of the prefrontal cortex as playing an important role, but many additional structures and networks also participate. Researchers also have begun to explore the neural basis for the effects of persuasive messages on the intention to behave and on actual changes in behavior. Southern Californians were scanned using fMRI (see Chapter 4) while receiving persuasive messages about using sunscreen (Falk, Berkman, Mann, Harrison, & Lieberman, 2010). The researchers identified areas within the prefrontal cortex that predicted participants’ changes in sunscreen use 1 week after the scan. Significant progress has been made in this area, but the problem is extremely complex. Thinking about and acting on a persuasive appeal, as noted in this chapter, combine a number of cognitive and emotional processes. It would be surprising if the underlying neural correlates were simple rather than complex. Some untapped possibilities for pursuing this research question involve studying the aging brain and manipulating brain activity through the use of magnetic stimulation (Cacioppo et al., 2016). The aging brain would enlighten this discussion because the prefrontal cortex undergoes more dramatic change during aging than many other brain structures. Older adults differ in their susceptibility to deceptive advertising and their willingness to change their attitudes in the face of conflicting information. Assessing the differences between these groups and between younger and older participants could be very useful. Magnetic stimulation, which changes the activity of underlying brain structures, could be used to investigate whether particular structures or networks were crucial for a persuasive outcome. While it might take some time to fully grasp the neural correlates of persuasion, efforts in this direction will help us better understand both the process of persuasion itself and the general information processing performed by the brain. Among the most damaging perceptions that we form about other people are those based on their group memberships, which can include factors such as race, gender, age, socioeconomic status, height, weight, disabilities, attractiveness, sexual orientation, and religion. Psychologists use a group of related terms to discuss the assessment of others based on group membership. PrejudicePrejudiceA prejudgment, usually negative, of another person on the basis of membership in a group.            Prejudice        A prejudgment, usually negative, of another person on the basis of membership in a group.            , which literally means to prejudge, is an attitude (usually negative) about others. Prejudice is supported by cognitions known as stereotypesstereotypesA simplified set of traits associated with membership in a group or category            stereotypes        A simplified set of traits associated with membership in a group or category            , which are simplified sets of traits associated with group membership. A person might believe a stereotype that all blondes are dumb, which leads to negative attitudes (prejudice) about blondes. Resulting behavior based on prejudice and stereotyping is considered to be discriminationdiscriminationA learned ability to distinguish between stimuli. (b) Unfair behavior based on stereotyping and prejudice            discrimination        A learned ability to distinguish between stimuli. (b) Unfair behavior based on stereotyping and prejudice            . Hiring practices, housing, educational opportunities, and many other facets of modern living can be conducted in discriminatory ways. Without excusing prejudice and stereotyping and their negative social consequences, it helps to understand that some tendencies to develop prejudice and stereotypes are extensions of the typical ways that we process information. Chapter 10 noted that humans organize information and experience into schemas made up of categories or concepts, which is generally useful. Our concepts are associated with a variety of characteristics, such as Birds have feathers. We frequently have exemplars, or best examples, of each category. If you think of the word bird, you are probably more likely to imagine a canary than an ostrich. Finally, we generalize from our known concepts when faced with new information. If we know that birds peck at objects placed before them, we are less likely to stick our fingers near the beak of a new type of bird. In other words, based on our past experience with or knowledge of birds, we are prejudging the behavior of an unfamiliar bird. This process of categorizing information enables us to make rapid decisions about new situations and objects, similar to the rules of thumb, or heuristics, discussed in Chapter 10. Effective concepts are inclusive, recognizing a full range of associated characteristics. When our categories are oversimplified, we are left with stereotypes. Although it is true that most birds fly, it is inaccurate to oversimplify the matter by just saying Birds fly, because ostriches do not. Stereotypes often contain an element of truth but become inaccurate because of the information that they exclude. More important, stereotypes conflict with our valid desire to be viewed and treated as individuals. If stereotypes can lead us astray, why are they so pervasive and persistent? People tend to misjudge correlations between groups and their stereotypical behaviors. They might overestimate the percentage of Asian students who excel in math and science or underestimate the number of White athletes who play professional basketball. Attributions, discussed earlier, also contribute to continued stereotyping. If you see an exception to your stereotype (perhaps a female who is running faster than her male friend), you might view her success as the result of situational variables. Perhaps the man is letting her win to be nice. This type of thinking allows us to maintain a women are weaker stereotype. People often have negative stereotypes about overweight people as a group. In one online survey, more than 75% of respondents believed that overweight people should either buy a second seat or pay a fat tax to travel on airplanes (Siegfried, 2010). Comedian Kevin Smith, ejected from a Southwest Airlines flight for infringing on a portion of the next seat, tweeted that he broke no regulation, offered no ‘safety risk’ (what, was I going to roll over on a fellow passenger?). You have probably noticed that people’s prejudice toward a group often conflicts with their behaviors toward individual members of that group. Someone might express dramatically prejudiced opinions about a group and yet treat members of that group well. Individuals can be viewed as being different or exceptions, allowing people to retain their stereotypes in the face of conflicting observations. Instead of rethinking a stereotype about truck drivers and country music, you view your truck-driving friend who enjoys opera to be one of a kind. Finally, stereotypes are often maintained by people’s expectations. In one experiment, students listening to a basketball game were informed that a certain player was either Black or White. After the game, students who believed the athlete was Black expressed their stereotypes by rating the player as being highly athletic, while those who believed that the athlete was White praised him for his intelligence and hustle (Stone, Perry, & Darley, 1997). Prejudice, like other types of attitudes, can result from direct experience (I had one boring history professor, so all history professors must be bad), or they can be indirectly absorbed from the social world of peers, parents, the community, and the media. Human prejudice based on group membership, however, might have deeper roots in our evolutionary past. Recognizing and reacting to individuals based on group affiliation was probably an important skill for early hunter–gatherers, whose survival often depended on quickly identifying others as potential trading partners or foes. This pressure could have led to a tendency to distinguish between in-groups, or the group to which we belong, and out-groups, or everybody else (see Figure 13.7). This categorization is made primarily on the basis of physical appearance, and people still instantly (and often unconsciously) categorize others on the basis of physically observable features, including age, gender, and race (Fiske, 1998). Out-groups often elicit mixed reactions. Some out-groups (rich people) are respected for their perceived competence but disliked for their supposed lack of warmth. Elderly people and the disabled are viewed as low in competence, but they are viewed warmly because of the lack of threat that they pose. Groups such as the poor are viewed most harshly as a result of having both low status and representing a threat of exploiting the in-group. This type of categorization was demonstrated by recording brain activity as people viewed photographs of faces and made judgments about either race or gender (Ito & Urland, 2003). Study participants reacted quickly to both race and gender information, even when the information was irrelevant to their task. In other words, a participant who was asked to differentiate between male and female faces still showed evidence of having paid attention to the race of the faces, without being asked to do so. Different emotional responses can accompany these instant categorizations. When study participants viewed faces from their own racial group, activity in the amygdala was lower than when they viewed faces from another racial group (Hart et al., 2000). As discussed in Chapter 4, activity in the amygdala is correlated with assessments of threat. Either realistic or imaginary threats from an out-group are viewed as a major source of prejudice (Stephan & Stephan, 2000). Furthermore, responses of the amygdala to race are correlated with several implicit measures of prejudice (Phelps et al., 2000). Implicit measures are designed to demonstrate subconscious associations between concepts (literature and math, for example) and attitudes (good and bad) (e.g., Nosek, 2007). Psychologists continue to debate the extent to which implicit measures predict actual prejudice (Oswald, Mitchell, Blanton, Jaccard, & Tetlock, 2013). White study participants who showed the greatest differences in amygdala activity while viewing Black and White faces were also more likely to associate positive words such as joy, love, and peace with Whites and negative words such as cancer, bomb, and devil with Blacks. The amygdala activity in response to viewing Black faces might reflect the fear and other negative assessments made by these participants based on racial stereotypes. Oversimplified categories, such as girl toys or boy toys, can lead to stereotypes. Understanding that biological and evolutionary processes are involved with in-group/out-group differentiation does not make prejudice inevitable or acceptable. By understanding why we develop prejudice, we can better identify ways to reduce it. As we will see later in this section, prejudice can change. Prejudice, whether subtle or overt, can lead to discriminatory behavior in a number of ways. Not only are African Americans convicted of murder more likely than Whites to be given the death penalty in the United States, but perceptions of Blackness are also predictive of a murderer being sentenced to death (Eberhardt, Davies, Purdie-Vaughns, & Johnson, 2006). Researchers presented photographs of African-American defendants to participants without telling them that the men depicted had been convicted of murder. The participants ranked the photographs on the basis of stereotypical Black features, such as broad nose, dark skin, and thick lips. The ratings were highly predictive of the men’s sentences when the victim was White. Men who appeared more stereotypically African American were more than twice as likely to be given the death sentence as men whose physical appearance was less stereotypical. This difference disappeared, however, when the victim was also African American (see Figure 13.8). Study participants’ ratings of stereotypical Black features predicted the men’s likelihood of being sentenced for murder with the death penalty, but they did so only when the victim was described as White. The men in these photographs have no criminal records but are examples provided by the authors for individuals who were viewed as less stereotypical (left) and more stereotypical (right). Prejudice affects not only the way we view others, but also the way we view ourselves. Raising awareness of a negative stereotype about a group to which we belong has the ability to reduce our performance, a phenomenon known as stereotype threat (Steele, 1997;  Steele & Aronson, 1995). Anxiety about confirming a negative stereotype attributed to your group can prevent you from doing your best work. In a set of studies demonstrating this effect, researchers asked Stanford University undergraduate students to complete difficult items taken from the verbal portion of the Graduate Record Exam (GRE), used as part of the admissions process for some graduate programs (Steele & Aronson, 1995). One group of students, the stereotype threat group, was told that the test accurately assessed their cognitive abilities, a statement that was designed to stimulate thinking about possible racial stereotypes about intelligence. The other group was told that the test was a routine laboratory procedure. Subsequently, the performance of Black and White students on the test was compared. After controlling for the participants’ previous Scholastic Aptitude Test (SAT) scores, the Black students in the stereotype threat group performed more poorly than did the White students. In contrast, in the nonthreatening condition, Blacks and Whites with equal SAT scores performed similarly on the GRE items, which is the result that we would expect because of the tests’ reliability and validity, discussed in Chapter 2 (see Figure 13.9). However, it would be an inaccurate interpretation of Steele and Aronson’s work to suggest that stereotype threat accounts for the bulk of group differences in performance (Stoet & Geary, 2012). Meta-analyses have supported the existence of the stereotype threat effect, but its contribution to overall performance appears to be small to moderate (Spencer,  Logel, & Davies, 2016). Nonetheless, any irrelevant influence on a person’s performance is noteworthy, as these small individual effects can add up to larger influences at the level of entire populations. The Implicit Association Test (IAT) measures reaction time to assess people’s unconscious attitudes. In this version of the test, either a face (African-American or European-American) or a word appears at the center of the screen. The participant pushes a specified key on a computer for the face (African-American) and then again for the word (joy is Good). Later in the test, the combinations at the top of the screen are switched to African-American or Bad and European-American or Good. An implicit bias in favor of African Americans would be shown if reaction time in the first condition (African-American or Good) is faster than in the second condition (African-American or Bad). Students at Stanford University were told that a verbal test measured their true abilities (stereotype threat). After controlling for prior performance on the SAT, the subsequent performance by African-American participants was worse than when they were told that the test was being used to understand the psychological factors involved in solving verbal problems (no stereotype threat). The different instructions did not affect the performance of the White students. Recent news stories have featured a number of tragic and often fatal shootings of unarmed African Americans by police. In many cases, the anger over these cases has boiled over to affect entire communities and the conscience of the nation. What does psychological science tell us about these situations? Research demonstrates a consistent pattern. In computer simulations, White Americans, both real police officers and college undergraduates, are more likely to shoot unarmed African-American than White suspects (Kenworthy, Barden, Diamond, & del Carmen, 2011). In many cases, the researchers explain their results in terms of cultural stereotypes. Stereotypical views of African-American men as more dangerous might shape these quick decisions. To directly assess the role of cultural stereotyping on the decision to shoot, researchers first gave participants a personality test that supposedly gave them a red or green personality (Miller, Zielaskowski, & Plant, 2012). In actuality, participants were randomly assigned to red or green groups. In addition, participants completed a questionnaire assessing how dangerous they thought the world was. Next, the researchers presented faces of White males against a background of either red or green, which the participants were told reflected the personality of the individual. Results showed that no shooter bias occurred when participants were low to moderate in their beliefs about the danger of the world, but a bias did occur among those who saw the world as very dangerous. They were more likely to shoot the faces that were of the other personality color. The researchers performed a second study using White, African-American, and Asian faces. They assumed that there would be no cultural stereotype that Asian males are dangerous. All participants were still more likely to shoot the African-American faces than the White or Asian faces, but participants with high scores on the world is dangerous scale were more likely to shoot Asian faces than White faces. What have we learned from this research? Certainly, these studies confirm that cultural stereotyping is an important source of influence on quick, defensive decisions. In addition, a person’s view of the world as a dangerous place is likely to accentuate a general defensiveness with regard to any out-group member. This research raises concerns about rhetoric among law enforcement personnel likening police work to combat and other factors that heighten a police officer’s sense of being in mortal danger. Viewing our cities as war zones as opposed to communities is likely to produce more, not less, conflict. A video game including individuals holding guns or other objects has been used to examine the effect of race and ethnicity on decisions to shoot. Civilians and police officers both make faster decisions to shoot an armed person if that person is African American and not to shoot an unarmed White target. Although we would prefer a world in which prejudice never occurred, psychologists at least have gained insight into how prejudice can be reduced. Increased contact can reduce prejudice (Skerry, 2000). However, not just any contact will have this effect. To be effective in reducing prejudice, contact must occur under certain circumstances. For example, contact between people of equal standing is more effective in reducing prejudice. If people are from different socioeconomic groups, greater contact may increase rather than decrease between-group resentment. Cooperative learning programs similar to Aronson’s jigsaw classroom are used in most public schools in the United States today. Contact is particularly beneficial in reducing prejudice when groups participate in cooperative activities (see Figure 13.10). In a study using two groups of boys at a summer camp, negative feelings between the groups decreased after the boys cooperated to solve false emergencies, such as interruptions to the camp’s water supply (Sherif, Harvey, White, Hood, & Sherif, 1961). Another classic example of the effectiveness of cooperation in reducing prejudice is the jigsaw classroom, designed to reduce racial tensions in newly desegregated school districts in the 1970s (Aronson, 2011). Children in cooperative groups were assigned a part of the answer that their group needed to learn, leading to interdependence among the group members. Rather than competing for the teacher’s approval, the children in the jigsaw task helped their teammates by listening to and teaching each other. Outcome research indicated that students in jigsaw classrooms mastered the material better, liked their group members better, were absent less frequently, and showed evidence of more empathy and higher self-esteem (Aronson, 2011). This type of cooperative contact is used in deliberate efforts to reduce prejudice in about 80% of elementary schools in the United States (Paluck & Green, 2009). After arbitrarily dividing campers into two groups, the Rattlers and the Eagles, Sherif and his colleagues observed considerable in-group/out-group bias between the groups. During this phase, few of the campers reported having friends from the out-group. To resolve the resulting discord, the researchers set up false emergencies, such as a problem with the camp’s water supply, which required the boys to cooperate. After cooperating, many of their previous biases were no longer present, and more boys reported having friends among the out-group. Because so much prejudice originates in categorizing people and then engaging in in-group/out-group thinking, another strategy for reducing prejudice is to expand the definition of the in-group. For example, if a White female and an Asian female concentrate on the similarities that they share as women, they should experience fewer feelings of bias toward each other on the basis of race (Deschamps & Doise, 1978). Although this effect is not always seen in the laboratory (Vescio, Judd, & Kwan, 2004), when combined with real-world extended contact, the strategy seems to be promising (Eller, Abrams, Viki, & Imara, 2007). Perhaps if we stop focusing on our group identities, we can consider ourselves members of the human race or residents of the planet Earth. If we cooperate to solve problems of hunger, sickness, poverty, and degradation of the environment, the result could be a reduction in prejudice. When acceptance by a group is important to us, we are motivated to behave in ways that increase the likelihood of gaining the group’s approval and avoiding rejection. Groups typically have rules for behavior, known as social normssocial normsUsually unwritten or unspoken rules for behavior in social settings.            social norms        Usually unwritten or unspoken rules for behavior in social settings.            . Social norms can be explicit, or conscious, like wearing shoes into a restaurant; they can also be implicit, or unconscious, like moving to the back of an elevator and turning to face the doors. We often conform to social norms without being asked to do so. In contrast, compliance and obedience lead us to fulfill a request from another person. In compliance, the person making the request has no authority over us, but in obedience, the request is coming from an authority figure. Matching your behavior and appearance to the perceived social norms of a group constitutes conformityconformityMatching behavior and appearance to perceived social norms.            conformity        Matching behavior and appearance to perceived social norms.            . In a classic study of conformity by Solomon Asch (Asch, 1951), participants were asked to match a reference line to one of three comparison lines. As you can see in Figure 13.11, the lines are sufficiently different that solving this task was not rocket science. In the conformity condition, one real participant was placed among a group of confederates (fake participants employed by the investigator). Before the real participant gave an answer, the confederates provided some wrong answers. Would the real participant tell the truth or conform to the answer given by the confederates? In about one third of the trials, the real participants conformed by deliberately choosing a response that they knew was wrong. About three-quarters of the study participants conformed on at least one trial. When a lone participant was asked to match the Exhibit 1 line to Line A, B, or C, the person chose the correct line, A. However, when placed in a group of confederates told to pick another line, people became more likely to go along. Conformity reached its peak in groups of three to four people and did not increase as the group became larger. Conformity to expectations associated with roles was investigated in the Stanford Prison Study (Haney, Banks, & Zimbardo, 1973). In a short time, young men who had been randomly assigned to prisoner or guard roles quickly adopted behaviors consistent with those roles, leading to such inappropriate behavior (violence, etc.) that the study was stopped after only 6 days instead of the planned 14 days. The study has been criticized on the basis of methodological problems due to the active role taken by researcher Philip Zimbardo, who served as the warden in the study and thus influenced how the guards behaved. A similar prison experiment was conducted for BBC television, and this time, few of the guards acted inappropriately (Reicher & Haslam, 2006). In contrast to Zimbardo’s active participation in his own study, the British prison experiment featured little direction from the investigators. However, the likelihood that we unconsciously fulfill the roles expected of us by others remains an active area of scientific investigation. In 2001, Stephen Reicher and Alexander Haslam re-created aspects of the classic Stanford Prison Study, which was conducted by Philip Zimbardo and his colleagues in the 1970s. Zimbardo argued that the roles of guard or prisoner overwhelmed individuality, but Reicher and Haslam believe that the study’s participants were simply responding to Zimbardo’s instructions. Scientists do not always agree with one another, and their debates bring us closer to realistic representations of our world. Why do people conform? Conformity can be useful in ambiguous situations. If you are uncertain about what to do, such as how to dress at work, observing the behavior of others and conforming to it can be helpful. In a computer simulation comparing the effectiveness of social learning strategies, which includes imitation of others, to nonsocial learning strategies, such as trial and error, the social strategies proved far more successful (Rendell et al., 2010). The availability of others to copy might contribute to the overall success of group over independent living. Conformity also reduces the risk of rejection by a social group, a situation that would have greatly threatened the survival of our human ancestors. In a study demonstrating the rejection of nonconformists, participants were asked to debate the fate of an imaginary juvenile delinquent named Johnny Rocco (Schachter, 1951). Three members of the group were confederates. One was instructed to agree with the group consistently, another to disagree with the group consistently, and the third to switch positions from disagreeing to agreeing with the group. When the real participants rated how well they liked other members of the group, members who agreed with the majority were all well liked, and the confederate who changed position was liked best. The participants were unanimous in their intense dislike of the confederate who disagreed with the group. ComplianceComplianceAgreement with a request from a person with no perceived authority            Compliance        Agreement with a request from a person with no perceived authority             occurs when we simply agree to do something because another person asks us to do it, even if that person has no authority over us. From messages in junk mail that tell us to Open immediately. Do not discard and advertising appeals to Buy now to medical instructions that we receive from our dentist or family physician, we are bombarded with requests for compliance. Fulfilling requests from authority figures such as teachers or police officers can provide considerable advantages to society. It is more difficult, however, to explain why people would develop the tendency to comply with requests made by strangers who have no authority. It appears that our natural tendency to affiliate leads us to behave in ways that encourage the development of relationships, including being compliant. Compliance is higher when factors that normally lead to the formation of relationships are present, such as perceived similarity and physical attraction. For example, we are more likely to comply with requests presented by physically attractive people (Lynn & Simons, 2000;  McCall, 1997). This behavior provides yet another example of the Beauty is good stereotype that influences persuasion in general. People are more likely to comply with a request from a person who shares some similarity, even when the similarity is irrelevant, such as a shared birthday (Burger, Messian, Patel, del Prado, & Anderson, 2004). Many seemingly simple yet sophisticated sales techniques exploit our natural tendencies to comply in response to perceiving a social tie with another person (Cialdini & Goldstein, 2004). Once a sense of relationship is established, powerful norms of reciprocation are brought into play. Reciprocation, in which we feel obligated to give something back to people who have given something to us, is one of the most powerful tools of social influence (Gouldner, 1960). The importance of reciprocation to the cohesion of early human groups probably can’t be overestimated. If somebody shared food with you, and you reciprocated by sharing yours, a powerful bond would be formed. Reciprocation has been used to explain compliance in the door-in-the-facedoor-in-the-faceA persuasive technique in which compliance with a target request is preceded by a large, unreasonable request.            door-in-the-face        A persuasive technique in which compliance with a target request is preceded by a large, unreasonable request.             scenario, in which a large, unreasonable demand is followed by a smaller request, which is the one that the requester expects to work (Cialdini et al., 1975). For example, you might be asked to donate a large amount to an organization, followed by a request to give what you can afford. By lowering the initial demand, the salesperson has made a concession. The rules of reciprocation now suggest that you have an obligation to that person and should respond to this concession with one of your own—sending a donation (see Figure 13.12). In a study by  Cialdini et al. (1975), participants in the door-in-the-face condition were first asked to volunteer for 2 hours per week for 2 years as a big brother or big sister at a local juvenile detention center. None agreed. Subsequently, they were asked to spend 2 hours chaperoning a group of delinquents to a local zoo, and 50% agreed. In comparison, only 16.7% of the participants hearing only the zoo request agreed, and 25% of people asked whether they would choose one of the two activities agreed to the zoo request. It is possible that feeling guilty for turning down the first request makes people more likely to agree to a second, smaller request. In addition to reciprocity, the need to appear consistent may drive a considerable amount of compliance. The foot-in-the-doorfoot-in-the-doorA persuasive technique in which compliance with a small request is followed by compliance with a larger request that might otherwise have been rejected.            foot-in-the-door        A persuasive technique in which compliance with a small request is followed by compliance with a larger request that might otherwise have been rejected.             technique, in which a small request is followed up by a larger request, is especially effective in gaining compliance. Liking an organization on Facebook (a small request) can lead to more purchases and brand loyalty (larger requests). Another technique based on the need to appear consistent is the low-ballinglow-ballingMaking further requests of a person who has already committed to a course of action.            low-balling        Making further requests of a person who has already committed to a course of action.             strategy, in which an initially favorable deal is revised upward once the buyer appears committed (Cialdini, Cacioppo, Bassett, & Miller, 1978). Once you have made a commitment to buy a car, the salesperson leaves to check the deal with a supervisor and invariably returns with a few more charges to make the deal work. The salesperson is counting on you to agree to the higher price because you have already made a public commitment to buy the car. If you stand up at this point and prepare to leave, it is likely that those extra charges will quickly disappear. According to the foot-in-the-door technique, agreeing with a small request, such as clicking the Like button on Facebook, makes it more likely that you will agree with larger requests, such as buying products from a company. ObedienceObedienceCompliance with a request from an authority figure            Obedience        Compliance with a request from an authority figure             is defined as compliance with the request of an authority figure. When you take an exam from a professor or show your driver’s license to a police officer, you are being obedient. The power of authorities to gain obedience is probably due to our perception of their legitimacy. Men dressed in normal attire had less influence than men dressed as security guards when making simple requests of passersby, even though the requests (Pick up that bag or Give that man a dime for the parking meter) had nothing to do with their perceived roles (Bickman, 1974). In a series of classic experiments, Stanley Milgram attempted to demonstrate the power of authority over personal ethics (Milgram, 1974). Milgram’s participants entered his laboratory at Yale University believing that they were to take part in a study of the effects of punishment on learning. The other participant present was a confederate, and the situation was rigged so that the real participant would take the role of teacher while the confederate took the role of learner. The punishment for failing to learn pairs of words was an electric shock, to be administered by the teacher to the learner using an intimidating piece of equipment. The learner or confederate received no actual shocks; his verbal responses to shocks were prerecorded. In most cases, Milgram’s experiment began uneventfully. However, the first challenge to the teacher occurred at 75 volts, at which time the learner’s first recorded protest could be heard. When faced with this situation, many of Milgram’s participants looked to the experimenter in the room for guidance. The experimenter assured the participant that the experiment must continue and you have no choice. Although most participants showed signs of stress and verbally stated their concerns about the procedure, all of Milgram’s original 40 participants went as far as 300 volts, and 26 out of the 40 (65%) fully complied by administering the maximum shock (450 volts). Milgram followed up his original scenario by modifying variables that might have influenced the large number of people willing to obey. No differences in outcome were observed when the experiment was conducted in an off-campus office or with participants of different ages and occupations. Among the variables that did seem to influence Milgram’s results was proximity, both between the teacher and the learner and between the teacher and the experimenter. The proximity between the teacher and the learner ranged from complete separation, in which the two people could neither see nor hear each other, to closeness, in which the teacher was instructed to physically hold the learner’s hand on a shock grid. As proximity between the teacher and the learner increased, rates of obedience dropped to about 30% (see Figure 13.13). After his initial study, in which the teacher could hear but not see the learner, Milgram varied the experimental circumstances to see what variables influenced the amount of obedience that his participants displayed. The teacher, learner together condition took place with both people in the same room. In the teacher touches learner condition, the teacher had to hold the learner’s hand down on a shock device. As the teacher and learner came into more contact, obedience dropped, but it remained surprisingly high overall. The implications of this result for modern war technologies are clear. People are more likely to operate a drone that harms a person whom they can neither see nor hear than they are to harm a person whose pain is apparent. The proximity of the teacher and the experimenter also was quite influential. The experimenter communicated with the teacher while in the room, by telephone, or by tape recording. Obedience dropped dramatically as interaction with the experimenter, who represented authority, became more remote. Obedience varies across cultures, but no one is immune. Milgram’s data suggest that about 65% of the American population, with equal numbers of men and women, would fully comply in his basic experiment (Milgram, 1963). The highest rates of obedience are found in the general public of the Netherlands (92%;  Meeus & Raaijmakers, 1995), Austria (80%; Schurz, 1985), and Germany (85%; Mantell, 1971). Australia reports some of the lowest levels of obedience, along with one of the few notable gender differences reported in the obedience literature—40% of Australian male students were fully obedient, but only 16% of Australian female students were fully obedient (Kilham & Mann, 1974). Milgram’s original goal was to illuminate some of the most troubling behavior of modern times, particularly the killing of millions of innocent people in the Holocaust, so it is no surprise that his results are among the most disturbing in modern psychology. Milgram clearly demonstrated that typical people are willing to harm others under the right sets of circumstances. But did he meet his goal of modeling Nazi obedience? He believed so. You may be more comfortable thinking that people who could carry out genocide are different from the students sitting next to you in class. But Milgram’s findings suggest otherwise:Ordinary people, simply doing their jobs, and without any particular hostility on their part, can become agents in a terrible destructive process. Moreover, even when the destructive effects of their work become patently clear, and they are asked to carry out actions incompatible with fundamental standards of morality, relatively few people have the resources needed to resist authority. (Milgram, 1974, p. 6)You might be thinking optimistically that people today are different from Milgram’s participants in the 1960s. Between the 1920s and the 1980s, parents in the United States shifted their preferences for their children’s outcomes from obedience to autonomy (Alwin, 1988). We can begin to evaluate the impact of this parenting change because Milgram’s experiment has been partially replicated fairly recently by Burger (Burger, 2009). By partially replicated, we mean that Burger made procedural changes for the purposes of meeting ethical requirements, such as informing participants that they could leave if they felt uncomfortable. Despite these changes, Burger’s results were similar to Milgram’s (82.5% of Milgram’s participants obeyed under the same circumstances in which 66.7% of Burger’s male participants obeyed). Burger’s participants were told explicitly that they could leave at any time, which raises questions about previous explanations of Milgram’s results in terms of coercion (Griggs, 2017). An alternative view of the reasons behind the behavior of Milgram’s participants suggests that something called engaged followership is more important than coercion (Haslam, Reicher, Millard, & McDonald, 2015). In engaged followership, people are willing to do unpleasant things, so long as they agree with the objectives. Leaders, like Milgram’s experimenter, promote followers’ identification with their objectives, which makes even the most unpleasant tasks seem virtuous. This in turn reduces the stress experienced by the followers. This model seems to present a clearer picture of Milgram’s participants, the Nazis that he wanted to understand, and fanatic followers of many types. If you believe in your cause, there aren’t too many things you wouldn’t do to reach your goals. No society can exist without obedience. However, history is full of instances in which blindly following orders leads people into highly unethical behavior. Even in more commonplace situations, obedience to authority can overwhelm individual judgment. In one chilling obedience experiment, 22 nurses were telephoned by a doctor they didn’t know and instructed to administer a drug not listed on a patient’s chart in double its normal dose (Hofling, Brotzman, Dalrymple, Graves, & Pierce, 1966). Astonishingly, 21 of the 22 nurses indicated that they would be willing to obey the order. We could argue that it is desirable for nurses to carry out the directions of physicians, but at what point do we want people to question authority? Our discussion of conformity, compliance, and obedience would be incomplete without considering the circumstances in which an individual or small group can influence a larger group. Asch provided a clue to the necessary conditions when he noted that conformity was greatest when a person faced a unanimous group. A single individual’s willingness to take an opposing view may encourage others to also resist the pressure to conform to a misguided action, or at least to reconsider their original position. To successfully influence the majority, a minority voice must display consistency and confidence. In a study designed to determine the factors that enabled a minority to influence a majority, people occasionally agreed with an individual who consistently stated that blue slides were green, but never agreed with an individual who judged blue to be green only part of the time (Moscovici, Lage, & Naffrechoux, 1969). Individual voices that are raised confidently, as in the case of Mahatma Gandhi or Martin Luther King, Jr., are more likely to influence the majority than are more tentative efforts. The anonymous Tank Man of the 1989 Tiananmen Square protests in Beijing, China, created a minutes-long standoff by maintaining his position in front of a column of tanks. Single individuals disagreeing with a majority with confidence and consistency can stimulate others to reconsider their positions. We do not know the fate of Tank Man today. We have explored many instances in which situations seem to have a large impact on people’s behavior. Given the sociability of our species, it should not be surprising to learn that one of the most powerful situational influences on our behavior is the presence of other people. How is our behavior different when we are with others compared to when we are alone? Social facilitationSocial facilitationA situation in which the presence of other people changes performance            Social facilitation        A situation in which the presence of other people changes performance             occurs when the presence of other people changes individual performance. In 1898, Norman Triplett reported that cyclists riding in pairs rode faster than cyclists riding alone. Social facilitation is not limited to cycling. Children spinning fishing reels did so faster when in groups than when they were alone. Because of social facilitation, a well-practiced skill (like pedaling by these professional cyclists) is enhanced by the presence of others. In contrast to Triplett’s observations, we have all seen skilled performers choke in front of an important audience. Why does performance improve in front of audiences in some situations and become worse in others? Psychologists noticed that well-practiced skills, like riding a bicycle, improved with an audience, but that when people were still learning a new skill, they performed badly (Zajonc, 1968;  Zajonc & Sales, 1966). When we’re performing a well-practiced skill, most of what we do is correct, but when we are learning something new, most of the things we do are wrong. The presence of the audience appears to enhance whichever patterns of behavior are dominant—the right moves in a well-practiced skill and the wrong moves in a new skill. The moral of this story is to make sure that you practice thoroughly before you perform in front of an audience. The complexity of a task also can interact with the presence of an audience to produce either improved or impaired performance (Bond & Titus, 1983). This result implies that one of the functions of the audience is to increase the performer’s level of arousal. The Yerkes–Dodson law, discussed in Chapter 7, states that performance on simple tasks improves steadily with arousal, but that performance on complex tasks first improves and then becomes impaired as arousal continues to grow (Yerkes & Dodson, 1908). Improvements in performance are more likely to occur when people are engaged in simple, well-practiced activities, such as riding a bicycle, as opposed to highly complex, novel activities, such as memorizing a difficult list of words. While working on group projects, you have probably had opportunities to observe social loafingsocial loafingReduced motivation and effort shown by individuals working in a group.            social loafing        Reduced motivation and effort shown by individuals working in a group.            , or the reduced motivation and effort shown by individuals working in a group as opposed to working alone. When engaged in a tug-of-war game at a picnic, members of a team exert less individual effort with each additional person added to their side (Kravitz & Martin, 1986). Individuals cheer less loudly in a large group than they do in a smaller group (Latané, Williams, & Harkins, 1979). Because of social loafing, we would expect the cheering by each of these fans in a nearly empty stadium to be louder than the cheering produced by individual fans in a packed stadium. Individual, gender, cultural, and task variables interact to produce social loafing. Women are somewhat less likely to demonstrate social loafing than men, and people from Eastern cultures are less likely to loaf than those from the West (Karau & Williams, 1993). Simple physical tasks, such as screwing and unscrewing nuts and bolts, reliably produce social loafing in most people (Petty, Cacioppo, & Kasmer, 1988). Task complexity interacts with a need for cognition, or the motivation to engage in and enjoy effortful cognitive endeavors (Cacioppo, Petty, & Kao, 1984). People with a low need for cognition showed social loafing during both a physical task and a brainstorming task in which they were asked to generate a list of possible uses for an object (Cacioppo, Petty, Feinstein, & Jarvis, 1996). In contrast, individuals who demonstrate a high need for cognition take pleasure in difficult mental tasks, and although they showed social loafing on the simple physical task, they did not socially loaf during the more intellectually demanding brainstorming task. Individuals appear less likely to take the opportunity to socially loaf on tasks that they find intrinsically enjoyable to perform. If you participate in online discussions, you might notice a difference in the way that people relate on platforms such as Facebook compared to the anonymous platforms in some message boards and chat rooms. Deindividuation predicts that anonymous message boards would feature less inhibition, such as less civil discussion, than would platforms where individual identity is obvious. DeindividuationDeindividuationImmersion of an individual within a group, leading to anonymity            Deindividuation        Immersion of an individual within a group, leading to anonymity             refers to the immersion of the individual within a group, which makes the individual relatively anonymous. Deindividuation can lead normally law-abiding people to commit uncharacteristic atrocities, including riots and lynchings. Why would people go along with the negative behavior of a surrounding group, especially if they are unlikely to behave this way on their own? One possibility is that anonymous people feel less accountable for their actions. In an experiment using a paradigm similar to Milgram’s obedience experiments described earlier, participants dressed in hoods and oversized lab coats administered longer shocks than control participants who were dressed in normal clothing with name tags (Zimbardo, 1969). Other psychologists suggest that you do not necessarily lose yourself in a group, but that your attention shifts from a personal identity to the identity of the surrounding group (Lea & Spears, 1991). If that surrounding group begins to act in antisocial ways, the personal controls normally present within you might not be enough to resist going along. Common sense suggests that discussion with others who express different opinions should produce more moderate attitudes for everyone in the group. Surprisingly, this is not always the case. In group polarizationgroup polarizationThe intensifying of an attitude following discussion            group polarization        The intensifying of an attitude following discussion            , a period of discussion pushes group members to take more extreme positions in the direction that they were already inclined to prefer (Moscovici & Zavalloni, 1969). Group polarization does not reverse the direction of attitudes, but rather accentuates the attitudes held at the outset (see Figure 13.14). If group members began mildly favoring gun control, their attitudes will be more favorable after discussion, whereas if group members began with a mildly unfavorable position on gun control, their attitudes will be more unfavorable after discussion. We like to think that we can come together and talk out our differences, but that does not always happen. Discussion often moves individuals to take more extreme versions of their original positions. After individually rating French leader Charles de Gaulle and Americans in general (both of which were rated somewhat positively), groups of four French college students were asked to reach consensus on the ratings and then provide a second set of individual ratings. Following the group discussion leading to consensual ratings, the students’ individual ratings were more extreme (more positive, in this example) than their first ratings had been. Two pressures appear to push individuals to take more extreme positions following a group discussion. First, conformity and desire for affiliation contribute to group polarization. If the majority of a group is leaning in a particular direction, what could be a better way of fitting in than agreeing with that majority, and maybe even taking its argument one step farther? There is also a tendency for like-minded people to affiliate with one another, which can provide reinforcement for existing opinions, increase people’s confidence in those opinions, lead to the discovery of new reasons for those opinions and counterarguments to opposing views, and reduce exposure to conflicting ideas. Second, exposure to discussion on a topic introduces new reasons for holding an attitude. If you are already opposed to gun control and you listen to additional arguments supporting your position, you might end up more opposed than you were originally. Group polarization is frequently studied in the context of the behaviors of juries. Following deliberations, final sentencing decisions by mock juries are typically more severe or more lenient than the original opinions expressed by individual group members (Bray & Noble, 1978). Group polarization appears to be even more powerful in online discussions, where people are exposed to many more arguments on issues than they might generate on their own (Himelboim, McCreery, & Smith, 2013;  Sia, Tan, & Wei, 2002). Deindividuation, or the loss of personal identity within a crowd, can lead people to do things that they would not do on their own. As we discovered in our discussion of conformity, the presence of others can influence the process of decision making. In Asch’s experiments, individuals appeared to make a choice in favor of group harmony over accuracy. If group members suppress dissenting opinions in the interests of group cohesion, the group may fall into a state of groupthinkgroupthinkA type of flawed decision making in which a group does not question its decisions critically            groupthink        A type of flawed decision making in which a group does not question its decisions critically            . This type of flawed decision making might account for a number of unfortunate group decisions throughout history (Janis, 1972). Among the examples most frequently described as results of groupthink are the Bay of Pigs invasion, the Korean and Vietnam Wars, and the Cuban missile crisis (Rose, 2011). These examples are notable because of their disastrous results, but they may not be representative of groupthink outcomes. For example, many terrorist organizations meet the criteria stipulated by  Janis (1972) for groupthink, and yet they have arguably met a number of their short-term objectives, including the terrorist attacks of 9/11 (Rose, 2011). Smart, experienced groups of people can make some not-so-smart decisions because of groupthink. In 2001, Yahoo! had the opportunity to buy a company called Google, started by Stanford University graduates Sergey Brin (left) and Larry Page (right). The Yahoo! executives hesitated, and the opportunity passed—a decision that they probably regret today. Groupthink is especially likely in cohesive groups with high morale whose members already share similar attitudes (Baron, 2005). Members of cohesive groups are less likely to consider risks or alternatives associated with their chosen plan of action. In many cases, the group is far from unanimous, but individuals are unwilling to step forward in dissent, just as many of Asch’s participants preferred to go along rather than rock the boat. This illusion of a unanimous group further stifles serious consideration of alternatives before making a decision. Part of our heritage as a social species involves the need not only to affiliate with a group, but also to form important social relationships with individual people. These relationships can be between family members, peers, coworkers, neighbors, or romantic partners. Psychologists have discovered a number of underlying themes that affect this important part of life. Attraction refers to our attitudes about other people and can vary along a continuum from strong liking to strong disliking. It varies in quality and strength, which in turn helps to determine the type of relationship that we form with another person. We are attracted to our romantic partners in stronger and more physical ways than we are attracted to our friends and acquaintances. We know what attraction to another person feels like, but it is more difficult to identify the variables that predict our attraction to a particular person. Before you can be attracted to a person, you need to have some contact with that person. Contact becomes more likely when people work, live, or spend leisure time in proximity to one another. In an early exploration of this concept, 5,000 marriage licenses in which one or both partners lived in Philadelphia were examined (Bossard, 1932). In about one third of the marriages, the partners had lived five blocks or less from each other before marrying. As distance between homes increased, the number of couples decreased. In the contemporary era of online dating, proximity can take forms other than physical closeness. Musicians recognize the advantages of the mere exposure effect. Hearing a song frequently makes it more likeable. Placement of a song in a television show or movie is virtually guaranteed to boost sales of the artist’s work. Proximity is important to relationships because simple contact can increase liking. In contrast to the proverb Familiarity breeds contempt, regular exposure to neutral or positive stimuli generally leads to increased liking, a principle known as the mere exposure effectmere exposure effectA situation in which repeated exposure increases liking            mere exposure effect        A situation in which repeated exposure increases liking             (see Figure 13.15). In one example of this effect, people who repeatedly viewed nonsense words described as Turkish words and Chinese-like symbols reported liking them better than those that they were seeing for the first time (Zajonc, 1968). The mere exposure effect applies to people too (Swap, 1977). From an evolutionary point of view, the mere exposure effect might have led people to avoid unfamiliar, and possibly dangerous, stimuli (Young & Claypool, 2010). Unfortunately, having an inclination to make more negative responses to unfamiliar people also might contribute to prejudice. More frequent exposure to these nonsense Turkish adjectives resulted in higher ratings among American college students on their goodness of meaning. To what extent do similarities and differences influence our friendships and romantic relationships? In one experiment, college students read the results of an attitude survey supposedly filled out by another student (Byrne, 1961). As the attitudes expressed became closer to the students’ attitudes, the students reported that the anonymous person was better liked, more intelligent, and better informed about current events. Apparently, assuming that we like ourselves, we find it easy to like people with whom we are similar. People tend to choose friends and romantic partners who are similar in race, ethnicity, religion, values, education, and age (Buss, 1985;  Kandel, 1978). However, the number of interracial couples in the United States has grown dramatically since 1960 (Joyner & Kao, 2005;  Lee, 2010). Although people may be initially attracted to those with whom they share similar characteristics, being attracted to another person also increases similarity, a process known as attitude alignment. Attitudes of dating partners grow closer together in a gradual process (Davis & Rusbult, 2001). At the same time, partners in stable relationships often overestimate the similarity of their attitudes (Murray, Holmes, Gellavia, Griffin, & Dolderman, 2002). Most long-term couples share an approximately equal level of attractiveness and may even have similar features or physique, as do Sting and Trudie Styler. Physical appearance is a dominant factor in romantic attraction. After completing questionnaires assessing personality and academic achievement, study participants were randomly paired for a computer dance. Male participants were subsequently asked whether they would ask their partner for a second date. The only significant variable out of all the personality and other data collected that predicted the likelihood of a second date was the woman’s physical attractiveness (Walster, Aronson, & Abrahams, 1966). Cultures frequently have their own definitions of physical beauty, but we appear to have some underlying, biological preferences. Beauty serves as a marker for health and fertility. By choosing a beautiful mate, you may be enhancing your abilities to produce healthy children with a strong chance of survival. Preferences for faces rated as beautiful by adults can be demonstrated in infants as young as 3 months of age (Langlois, Roggman, & Rieser-Danner, 1990). Subjective judgments of beauty are heavily influenced by symmetry, or the degree of similarity between one half of the body or face and the other half. Symmetrical bodies are more likely to be healthy, so they are highly sought in prospective mates (Thornhill & Gangestad, 1994). As further evidence of the importance of symmetry in mate choice, symmetry of faces was more important to heterosexual research participants in opposite-sex judgments of attractiveness than in same-sex judgments of attractiveness (Little, Jones, DeBruine, & Feinberg, 2008). People show strong preferences for symmetrical faces (bottom row) compared to the less symmetrical versions of the same faces (top row). Symmetry may serve as an outward sign of genetic health. Most romantic couples share a common level of physical attractiveness, a concept known as the matching hypothesis. Couples who were most similar in terms of physical attractiveness were more likely to be together 9 months later (White, 1980). However, when longer-lasting heterosexual relationships are examined, female partners tend to be rated as slightly more attractive than their male partners (McNulty, Neff, & Karney, 2008). Self-disclosure, or sharing personal information about yourself, follows a regular progression as people become more intimate with one another (Taylor & Altman, 1987). Acquaintances talk about general topics other than themselves. New friends might discuss their personal abilities and attitudes, whereas close friends move on to consider dreams and goals. Intimate friends might share fears and negative past behaviors. Intimacy is highest when falling in love, but what does love look like under the microscope? Robert Sternberg’s triangular model of love distinguishes the building of different types of relationships along dimensions of intimacy, passion, and commitment (Sternberg, 2004). Intimacy refers to the closeness or bonding that we experience with another person, passion includes sexual attraction and romance, and commitment describes the intent to maintain the relationship over time. Intimacy without passion or commitment is characteristic of most friendships. Passion by itself characterizes an infatuation, or love at first sight. Commitment alone could describe the situation in which a person remains loyal to a partner whose health makes intimacy and passion no longer possible, as in the case of a partner with dementia (see Figure 13.16). According to Sternberg, close relationships vary along the dimensions of intimacy, passion, and commitment. Consummate love combines all three. Combinations of intimacy and passion define romantic love. Without the addition of commitment, this type of love can fade quickly. Intimacy and commitment occur together as companionate love. This love exists without passion in deep friendships, family relationships, and long-term partnerships in which sexuality no longer plays a significant role. A combination of passion and commitment can occur in an impulsive whirlwind relationship, in which insufficient time has elapsed for the development of true intimacy. Finally, people who are lucky enough to find all three components in the same relationship experience consummate love. Type of LoveCombinesExcludesRomantic love Intimacy + passionCommitmentCompanionate loveIntimacy + commitment. PassionConsummate loveIntimacy + commitment + passionNothingWhat factors keep people together? Some strategies seem particularly important to many types of relationships, including friendships, family relationships, and romantic relationships. People continue to use openness and self-disclosure as relationships mature, along with participation in joint activities, provision of reassurance, and communication (Berscheid, 2004;  Canary, Stafford, Hause, & Wallace, 1993). Research by Gottman and Gottman (Gottman & Gottman, 2017) describe how turning toward a bid for attention offered by a partner rather than turning away or against is predictive of happier relationships. For example, one person might bid for a response from a partner by commenting on something of personal interest, such as Look at that beautiful dog! The partner can turn toward the bid (Yes, I agree. That’s a really pretty dog, but our dog is even prettier!), away from the bid (ignoring it), or against the bid (Don’t bother me now. I’m reading!). The researchers found that couples divorced after six years of marriage turned toward bids only 33% of the time, compared to 86% of the time for couples still married. In research by Gottman and Gottman (2017), successful couples are much more likely to turn toward a bid made by a partner, such as interest in the Super Bowl. Less successful couples are more likely to turn away from (don’t bother me) or against (I hate football) a partner’s bid. Maintaining a relationship may become easier over time. As couples spend more time together, they build a shared history, which provides them with a stronger basis for understanding each other. Couples who have been together for many years appear to have the ability to communicate a thought or emotion to each other with a single look (Schober & Carstensen, 2010). Long-term relationships can become boring, however. Couples who continue to engage in novel and exciting activities have higher relationship satisfaction (Aron, Norman, Aron, McKenna, & Heyman, 2000). As you are no doubt well aware, real life does not typically follow the scripts of fairy tales, in which the lovers ride off into the sunset to live happily ever after. Maintaining relationships takes time and effort, and sometimes people are wrong for one another. A sense of inequity or unfairness in a relationship may cause it to end. Equity in a relationship suggests that costs, such as curtailed freedom and less time to oneself, are offset by benefits, such as companionship and security. In general, more equitable romantic relationships are stable, enjoyable, and sexually intimate. Most couples experience their share of ups and downs, but the relative number of their positive and negative experiences is related to the likelihood that the relationship will survive or end (Gottman, Swanson, & Swanson, 2002). Couples appear to need about five positive interactions to offset one negative interaction. As the ratio of positive to negative interactions slips toward 1:1, the likelihood that the relationship will end increases. Couples with high levels of apparent conflict might still stay together, so long as their conflict is offset by a larger amount of positive times together. Infidelity often contributes to the ending of a romantic relationship. Data describing how frequently romantic partners cheat on each other are difficult to obtain and show wide variations, as in the 33%–75% of men and 26%–70% of women reported to cheat on their partners in one review of the literature (Buss & Schmitt, 1993). Men and women also appear to respond to different aspects of cheating behavior (see Figure 13.17). Males become jealous of sexual infidelity, whereas females are more distressed by emotional infidelity (Buss, Larsen, & Westen, 1992). In other words, a woman would be less distressed if her partner had a one-night stand while drunk than if he had an ongoing relationship with a mistress whom he supported financially. In any case, relationship jealousy can have dangerous implications. More than one-third of female homicides worldwide are committed by intimate partners (Stöckl et al., 2013). When asked which would cause greater distress, your partner cheating on you sexually or becoming deeply emotionally involved with a rival, men reported greater distress from sexual infidelity, whereas women reported greater distress from emotional infidelity. Our social interactions with one another can feature cooperation, or working together toward common goals, or competition, where we struggle with one another to obtain limited resources. Both cooperation and competition have a long history in shaping our behavior. In Chapter 3, we described natural selection as an ongoing race among organisms competing for scarce resources and opportunities to reproduce (Darwin, 1859). The stakes in this race are high. Individuals with characteristics that give them an advantage over others are able to pass their genes along, while losers fail to reproduce and die. At the same time, because humans do not live alone, our survival depends on cooperation. Individual animals frequently compete for food or opportunities to mate. Beginning with observations of pecking order among chickens (Schjedlderup-Ebbe, 1975), biologists have cataloged many species that use competition, usually in the form of fighting, to determine the allocation of mating opportunities, food, and other resources both within and between groups. The advantages of cooperation can be illustrated by a real-life tale of two 1864 shipwrecks in the South Pacific. Members of one crew worked cooperatively with one another as equals, while members of the other maintained their formal command structure. After 1 year, the first crew managed to build a boat and get rescued, while the second was decimated by cold, hunger, and cannibalism (Druett, 2007). Too much competition among individual members hampers the cohesion of a group, breaking it into factions and possibly resulting in injuries among its members. Consequently, many animals minimize the extent of within-group conflict by displaying clear signals of dominance, or high status. The mandrill, a type of baboon, uses facial expressions and gestures to signify its position in a dominance hierarchy. The coloring of the mandrill’s face, rump, and genitalia is also reflective of individual fighting abilities (Setchell & Wickings, 2005). Do humans have analogous signals of dominance? It appears that the male of the species does (Mazur & Mueller, 1996). Dominant human male faces are handsome and muscular, featuring strong chins, deep-set eyes, and prominent brows. In contrast, baby-faced males are rated by observers as being more submissive (McArthur & Berry, 1987). Humans have the advantage of augmenting their natural signals of status by purchasing and displaying expensive automobiles, homes, and jewelry. Animals minimize the damage caused by competing for dominance by displaying clear signals of submission. The dog on its back is letting the other dog know that its dominance has been recognized. Individuals vary in their predisposition to cooperation or competition. In one experiment, groups of participants were given the opportunity to compete (by keeping tokens) or cooperate (by sharing tokens) while they observed the decisions made by their group members (Kurzban & Houser, 2005). Three types of dispositions emerged: 17% of the participants were cooperators, who gave their tokens freely without paying much attention to the behavior of other group members; 20% were free riders, who didn’t give up any of their tokens; and 63% of the participants were reciprocators, who cooperated only after seeing others do so. Even though cooperators are in the minority, their behavior is important to group survival. By contributing selflessly, they encourage the majority of the group to cooperate too. Members of successful groups are more likely to survive and reproduce than members of unsuccessful groups. Because it is in our best interest to belong to effective groups, we cooperate with other group members and are quick to conform to their norms and judgments. Cooperation becomes ingrained in the group’s culture and is passed along to new members. Why is cooperation so essential to successful group behavior? Without cooperation, there would be no basis for gathering diverse areas of individual expertise into a meaningful, useful whole. Under the influence of culture, humans have progressed from simple divisions of labor to highly specialized areas of expertise. Individuals might spend their entire academic careers exploring the structure of the frog retina or the surface characteristics of one of Saturn’s moons. At the same time, no single person working alone would be able to put humans on the Moon or produce all the special effects expected by contemporary movie audiences. Few fans would pay to see an individual football player demonstrate his skills on an otherwise empty field on Sunday afternoon. Cooperation is the glue that holds productive groups together, from the operating room of a hospital to the baseball diamond. The importance of group membership to survival is reflected in the remarkable loyalty that humans show to their culturally defined groups. Fans of a particular sports team may be every bit as loyal to their group (and as potentially violent toward others) as our early hunter–gatherer ancestors were to their tribe or clan. Unfortunately, the same loyalty and cultural distinctiveness that bind us to our groups may lead us into conflict with other groups, as discussed later in this chapter. Cooperation is essential to successful group behavior. How do people decide when to compete and when to cooperate? A classic model of competition and cooperation within groups is known as the prisoner’s dilemma (Poundstone, 1992). Assume that you and another person have been arrested for a crime, but that the police have limited evidence against you. Without being able to communicate with your partner, you are given the choice to confess (defect from your partner) or to remain silent (cooperate with your partner). If you both remain silent, you will each be sentenced to 1 year. If you both confess, you will each receive 10 years. However, if one confesses and the other remains silent, the prisoner who confesses will be set free, and the silent prisoner will receive a 20-year sentence. The dilemma arises because the best individual strategy is to confess or defect from your partner, but if both of you figure this out and subsequently confess, your outcome is worse than if you both had remained silent (see Figure 13.18). In the prisoner’s dilemma, cooperation is moderately rewarded (a 1-year sentence), while defection is punished (10 years). If only one of you confesses, however, the confessor is highly rewarded (freedom), while the other person is severely punished (20 years). For real people, computers, and even animals, the most successful strategy in the prisoner’s dilemma situation is the tit-for-tat (TFT) strategy (Axelrod & Hamilton, 1981). Using the TFT strategy, you make cooperation your first move and then repeat your partner’s successive moves. The success of the TFT strategy has been attributed to three aspects: It is nice (you begin with cooperation), it is able to retaliate and punish uncooperative behavior (defection by your partner is followed by your own defection), and it is forgiving (you return immediately to cooperation following cooperation on the part of your partner (Brembs, 1996). The research on individual differences in cooperation discussed earlier found that most human participants were reciprocators, whose behavior is most similar to the TFT strategy (Kurzban & Houser, 2005). The prevalence of this type might reflect its typical success. If most people are reciprocators, how can we explain the existence of the other types? Free riders clearly benefit from their selfish behavior, but what advantages might increase the chances of the cooperators’ survival and reproductive success? It is easier to be generous when you are well off, so acting like a cooperator might make people believe that you have extensive resources, improving your chances of attracting a mate (Milinski, Semmann, & Jrambeck, 2002). A person’s reputation is also a powerful motivator for cooperation. In ongoing prisoner’s dilemma games, participants use other people’s record of cooperation (or lack thereof) to determine whether to cooperate with them (Milinski et al., 2002). In small hunter–gatherer bands or basketball teams, having a reputation for selfishness is not likely to make you successful. Culture can shape a person’s choice of competition and cooperation. When two people from different countries played the prisoner’s dilemma game, less cooperation and more competition occurred than when two people from the same country participated together (Matsumoto & Hwang, 2011). Furthermore, the bigger the difference between the native countries of the participants along the dimension of individualism or collectivism, the less cooperation occurred. This finding reflects the in-group bias mentioned earlier in the chapter. Contributing to our cooperative abilities is a deep-seated, possibly biological sense of fair play that is shared with other species in the animal kingdom. Capuchin monkeys will happily trade pebbles for slices of cucumber until they see another monkey receiving the more preferred grapes for its pebbles. After seeing the unfairness of their situation, they are no longer willing to work for cucumber slices (Brosnan & De Waal, 2003). When people playing a prisoner’s dilemma game were observed with fMRI, viewing the faces of cooperators produced activation in brain areas normally linked to reward (T.  Singer, Kiebel, Winston, Dolan, & Frith, 2004). In contrast, when people were treated unfairly, activation occurred in parts of the brain associated with anger, disgust, and pain (Sanfey, Rilling, Aronson, Nystrom, & Cohen, 2003). A capuchin monkey refuses to exchange a pebble for a slice of cucumber after seeing another monkey receive a more highly valued grape for its pebble, suggesting that nonhuman primates have a sense of fairness. We have all read stories about soldiers, firefighters, and others who have risked or even sacrificed their lives to save the lives of others. These instances are examples of altruism, a helping behavior that either fails to reward or actually harms the person who performs it, which was discussed in Chapter 3. Instances of altruism in which the helper is exposed to risk or experiences harm are often described as heroism (Franco, Blau, & Zimbardo, 2011). The designation of hero is often given to people who are altruistic to the point of risking their lives to help others. In 2007, Wesley Autrey (shown here with his daughter) selflessly risked his life to save a young man who fell onto New York subway tracks after suffering a seizure. If behavior usually leads to improved survival and reproduction, how can we account for altruism? Charles Darwin suggested that [a] tribe including many members who were always ready to give aid to each other and sacrifice themselves for the common good, would be victorious over most other tribes; and this would be natural selection (Darwin, 1871, p. 166). Although this suggestion is attractive, it suffers from a major flaw in logic. The system suggested by Darwin would produce free riders who were not altruistic themselves, but who would benefit from the altruistic behaviors of the members of their group (Dawkins, 1976). These individuals, because they would undoubtedly live to reproduce (as opposed to the altruistic, who might not survive), should eventually swamp the group with selfish genes. Several evolutionary mechanisms have been proposed that account for the continuation of cooperative, altruistic behavior (Nowak, 2006). Although altruistic behaviors may have costs for an individual, they may benefit the altruist’s genetic relatives. Altruistic genes will survive in the population so long as the cost to an altruist is associated with a large-enough benefit to relatives (Hamilton, 1964). Japanese macaque monkeys are more likely to defend close relatives from attack than they are to defend unrelated monkeys (Kurland, 1977). People are more likely to help their relatives than they are strangers, such as in helping to raise their children. Altruism is also maintained by expectations that a favor might someday be returned by those you have helped, a process known as reciprocal altruism (Trivers, 1971). Vampire bats often regurgitate blood to share with bats who did not hunt successfully; bats are more likely to share with those who had shared with them recently (Wilkinson, 1984,  1990). Selfishness might be limited by punishment administered by other group members. Study participants who had the opportunity to punish those who had not cooperated activated reward circuits in the brain, lending credence to the phrase Revenge is sweet (Fehr & Rockenbach, 2004). Unfortunately, the willingness to pay a big price to carry out revenge against those who seem unfair might also lead to irrational acts, such as suicide bombings. However, other instances of altruism do not fit these models. People have been known to adopt unrelated children. Arland Williams, a passenger on an airplane that crashed into the freezing Potomac River in 1982, passed lifelines to other passengers before disappearing beneath the water. These altruistic acts represent psychological altruism, or conscious efforts to help. This type of altruism can be transmitted by a culture through learning (Dawkins, 1976). The role of learning in altruism is supported by observations that the tendency to feel pleasure when helping others is quite low in children and gradually increases into adulthood (Cialdini, Kenrick, & Baumann, 1981). Sensational (though inaccurate) news accounts of the murder of Kitty Genovese stimulated research in bystander intervention, which asked questions about why people seem to help in some situations but not others. If cooperation confers so many benefits to individuals and groups, how can we explain the failure of some individuals to provide help when it is desperately needed? In 1964, a young woman named Kitty Genovese was murdered on a New York City street near her home late at night. Sensational news reports at the time stated that 38 of her neighbors watched for more than half an hour while Kitty struggled with her attacker without intervening or calling the police. Although these reports were not confirmed by later court testimony in the case (Manning, Levine, & Collins, 2007), the public outrage that they stimulated led to a series of experiments into bystander interventionbystander interventionThe study of situational variables related to helping a stranger, most notably the decreased likelihood of helping as the number of bystanders increases            bystander intervention        The study of situational variables related to helping a stranger, most notably the decreased likelihood of helping as the number of bystanders increases            . This research demonstrated that an individual’s likelihood of helping drops as the number of other observers grows. In a typical experiment investigating this phenomenon, college students filled out a questionnaire as their room filled with smoke (Latané & Darley, 1968). The researchers were interested in how frequently and quickly the students reported the smoke. When the college students were alone, nearly 80% sought help within 6 minutes. When three study participants were tested together, however, only 40% reported the emergency within 6 minutes. The effect of the number of observers on the likelihood of intervention is probably influenced by a sense of individual responsibility. When you are the only person available to help, your responsibility is clear. When a crowd of people is gathered around the scene of an accident, it is easier to assume that others have already called 911 or that a physician or nurse who is better qualified to help will be present. People also fail to act because of fears of appearing foolish as a result of misinterpreting a situation. Emergencies sometimes can be ambiguous. What sounds like a case of domestic violence next door might be a loud movie or video game or actors practicing a scene for a play. People who feel anonymous are less likely to help, which is consistent with our previous observations of how anonymity can lead to less socially desirable behavior. Finally, people are more likely to help when it is safe and convenient for them to do so. If you are heading for your car at the end of the day when you notice a classmate with a flat tire, you are likely to lend a hand. However, if you are late for work or a final exam, you may conveniently assume that someone else will take care of the situation as you continue on your way. AggressionAggressionThe conscious intent to harm another.            Aggression        The conscious intent to harm another.             is the conscious intent to harm others, and it can take several forms. Instrumental aggression is the intentional harm, usually physical, done to others to obtain a goal, such as attacking a person to steal a wallet or purse. Relational aggression harms another person’s social standing through behaviors such as ignoring, excluding, and gossiping. Other types of aggression include defensive aggression, in which the person may do harm to others in self-defense; passive aggression, in which people who aren’t comfortable being openly aggressive get what they want under the guise of still trying to please others; and maternal aggression, a rather common phenomenon in the animal world (and occasionally among humans) in which sickly or unwanted offspring are killed. Like most of the behaviors discussed in this textbook, factors leading to human aggression form complex interactions. Efforts to find simple correlations between violence and single variables such as economic growth, population trends, illegal drug trade, availability of guns, and domestic violence versus stranger violence do not tell the whole story. We can capture the flavor of this complexity by examining homicide rates across cultures. The average homicide rate worldwide is about 10 per 100,000 residents per year, or twice the number killed in war (UNODC, 2014). South Africa has a murder rate of 60 homicides per 100,000 residents, compared to Singapore’s rate of 0.2 per 100,000 (UNODC, 2014). Rates in large cities within the United States vary substantially, from 50.3 homicides per 100,000 residents of St. Louis, Missouri, to 0.4 per 100,000 in Honolulu (FBI, 2010). Across all cultures, women are more likely than men to be killed by intimate partners or family members, but rates again vary. Only 24% of female victims in Australia are killed by someone they don’t know, while 66% of female victims in Singapore don’t know their murderer (UNODC, 2014). Is a capacity for aggression simply part of being human? Biological explanations of aggression point to several possible sources of human aggression: genetics, biochemistry, and nervous system structure and activity. Aggression is clearly a disposition that can be selectively bred in animals, as evidenced by the development of the famous fighting bulls of Spain and hyperaggressive laboratory mice (Lagerspetz & Lagerspetz, 1983). Human twin and adoption studies imply that aggressive tendencies in humans are at least partly influenced by genetics (Rushton, Fulker, Neale, Nias, & Eysenck, 1986). As we have seen so frequently in this textbook, genes associated with aggressive behavior interact with nurture or experience. Among children who were abused, those with one version of a gene linked to aggression in animals were likely to behave aggressively, while those with the other form rarely did so, even when the abuse experienced was severe (Craig, 2007;  Fergusson, Boden, Horwood, Miller, & Kennedy, 2011). Both naturally occurring chemicals and recreational drugs can influence aggression. Among the naturally occurring chemicals implicated in aggression are androgens, or male hormones such as testosterone (see Figure 13.19). Prenatal exposure to high levels of androgens, which can occur naturally or because of medications occasionally given to pregnant women, increases the aggressive play of both male and female preschoolers (Reinisch, Ziemba-Davis, & Sanders, 1991). Adult men with higher levels of prenatal testosterone exposure score higher on standardized questionnaires of aggression (Bailey & Hurd, 2005;  Burton, Henninger, Hafetz, & Cofer, 2009). Testosterone levels on the high end of the typical range in teen and adult males are positively correlated with delinquency, drug abuse, and aggression (Dabbs & Morris, 1990). Testosterone levels for both male and female criminals correlate with the violent nature of the crimes for which they were sentenced, as well as the dominance and violence that they demonstrate while in prison (Dabbs, Frady, Carr, & Besch, 1987;  Dabbs & Hargrove, 1997). Testosterone appears to affect aggressive behavior by increasing the sensitivity of the amygdala to threatening stimuli, such as angry faces (Derntl et al., 2009). When feeling threatened, a person might engage in more aggressive behavior as a preemptive strike. Both male and female preschoolers who had been exposed prenatally to higher than normal levels of testosterone were more physically aggressive than their same-sex siblings who experienced normal levels of prenatal testosterone. Serotonin levels are negatively correlated with aggressive behavior. Male vervet monkeys form precise social hierarchies, and higher rank is associated with higher levels of serotonin activity (Raleigh, Brammer, McGuire, Pollack, & Yuwiler, 1992). Monkeys at the bottom of the hierarchy had lower serotonin levels and were much more aggressive than high-status monkeys. Serotonin might reduce aggression by enhancing empathy (Siegel & Crockett, 2013) or by promoting activity in frontal-lobe areas associated with inhibiting impulsive behavior (Heinz, Beck, Meyer-Lindenberg, Sterzer, & Heinz, 2011). Many psychoactive substances affect the likelihood that a person will behave aggressively. For instance, alcohol has been implicated in between 32% and 63% of violent crimes (Giancola, 2013). In addition, many suicides are committed under the influence of alcohol (Sher, 2006). The most likely mode of action for alcohol in stimulating aggression is its general disinhibiting effects on behavior. As discussed in Chapter 4, alcohol silences higher cortical areas responsible for impulse control, often leading to behavior that is normally actively suppressed, including aggression. Keeping in mind the complexity of human aggression, we can point to variations in brain structure and function that are associated with senseless violence. Antisocial, violent behavior is correlated with some types of brain damage, although these findings interact with experience in ways similar to the genetics studies discussed earlier (Davidson, Putnam, & Larson, 2000). The diversity of patterns of brain activity leading to aggression was demonstrated in a study comparing brain activity in murderers who had experienced neglect, poverty, physical abuse, or sexual abuse and murderers who had experienced none of these social deficits (Raine, Stoddard, Bihrle, & Buchsbaum, 1998). The brain activity of the neglected and abused murderers was quite similar to the nonviolent control participants. In contrast, the brain activity of the nonabused murderers was quite different, suggesting that their violent behavior had different roots than in the murderers with a history of child abuse. The nonabused individuals showed an unusually reduced level of activity in the frontal lobes. Given the essential role of the frontal lobes in higher-order judgment and impulse control, this lower level of activity might help account for their inability to control their aggression. These brain images were taken from (a) a person with no history of criminal behavior, (b) a person who was convicted of murder and had a strong history of childhood abuse and neglect, and (c) a person who was convicted of murder but had no history of childhood abuse or neglect. The murderer who had been abused and neglected shows activity that is similar to that of the person without a criminal history. The murderer with no history of abuse or neglect shows little activity in the frontal lobes (toward the top of the image) or in the middle parts of the brain that usually respond to emotional stimuli. Although it is probably safe to say that our human heritage contains a capacity for aggression, the actual expression of aggression is elicited by environmental factors and modified by experience. As discussed in Chapter 8, Albert Bandura provided compelling evidence that children who observe aggression are likely to behave aggressively themselves (Bandura, 1973). In Bandura’s classic demonstration of social learning, preschoolers who observed an adult attack an inflatable Bobo doll mimicked not just the violent actions, but also the yells of Sock him in the nose, Knock him down, and Kick him. Opportunities to observe violence are plentiful. Within the family, children who are disciplined with screaming, slapping, and beating are more likely to be physically aggressive with their peers at school (Patterson, Chamberlain, & Reid, 1982). Even when the parents are violent toward each other, without involving the child directly, the child’s level of violent behavior may be affected. In a study of more than 2,000 violent children and teens, recent exposure to violence in the home was a useful predictor for the child’s violent behavior outside the home (Singer, Miller, Guo, Slovak, & Frierson, 1998). Additional opportunities to observe violence are provided by the media. By the age of 18, the average American has seen an estimated 200,000 acts of violence on television alone (Strasburger, 2007). Statistics show that 61% of television programming includes violence, with only 4% featuring an antiviolence theme (Federman, 1998). Children and youth are exposed to increasingly violent and graphic images and ideas through films, video games, and music lyrics. As discussed in Chapter 2, however, care must be taken to avoid misinterpretation of the research results showing correlations between media exposure and aggression. The human species loses a smaller percentage of the population to war today than in the time of our hunter–gatherer ancestors (LeBlanc, 2003), but we still have a long way to go. As in the case of reducing prejudice, redefining the boundaries of the in-group might be contributing to this improvement. Instead of hunter–gatherer clans of 100 or so individuals fighting with neighboring clans, we have nations of millions that cooperate as units. Increasing economic dependencies among nations further reduces the advantages of between-group violence. Damage to one nation’s critical infrastructure, such as its ability to produce or process oil, is likely to have adverse effects for all nations, including the attacker. Despite this progress, we remain plagued not only by continued war deaths, but also by homicides, domestic violence, rape, bullying, and other destructive aggressive behavior. Psychologists continue to search for effective ways to reduce aggression. There are no simple solutions to the problem of aggression. You might think that we could take advantage of catharsis, or the reduction of an emotion through its expression, but the research results do not support this approach. Catharsis theories of aggression are common but suffer from the same weaknesses shown by catharsis explanations of emotion discussed in Chapter 7. Just as expressing an emotion typically heightens rather than decreasing a feeling, people who engage in aggression experience an increase in aggression. In one typical experiment, participants first were angered by a person who criticized essays they had written (Bushman, 2002). Subsequently, the participants who were given opportunities to hit a punching bag while thinking about the critic administered louder sound blasts to the critic when given the opportunity to retaliate than did participants in control groups. Despite the intuitive appeal of the notion of catharsis, when it comes to aggression, catharsis is generally ineffective. Research using surveillance camera footage shows that third parties often intervene in an effort to deescalate violence (Levine, Taylor, & Best, 2011). Unlike most laboratory studies of bystander intervention, third parties in the filmed incidents were more, not less, likely to intervene as groups of onlookers became larger. Further research should help clarify the exact conditions leading to helping behavior. Another frequent suggestion for reducing aggression that has failed to gain research support is an active teaching approach in which children are rewarded for sensitivity and cooperation. Cross-cultural data show that homicide rates are often high in cultures that feature a loving socialization of children and aversion to interpersonal conflict, such as the Inuit, !Kung Bushmen, and Gebusi of New Zealand, yet very low among cultures where children are explicitly taught to fight, like the Turkana of Kenya (Dyson-Hudson & Dyson-Hudson, 1999; Dyson-Hudson & Dyson-Hudson, 1995). Much of the research discussed so far in this chapter focuses on individual variables, whether they are related to a person’s genetic makeup or socialization. To truly understand aggression, we need to zoom out to see this behavior within the social context. For example, we know that if groups rarely interact, they are more likely to fight than to cooperate. In contrast, if groups interact regularly and anticipate future interactions, they may still compete, but they tend to do so in more prosocial ways. The social environment also can determine whether aggressiveness escalates to actual violence. In a study using surveillance footage of public places, researchers observed actual fighting behavior (Levine, Taylor, & Best, 2011). In many cases, one or more third parties (usually friends of the combatants) intervened to stop the fight. In an interesting contrast with the helping behavior observed in the bystander intervention literature discussed previously, third parties are more likely, not less likely, to intervene in a fight when the number of bystanders is larger. Perhaps the implications of the aggression spreading to include the whole group makes stopping the fight a very high priority for members of the group. Homicides among the Turkana of Kenya are quite rare, even though children are explicitly encouraged to be aggressive toward one another, under the watchful eyes of adults. However, the Turkana are exceedingly aggressive toward their neighbors. Over human history, the trait of aggression has probably been valued when turned against other groups, but not when it is turned inward. The advent of the internet is changing our social interactions right in front of our eyes. Classic research reviewed earlier in this chapter showed that Americans typically married someone who lived within five blocks of their home (Bossard, 1932). The Internet has changed all of that. Between 2005 and 2012, a surprising number of Americans, one out of three, met their spouses online (Cacioppo, Cacioppo, Gonzaga, Ogburn, & VanderWeele, 2013). Despite the comfort level of younger adults with the Internet, 30- to 39-year-olds were more likely to have met their spouse online than were those under 30. Those who met their spouse online were also more educated and had higher incomes, suggesting that the Internet has provided busy people opportunities to socialize. Marriages begun online were slightly but significantly less likely to end in divorce or separation than marriages begun offline (see Figure 13.20). Online marriage partners rated their marriages as more satisfying. Even when factors such as age, education, and income were controlled, because these might account for higher satisfaction by themselves, the online marriages remained more successful. Researchers at eHarmony reported that their now-married clients are much less likely to divorce than couples meeting in more traditional ways, such as through school, family, church, and work. What other factors might account for the success of the online marriages? People who seek a spouse online might differ from those who don’t in important ways. Perhaps they are more motivated to maintain long-term relationships, or they might be more selective in their choices. Self-disclosure differs in the online and offline settings, leading to greater liking for people you meet in the online realm (Cacioppo et al., 2013). Finally, the matching algorithms used by online dating sites might result in people finding more compatible partners than they would through chance meetings in their face-to-face lives. Further research is necessary to fully understand the impact of online meetings on relationship outcomes, but these initial results indicate that many Americans are using online dating with surprising success. In this chapter, you have read about a number of studies that explored the phenomenon of bystander intervention, or the circumstances in which a person is most likely to offer help to others in distress. Bystanders can play a significant role in the perpetration of both traditional bullying and cyberbullying. They can either reinforce the bully, increasing the harm done to the victim, or they can attempt to stop the bullying, which could decrease the harm done to the victim (Bastiaensens et al., 2016) (see Figure 13.21). What determines the path that the bystander chooses? Teens were asked if they would imitate cyberbullying in the presence of bystanders who were reinforcing a bully or defending the victim, when those bystanders were either good friends or acquaintances. In the presence of good friends who were supporting the bully, the teens indicated a greater likelihood of modeling the bullying behavior. The type of bystander (friend or acquaintance) had less impact on whether a teen would defend the victim. Not too surprisingly, given our social nature and dependency on our social groups, having good friends who approve of and participate in cyberbullying is more important to predicting when bystanders will join in than having acquaintances who bully (Bastiaensens et al., 2016). The severity of the bullying also plays a role, as bystanders were more likely to intervene on behalf of a victim when the bullying was perceived as severe (Bastiaensens et al., 2014). Finally, the amount of exposure to cyberbullying experienced by a bystander might contribute indirectly to their participation by lowering empathy (Pabian, Vandebosch, Poels, Van Cleemput, & Bastiaensens, 2016). These empirical results seem particularly relevant to the design of effective interventions aimed at reducing the frequency of cyberbullying. Identifying one cyberbully could lead to interventions aimed at the cyberbully’s social circle as well. Reframing all cyberbullying as severe could increase the likelihood that bystanders would intervene on behalf of a victim. Recognizing that continuing exposure to cyberbullying on the part of bystanders reduces empathy could promote increased efforts at empathy training. What Is It? Influential FactorsCooperation Working together to reach a goalIndividual differencesTFT strategyCultureInnate sense of fair playCompetition A contest for resourcesAvailability of resourcesSignals for dominance and submissionAltruism Helping others without personal gain or with personal costNumber of potential helpers availableRelatedness to person needing helpPunishment for selfishnessExpectations for the future reciprocationConscious desire to helpSense of personal responsibility to helpAggression Conscious intent to harm anotherInteractions between genetics and child maltreatmentUse of alcohol and other drugsTestosterone and serotonin levelsBrain structure and activityObservational learningBe sure you can define these terms and use them correctly.`;\n","export const text = `An interactome among genes implicated in ASD with other cellular proteins confirmed links between ASD and several related conditions and identified new genes that warranted further study. Of all the reasons to study Psychology, Finding ways to improve the lives of people with psychological disorders might be one of the most rewarding. This process begins with understanding the features and possible causes of a disorder. Identifying the causes of psychological disorders requires us to pull together multiple threads leading from the genetic, biological, cognitive, developmental, and social aspects of being human. These causal factors interact with one another in complex ways, so the same disorder might look one way in one individual and quite different in another. We do not know what causes children to develop the symptoms of autism spectrum disorder (ASD), discussed in detail later in this chapter, but at least we know where to start looking. We have cautioned you on several occasions in this textbook to avoid the idea that we have genes for this trait or the next because the reality is more complex. As we zoom in to analyze the genetic contributions to ASD, you will see that we have zoomed way past the traditional array of 46 human chromosomes featured in Chapter 3. Instead, we are looking at a visual representation of something that biologists call an interactome—a map of all interactions among proteins in a cell. In this particular case, the researchers were attempting to identify the relationships among the 539 proteins that formed interactions with proteins produced by 26 genes already implicated in ASD (Sakai et al., 2011). The resulting map not only confirmed links between ASD and several related conditions, but also identified genetic relationships that warranted further study. A self-portrait by one of the authors’ (LF) daughters, who has ASD. We refer to people with autism spectrum disorder instead of autistic person because people are so much more than their disorders. Many individuals with ASD, including author LF’s daughter, enjoy varied interests, including art. The pig races at the local Mid-State Fair have been a favorite source of inspiration. What does an interactome mean for a person with autism? Unfortunately, understanding more about the sources of ASD has not yet led to improved treatments. As mentioned in Chapters 8 and 15, the only effective treatment for ASD today is the use of behavioral therapies. Still, such discoveries have the potential to improve the lives of individuals with ASD down the road or to provide information that eventually will help us prevent further cases. To fully understand the experience of individuals with ASD requires us to zoom out to consider overall development. Psychologists point out that we say a person with autism spectrum disorder instead of an autistic person for good reason. People are not their disorders, and much is happening in each individual’s life that has nothing to do with ASD. We could consider the impact of parents, whose age might have been a risk factor for a child’s ASD, or study their ability to manage their child’s behavior. Zooming even further out, we can investigate the effects of ASD on the social connectedness children with ASD experience as they venture out into the world. In this chapter, you will discover a number of types of psychological disorders, each with a unique history and set of characteristics. The next chapter, Chapter 15, will explore the treatments and therapies that psychologists have developed to address the challenges to well-being posed by these disorders. Mental health experts define a psychological disorder as a syndrome characterized by clinically significant disturbance in an individual’s cognition, emotion regulation, or behavior that reflects a dysfunction in the psychological, biological, or development processes underlying mental functioning (APA, 2013, p. 20). But what does this definition actually mean? Many behaviors considered abnormal are quite similar to normal behaviors. We all know what it feels like to be depressed, but how is that different from having a major depressive disorder? This chapter will help you understand the distinctions, but because those distinctions can be subtle, we also caution you against first-year medical student’s disease, or the sense that you have all the disorders that you are studying. The study of psychological disorders is called abnormal psychologyabnormal psychologyThe study of psychological disorders.            abnormal psychology        The study of psychological disorders.            . It is surprisingly difficult, but necessary, for psychologists to agree on the differences between typical and abnormal behaviors because deciding that an individual’s behaviors and mental processes are abnormal is the first step toward labeling that person with a psychological disorder and providing treatment (see Figure 14.1). Psychological disorders can be understood both categorically, which means that each has its own set of distinct characteristics used for diagnosis, and as part of a continuum from normal behavior to severely disordered behavior. The diagnostic criteria discussed in this chapter guide psychologists in their identification of the types and severity of disorders their patients and clients might have. NORMALMILDMODERATEPSYCHOLOGICAL DISORDER—LESS SEVEREPSYCHOLOGICAL DISORDER—MORE SEVEREGood alertness and positive emotional state. Feeling sad or down temporarily, but not for long. Feeling sad, but a strong positive experience such as a good grade could lift mood. Intense sadness most of the day with some trouble concentrating and some loss of appetite. Extreme sadness all of the time with great trouble concentrating and complete loss of appetite. I’m not getting the grades I want this semester, but I’ll keep trying to do my best. I’m struggling at school this semester. I wish I could study better, or I’ll fail. These bad grades really hurt. This may set me back for a while. I’m really worried. I’m so worried about these grades that my stomach hurts. I don’t know what to do. These bad grades just show what a failure I am at everything. There’s no hope; I’m not doing anything today. Going to classes and studying for the next round of tests. Talking to professors. Going to classes with some trouble studying. Less contact with others. Skipping a few classes and feeling somewhat unmotivated to study. Avoiding contact with professors and classmates. Skipping most classes and unable to maintain eye contact with other people. Strong lack of motivation. Unable to get out of bed, eat, or leave the house. Lack of energy and frequent crying. We can start with the literal meaning of the word abnormal. In Latin, the prefix ab means away from and norma means the rule. In other words, abnormal behavior is literally behavior that is not typical, usual, or regular. This meaning implies a statistical definition of abnormality. By this definition, behaviors that most people do are normal, whereas behaviors that characterize a minority of people are abnormal. This statistical approach has the advantage of being clear (see Figure 14.2). For example, intellectual disability is one of the categories in the Diagnostic and Statistical Manual of Mental Disorders, a handbook for diagnosis that we discuss in more detail later in the chapter. The diagnosis of intellectual disability relies heavily on statistics. A score of 70 or below on a standard IQ test is the typical cutoff for identifying an individual with intellectual disability (APA, 2013). Even in this relatively clear situation, however, the statistical approach represents merely a starting point. A diagnosis of intellectual disability also requires an individual to demonstrate difficulties with adaptive skills, which include factors such as communication, self-care, safety, and use of community resources. The word abnormal literally means away from the rule. Both intellectually gifted individuals, usually defined as having an IQ over 130, and individuals with intellectual disability, usually defined as having an IQ below 70, are equally abnormal in this statistical sense of the word. Academy Award–winning producer Quentin Tarantino (Pulp Fiction, Kill Bill, Django Unchained, and The Hateful Eight) has a reported IQ of 160. Although neither he nor his films are typical, we are unlikely to refer to Tarantino as abnormal. Statistics alone do not capture what we mean by abnormal behavior. A purely statistical approach has several major drawbacks. First, it implies a cultural specificity that we would like to avoid. Because most members of one culture may behave in ways different from those of people in another, the purely statistical approach to abnormality might result in different definitions of psychological disorder from one culture to the next. Second, a statistical definition fails to capture the distress that often accompanies a psychological disorder, both for the person experiencing the disorder and for those interacting with that person. Our tolerance for people engaging in acts that are harmful to themselves is variable. For example, most states in the United States have prohibitions against committing suicide (the irony in prosecuting under such laws appears to have been lost on the legislators). To address these concerns, we might suggest that abnormal behavior deviates from some ideal manner of behaving. This approach succeeds in capturing the general distress that most of us experience when observing or experiencing abnormal behavior. Unfortunately, this approach does nothing to avoid cross-cultural differences in defining disordered behavior. Different cultures are often characterized by diverse ideals. In Western nations, hearing voices that others cannot hear is generally considered abnormal. In other cultures, hearing voices may be viewed more positively, as a sign of religious giftedness (Al-Issa, 1977;  Kurihara, Kato, Sakamoto, Reverger, & Kitamura, 2000). More troublesome is the simple fact that it is difficult, if not impossible, to reach consensus regarding what constitutes ideal behavior. People, including experts in psychology, have deeply divided value judgments regarding sexual conduct, recreational drug use, and a host of other activities. A simple modification of this deviation from the ideal approach can help us to achieve some consensus. Behaviors that cause harm to others are viewed as less than ideal. Unless the person is acting defensively, most of us would agree that such behavior is negative and undesirable. We are also sympathetic to those who have conditions that cause harm to themselves. We don’t know how many people have psychological disorders. A person can be diagnosed with comorbidcomorbidTwo or more disorders in the same individual.            comorbid        Two or more disorders in the same individual.             disorders, which means more than one disorder occurs at the same time, so simply adding up the rates for each type of disorder is not a solution. Statistics can look different for different time frames. The estimate of the number of people who experience a psychological disorder during the previous year is around 26% (Kessler, Chiu, Demler, & Walters, 2005), but estimates of the number of people experiencing a disorder as least once during their lifetime can be much higher. In one longitudinal study of individuals from age 19–20 to age 50, the cumulative probability of experiencing at least one disorder was 73.9% (Angst et al., 2016). These numbers certainly challenge our conceptions of abnormal behavior as unusual. Defining abnormal behavior as a deviation from what is considered ideal immediately runs into problems of cultural specificity. Behaviors like hearing voices that others cannot hear might be considered less than ideal in Western cultures but a gift in others. The ability to hear voices was considered an advantage for shamans, like the one represented transforming into his jaguar spirit companion in this Costa Rican piece from 1000–1500 CE. We can combine the best aspects of each of these approaches to construct a formal definition of abnormal behavior: Abnormal behavior is distressing and harmful to self or others. Although college students have the same rate of psychological disorders as their age peers who are not attending college, as shown in Figure 14.3, the rate of psychological disorders in young adults is higher than in the general population and appear to be increasing (Hunt & Eisenberg, 2010). Nearly half of the young adults under the age of 24 who were surveyed met criteria for at least one psychological disorder in the previous year, including 18% for a personality disorder, 12% for an anxiety disorder, and 11% for either major depressive disorder or bipolar disorder (Blanco et al., 2008). Each of these categories is discussed in more detail in this chapter. Based on face-to-face interviews, college students and their same-age peers who do not attend college have a high rate (nearly 50%) of meeting the criteria for at least one psychological disorder over the past 12 months. College students and age peers not in college did not differ significantly in their rates of psychological disorder, with the exception of alcohol-use disorders, which were higher among those attending college. The first official effort in the United States to gather data on psychological disorders was the recording of a single category, idiocy/insanity, in the 1840 census (APA, 2000). When you visit your physician or student health center complaining of a sore throat, your health care provider notes your symptoms, compares them to known categories of illness, swabs your throat for a sample, and then determines your diagnosis: You have strep throat. Based on that diagnosis, you are given a prescription for an antibiotic. Unlike medical illnesses, however, psychological disorders are diagnosed purely on the basis of observable behaviors. There are no blood tests, scans, or other medical diagnostic procedures that can be used to determine whether a person has a psychological disorder. To promote consistency in the diagnosis of psychological disorders, mental health professionals typically refer to the Diagnostic and Statistical Manual of Mental Disorders (DSM)Diagnostic and Statistical Manual of Mental Disorders (DSM)A system for classification of psychological disorders published by the American Psychiatric Association.            Diagnostic and Statistical Manual of Mental Disorders (DSM)        A system for classification of psychological disorders published by the American Psychiatric Association.            , first published by the American Psychiatric Association (APA) in 1952. The current, fifth edition is known as DSM-5 (APA, 2013). A second classification system in wide use is the International Statistical Classification of Diseases and Related Health Problems, 10th edition (ICD-10), which was published by the World Health Organization (WHO) in 1992. An updated 11th edition is due to be published in 2018 (WHO, 2017). Professionals responsible for the DSM and ICD-10 classification systems have coordinated their efforts to reduce the differences between the two systems (APA, 2013). The DSM is not without its critics. Beginning with DSM-III in 1980, some normal behaviors seem to have been categorized as abnormal by the system. Allen Frances, a psychiatrist who chaired the task force in charge of writing the DSM-IV (published in 1994), expressed concern that the committee’s well-meaning efforts had led to three false epidemics—attention deficit hyperactivity disorder (ADHD), autism, and childhood bipolar disorder—and that the DSM-5 was even worse (Frances, 2010). For example, the DSM-5 lists the following (very normal) behavior as a criterion for ADHD:often avoids, dislikes, or is reluctant to engage in tasks that require sustained mental effort (e.g. schoolwork or homework; for older adolescents and adults, preparing reports, completing forms, reviewing lengthy papers). (APA, 2013, p. 59)The Diagnostic and Statistical Manual of Mental Disorders (DSM) has undergone a number of revisions since its first edition in 1952. Over its history, the DSM has become less Freudian and more sensitive to cross-cultural issues. The DSM is published by the American Psychiatric Association. Despite its flaws, the DSM remains the most frequently used tool in the United States for the diagnosis of psychological disorders. Our discussion of types of psychological disorders, therefore, is grounded in the DSM system. We have seen many examples in this textbook of behaviors that are best understood when viewed from multiple perspectives. The study of psychological disorders provides yet another example of how an integration of multiple perspectives can be useful. This is particularly the case when considering the possible causes for abnormal behaviors. What might an integrated approach to psychological disorder look like? First, we propose that the various perspectives discussed in this textbook (biological, clinical, developmental, cognitive, and social/personality) all have a great deal to say about disorders, but each has more to say about some disorders than about others. By considering the contributions of factors from multiple perspectives, we should have a greater appreciation of the complex interactions that occur among perspectives. Philippe Pinel (1745–1826) is often called the father of modern psychiatry. After observing the more humane treatment of inmates by a former patient turned employee named Jean-Baptiste Pussin at Bicêtre Hospital outside Paris, Pinel followed Pussin’s example by removing the patients’ iron shackles. Instead of the usual treatments of the day—bleeding, purging, and blistering—Pinel made a practice of conversing with the patients regularly. In addition, an integrated perspective helps us understand the reciprocal relationships among factors leading to psychological disorders. Just as a person who is diagnosed with depression might show low levels of serotonin activity in the brain, being in a leadership position boosts a person’s serotonin levels. A simple biological explanation suggesting that chemical imbalances lead to depression, or a simple psychological explanation stating that people who feel powerless are more likely to be depressed, misses the nuances of these reciprocal relationships. The richer understanding of the causal factors leading to a psychological disorder pays off in the development of more effective treatments. Single-perspective thinking usually leads to single-perspective treatments. If you believe that depression is purely the result of chemical imbalances of the brain, then a simple prescription should do the trick. If you believe that hearing voices that others can’t hear is the result of childhood communication patterns, you will miss the opportunity to provide medication that can effectively end these troubling symptoms. Inclusive models describing the entire range of causal factors are more likely to result in effective treatments tailored to the needs of individuals. Many disorders might have roots in childhood but are more typically diagnosed in adolescence and adulthood. The DSM-5 uses the term neurodevelopmental disorders to refer to disorders that instead are diagnosed typically in childhood, yet often continue throughout the life span (APA, 2013). Two of these neurodevelopmental disorders are examined in this section: ASD and ADHD. The word autism literally means within oneself. In 1943, psychiatrist Leo Kanner worked with a group of children who shared problems in social communication and interaction and restricted, repetitive patterns of behavior, interests, or activities. Kanner’s observations form the basis of our current diagnostic criteria for autism spectrum disorderautism spectrum disorderA disorder characterized by deficits in social relatedness and communication skills that are often accompanied by repetitive, ritualistic behavior.            autism spectrum disorder        A disorder characterized by deficits in social relatedness and communication skills that are often accompanied by repetitive, ritualistic behavior.             (ASD; APA, 2013), which combines previous categories of autism and Asperger’s syndrome. ASD represents a spectrum because the severity of the observed deficits can vary widely from individual to individual. Adjustment can range from relatively normal, allowing independent living, to intellectual disability, requiring living with parents or in institutional settings. Rates of ASD have been increasing rapidly over the last two decades (see Figure 14.4). Approximately 1 child out of every 68 has ASD, with rates for boys (1 in 42) significantly higher than rates for girls (1 in 189) (Christensen, 2016). Whether the rates are actually increasing or diagnostic criteria and awareness are changing remains unknown. In the United States, individuals can receive a medical diagnosis using the DSM-5 or be educationally verified by teams of school personnel (Ramsey, Kelly-Vance, Allen, Rosol, & Yoerger, 2016). These different methods of assessment produce different numbers, with higher rates of diagnosis in the educational than in the medical/psychological setting. Rates of ASD continue to increase, but the reasons for this change remain a source of debate in the scientific community. It is possible that the rates are truly increasing, but greater awareness by parents and health care providers, relaxed applications of the diagnostic criteria, and improved availability of services might also be contributing to the observed change. Problems with social relatedness are at the core of this disorder, regardless of the individual’s level of intelligence and adjustment. Beginning in infancy, most children with ASD do not make eye contact or take pleasure in reciprocal games like peekaboo. Insight into the thoughts and points of view of others is particularly lacking. As discussed in Chapter 11, children develop a theory of mind by the time they are 3–4 years of age. Many researchers believe that a failure to develop a normal theory of mind is responsible for many of the social deficits observed in ASD (Baron-Cohen, 1991;  Senju, Southgate, White, & Frith, 2009). Language skills of individuals with ASD can vary widely, from having no language abilities to delayed acquisition of language to normal skills (Ellis Weismer, Lord, & Esler, 2010). Even when language skills are relatively normal, though, individuals with ASD usually experience difficulty maintaining conversations with others because of their social skills deficits. Individuals with ASD often object strenuously to changes in their environments and show a high level of repetitive, routine behavior. Rituals may include repetitive movements, such as rocking, hand flapping, head banging, and twirling. Other individuals may engage in extremely limited preoccupations, such as learning all models of cars ever made by Ford. One possible source of this ritualistic behavior is a general dysfunction in sensory networks. Most individuals with ASD show unusually increased or decreased sensitivity to stimuli (Lane, Young, Baker, & Angley, 2010). They may be relatively insensitive to pain or cold but distressed by normal sound levels (see Figure 14.5). Ritualistic behavior may control or override these disparate sensations. Children with ASD often show differences from healthy children in their sensitivity to environmental stimuli. This graph shows the percentage of children with ASD whose scores on an instrument measuring sensitivity were the same as those of typical children, mildly different from those of typical children (between one and two standard deviations from the mean), and rare compared to those of typical children (more than two standard deviations from the mean). The domains measured included sensitivity to touch (tactile), sensitivity to taste and smell, sensitivity to movement (e.g., fear of falling), underresponsiveness (e.g., touches people), auditory filtering (e.g., fails to respond to name when called), low energy (e.g., tires easily), and visual or auditory (e.g., responds negatively to loud noises or bright lights). Although the causes of ASD remain somewhat mysterious and probably show variable patterns from case to case, scientists are making progress in their understanding. Family and twin studies provide strong evidence that ASD is influenced by genetics (Frazier et al., 2014). The concordance rate for ASD between identical twins may be 76% to 88%, and possibly more (Ronald & Hoekstra, 2011), which means that if one twin has ASD, the other twin has between a 76% and an 88% chance of also having ASD. Hundreds of genes are probably involved, and research attention is focusing on the expression of these genes during brain development (Sakai et al., 2011;  Vorstman et al., 2017). Autopsies of the brains of people with ASD and people with no history of psychological disorder show dramatic differences (Voineagu et al., 2011). In the typical control brains, 174 genes were expressed differently in the frontal lobes than in the temporal lobes, but in the brains of people with ASD, no gene showed evidence of being expressed differently in these two areas. Abnormalities in cortical development might lead to unusual minicolumns, vertical arrays of neurons perpendicular to the surface of the cerebral cortex that represent the smallest processing units of the brain. Individuals with ASD have narrower minicolumns containing normal numbers of cells, but spaced farther apart than those found in healthy individuals. These structural differences are consistent with a pattern of connectivity that favors detailed focus, which could produce the unusual interests and hobbies of people with ASD, over more global processing, like understanding the social environment (Opris & Casanova, 2014). Narrow minicolumns similar to those found in the brains of people with ASD were observed in the brains of three distinguished scientists, none of whom had ASD symptoms, who donated their brains for scientific study. These similarities suggest that the minicolumn structure might account for the extreme focus of interests typical in ASD (Casanova, Switala, Trippe, & Fitzgerald, 2007). Outstanding scientists, like people with autism, have been known to study minute details for long periods of time. Additional structural abnormalities in cases of ASD have been observed in the amygdala, hippocampus, and cerebellum (Barnea-Goraly et al., 2014;  Stoodley, 2014). Researchers continue to debate a possible role in ASD for the mirror system, which has been implicated in empathy, imitation, and language (see Chapter 8). Environmental factors probably interact with genetic factors associated with ASD during sensitive periods of brain development (Engel & Daniels, 2011). Parental age is also a risk factor, with older parents more likely than younger parents to give birth to a child with ASD (Grether, Anderson, Croen, Smith, & Windham, 2009; Shelton, Tancredi, & Hertz-Picciotto, 2010), although the effect is rather small. Prenatal exposure to infection and nutritional factors increase the risk for ASD (Hamlyn, Duhig, McGrath, & Scott, 2013). Use of common antidepressants known as selective serotonin reuptake inhibitors (SSRIs) during pregnancy is associated with a higher risk for producing a child with ASD (Boukhris, Sheehy, Mottron, & Bérard, 2016; Harrington, Lee, Crum, Zimmerman, & Hertz-Picciotto, 2014). One of the unfortunate consequences of the uncertainty surrounding the causes of ASD has been the vulnerability of concerned parents seeking answers. In a paper later retracted by the British medical journal Lancet, unsubstantiated claims that the routine measles, mumps, and rubella vaccination caused ASD were published. Similar controversies ensued in the United States about thimerosal, a mercury-containing preservative used in some vaccines. Minicolumns in the cerebral cortex function like the microprocessors in modern computers by serving as the basic unit that receives input, processes it, and responds. Individuals with ASD (lower image) have smaller minicolumns than do healthy controls (top image). What does this difference mean for information processing? Smaller minicolumns favor the process of discrimination, described in Chapter 8 as distinguishing among stimuli, while larger minicolumns favor generalization, or applying a response to similar stimuli. Behavioral domains that are difficult for people with ASD, such as language, face recognition, and following another person’s gaze, require more generalization than discrimination. In a meta-analysis involving over 1 million children, no relationship was found between ASD and vaccination in general, vaccination with the measles-mumps-rubella combination, or thimerosal (Taylor, Swerdfeger, & Eslick, 2014; see Figure 14.6). Despite the clear data and reassurances from medical experts, however, worried parents have withheld vaccinations from their children, leading to increasing numbers of cases of life-threatening, preventable diseases that had previously been believed to be under control. For example, in the first half of 2008, measles cases in the United States doubled compared to the rates observed between 2000 and 2007, and all cases involved unvaccinated school children (CDC, 2008). Cumulative exposure to thimerosal, a mercury-containing preservative that has been used in vaccines, was the same for children diagnosed with ASD and healthy controls. Despite clear scientific evidence contradicting the link between vaccines and autism, many people have been influenced by celebrities such as model Jenny McCarthy and have withheld vaccinations from their children. As a result, communities are facing epidemics of clearly avoidable and disabling diseases, such as measles. Attention deficit hyperactivity disorder (ADHD)Attention deficit hyperactivity disorder (ADHD)A disorder characterized by either unusual inattentiveness, hyperactivity with impulsivity, or both.            Attention deficit hyperactivity disorder (ADHD)        A disorder characterized by either unusual inattentiveness, hyperactivity with impulsivity, or both.             is perhaps one of the most contentious categories described in the DSM-5. The criteria for the disorder are difficult to distinguish from the behaviors of many typical young children. Because many children diagnosed with ADHD are treated with medication, the stakes for accurately diagnosing the condition are high indeed. One of the DSM-5 criteria for ADHD is often fidgets with hands or feet or squirms in seat. If you have visited an elementary school classroom lately, you might have noticed that most children are active. At what point does squirming become a psychological disorder requiring medication? ADHD involves inattention and hyperactivity. Some individuals show both inattention and hyperactivity, but others show inattention without hyperactivity or vice versa (APA, 2013). The core feature of inattention is the inability to maintain sustained attention or on-task behavior for an age-appropriate length of time. This problem is evidenced in the diagnostic criteria for inattention, such as difficulties in following instructions, in organizing and completing work, and in avoiding careless mistakes. Children with hyperactivity express a high level of motor activity and find engaging in structured activities, such as waiting in line or sitting quietly in class, challenging. These children are noisy, active, and boisterous, and they often appear to take action without thinking it through. Even as adults, individuals who were diagnosed with ADHD as children have more traffic accidents than do people without the disorder (Barkley & Cox, 2007). Many of these behaviors are also seen in children who do not have psychological disorders. Although the DSM-5 provides guidelines for distinguishing between normal and abnormal inattentiveness and hyperactivity, fewer than 40% of surveyed pediatricians reported using those criteria to evaluate cases of ADHD (Wasserman et al., 1999). More than half the children in a large sample who were receiving medication for ADHD did not meet even the relaxed diagnostic criteria for the disorder, let alone the carefully constructed criteria spelled out in the DSM-5 (Angold, Erkanli, Egger, & Costello, 2000). In 2011, 11% of children between the ages of 4 and 17 years had been diagnosed with ADHD in the United States, representing an increase of 42% over rates of ADHD as recently as 2003 (Visser et al., 2014; see Figure 14.7). ADHD is diagnosed at least twice as frequently in males as in females, and females are more likely than males to be diagnosed with inattentiveness without hyperactivity (APA, 2013). The number of children diagnosed with ADHD in a 2007 report from the Centers for Disease Control and Prevention (CDC) varies dramatically by region in the United States, from a low of 5.6%–7.9% in the Southwest to 11%–15.9% in parts of the Midwest and Southeast. Reasons for these regional discrepancies are not clear, although low socioeconomic status is considered a risk factor for ADHD. The differences also might represent variations in how the diagnostic criteria are applied by local health care providers. The National Institute of Mental Health (NIMH, 2009, p. 3) concluded that scientists are not sure what causes ADHD. However, twin and adoption studies support a significant role for genetics in the development of ADHD. Heritability may be 70% or more (Faraone & Mick, 2010). Environmental factors might interact with genetic risk. Known environmental risks for ADHD are lead contamination, low birth weight, and prenatal exposure to tobacco, alcohol, and other drugs (Banerjee, Middleton, & Faraone, 2007). All children can be active and noisy, but most usually learn quickly to restrict those behaviors to the right times and places. Children with ADHD often have trouble sitting quietly in class or waiting in line. ADHD is correlated with a number of structural and functional differences in the brain. The frontal lobes may be underactive in cases of ADHD (Barkley, 1997). Because the frontal lobes inhibit unwanted behavior, lower activity in this part of the brain may lead to hyperactivity and impulsivity. In addition, smaller volume in the amygdala, basal ganglia, and hippocampus, and the brain as a whole was observed in ADHD (Hoogman et al., 2017). Disruption in the corpus callosum, a major white-matter pathway connecting the two cerebral hemispheres, was correlated with the severity of symptoms in ADHD (Ameis et al., 2016). The frontal lobes, and the prefrontal areas in particular, appear to mature more slowly in children with ADHD than in healthy controls (Shaw et al., 2007). Peak cortical thickness, a measure of brain maturity, occurred in healthy controls around the age of 7.5 years, but not until the age of 10.5 years in children with ADHD. Patterns of cortical thinning during adolescence predicted which individuals would continue to experience ADHD symptoms in adulthood and which would outgrow their symptoms (Shaw et al., 2013). White-matter circuits connecting the basal ganglia and the frontal lobes matured differently in people with and without ADHD (Helpern et al., 2011). A study of 1,713 individuals between the ages of 4 and 63 years with ADHD supported the idea that ADHD involves delays in maturation (Hoogman et al., 2017). The parts of the brain implicated in ADHD, such as the prefrontal cortex and the basal ganglia, feature large amounts of dopamine activity. As will be shown in Chapter 15, most of the medications used to treat ADHD, such as methylphenidate (Ritalin), dextroamphetamine (Dexedrine or Dextrostat), or amphetamine salts (Adderall), act by boosting the activity of dopamine, suggesting that dopamine activity might be lower than usual in cases of ADHD (Volkow et al., 2009). Among the many differences observed in the brains of children with ADHD compared to those of healthy controls is the rate of brain maturity, as measured by cortical thickening. These images demonstrate areas of the brain that developed later in children with ADHD than in healthy controls. Peak cortical thickness occurred around the age of 7.5 years in healthy children but was not seen in children with ADHD until an average age of 10.5 years. This finding implies that children with ADHD can be expected to lag their age peers in some tasks, but will eventually experience improvement. As in ASD, myths about the causes of ADHD are common. Sugar is often blamed for hyperactive behavior, but evidence from carefully controlled studies does not support this belief (Milich, 1986;  Wolraich, 1996). However, as discussed in Chapter 2, a well-controlled study suggested that combinations of common food additives made normal children demonstrate more hyperactivity (McCann et al., 2007). It is unlikely that poor parenting is responsible for these symptoms (Schroeder & Kelley, 2009). Parents, however, can learn new behavioral management techniques that greatly improve their child’s behavior. DisorderSymptomsPossible Causal Factors Under InvestigationAutism spectrum disorder (ASD)Problems with social relatednessProblems with communicationRitualistic behaviorGeneticsParental ageDisruptions in brain developmentUnknown environmental influencesAttention deficit hyperactivity disorder (ADHD)InattentivenessHyperactivity/ImpulsivityGeneticsBrain development/exposure to toxinsFrontal lobe activityFood additivesSchizophreniaSchizophreniaA disorder characterized by hallucinations, delusions, disorganized thought and speech, disorders of movement, restricted affect, and avolition or asociality.            Schizophrenia        A disorder characterized by hallucinations, delusions, disorganized thought and speech, disorders of movement, restricted affect, and avolition or asociality.             is not the most common type of disorder, affecting approximately 1% of the human population worldwide (Kessler et al., 2007), but it is one of the most dramatic. This condition influences and distorts a range of behaviors, including perception, cognition, movement, and emotion. The DSM-5 places schizophrenia within a group of disorders called schizophrenia spectrum and other psychotic disorders (APA, 2013). Among the symptoms of schizophrenia are delusions, hallucinations, disorganized speech, and disorders of movement, which together indicate a state of psychosis. Schizophrenia also features negative symptoms (APA, 2013). Negative symptoms are behaviors that are seen in healthy people but not in patients. These symptoms include diminished emotional expression and avolition (APA, 2013, p. 88). A person with diminished emotional expression does not show typical outward signs of emotion, such as facial expressions and tone of voice, when an emotional response is expected. Avolition (the a means lack of) refers to a patient’s lack of volition, or goal-oriented behavior. DelusionsDelusionsA false, illogical belief.            Delusions        A false, illogical belief.             are defined as unrealistic beliefs. These may take a number of forms, including delusions of persecution by others (paranoia), feelings of unrealistic power or importance (grandiosity), or beliefs that others are directing one’s behavior (control). Despite having been diagnosed with schizophrenia, John Nash shared the 1994 Nobel Prize in Economic Sciences. Nash was the subject of the 2001 film A Beautiful Mind, starring Russell Crowe. Sadly, both Nash and his wife were killed in a traffic accident in 2015. Some people with schizophrenia experience catatonia, which means that they maintain awkward or unusual body positions for hours at a time. HallucinationsHallucinationsA false perception.            Hallucinations        A false perception.             are false perceptions. Although hallucinations may occur in several sensory modalities, most hallucinations in schizophrenia are auditory (Tien, 1991). Patients often report hearing voices, which can be accusatory or otherwise unpleasant, contributing to the distress associated with the disorder. Auditory hallucinations are not imaginary; they are real sensations correlated with increased activity in the primary auditory cortex of the temporal lobe (Dierks et al., 1999). Simply asking patients to remember or imagine sounds did not produce the type of activity in the auditory cortex seen during an auditory hallucination, indicating that the hallucination experience is different from simple memory or imagination. A further symptom of schizophrenia involves disorganized patterns of speech. The patient jumps inexplicably from one topic to the next. People with schizophrenia appear to have difficulty inhibiting secondary meanings for some words (Titone, Levy, & Holzman, 2000). For example, the word jam can refer to either a fruit spread for toast or an impromptu musical session. Most people would use context (a conversation about food or music) to decide which meaning was appropriate. Patients with schizophrenia might not experience this filtering, leading their thoughts to jump from food to jazz and then on to other atypical connections. This loosening of associations among ideas might occur because of the reduced latent inhibition in patients with schizophrenia, as discussed in Chapter 8. According to this argument, latent inhibition typically results in fewer associations being made to familiar stimuli. If you have had a lot of experience connecting jam and food, you are unlikely to consider other uses of the word jam. The person with schizophrenia, however, would be less inhibited in making connections among ideas, leading to trains of thought that seem bizarre to the rest of us. Schizophrenia also features grossly disorganized or abnormal motor behavior (APA, 2013, p. 88). Some people may be unusually active, while others may barely move throughout an entire day. Unusual behaviors can occur, including grimaces and gestures. The maintenance of awkward or unusual body positions for hours at a time is called catatonia. People experiencing catatonia appear aware of their surroundings, but they don’t move. Most of us would find it difficult to sit for a few minutes, let alone hours, without shifting position. Like many other types of psychological disorders, it is likely that schizophrenia has multiple sources of causality. No one factor is likely to be sufficient to produce the disorder. Significant evidence points to a genetic vulnerability for schizophrenia. People with close family members who have been diagnosed with schizophrenia are more likely to develop the disorder themselves (Gottesman, 1991). If one identical twin is diagnosed with the disorder, the other twin has nearly a 50% lifetime risk of developing the disorder (see Figure 14.8). Comparisons of adopted children with their biological and adoptive parents also support a role for genetics (Kety, Rosenthal, Wender, & Schulsinger, 1968). A large number of genes have been implicated in the development of schizophrenia and overlap with those involved with bipolar disorder (Cross-Disorder Group of the Psychiatric Genomics Consortium, 2013;  Owen, Craddock, & Jablensky, 2007). In one case of identical triplets, two of the triplets were diagnosed with schizophrenia and the third was diagnosed with bipolar disorder (McGuffin, Reveley, & Holland, 1982). A person’s lifetime risk of developing schizophrenia increases when closely related family members have been diagnosed with the disorder, suggesting that genes play a significant role. Having a spouse with schizophrenia doubles the risk (from 1% to 2%), which probably reflects the tendency for people to marry others with whom they share similarities. A number of structural and biochemical features accompany schizophrenia. One reliable correlate of schizophrenia is the presence of enlarged ventricles (Yotsutsuji et al., 2003). As discussed in Chapter 4, the ventricles are fluid-filled spaces in the brain that are not responsible for any particular behavior. The ventricles enlarge in response to any condition resulting in a loss of neural tissue in adjacent areas, so we can assume that schizophrenia is associated with neural degeneration. Patients with schizophrenia demonstrate differences in brain activity compared with healthy people. For example, patients with schizophrenia often show a lower level of frontal lobe activity than that of healthy controls, both at rest and during effortful cognitive tasks (Berman, Torrey, Daniel, & Weinberger, 1992). The importance of the frontal lobes to higher cognitive processes and attention suggests that this difference would have significant influences on behavior, and lower levels of activity might account for the emotional disturbances and social withdrawal seen in these patients (Andreasen et al., 1992). Pathways in the brain that manage olfaction, or the sense of smell, travel through the frontal lobe. Lower overall frontal lobe activity might account for some difficulties that patients with schizophrenia experience with their sense of smell. In addition, distortions in the brain’s default mode network (see Chapter 6) predict the severity of patients’ psychotic symptoms (Garrity et al., 2007; Pomarol-Clotet et al., 2008). Magnetic resonance images of the brains of a pair of identical twins show the differences in the size of the lateral ventricles (arrows) found in healthy people (left) and in people with schizophrenia (right). The ventricles are fluid-filled spaces and enlarge whenever significant numbers of nearby neurons have died. Schizophrenia might involve abnormal brain development during adolescence. Teens typically experience a burst of cortical gray matter growth at puberty, followed by a wave of thinning of the gray matter extending into their early 20s. Healthy teens experience little loss of gray matter, whereas teens diagnosed with schizophrenia experience a loss that has been likened to a forest fire (Thompson et al., 2001). Abnormalities in dopamine activity might be the major biochemical culprit in schizophrenia. Drugs that boost dopamine activity, such as amphetamine and the Parkinson’s disease treatment l-dopa, can produce hallucinations and paranoid delusions (Goetz, Leurgans, Pappert, Raman, & Stemer, 2001). Medications that block dopamine activity are usually quite effective in reducing these same symptoms. However, this dopamine hypothesis of schizophrenia is not perfect. About one fourth of patients with schizophrenia do not respond favorably to drugs that reduce dopamine activity. In particular, the negative symptoms, such as diminished emotional expression, are not improved by these drugs (Goff & Evins, 1998;  Kane & Freeman, 1994). In addition, medications that influence neurotransmitters other than dopamine are often effective (Syvalahti, 1994). Finally, phencyclidine (PCP, or angel dust) produces a syndrome that is quite similar to schizophrenia, although PCP acts on synapses using glutamate, not dopamine (Mössner et al., 2008). Because glutamate and dopamine are often important to the same brain circuits, it is possible that disturbances in one, the other, or both could lead to similar behavioral outcomes. Disturbances in the balance between brain excitation and inhibition, not related to any one neurochemical system, might also contribute to symptoms of schizophrenia (Jardri et al., 2016). The nearly 50% concordance rate for schizophrenia in identical twins indicates a role for genetics in the development of the disorder. However, 50% is not 100%, and we need to account for the remaining factors. When the brain activity of these identical twins is observed through brain imaging, the frontal lobes (arrows) appear to be more active in the healthy twin (left) than in the twin with schizophrenia (right). Red and yellow areas indicate greater activity, while green and dark areas are relatively inactive. Extreme stress contributes to the appearance and severity of schizophrenia among genetically vulnerable people. Schizophrenia appears nearly five times more frequently in members of lower than in those of higher socioeconomic groups (Keith, Regier, & Rae, 1991). The stress of living in poverty may trigger schizophrenia in vulnerable individuals. However, others argue that people who are susceptible to schizophrenia drift into lower-paying jobs. In other words, the disorder reduces the patients’ socioeconomic status, accounting for the higher rates of the disorder among the poor. Other stresses might accompany a person’s minority status in a community (Boydell, 2003). African immigrants living in London neighborhoods with the lowest proportions of African immigrants were twice as likely to be diagnosed with schizophrenia as African immigrants living in neighborhoods with the highest proportion of minorities (see Figure 14.9). The stress of social isolation because of minority status may have contributed to higher rates of schizophrenia. African immigrants living in South London were more likely to be diagnosed with schizophrenia if they lived in a predominantly White neighborhood than if they lived in a primarily non-White neighborhood; that is, if you divide neighborhoods into thirds based on the percentage of non-Whites (high, medium, and low), schizophrenia was more common in the middle and lower groups. An explanation based on self-selection (people choosing to live in each type of neighborhood) is unlikely in this case because nearly all the study participants were assigned to public housing units rather than choosing where to live. These findings suggest that the stress of minority status might contribute to higher risk for schizophrenia. When we talk about environmental influences, factors such as stress come to mind (see Chapter 16). However, environmental influences also include biological variables such as the prenatal environment, including the pregnant woman’s exposure to viral illness. Another environmental variable that might trigger schizophrenia in genetically vulnerable individuals is marijuana use (Kelley et al., 2016;  Marconi, Di Forti, Lewis, Murray, & Vassos, 2016). A 25-year longitudinal study concluded that cannabis use nearly doubled the risk of schizophrenia (Fergusson, Horwood, & Ridder, 2005). As mentioned in Chapter 11, adolescence is a time of brain growth, and heavy marijuana use during this period produces a reduction in white matter volume in the frontal lobes that might interact with a person’s genetic vulnerabilities for schizophrenia (Ho et al., 2011). Teens typically experience a burst of gray matter growth at puberty, followed by a wave of gray matter thinning that extends into their early 20s. Compared with their healthy peers, teens diagnosed with schizophrenia experience much greater gray matter loss. Purple and red areas indicate the greatest amount of loss, followed by yellow and green areas. Bipolar disorderBipolar disorderA mood disorder characterized by alternating periods of mania and depression.            Bipolar disorder        A mood disorder characterized by alternating periods of mania and depression.             serves as a bridge between psychotic disorders, such as schizophrenia, and depressive disorders with respect to symptoms, family histories, and genetics (APA, 2013, p. 123). Bipolar disorder features a period of maniamaniaA period of unrealistically elevated mood.            mania        A period of unrealistically elevated mood.            , which may or may not be followed by a period of depression. The symptoms of depression that can accompany bipolar disorder are identical to those used to diagnose major depressive disorder, which is described in What Is Major Depressive Disorder (MDD)? of this chapter. A manic phase consists of a distinct period of abnormally and persistently elevated, expansive, or irritable mood, abnormally and persistently increased goal-directed activity or energy (APA, 2013, p. 124). Patients may demonstrate grandiosity, in which they feel unrealistically special or important. The person’s behavior, characterized by little need for sleep, rapid speech, difficulty concentrating, and rapidly shifting ideas, seems to be running at an abnormally high speed. Unlike many other disorders, however, mania increases productive, goal-directed behavior. However, because of the person’s tendency to meet the DSM-5 criterion for excessive involvement in activities that have a high potential for painful consequences, hospitalization is often necessary for the patient’s protection (APA, 2013, p. 124). Many variations occur in the timing and severity of the mania and depression observed in bipolar disorder. Bipolar disorder affects approximately 2.6% of American adults each year (Kessler et al., 2005). Women are more likely to be diagnosed with bipolar disorder than men, by a ratio of approximately 3:2 (CDC, 2013). The average age of onset for bipolar disorder is 25 years. Children and youth under the age of 18 years with symptoms of bipolar disorder are diagnosed instead with disruptive mood dysregulation disorder (APA, 2013). Actors, actresses, poets, painters, and musicians seem to have a disproportionate risk for bipolar disorder compared to people in other occupations. The late Carrie Fisher, of Star Wars fame, was outspoken regarding her challenges with bipolar disorder. We do not know whether having bipolar disorder allows these creative people to express emotion more effectively in their art, the arts are simply more welcoming than the corporate boardroom to those who have psychological disorders, or some combination of the two. Genetic predispositions play a significant role in bipolar disorder. Concordance rates between identical twins for bipolar disorder are commonly reported to be as high as 70% (Craddock & Sklar, 2013). Adoption studies also support a powerful role for genetics (Taylor, Faraone, & Tsuang, 2002). As mentioned previously, there is considerable overlap between the genes believed to play a role in bipolar disorder and those implicated in schizophrenia. Not surprisingly because of their genetic overlap, bipolar disorder shares some of the same abnormalities in brain structure and function as observed in schizophrenia (De Peri et al., 2012; Van Haren et al., 2012). Among the many possible environmental factors that could interact with genes associated with bipolar disorder is diet. Omega-3 fatty acids, generally found in fish, may provide some protection from bipolar disorder (Noaghiul & Hibbeln, 2003). As shown in Figure 14.10, prevalence rates for bipolar disorder are highest in countries where fish is rarely consumed (such as Germany) and lowest in countries where fish is an important diet staple (such as Iceland). However, attempts to improve symptoms in patients with bipolar disorder by administering omega-3 supplements have produced weak results (Murphy et al., 2012;  Stahl, Begg, Weisinger, & Sinclair, 2008). The impact of omega-3 fatty acids on a person’s vulnerability for bipolar disorder might be more important during prenatal development than later in life. Rates of bipolar disorder are negatively correlated with a nation’s consumption of seafood, suggesting that the omega-3 fatty acids contained in seafood might help prevent bipolar disorder. Many unanswered questions regarding the development of this condition remain. Bipolar disorder may be overrepresented in groups of people with artistic and creative talent (Jamison, 1993,  1995). Based on biographical accounts, the poet William Blake, the composers George Frideric Handel and Gustav Mahler, and visual artists Vincent van Gogh and Michelangelo might have had bipolar disorder. Many noted actors and actresses, including Carrie Fisher (Star Wars) and Vivien Leigh (Gone with the Wind), have been diagnosed with bipolar disorder. Comparisons between people with bipolar disorder and major depressive disorder and healthy people in creative and noncreative professions support this hypothesis. The patients with bipolar disorder scored similarly on tests of creativity to healthy people in creative occupations and higher than people with major depression and healthy people in noncreative occupations (Santosa et al., 2007). The parts of the brain that are affected in bipolar disorder are similar to those seen in cases of frontotemporal dementia. In both conditions, lower activity in these areas may reduce inhibition, which in turn leads to more creativity in some individuals (Seeley et al., 2008). Although most of us experience the occasional blues, these feelings are not as severe or chronic as the depressed feelings that characterize major depressive disorder. According to the DSM-5, major depressive disorder (MDD)major depressive disorder (MDD)A disorder characterized by lengthy periods of depressed mood, loss of pleasure in normal activities, disturbances in sleep and appetite, difficulty concentrating, feelings of hopelessness, and possible thoughts of suicide.            major depressive disorder (MDD)        A disorder characterized by lengthy periods of depressed mood, loss of pleasure in normal activities, disturbances in sleep and appetite, difficulty concentrating, feelings of hopelessness, and possible thoughts of suicide.             is characterized by depressed mood most of the day, nearly every day, for a period of at least 2 weeks (APA, 2013). Patients with MDD complain about feeling sad and empty, and periods of tearfulness are common. The DSM-5 also notes that MDD can produce anhedonia, or loss of pleasure. You are probably already familiar with the term hedonist, which refers to a person who is a pleasure seeker. Anhedonia refers to a person’s disinterest in activities that previously provided pleasure, such as sex, eating, or social activities. To be diagnosed with MDD, a person must show at least five symptoms, one of which must be either depressed mood or anhedonia. The remaining symptoms can be divided into physical and cognitive groups. The physical symptoms of MDD are related to disturbances in autonomic function typically found in the presence of high levels of stress, described in Chapter 16. Appetite and sleep may be disturbed. Some patients experience a loss of appetite, whereas others begin to eat too much. As observed in Chapter 6, many patients with depression experience frequent wakefulness, whereas others experience oversleeping (more than 9 hours per night). Fatigue or restlessness may occur. Among the cognitive symptoms of depression are difficulty concentrating, feelings of hopelessness and worthlessness, and in some cases, thoughts of suicide (APA, 2013). MDD is one of the most frequently diagnosed psychological disorders, affecting approximately 7% of the adult population each year (APA, 2013). MDD decreases with age, and people between the ages of 18 and 29 years have rates that are three times as high as those for people over the age of 60 years. Women experience MDD more frequently than men do (Nolen-Hoeksema, 1987;  Nolen-Hoeksema, Larson, & Grayson, 1999). This discrepancy between rates of MDD in men and those in women has been observed to be independent of race, ethnicity, social class, and country of residence (Strickland, 1992;  Üstün, Ayuso-Mateos, Chatterji, Mathers, & Murray, 2004; see Figure 14.11). It is possible that female hormones participate in mood through some unknown mechanism. Mood disturbances can be associated with hormonal changes in women, including postpartum depression and mood changes accompanying menopause (Rapkin, Mikacich, Moatakef-Imani, & Rasgon, 2002). However, it is also possible that women are more likely than men to admit feeling depressed to others and to seek help with depression. Men may mask or cover their depressed mood by engaging in activities such as drinking alcohol. Across many cultures, women are diagnosed with depression more frequently than men. Theories attempting to explain MDD range from the learning, social, and cognitive to the strictly biological. It is likely that some combination of these approaches provides the greatest understanding of the sources of depression. According to learning theories, MDD occurs when a person experiences a reduction in positive reinforcement or an increase in negative outcomes. The loss of an important relationship can lead to depression because it reduces the amount of positive reinforcement that a person experiences. A variation of the learning approach suggests that MDD results from learned helplessnesslearned helplessnessA state in which experiencing random or uncontrolled consequences leads to feelings of helplessness and possibly depression            learned helplessness        A state in which experiencing random or uncontrolled consequences leads to feelings of helplessness and possibly depression            , which is an application of operant conditioning. Instead of experiencing consequences that are clearly linked to your previous behavior (I studied hard and earned an A on my test), learned helplessness occurs when consequences of behavior appear to be random or uncontrolled (the amount of studying that I do doesn’t seem to make a difference in my test grades). If you get low grades whether you study hard or not, you might begin to believe that grades are outcomes that cannot be controlled. You may begin to feel helpless in preparing for your exams, and this belief in your own helplessness can lead to depression. Cognitive theories frame depression as the result of a combination of negative thoughts about the self, the world, and the future (Beck, 1975). For example, students failing an exam might respond by doubting their academic abilities (self), assuming that they will fail the course (future), and deciding that they hate the class (world). The resulting dysfunctional beliefs would lead to depression. Beck’s view of the development of depression led directly to the use of cognitive therapies for depression, which are discussed in Chapter 15. Rates of both depression and rumination drop in middle adulthood. Young adults often find it frustrating when they are trying to ruminate about problems with their parents, only to have their parents immediately shift into problem-solving mode. The young adult might not feel ready to fix the problem yet. Another cognitive process that contributes to the development and maintenance of depressed mood is rumination (Nolen-Hoeksema, 2003). Susan Nolen-Hoeksema describes rumination in depression as repetitively focusing on the fact that one is depressed; on one’s symptoms of depression; and on the causes, meanings, and consequences of symptoms of depression (Nolen-Hoeksema, 1991, p. 569). Rumination can arise from attempts to gain insight into one’s problems, but too much rumination interferes with problem solving (Watkins & Brown, 2002). When rumination is encouraged by asking study participants to focus on their mood, they find it difficult to solve several types of problems (see Figure 14.12). When the participants are distracted from this inward focus by thinking about neutral topics (such as think about the Statue of Liberty), their problem-solving abilities improve immediately. Typically, rumination is correlated with negative outcomes, including delaying recovery from MDD during treatment (Siegle, Sagratti, & Crawford, 1999), predicting MDD following the death of a loved one (Nolen-Hoeksema & Girgus, 1994) and increasing a person’s likelihood to abuse alcohol (Nolen-Hoeksema, 2003). Responding to problems by either ruminating or distracting oneself might help explain some gender differences in depression rates. Women are more likely to ruminate than men, whose coping strategies are more likely to consist of finding distractions (Nolen-Hoeksema, 1987;  Nolen-Hoeksema, Larson, & Grayson, 1999). Rumination and distraction were induced in depressed and nondepressed study participants, who then completed a problem-solving task. To induce rumination, participants were instructed to think about what your feelings mean. To induce distraction, participants were asked to think about the shape of a large black umbrella. Subsequently, all participants were asked to complete a problem-solving task. More errors occurred in the depressed ruminating group than in the other groups. These results suggest that rumination competes with other cognitive processes, possibly contributing to the problems with concentration experienced by people with depression. MDD might be influenced by the types of attributions that a person makes (Abramson, Seligman, & Teasdale, 1978). As mentioned in Chapter 13, an attribution is a belief about causality. Attributions can vary along several dimensions (internal–external, stable–unstable, or global–specific), and making some types of attributions might predispose a person to depression. The internal–external dimension, which is similar to the locus of control discussed in Chapter 12, represents beliefs that outcomes occur because of personal effort or because of luck or chance. The stable–unstable dimension captures beliefs about whether circumstances can change. People make global attributions (I’m stupid) or more specific attributions (I’m not great in algebra, but I’m good at geometry). Overall, people who make internal, stable, and global attributions (what happens to me is my fault, circumstances never change, and I’m always this way) are more prone to depression. People with this attributional style earn lower grades in college, perform more poorly as sales representatives, and experience worse health (Seligman, 1987). Depression reflects feeling generally sad, whereas loneliness is a more social process that reflects feeling badly about the state of one’s relationships with others. The two states are related, yet they can occur independently. In older adults, loneliness increased depressive symptoms, but depression did not increase loneliness (Cacioppo, Hawkley, & Thisted, 2010). Social and evolutionary theories of depression and loneliness suggest that these states might promote an individual’s survival by promoting better relationships with others (Allen & Badcock, 2003;  Cacioppo, Cacioppo, & Boomsma, 2014;  Cacioppo, Hughes, Waite, Hawkley, & Thisted, 2006). Expressions of depression, such as sad facial expressions and crying, can serve as a safe call to others for connection and comforting responses in what the person may feel is a threatening social context. The response of others to these signals may foster the repair of frayed or broken social connections. Even when others do not respond, the symptoms of MDD may protect people from additional negative interactions by reducing their social activities. The biological perspective identifies a number of factors contributing to depression. Twin studies suggest that the heritability of MDD is about 40% (Shi et al., 2011). A number of genes have been implicated in depression, including genes affecting serotonin function (Wurtman, 2005). A large body of research points to a role for serotonin in the regulation of mood. Serotonin’s role in brain systems responsible for mood, appetite, and sleep corresponds closely to the symptoms of MDD outlined in the DSM-5. Most effective medications for the treatment of depression boost the activity of serotonin at the synapse. The anterior cingulate cortex plays an important role in regulating emotion through its connections with the amygdala. In many cases of MDD, the cingulate cortex appears to be overactive. Reductions in activity in the anterior cingulate cortex are correlated with improved mood following several different types of treatments. MDD might be part of a larger disturbance in daily, or circadian, rhythms (Soria et al., 2010). Among the genes suspected of being related to MDD are those involved with circadian rhythms (see Chapter 6; also see McClung, 2007). The DSM-5 criteria for MDD include sleeping either less than or more than a normal number of hours per night (7–9 hours) (APA, 2013). People who are depressed spend too much time in rapid eye movement (REM) sleep, and most medications that are used to treat MDD reduce REM sleep (Rijnbeek, de Visser, Franson, Cohen, & van Gerven, 2003) (see Figure 14.14). These sleep records show the normal progression of non-REM and REM sleep (upper row) and the disrupted sleep of a person with major depressive disorder (bottom row). Note the lack of Stage 3 and 4 non-REM sleep and the frequent waking experienced by the person with depression (see Chapter 6 for more information). How does the world look to a person who is depressed? Does the experience of depression affect a person’s attention to negative stimuli? As mentioned in Chapter 7, positive and negative emotions often serve as guides to approach and avoidance behaviors (Davidson & Irwin, 1999). Because it is usually more important to survival that we avoid predators and other dangers than that we miss approaching something positive, such as a tasty food, we have a bias toward noticing the negative. If we’re experiencing an ongoing negative mood, does that have an effect on our sensitivities to positive and negative stimuli? Among the important types of positive and negative stimuli that we need to process are facial expressions, because these are our clues to navigating the social environment successfully. Few of us would miss dramatic facial expressions, but the subtle ones require more attention (Gollan, McCloskey, Hoxha, & Coccaro, 2010). The Question: How do people with depression compare to control participants in their ability to detect subtle facial expressions of emotion? In one study, 88 participants, half of whom had been diagnosed with major depressive disorder but were not treated with medication, observed photographs of facial expressions on a computer screen. The photographs depicted four basic emotions: happiness, surprise, sadness, and harsh (a category that combined photos showing disgust, fear, and anger). The photos were morphed to present intensities ranging from mostly neutral to very intense (see Figure 14.13). A total of 200 photos were presented for only 0.5 second each. The participants pushed one of six keys to identify the emotion being expressed. Study participants see one photo for only 0.5 second and must determine which of six emotions (happiness, surprise, sadness, disgust, fear, or anger) is being displayed. Morphed images are made by combining neutral (0%) and high-intensity (100%) images. Each emotion is easier to identify as its intensity increases. Participants with depression needed to see the same amount of intensity as healthy control participants before correctly identifying happy, surprised, or harsh facial expressions but needed to see less intensity than control participants before correctly identifying sad faces. This heightened sensitivity to the sad moods of others might reinforce the negative feelings of the person with depression. It is unlikely that this procedure would produce distress in any of the participants. Because some of the participants are identified as having a psychological disorder, great care should be taken to protect their confidentiality. The participants with depression were more sensitive to sad faces than the control participants were. Specifically, all participants identified strong expressions of sadness, but participants with depression were more accurate in the identification of sadness when the emotion was expressed at a lower intensity. The groups were equally accurate in identifying the positive emotions (happiness and surprise) and the harsh emotions (fear, anger, and disgust). The severity of the participants’ depression was positively correlated with their accuracy in recognizing sad faces, but not with their accuracy in recognizing other emotional expressions. The participants with depression were more sensitive to subtle expressions of sadness than the healthy control participants, and they were more likely to misidentify other emotional facial expressions as sad. In contrast, they were just as likely as control participants to misidentify happy, surprised, or harsh facial expressions. These findings indicate that people with depression not only are more likely to attend to sadder stimuli, but also might misinterpret neutral facial stimuli as sad. These tendencies might reinforce a person’s depressive feelings. Major depressive disorder might result from more global problems with circadian or daily biorhythms. Among the candidate genes producing vulnerability for major depressive disorder are those that contribute to individual circadian rhythms. People who are depressed sometimes think about suicide or even make suicide attempts (see Figure 14.15). Because of the greater prevalence of major depressive disorder in younger people, they are also at greater risk of suicide. Approximately 1,100 college students in the United States die in suicides each year, making suicide the second-leading cause of death in this population (Wilcox et al., 2010). Among individuals experiencing at least one depressive episode in the past year, thinking seriously about suicide and making a suicide attempt happened frequently. These statistics raise the possibility that as a college student, you may become aware of another person’s risk for suicide. According to the American Foundation of Suicide Prevention (AFSP), we can make ourselves aware of risk factors and take actions that might save a life (AFSP, 2014). Among these risk factors are evidence of a psychological disorder (particularly depression), alcohol and other substance use, a history of suicide attempts, being male (males have a three to five times higher suicide completion rate than females), and impulsivity. More immediately, the occurrence of a traumatic event (job loss or ending of an important relationship) might increase a person’s risk. Approximately 75% of individuals who attempt suicide show prior evidence of their intent (AFSP, 2014). People might say things like My family would be better off without me or speak in ways that indicate they are saying good-bye. Purchasing a gun or putting one’s affairs in order (e.g., giving away prized possessions like a computer) is a sign of risk. Hopelessness, rage, or increased use of drugs and alcohol is also a common response to feeling suicidal (AFSP, 2014). If you suspect someone of being suicidal, experts advise you to take the situation seriously. Yes, it may be true that some people cry wolf, but with a life at stake, who wants to take that chance? Be a good listener, and reassure the person that depression can be treated, that he or she is not alone, and that you care. If possible, take your friend to a place where professional help is available, whether that is your campus health center or a local emergency room. Do not leave the person alone, and remove possible means of self-harm (drugs, weapons, etc.). If such steps are not possible, call the National Suicide Prevention Lifeline (800-273-TALK) or 911. Understanding that the combination of depression, impulsivity, alcohol or substance use, a history of suicide attempts, and a recent breakup or other traumatic event may increase the risk of suicide can help us offer support to a person who exhibits these characteristics. The experience of severe stress, whether arising from the death of a loved one, illness, or frustration of major life goals, frequently occurs immediately before the onset of a depressive episode (Brown & Harris, 1989;  Mazure, 1998). The physical and cognitive responses to high levels of stress, such as difficulty sleeping or concentrating, are quite similar to the symptoms of MDD. However, most people exposed to such stressors do not respond by becoming depressed, reinforcing the complexity of the development of this disorder. MDD provides an example of a diathesis–stress modeldiathesis–stress modelA model that suggests that the experience of stress interacts with an individual’s biological predisposition to produce a psychological disorder.            diathesis–stress model        A model that suggests that the experience of stress interacts with an individual’s biological predisposition to produce a psychological disorder.            , which suggests that biological vulnerabilities for a disorder interact with a person’s experience with stress (Zubin & Spring, 1977). The picture emerging from this discussion of causal factors in MDD is complex. It is likely that underlying biological predispositions interact with cognitive patterns and life events to produce the disorder. For example, the serotonin transporter gene, which has been discussed previously in this book in the contexts of personality and responses to bullying, does little by itself to help predict a person’s risk for MDD. However, among people experiencing significant stress, those with one or two copies of the short version of the gene were more likely than those with two copies of the long version to develop MDD (Caspi et al., 2003). A biological bridge is formed across the experience of stress, circadian rhythms, and the development of depression by the release of hormones (e.g., cortisol) when the stressor is perceived. Cortisol levels follow circadian patterns, with the greatest cortisol release in the early morning followed by a gradual drop throughout the day and evening. However, cortisol is also released at times of stress and helps prepare the body for fight or flight. Feedback loops involving the hippocampus usually prevent too great a release of cortisol. It is possible that in cases of MDD, this feedback loop is not working correctly, and cortisol levels remain high (Stokes, 1995). The body simply cannot maintain high levels of arousal indefinitely, and MDD may result. DisorderSymptomsPossible Causal Factors Under InvestigationSchizophreniaDelusionsHallucinationsDisorganized thought and speechMovement disordersRestricted affectAvolition or asocialityGeneticsReduced frontal lobe activityDisruption of brain developmentDopamine and glutamate abnormalitiesStressPrenatal environmentMarijuana useBipolar disorderElevated moodGrandiosityLittle need for sleepRapid speechDifficulty concentratingPossible depressionGeneticsSerotonin disturbancesDietMajor depressive disorderChronic depressed moodLack of pleasureSleep disturbancesAppetite disturbancesDifficulty concentratingHopelessnessPossible suicidal thoughtsLoss of positive reinforcementLearned helplessnessNegative cognitionsRuminationDepressive attributionsStressGeneticsReduced serotonin activityPatterns of brain activityCircadian rhythmsAnxiety disorders take many forms, but all share the core characteristic of unrealistic and counterproductive levels of anxiety. Anxiety has two major components: strong negative emotions andphysical tension because of the anticipation of danger (Barlow, 1988). It is the anticipation of danger that separates anxiety from the closely related emotion of fear. When we are afraid, usually something is happening in the present to produce that feeling, whereas anxiety occurs when we are worried about the future. Anxiety disordersAnxiety disordersA disorder featuring anxiety that is not proportional to a person’s circumstances.            Anxiety disorders        A disorder featuring anxiety that is not proportional to a person’s circumstances.             represent an exaggeration of what is normally a useful response. Normal levels of anxiety protect us from engaging in risky activities, such as running up debts, driving recklessly, or failing to prepare for work or school, but excessive anxiety can cause a person to withdraw from positive life experiences and interpersonal relationships. Nearly 30% of all Americans experience one or more anxiety disorders during their lifetime, although not all seek treatment (Kessler et al., 2005). People have genetic vulnerabilities for anxiety disorders in general, but not for specific types of anxiety disorders (DiLalla, Kagan, & Reznick, 1994). Families with members who are diagnosed with anxiety disorders are also likely to have members diagnosed with depression because these types of problems appear to share an underlying genetic basis (Weissman, Warner, Wickramaratne, Moreau, & Olfson, 1997). Anxiety disorders differ across gender and ethnicity for reasons that are not well understood. Women are more likely to be diagnosed with anxiety disorders than are men, and African Americans and Hispanics living in the United States are less likely to be diagnosed with anxiety disorders than are White Americans (Kessler et al., 2005). Anxiety is the anticipation of danger. Normal levels of anxiety remind us about the risks of engaging in dangerous activities, but disordered anxiety can prevent people from engaging in everyday activities. Evidence of a predisposition to anxiety appears early in life (Biederman et al., 1990;  Schwartz, Snidman, & Kagan, 1999). As observed in Chapters 11 and 12, children’s initial temperament and reactivity can predispose them to anxiety. In one study, young infants showed consistent levels of response to novel stimuli, such as the smell of cotton swabs dipped in alcohol (Kagan, 1997), and 20% of the infants were classified as high reactives to these stimuli because of their increased activity and distress when novel stimuli were presented. When the high-reactive children were placed in a novel laboratory environment at the ages of 4–5 years, they still showed heightened levels of anxiety compared to their peers. For reasons that are currently poorly understood, a number of psychological disorders appear to be more prevalent in some racial and ethnic groups than others. Possible reasons for these discrepancies include true differences in risk, access to diagnosis and treatment, poverty, and willingness to discuss symptoms. As shown in Figure 14.16, differences appear between non-Hispanic Whites, Caribbean Blacks, and African Americans in prevalence for major depressive disorder (MDD) and general anxiety disorder (GAD) (Watkins, Assari, & Johnson-Lawrence, 2015). These data were collected from about 6,000 participants as part of the National Survey of American Life (NSAL). Data from over 6,000 participants in a national sample showed that non-Hispanic Whites, Caribbean Blacks, and African Americans differ in their rates of major depressive disorder (MDD) and GAD. The exact reasons for these differences remain unknown but might include true differences in risk, access to diagnosis and treatment, poverty, and willingness to discuss symptoms. While it is interesting and important to understand why disorders might be diagnosed more frequently in some populations compared to others,  Watkins et al. (2015) suggest an even more pressing need. In general, MDD and GAD are associated with chronic medical diseases (see Chapter 16), such as cancer, diabetes, stroke, and heart disease. However, these researchers found different patterns of connections across racial and ethnic groups. MDD, but not GAD, was associated with at least one chronic medical condition for both Caribbean Blacks and African Americans, but not for non-Hispanic Whites. Caribbean Blacks showed stronger associations with MDD and GAD and having one or more chronic medical conditions. Watkins et al. (2015) recommend that primary health-care screenings for psychological disorders among patients with chronic medical diseases could benefit from tailoring based on race and ethnicity. They argue for the importance of thinking about services for patients with chronic medical conditions who are at the apex of various racial, ethnic, and mental health intersections. Unlike most other types of psychological disorders, anxiety disorders do not impair a person’s ability to think realistically. In most cases, adults with anxiety disorders recognize that their circumstances do not warrant their extremely anxious responses, but they feel unable to control them. A person looking out the window of a skyscraper is well aware that the likelihood of falling is quite low, but feelings of fear and anxiety persist. Young children with anxiety disorders are usually less aware than adults that their feelings are unrealistic. Specific phobiasSpecific phobiasFears of objects other than those associated with agoraphobia or social anxiety disorder            Specific phobias        Fears of objects other than those associated with agoraphobia or social anxiety disorder             are fears of objects or situations other than those associated with agoraphobia and social anxiety disorder (see Figure 14.17). These fear stimuli may be animals, natural phenomena (water or earthquakes), blood and injury, or situations (flying, heights, bridges, and so on). People do not ordinarily form phobias about harmless objects such as coffee cups and keys (Mineka, Sutton, Craske, Hermans, & Vansteenwegen, 2006). In a large random sample of adults, 21.2% of women and 10.9% of men reported having at least one specific phobia, as defined by the DSM-5. Gender differences were seen in the prevalence of situational phobias (lightning, enclosed spaces, darkness, flying, and heights) and animal phobias (primarily spiders and snakes). However, no gender differences occurred in injury phobias (injections, dentists, and injuries). More than one type of specific phobia was reported by 5.4% of the women and 1.5% of the men. The evolutionary perspective suggests that phobias might be exaggerations of an otherwise useful sense of caution. The items that become fear stimuli have some realistic potential relationship to harm, such as snakes, spiders, and the sight of blood. As noted in Chapter 8, the connections formed in phobias can arise from classical conditioning. A stimulus (conditioned stimulus, or CS) becomes associated with a negative experience (unconditioned stimulus, or UCS) that results in fear (unconditioned response, or UCR), giving the CS the ability to elicit fear (conditioned response, or CR). We might be biologically prepared to form connections between certain stimuli and feelings of fear (Seligman, 1971). Learning also can influence the development of phobias through imitation and observation, such as when a child sees a parent scream when a spider creeps across the room. People with spider phobias are often convinced that the spider is intent on harming them, when spiders are usually just being spiders. In Japan, some people experience taijin kyofusho, which is a fear of offending or embarrassing other people with their odor, eye contact, or appearance (Tarumi, Ichimiya, Yamada, Umesue, & Kuroki, 2004). This condition is significantly different from the emphasis on a person’s own embarrassment that is typical in social anxiety as experienced in Western cultures. Cognitions influence specific phobias. Even though a spider in your house or garden is simply minding its own business while doing spider things, people with spider phobias tend to believe that the spider is intending to attack them or move toward them (Riskind, Moore, & Bowley, 1995). People who have height phobias misjudge real physical heights, believing that they are higher than they really are (Stefanucci & Proffitt, 2009). Social anxiety disorderSocial anxiety disorderA disorder characterized by an unrealistic fear of being scrutinized and criticized by others            Social anxiety disorder        A disorder characterized by an unrealistic fear of being scrutinized and criticized by others             is the fear of being scrutinized and criticized by others, particularly during public speaking and when meeting new people. Many of us are a little uneasy in these situations, but for people with social anxiety disorder, the fear and anxiety caused by these situations are out of proportion for the particular situation. In other words, the person with social anxiety disorder feels crippling anxiety while doing things that most people take in stride, like introducing yourself to others at a party. In most cases, social anxiety disorder begins during adolescence. As discussed in Chapter 12, people experience a spotlight effect, or a belief that others are scrutinizing them. For an anxious teen, this spotlight effect might be exaggerated, leading to social anxiety disorder. Morita Masatake (1874–1938) founded the Morita Therapy for the treatment of anxiety disorders, including the culture-bound syndrome taijin kyofusho, which affects 10% to 20% of the Japanese population. This condition, recognized in the DSM, is characterized by fear of offending others by blushing or by having an offensive appearance or body odor. Morita recommended treating anxiety by accepting your feelings, knowing your purpose, and doing what needs doing. His therapy continues to be used today. Cultural variables appear to be particularly significant in the development of social anxiety. Social anxiety is more prevalent in collectivistic cultures, with their emphasis on shame and the opinions of other people, than in individualistic cultures. Asian American college students were more likely than White American students to report negative emotions in social situations (Lee, Okazaki, & Yoo, 2006). The DSM-5 distinguishes between the experience of a single panic attackpanic attackThe experience of intense fear and autonomic arousal in the absence of real threat            panic attack        The experience of intense fear and autonomic arousal in the absence of real threat            , characterized by intense fear and autonomic arousal, and the experience of panic disorderpanic disorderA disorder characterized by repeated panic attacks and fear of future attacks.            panic disorder        A disorder characterized by repeated panic attacks and fear of future attacks.            , which features repeated panic attacks and fear of future attacks (APA, 2013). Just having a panic attack does not mean that you should be diagnosed with panic disorder. It is not unusual for people to experience one or two panic attacks in their lifetime. One quarter to one third of college students report having had at least one panic attack in the previous year (Asmundson & Norton, 1993;  Brown & Cash, 1990). However, panic disorder is much less common, affecting 2.7% of the population in any given year (Kessler et al., 2005). Women are more likely to experience panic attacks and panic disorder than are men. More than half of people with panic disorder also have been diagnosed with depression or another type of anxiety disorder (Kearney, Albano, Eisen, Allan, & Barlow, 1997). Panic attacks begin to occur in adolescence and young adulthood (Robins & Regier, 1991). The Scream, a famous series of four paintings by Edvard Munch (1863–1944), was reportedly based on a panic attack that he suffered while out with two friends at sunset. By all reports, panic attacks are extremely unpleasant. Symptoms typically last about 10 minutes. Feelings of intense fear or discomfort are accompanied by both physical and cognitive symptoms. Strong arousal of the sympathetic nervous system leads to the experience of a pounding heart, sweating, trembling, shortness of breath, chest pain, nausea, and dizziness or faintness. Not too surprisingly, people experiencing such symptoms may believe they are dying or going crazy. For some individuals, panic attacks may be brought on by identifiable stimuli, such as being swept along in a crowd. For others, the attacks seem to come out of nowhere. In either case, many people experiencing repeated attacks may begin to fear leaving their homes, a condition known as agoraphobia, which will be discussed in a later section. A clue to the biological origin of panic attacks is that they can be produced artificially in people with panic disorder, but not in people who do not have the disorder, by administering an injection of sodium lactate (Papp et al., 1993;  Pitts & McClure, 1967). Sodium lactate interacts with orexins, chemical messengers that were discussed in Chapters 6 and 7. Orexins, which are released by cells in the hypothalamus, play important roles in wakefulness, vigilance, and appetite. People with panic disorder have larger quantities of orexins than do people without the disorder (Johnson et al., 2010). It is possible that disturbances involving the orexins might lead to panic attacks. Cognitive theories of panic attacks suggest that the interpretation of body symptoms, such as an increased heart rate, could lead to increasing anxiety and, ultimately, to panic (Bakker, Spinhoven, van Balkom, & van Dyck, 2002;  Harvey, Watkins, Mansell, & Shafran, 2004). A person might interpret increased heart rate and rapid breathing as a heart attack instead of the result of having walked briskly from a parking lot. Concern about having a heart attack leads to further physiological arousal, which leads to more anxiety, and so on until a full state of panic is reached. Unfortunately for such theories, panic surprisingly does not increase levels of the stress hormone cortisol, which suggests that there is more to a panic attack than just responding to the stress of thinking that you are ill (Hollander et al., 1989). Social influences can be seen in the symptoms related by people with panic disorder. Many people worry about appearing weird or crazy to other people, which can lead to increased anxiety and more panic attacks (Hicks et al., 2005). Culture plays an important role in how panic attacks are interpreted. Although the underlying physical characteristics remain similar from country to country, interesting variations have been observed. For example, dizziness is an important aspect of panic attacks in China, constipation and shortness of breath are common symptoms of panic in Rwanda, and gastrointestinal symptoms signal panic for patients in Thailand (Hinton & Good, 2009). Social factors also influence the development of panic disorder. Panic disorder is more common in children who have experienced parental loss or separation and in adults who have recently experienced significant, stressful life events (Klauke, Deckert, Reif, Pauli, & Domschke, 2010). A combination of the factors just discussed is likely to provide the best explanation for the development and experience of panic disorder. A panic attack could occur as a result of a chain of events: A person with a biological predisposition to panic, possibly involving the orexin systems of the brain,is exposed to social stressors in the form of parental loss or separation or significant life stressors,interprets physical symptoms of arousal as threatening or embarrassing, and finallyhas cognitions leading to panic that are modulated by that individual’s cultural expectations. AgoraphobiaAgoraphobiaUnrealistic fear of open spaces, being outside the home alone, or being in a crowd.            Agoraphobia        Unrealistic fear of open spaces, being outside the home alone, or being in a crowd.             is the fear of open spaces and is named after the ancient Greek agora, or marketplace. As mentioned previously, agoraphobia is a common outcome of panic disorder. When people become afraid of the embarrassment of having panic attacks in public, they might opt to simply stay home. Fear can occur in a number of situations, including being outside the home alone, using public transportation, being in open spaces such as parking lots or in shops, or standing in line or in a crowd. Agoraphobia often prevents working or engaging in normal social activities (Kessler et al., 2005). People with agoraphobia or fear of open places often fear being in crowds, like this one in Times Square in New York City. Generalized anxiety disorder (GAD)Generalized anxiety disorder (GAD)A disorder characterized by excessive anxiety and worry that is not correlated with particular objects or situations.            Generalized anxiety disorder (GAD)        A disorder characterized by excessive anxiety and worry that is not correlated with particular objects or situations.             is diagnosed when a person has experienced excessive anxiety and worry for 6 months that is not correlated with particular objects or situations (APA, 2013). In other words, the person experiences a great deal of worry, but the worry is focused on life in general, not upcoming midterms or spiders. GAD is associated with physical complaints, including headache, stomachache, and muscle tension. GAD is often comorbid, or coexisting, with other anxiety disorders, substance abuse, and depression (APA, 2013). Like all anxiety disorders, GAD shows evidence of genetic predisposition (Kendler et al., 1995;  Molina et al., 2011). A reasonable place to start looking for correlates of anxiety in brain structure and function is the fear circuit involving the amygdala, which was discussed in Chapters 4 and 7. The amygdala is particularly rich in receptors for GABA, a neurotransmitter that inhibits brain activity. As discussed in Chapter 6, drugs such as alcohol and the benzodiazepine tranquilizers (e.g., Valium) have their main anxiety-reducing effects at these GABA receptors. Not too surprisingly, then, people often self-medicate with alcohol and tranquilizers to reduce their anxiety level, leaving them vulnerable to substance abuse disorders. The amygdala forms complex connections with the prefrontal areas of the frontal lobe (see Figure 14.18). As observed in the discussion of aggression in Chapter 13, the prefrontal cortex coordinates our decisions based on potential dangers identified by the amygdala. For example, if you see a driver’s angry face in your rearview mirror, your amygdala alerts the prefrontal cortex about a potential threat or danger, and your prefrontal cortex uses memories of similar past experiences and reasoning to decide to pull over safely to the slower lane to let the angry driver go by. It is possible that this circuit is not responding typically in cases of GAD. People with GAD have higher levels of activity in the cortex, thalamus, amygdala, and hypothalamus (Wittchen & Hoyer, 2001). The prefrontal cortex was especially active when people with GAD saw angry faces, suggesting that this area had to work harder to control incoming danger signals from the amygdala (Monk et al., 2008). Several cognitive models of GAD have been proposed (Beck, 1985). As discussed in Chapters 10 and 11, we often organize our thinking about the world into schemas, or systems of belief. If a high-reactive child develops a schema that says The world is a dangerous place, even safe experiences begin to look dangerous through this filter. A different cognitive approach suggests that worrying can become a coping strategy for anxiety. By examining what ifs, people might believe that they are coping, but so many situations are capable of triggering the coping response that the result is nearly constant worry (Wells, 2006). Bill Murray’s character in the 1991 film What About Bob? expresses this type of worry when he asks his psychiatrist, What if my bladder explodes? Individuals in lower socioeconomic classes in the United States are about twice as likely to be diagnosed with GAD as people in middle or upper socioeconomic classes (Kessler et al., 2005). Disruptions in social connectivity because of divorce, separation, death of a spouse, or loss of a job also are associated with higher rates of GAD (Wittchen & Hoyer, 2001). Pulling these perspectives together into an integrated model, we might predict that GAD is most likely to occur in individuals with a high-reactive temperament, possibly because of their genetic background, who subsequently develop schemas and other cognitive patterns that maintain high levels of worry. When these individuals experience unusual amounts of stress because of socioeconomic factors and disruption of social networks, their risk for developing GAD is likely to be even higher. The DSM-5 groups obsessive-compulsive disorder (OCD), which was formerly an anxiety disorder, with body dysmorphic disorder, hoarding disorder, and other similar problems (APA, 2013). Individuals with obsessive-compulsive disorder (OCD)obsessive-compulsive disorder (OCD)A disorder associated with intrusive obsessions and compulsions            obsessive-compulsive disorder (OCD)        A disorder associated with intrusive obsessions and compulsions             are haunted by distressing, intrusive thoughts (obsessionsobsessionsAn intrusive, distressing thought.            obsessions        An intrusive, distressing thought.            ); the need to engage in repetitive, ritualistic behaviors (compulsionscompulsionsRepetitive, ritualistic behavior associated with high anxiety.            compulsions        Repetitive, ritualistic behavior associated with high anxiety.            ); or both. Between 2% and 3% of the U. S. population report experiencing OCD during their lifetime (Ruscio, Stein, Chiu, & Kessler, 2010). Obsessions are distinct from everyday worries, which usually focus on real-life problems. Common obsessions include concerns about contamination (catching diseases from shaking hands), repeated doubts (wondering whether you have hurt somebody), ordering (needing things arranged in a particular order or configuration), inappropriate impulses (hurting somebody else or shouting obscenities at a funeral), and sexual imagery (APA, 2013). Some people with obsessive-compulsive disorder must arrange their possessions in particular ways; otherwise, they will experience overwhelming anxiety. The childhood rhyme Step on a crack, break your mother’s back captures the ineffectiveness of obsessive-compulsive thinking. Compulsions appear to be efforts to ward off the anxiety produced by obsessions or some other feared event. For example, people troubled by obsessions about contamination may engage in compulsive hand washing hundreds of times per day. Other common compulsions include checking (for locked windows or faucets turned off), counting, ordering objects, and requesting or demanding assurances. Twin studies suggest a strong genetic vulnerability for the disorder, with concordance rates between identical twins of 63%–87% (Menzies et al., 2008). It is probable that significant interactions between this genetic vulnerability and the environment occur. Symptoms of OCD may arise following head trauma, the brain inflammation caused by encephalitis, and seizure disorder (Grisham, Anderson, & Sachdev, 2008). Young children who experienced birth complications or streptococcal infections (e.g., strep throat) may be more likely to develop the disorder later in life (Swedo et al., 1997). Imaging studies provide insight into the patterns of brain activity that accompany obsessions and compulsions (see Figure 14.19). Several structures, including the orbitofrontal cortex, prefrontal cortex, the anterior cingulate gyrus, and the caudate nucleus of the basal ganglia, appear to be more active than usual in people with OCD (Saxena, 2003;  Szeszko et al., 2004). Individuals with OCD appear to have lower than normal amounts of serotonin activity, and individuals with the lowest levels of serotonin activity experience the most severe obsessions and compulsions (Piacentini & Graae, 1997). When compared to healthy controls, people with OCD have greater gray matter density in the areas shown in red. When people with OCD experience symptoms, the areas in yellow become especially active. Areas of the orbitofrontal cortex, shown in orange, are anatomically different from those in healthy controls and are activated during OCD symptoms, suggesting an important role for the orbitofrontal cortex in OCD. The evolutionary perspective suggests that OCD might have its roots in normal grooming and territoriality behaviors that somehow become overly exaggerated in some cases (Rapoport, 1989). Humans who anticipated threats like disease from dirty hands or threats from the perimeters of their territories probably benefited from proactive strategies like hand washing and checking their surroundings (Brune, 2006). Disordered behavior similar to OCD occurs elsewhere in the animal kingdom. Dogs may lick and chew their front paws to the point of causing extensive tissue damage, which can be prevented by giving the dogs the same medications used to treat humans with OCD (Seksel & Lindeman, 2001). Learning explanations for OCD focus on the rewarding aspects of compulsions. As pointed out in the previous discussion of operant conditioning, any behavior that produces a positive outcome (reducing anxiety associated with an obsession) is likely to be repeated in the future. For example, if hand washing reduced anxiety about germs and disease, hand washing might become progressively more frequent to the point where it would be considered abnormal. The DSM-5 specifies that engaging in compulsive behavior for more than 1 hour per day is considered abnormal (APA, 2013). Culture plays a role in how frequently OCD occurs and what obsessions and compulsions are more likely. Anthropologists have identified interesting parallels between cultural rituals and OCD. For example, many normal cultural rituals involve elements similar to OCD, including washing, special colors, stereotyped actions, and rigid rules (Dulaney & Fiske, 1994). Like the evolutionary perspective, this cultural view suggests that OCD might represent an exaggerated version of normal behaviors. Body dysmorphic disorderBody dysmorphic disorderA disorder characterized by the unrealistic perception of physical flaws            Body dysmorphic disorder        A disorder characterized by the unrealistic perception of physical flaws             is characterized by the unrealistic perception of physical flaws. Many people with this condition undergo numerous cosmetic surgeries. Others continually try to perfect their bodies through bodybuilding activities and other strategies. Pop entertainer Michael Jackson was described by his mother as addicted to plastic surgery. She related that during adolescence, her son became obsessed with his perceived physical flaws. Although Jackson was never formally diagnosed with body dysmorphic disorder, many people with this disorder engage in similar types of body dissatisfaction and repeated requests for plastic surgery. Body dysmorphic disorder shows a significant influence of heredity (Feusner, Neziroglu, Wilhelm, Mancusi, & Bohon, 2010). In addition, people with this disorder might not be as out of touch with reality as you might think. Their visual perception of subtle facial distortions appears to be better than normal, so they may be noticing flaws that the rest of us cannot see (Reese, McNally, & Wilhelm, 2010). Posttraumatic stress disorder (PTSD)Posttraumatic stress disorder (PTSD)A disorder caused by the experience of trauma, which leads to flashbacks, dreams, hypervigilance, and avoidance of stimuli associated with the traumatic event.            Posttraumatic stress disorder (PTSD)        A disorder caused by the experience of trauma, which leads to flashbacks, dreams, hypervigilance, and avoidance of stimuli associated with the traumatic event.             is the current term used to describe a condition formerly known as shell shock or battle fatigue. PTSD is unique among disorders listed in the DSM-5 because it is the only one with a specified cause: To be diagnosed with PTSD, you must have experienced trauma (APA, 2013). Although combat, especially hand-to-hand combat, is still recognized as one of the most likely experiences leading to PTSD, we now understand that other types of trauma may produce the same results (Insel, 2007). PTSD may follow the experience of automobile accidents, assaults, abuse, and natural disasters. Although combat remains the most frequent cause of PTSD, people who experience natural disasters, like the 2011 Japanese earthquake and tsunami, are also at risk for the disorder. PTSD symptoms include hypervigilance, avoidance of stimuli associated with the trauma, emotional numbing, and reexperiencing in the form of repetitive, intrusive thoughts, flashbacks, and dreams about the traumatic event (APA, 2013). The last thing that a person in this situation wants to do is to relive the traumatic event, so these symptoms are upsetting. Children with PTSD often reenact the traumatic event in their play (Cohen, Chazan, Lerner, & Maimon, 2010). In other words, they may play terrorist attack, car crash, or tsunami, possibly in an effort to reduce their fear. This reenactment play can have either positive or negative effects on the child. Children whose play included a reworking of the traumatic event (revenge or a happy ending) were less disturbed than children whose play was repetitive, without resolution. PTSD affects between 3% and 4% of the adult population in the United States every year (Kessler et al., 2007). After large-scale traumatic events, health workers typically expect about 10% of the population to develop PTSD (Kessler, Sonnega, Bromet, Hughes, & Nelson, 1996). For example, more than 12% of lower Manhattan residents developed PTSD following the terrorist attacks of 9/11 (DiGrande et al., 2008). Children are more vulnerable than adults, with 25% developing PTSD following automobile accidents in which they were injured, compared to 15% of their parents (de Vries, 1999). Between 8.5% and 14% of combat soldiers serving in Iraq and Afghanistan experienced severe impairment because of symptoms of PTSD after returning home, and up to 31% experienced some impairment (Thomas et al., 2010). There are Hidden Costs in War. Soldiers, most of whom are about the same ages as those of you reading this textbook, are permanently changed by their experiences in ways that are often difficult for those of us who have not shared those experiences to understand. They have faced the fear of death and the realities of injury, held dying friends in their arms, and felt guilty that their lives were spared. Some cope with these challenges better than others. As stated earlier in this chapter, between 8.5% and 14% of U. S. veterans serving in Iraq and Afghanistan experience severe PTSD, and up to 31% experience some impairment (Thomas et al., 2010). Veterans usually come home to their loved ones and families, who are also affected by the soldiers’ combat experiences in what has been called secondary traumatization (Galovski & Lyons, 2004). Although not directly threatened, the soldiers’ families have had challenges of their own as they cope with their helpless fear of loss. Understanding that bad news comes to the front door in the person of a military chaplain makes each FedEx delivery or visit by a neighbor a heart-pounding experience. The spouses who have learned over the course of a year to run households independently and balance the checkbook might not want to share some of these responsibilities with their returning partners. Among the biggest challenges for veterans with PTSD and their families are the results of having symptoms of hyperarousal, emotional numbing, and stimulus avoidance (Monson, Taft, & Fredman, 2009). Although veterans without PTSD have the same rates of intimate aggression as the general population, the risk of intimate aggression is much higher in those with PTSD, and the severity of symptoms predicts the likelihood of aggression. Because of the emotional numbing and avoidance symptoms of PTSD, returning veterans do not engage in much self-disclosure, or discussion of their thoughts, feelings, and experiences. As discussed in Chapter 13, self-disclosure is an important component of intimacy. Both soldiers and their families are changed by war, and their reunions, while happy, are not always smooth. Strong social support is a critical variable in the development and course of PTSD resulting from combat. A soldier’s existing level of social support is one of the strongest protections against the initial development of PTSD (Brewin, Andrews, & Valentine, 2000). After trauma, a person with PTSD recovers more quickly with strong social support. Unfortunately, with ongoing PTSD, that social support tends to diminish as the symptoms of PTSD take their toll on relationships (Kaniasty & Norris, 2008). By recognizing the importance of the larger social network to PTSD, psychologists can begin to develop more effective preventions and interventions that take the entire social system into account, not just the individual with PTSD symptoms. Biological explanations for PTSD focus on the hippocampus, a structure that has been discussed previously in the context of the biology of memory formation. Brain imaging studies consistently show that PTSD is correlated with smaller hippocampal volume (Bossini et al., 2008;  Bremner et al., 2002). This leaves us with two competing hypotheses: Are people with a naturally smaller hippocampus more vulnerable to PTSD? Or is there something about the experience of trauma that is toxic to the hippocampus? Both hypotheses seem reasonable. As we observed in Chapter 7, stress increases the release of cortisol, which in turn might have toxic effects on the hippocampus (Sapolsky, Krey, & McEwen, 1985). In Vietnam veterans with PTSD, decreases in hippocampal volume are greater as a function of the amount of time that has elapsed since a traumatic event, suggesting that hippocampal damage can continue steadily because of the disorder (Chao, Yaffe, Samuelson, & Neylan, 2014). Comparisons of twins have also identified PTSD-related damage to the anterior cingulate cortex (Kasai et al., 2008). PTSD is correlated with lower levels of naturally occurring benzodiazepine activity in the frontal cortex (Geuze et al., 2008) (see Figure 14.20). Extra benzodiazepines can be administered in the form of major tranquilizers, such as Valium, which produce relaxation and lack of anxiety. If naturally occurring benzodiazepine activity was low, anxiety would be the logical result. Lower levels of natural benzodiazepine activity might also explain the unfortunate tendency for people to self-medicate for PTSD with alcohol (Jakupcak et al., 2010). Alcohol reduces anxiety using biological mechanisms similar to those of the benzodiazepines. Yellow areas show reduced binding of benzodiazepine, which has a tranquilizing effect, in the brains of combat veterans with PTSD compared to those of combat veterans who do not have PTSD. These biological correlates form a pattern of dysregulation of circuits that allow us to process information within its proper context (Liberzon & Abelson, 2016). For example, seeing a mountain lion in your backyard should evoke a very different response than viewing a mountain lion safely within its compound at a zoo. This basic use of contextual information allows us to behave appropriately (whether it is to enjoy the view or act defensively). If the networks that help us connect context to behavioral and emotional responses become disrupted, we might see danger in realistically safe situations. Learning, and classical conditioning in particular, contribute to the symptoms of PTSD. Individuals with PTSD show conditioned responses to stimuli associated with the traumatic event. Combat veterans with PTSD seem particularly sensitive to smells such as burning rubber and diesel (Rothbaum, Rizzo, & Difede, 2010). Individuals with PTSD show increased generalization to conditioned fear stimuli, suggesting that over time, they will become responsive to more stimuli, not less (Kaczkurkin et al., 2016). Social and cultural factors play significant roles in the development of PTSD. People reporting low social support in the 6 months before 9/11 were twice as likely to be diagnosed with PTSD as people who reported strong social support during this period (Galea et al., 2002). Cultural factors also influence the likelihood of PTSD. Following 9/11, PTSD was more common in New York City residents of some ethnicities than others, affecting 13.4% of Hispanics, 9.3% of Black Americans, 6.5% of White Americans, and 3.2% of Asian Americans (Galea et al., 2002). Dissociative disordersDissociative disordersA disorder characterized by disruptions in a person’s identity, memory, or consciousness            Dissociative disorders        A disorder characterized by disruptions in a person’s identity, memory, or consciousness             feature disruptions in a person’s identity, memory, or consciousness (APA, 2013). Dissociative disorders take several forms. Dissociative amnesia occurs when a person forgets important information about specific events, often following a frightening or traumatic event. In dissociative fugue, people become confused over their identity and often combine this loss of identity with sudden travel and the assumption of a new identity. These situations typically last a few hours or days, but not longer periods of time (Kopelman, 2002). Heisman Trophy winner, NFL star, and mixed martial arts fighter Herschel Walker has spoken about his experience with dissociative identity disorder to encourage military veterans to seek help for psychological disorders. Depersonalization or derealization disorder occurs when a person experiences strong feelings of unreality about either the self or the surrounding environment (APA, 2013). People might feel as though they are watching their behavior from outside their bodies or that the world is dreamlike. However, they retain their ability to understand that their feelings are not realistic. In other words, they feel as if they’re outside their bodies, but they don’t believe that this is really happening. Possibly the most controversial of the dissociative disorders is dissociative identity disorder, which was previously known as multiple personality disorder. According to the DSM-5, dissociative identity disorder is characterized by the experience of two distinct personality states (APA, 2013). Dissociative identity disorder experienced a spike in popularity as a diagnosis in the mid-1990s but has since dropped in frequency. Media attention might have contributed to the frequency with which this disorder was diagnosed. Before the popularization of patients named Sybil and Eve in books and films, few cases of dissociative identity disorder had ever been described in the scientific literature. Following this media interest, thousands of cases were diagnosed, primarily in the United States and Canada. Responses to extreme stress frequently include detachment, numbness, or altered memory and cognition, but this is quite different from the development of additional identities. Well-meaning therapists can misinterpret these stress symptoms as dissociative identity disorder and, worse yet, suggest this possibility to their clients. People diagnosed with dissociative identity disorder, like those with other dissociative disorders, score very high on tests of suggestibility that predict a person’s susceptibility to hypnosis, as well as in measures of being likely to fantasize (Giesbrecht, Lynn, Lilienfeld, & Merckelbach, 2010). These findings imply that these people may have difficulty distinguishing between reality and fantasy and might accept a therapist’s suggestion without considering it critically. Historically, somatic symptom disordersomatic symptom disorderA disorder characterized by physical symptoms that do not have an underlying medical cause            somatic symptom disorder        A disorder characterized by physical symptoms that do not have an underlying medical cause             involved physical symptoms that do not have an underlying medical cause (APA, 2013). The DSM-5 notes that these patients spend excessive amounts of time thinking about the seriousness of their symptoms, which is accompanied by a high level of anxiety. To the patient, the somatic symptoms appear quite serious and often disabling. Among the different symptoms are vague pain complaints, gastrointestinal upset, sexual problems, amnesia, breathing problems, or unexplained sensory or motor problems. People with somatic symptoms usually visit physicians frequently, report high numbers of physical complaints, and are at risk of becoming dependent on pain medication. They can become preoccupied with their health and often insist on unnecessary medical tests and procedures. A personality disorderpersonality disorderA disorder characterized by impairments in identity, in personality traits, and in establishment of empathy or intimacy.            personality disorder        A disorder characterized by impairments in identity, in personality traits, and in establishment of empathy or intimacy.             is characterized by impairments in identity, in personality traits, and in establishment of empathy or intimacy. Personality disorders cannot be diagnosed in people under the age of 18. To illustrate this category of disorders, we will now look at three personality disorders in greater detail: antisocial personality disorder (ASPD), borderline personality disorder, and narcissistic personality disorder. Anders Behring Breivik killed 8 people with a bomb before shooting another 69 people at a nearby youth camp on July 22, 2011. Police told reporters that during a reenactment of the shootings, Breivik didn’t show any remorse. Although we don’t know whether Breivik could be diagnosed with antisocial personality disorder, a callous lack of empathy and remorse is typical of people with this diagnosis. Antisocial personality disorder (ASPD)Antisocial personality disorder (ASPD)A disorder characterized by an unusual lack of remorse, empathy, or regard for normal social rules and conventions.            Antisocial personality disorder (ASPD)        A disorder characterized by an unusual lack of remorse, empathy, or regard for normal social rules and conventions.             is characterized by a pervasive pattern of disregard for and violation of the rights of others (APA, 2013, p. 659). Individuals with this disorder have little regard for normal social rules and conventions. Their behavior is frequently risky and irresponsible, and they form shallow, fleeting relationships with others. ASPD is quite similar to the term psychopath, which has not been used by the DSM system since its third edition was published in 1980. Criteria for ASPD in DSM-III and DSM-IV focused on a person’s behaviors, especially criminal behaviors, whereas the term psychopath implies flaws in the person’s consistent traits or character (Hare, 2006). Between 0.2% and 3.3% of the population experiences ASPD in a given year (APA, 2013).  Hare (2006) has estimated the number of psychopaths in the United States to be approximately 1%. Most individuals with ASPD or psychopathy do not engage in outright criminal behavior, and not all criminals have ASPD or psychopathy. Noncriminal individuals with ASPD were easily recruited by placing a newspaper ad calling for charming, aggressive, carefree people who are impulsively irresponsible but are good at handling people and at looking after number one (Widom, 1978, p. 72). Research on individuals with antisocial parents suggests a genetic contribution to this disorder. Related traits such as fearless dominance, impulsive antisociality, and callousness show substantial heritability rates between .46 and .67 (Blair, Colledge, Murray, & Mitchell, 2001). Children adopted at birth who have antisocial biological parents are more likely to engage in antisocial behavior (Cadoret & Cain, 1981). When children of biological parents who are not antisocial are adopted by antisocial parents, they are less likely than children with biological parents who are antisocial to engage in antisocial behavior (Mednick & Kandel, 1988). As mentioned previously in this textbook, child maltreatment interacts with a genetic predisposition to violent antisocial behavior (Caspi et al., 2003). A genetic influence on a trait usually implies that the trait provides some advantage, but how can this be the case for a tendency toward antisocial behavior? One suggestion is that fearlessness characterizes both heroes and people with antisocial behavior (Lykken, 1982). Having heroic people in your population might offset the negative impact of having some antisocial people. To test his hypothesis about the similarities between these two groups, Lykken devised an Activity Preference Questionnaire, which asks respondents to choose between a frightening or embarrassing situation and one that is simply unpleasant. For example, would you prefer to wash a car or drive a car 95 miles per hour? People who either engage in antisocial behavior or who have behaved heroically typically choose the risky alternative (driving fast) over a chore (washing car), whereas cautious people typically choose the chore over the risky alternative. The areas marked in blue showed less activity in criminal psychopaths than in noncriminal control participants in a study during the processing of negative emotional words like rape or torture. These areas are associated with emotional processing and include (a) the cingulate cortex and basal ganglia and (b) the hippocampus and amygdala. The failure of negative emotional stimuli to produce normal levels of arousal might allow the psychopath to engage in harmful behaviors without experiencing normal remorse or empathy. A relative lack of response to emotions, especially those of others, might also contribute to antisocial behavior disorder. While undergoing brain imaging, people with ASPD, unlike the control participants in a study, showed little activation of parts of the brain that are usually active during emotional processing when listening to words such as torture, rape, or cancer (Hare, 2006;  Kiehl et al., 2001). Participants also were shown neutral scenes and rather gruesome images of murder scenes. Again, the people with ASPD showed different patterns of brain activation than did the typical control participants. In particular, the amygdalae of people with ASPD showed relatively little activity. As discussed in Chapters 4 and 7, the amygdala normally participates in the recognition of fear and other negative emotions in others. Other research indicates that children with antisocial tendencies are impaired in their ability to identify expressions of sadness and fear in others, a finding consistent with lower activity in the amygdala (Blair et al., 2001). Antisocial behavior is associated with abnormalities of the orbitofrontal cortex. Orbitofrontal abnormalities occur at higher rates among murderers and those diagnosed with ASPD (Davidson, Putnam, & Larson, 2000). As observed in Chapter 4, damage to the frontal areas of the cortex often leads to poor judgment and impulsivity. In one representative case, two adults who had been raised in stable, middle-class homes but who had experienced damage to the orbitofrontal cortex in infancy both showed a profound inability to understand the consequences of their negative behavior (Anderson, Bechara, Damasio, Tranel, & Damasio, 1999). They engaged in stealing, lying, and aggressive behavior and were poor parents. At a 2002 Convention of Law Enforcement Officers in Canada, psychologist Robert Hare made a startling suggestion (Hare, 2002). With slides in the background depicting convicted WorldCom chief executive officer Bernard Ebbers and Enron chief financial officer Andrew Fastow, Hare asked his audience why law enforcement personnel are screened for psychopathy, but not executives responsible for millions of dollars of people’s savings and retirement funds. When we hear the term psychopath, most of us think of characters like Hannibal Lecter from The Silence of the Lambs. However, Hare and other psychologists suggest that psychopaths are more likely to be the people next door, your coworkers, and maybe even your friends and relatives. Not only are psychopaths relatively common (around 1% of the population), but their characteristics also might allow them to be disturbingly successful. When profiles of 39 top British executives were compared to those of criminals and psychiatric patients, the executives were found to be more charming, egotistical, manipulative, and lacking in empathy (Board & Fritzon, 2005). Consequently, researchers began to refer to the executives as successful psychopaths, whereas the criminals and patients were unsuccessful psychopaths. Hare and his colleague, Paul Babiak, would agree. Using scales that they hope to market to corporations wishing to avoid hiring the next Fastow, these researchers have shown that corporate psychopaths score very high on items measuring the callous, selfless, and remorseless use of others (Babiak & Hare, 2006). Unlike criminal psychopaths, however, the corporate psychopaths did not show patterns of instability and social deviance. Unfortunately, rapid and uncertain change in the business environment may provide fertile ground for corporate psychopaths to rise to the top. In a world of downsizing, an executive like Sunbeam’s Chainsaw Al Dunlap, who fired half the corporation’s workforce, might look like the right person for the job. But Dunlap also had been divorced by his first wife for extreme cruelty, thrown a chair at one of his executives, allegedly altered Sunbeam’s financial records to make his performance look better, and demanded additional severance pay when he was fired for cause. Bernie Madoff built one of the most successful businesses on Wall Street and served as the nonexecutive chairman of the NASDAQ stock market before being convicted of perpetrating one of the largest financial frauds in U. S. history. Psychologists Paul Babiak and Robert Hare suggest that it might be difficult to distinguish between tough corporate leaders and those with antisocial tendencies. Modern corporations often find it difficult to distinguish between the behavior of the corporate psychopath and the person who is a genuinely visionary leader. Leaders such as Bill Gates may seem insensitive to others’ feelings, according to leadership experts, but they turn their grandiosity toward making their organizations great rather than plundering them for personal gain (Maccoby, 2003). The term borderline personality disorder (BPD)borderline personality disorder (BPD)A disorder characterized by instability in interpersonal relationships, self- image, and emotion            borderline personality disorder (BPD)        A disorder characterized by instability in interpersonal relationships, self- image, and emotion             originated from the idea that the characteristics of this disorder fell on the border between anxiety and psychosis. According to the DSM-5, BPD is characterized by instability in interpersonal relationships, self-image, and emotion. Individuals with this disorder may engage in frantic efforts to avoid abandonment, either real or imagined, and may behave in impulsive, self-destructive ways (see Figure 14.21). Suicide and self-mutilating behaviors, which were discussed in Chapter 8, are quite common among people with this disorder. Relationships are often initially intense and idealized, but if the individual perceives any slight rejection, love can turn to anger quickly. Not only do people with borderline personality disorder perform differently than healthy controls in experimental activities measuring trust, but their brain activity during a trust game is also different. In healthy study participants, activity in the anterior insula decreases when an offer is higher or fairer. In the participants with borderline personality disorder, insula activity did not correspond well with the offers that they received. This inability to respond to trust normally might explain some difficulties these people experience in building healthy relationships with others. Borderline personality disorder affects about 2% of the population, with women outnumbering men by a factor of 3 to 1 (Skodol & Bender, 2003). People with BPD appear to be vulnerable to a number of additional disorders, including bipolar disorder, substance abuse, eating disorders, and other personality disorders. Borderline personality disorder is five times more common among close relatives with the disorder than among the general population, suggesting a biological predisposition (APA, 2013). However, as many as 70% of individuals with this diagnosis have histories of abandonment, neglect, and physical or sexual abuse, or both types of abuse (Hunt, 2007). These findings suggest that traumatic experiences interact with a possible genetic predisposition to produce disordered behavior. DisorderSymptomsPossible Causal Factors Under InvestigationPhobiaExcessive fear of an object or situationExaggeration of normal cautionClassical conditioningCognitions about fear stimulusCollectivist cultures emphasizing shamePanic disorderRepeated panic attacks, accompanied by excessive worry about having more panic attacksPossible abnormality in orexin functionCognitions regarding symptomsExpectations and embarrassmentCultural influencesGeneralized anxiety disorder (GAD)Excessive anxiety for six months that is not correlated with specific objects or situationsGenetic predispositionStressFearful schemasLow socioeconomic statusDisruptions in social connectivityObsessive-compulsive disorder (OCD)Intrusive, repetitious, and anxiety-producing thoughts and behaviorsGenetic predispositionLow serotonin activityPrefrontal and basal ganglia activityExtension of grooming and territorial behaviorsRewarding aspects of compulsionsExtension of cultural ritualsBody dysmorphic disorderUnrealistic perception of physical flawsGenetic predispositionSuperior perception of facial distortionsPosttraumatic stress disorder (PTSD)Exposure to a traumatic event, followed by flashbacks, hypervigilance, and nightmaresHippocampus and prefrontal cortexBenzodiazepine receptorsClassical conditioningSocial supportAs defined by the DSM-5, narcissistic personality disorder (NPD)narcissistic personality disorder (NPD)A disorder characterized by grandiosity, need for admiration, and low empathy            narcissistic personality disorder (NPD)        A disorder characterized by grandiosity, need for admiration, and low empathy             is characterized by grandiosity, need for admiration, and lack of empathy (APA, 2013). Renewed interest in NPD has occurred in part due to questions about possible intersections between narcissism and social media use (McCain & Campbell, 2016). Other psychologists are tracking narcissistic traits as a function of age, with younger individuals showing more narcissism than older adults (Twenge & Campbell, 2009). Narcissistic personality disorder combines grandiosity, need for attention, and low empathy. The social media phenomenon of the selfie is the perfect outlet for people with narcissistic tendencies (Weiser, 2015). Although the DSM-5 has only one category for NPD, psychologists continue to debate the relationship between two apparent subtypes—the grandiose narcissist and the vulnerable narcissist (Miller et al., 2011;  Miller, Lynam, Hyatt, & Campbell, 2017). The grandiose narcissist tends to show inflated views of self-worth, aggressiveness, and dominance, while the vulnerable narcissist combines inflated views of self-worth with insecurity, defensiveness, and negative emotions. According to Miller et al. (2011, 2017), the DSM emphasizes the grandiose version, which might result in clinicians being less aware of the vulnerable version among their clients. One possible solution is to use trait models like the Big Five traits, described in Chapter 12, to distinguish between the two patterns. For example, the grandiose narcissist is extroverted, while the vulnerable narcissist is shy and neurotic. We are not certain how narcissistic behavior develops. The personality disorder itself is not to be diagnosed officially until adulthood, but the roots of narcissism might be found in the development of self-esteem strategies in the child (Thomaes, Bushman, De Castro, & Stegge, 2009). As such, narcissism might be simply an extreme version of the impression management and search for esteem that are considered normal in children and teens. Narcissistic traits have been shown to be somewhat heritable, and they possibly have their beginnings in basic temperament (Thomaes et al., 2009). Chapter 12 discussed reactivity, or a child’s tendency to either approach or avoid a stimulus. The pattern of behavior seen in the grandiose narcissist is consistent with approach, while the vulnerable narcissist seems to show both approach and avoidant tendencies. As discussed in Chapter 12, temperament quickly interacts with environmental factors, particularly those in the surrounding social setting. Parents who overpraise their children might produce habituation to praise, leading a child to seek constantly higher levels of positive feedback. Cold parents with very high expectations for their children might also stimulate narcissistic behaviors as children attempt to protect themselves while craving positive feedback from others as compensation. As you have Read in this Chapter, narcissism is characterized by grandiosity, entitlement, and low empathy. While this doesn’t sound very attractive, people who score high in narcissistic traits are also likely to score high on measures of creativity, happiness, and self-esteem, while scoring low on measures of anxiety and depression (Konrath, Meier, & Bushman, 2014). Measuring narcissism is difficult because of the possible dual nature of this trait. In addition to the grandiose behaviors, many individuals show a vulnerable, hypersensitive quality, which makes them shy and fragile in the face of criticism. As a result, the many self-report scales for measuring narcissism do not always agree well with one another. What we present here are two short scales of narcissism, the Hypersensitive Narcissism Scale (Hendin & Cheek, 1997) and the Single-Item Narcissism Scale (Konrath et al., 2014). As you complete the scales, your results are more likely to be meaningful to you if you answer the items as honestly as possible (attempting to avoid social desirability effects). See if the two scales provide you with consistent results. Do you think these scales are valid? Place a check in the appropriate box, and then add your scores for the 10 items. Very uncharacteristic or untrue; strongly disagree1234Very characteristic or true; strongly agree5I can become entirely absorbed in thinking about my personal affairs, my health, my cares, or my relations to others. My feelings are easily hurt by ridicule or by the slighting remarks of others. When I enter a room, I often become self-conscious and feel that the eyes of others are upon me. I dislike sharing the credit of an achievement with others. I am appreciated by at least one of those present. I feel that I am temperamentally different from most people. I often interpret the remarks of others in a personal way. I easily become wrapped up in my own interests and forget the existence of others. I feel that I have enough on my hands without worrying about other people’s troubles. I am secretly put out when other people come to me with their troubles, asking me for my time and sympathy. Add up all your scores. The totals can range from 5 to 50. There are no right or wrong answers on these types of scales, nor do we have a cutoff value that absolutely determines a person’s status as a narcissist. The mean reported for college students is generally about 29. To what extent do you agree with this statement: I am a narcissist. (Note: The word narcissist means an egotistical, self-focused, and vain person.)1234567Not very true of me     Very true of meUnderstanding the roots of narcissistic behavior will help clinicians develop more effective interventions, which will be discussed in more detail in Chapter 15 on treatments. In the meantime, you have the opportunity to assess yourself for narcissism using several standardized instruments. In Previous Chapters, we have explored many factors that might contribute to someone’s participating in cyberbullying another person. In this chapter, we reflect on the role of psychological disorder in producing this behavior. Our general definition of abnormal behavior includes harm to others, and cyberbullying certainly fits that criterion. Research on traditional, face-to-face bullying has implicated traits known as the Dark Triad of Machiavellianism, narcissism, and psychopathy (Paulhus & Williams, 2002). Narcissism and psychopathy already have been described in detail in this chapter; Machiavellianism refers to coldness, dishonesty, and calculated manipulation to achieve a person’s goals. More recently, scientists have argued for adding a fourth trait, sadism, to the Dark Triad (van Geel, Goemans, Toprak, & Vedder, 2017). Sadism refers to the tendency to take pleasure in the suffering of others, and it is uniquely correlated with many types of antisocial behavior. This trait can be assessed using the Varieties of Sadistic Tendencies Scale (VAST;  Paulhus, Jones, Boyle, Saklofske, & Matthews, 2015). In a large sample of older teens and young adults, Machiavellianism, psychopathy, and sadism were related to traditional bullying, but only sadism was related to cyberbullying (van Geel et al., 2017). This suggests that looking for elements of psychopathology associated with traditional bullying to predict likely cyberbullies might not be fruitful. Assessing sadism, however, should not only identify individuals prone to cyberbullying, but to antisocial behaviors in general. The Dark Triad characteristics of Machiavellianism, narcissism, and psychopathy commonly occur together and are predictive of traditional, but not cyberbullying. We apparently do a pretty good job of detecting Dark Triad traits in other people simply by viewing their appearance (Holtzman, 2011). This might help you identify people who might engage in traditional bullying, but not necessarily those prone to cyberbullying. A fourth potential Dark trait—sadism—is more predictive of cyberbullying behavior. Be sure that you can define these terms and use them correctly.`;\n","export const text = `Both medication and behavioral treatments reduce abnormal activity in the basal ganglia that is associated with obsessive-compulsive disorder. We have all been told how important it is to wash our hands to avoid illness, especially during flu season. But people with obsessive-compulsive disorder (OCD) might wash their hands several hundred times each day. Why do people with OCD behave this way? Chapter 14 highlighted some possible causes of OCD, including genetic vulnerability and the presence of too much activity in several brain structures, including the basal ganglia. This chapter will discuss how psychotherapists can improve the situations of people with OCD and the other disorders discussed in Chapter 14. The two most common approaches to treating OCD are cognitive behavioral therapy (CBT), covered in detail in this chapter, and medications. CBT was developed based on the principles of classical and operant conditioning that were discussed in Chapter 8. In the case of OCD, a woman with hand-washing compulsions could be trained to anticipate when she might feel like washing her hands inappropriately. Then, she would engage in some competing activity—perhaps going out to her garden to pull some weeds or pick some flowers. Finally, she would reward herself for resisting the temptation to wash her hands too much. Is it possible to show differences in brain activity resulting from CBT? The answer is yes (Baxter et al., 1992). Zooming in, we can see what happens to a person’s brain by using a positron emission tomography (PET) scan following therapy for OCD. A PET scan provides a measure of brain activity. The red and yellow areas are considerably more active than the green, blue, and dark areas. When people with OCD who have been treated with CBT are compared to people treated with medication, their PET scans show the same decrease in activity in the basal ganglia. Because medication can produce unwanted side effects and is not effective for all people, it is fortunate that alternate therapies are available. This chapter will explore a number of approaches that have been used to improve the situations of people with psychological disorders. Some represent a variety of psychological treatments, like the behavioral approach to OCD discussed here, while others take the form of medication and other biological treatments. The effects of these treatments might have more in common than you think, as in the case of the changes in brain activity that result from the psychological and biological treatments of OCD. Just as we have argued for the advantages of integrating psychological perspectives to understand behavior and mental processes, the field of clinical psychology is recognizing the benefits of applying multiple perspectives to the treatment of psychological disorders. Chapter 14 explored a diverse range of problems that can contribute to personal and interpersonal distress. Once considered virtually untreatable, many of these disorders respond positively to the treatments available today. Unfortunately, many people with disorders still do not seek help, possibly because of stigma related to having a psychological problem or a lack of awareness that effective help is available (see Figure 15.1). Many people diagnosed with schizophrenia 60 years ago faced a discouraging future of institutionalization and little improvement (see Figure 15.2). Today, although about 30% of patients with schizophrenia do not seem to respond to available treatments, the other 70% experience some relief from their troubling symptoms (Bobo & Meltzer, 2010). As symptoms of depression become more severe, people become more likely to obtain treatment from a psychiatrist or mental health professional, which would include clinical psychologists. It is difficult to tell whether the remaining individuals receive no care or are seen by their family physicians or other health care providers. Scientists are making progress in identifying which patients with schizophrenia are more responsive to current treatments. A deficit group of patients differed from a nondeficit group primarily in having more emotional disturbances. Recovery was defined strictly in this study as no psychiatric hospitalizations, no major symptoms, and at least half-time work or the equivalent level of functioning. In a 20-year study of the patients’ outcomes, using a group of patients diagnosed with depression as a comparison group, the nondeficit group made more progress than the deficit group. Identifying which patients respond to existing treatments will help researchers and health care providers develop more effective treatment plans for individuals. At the same time, significant challenges remain. An analysis of data from Australia, Canada, the United Kingdom, and the United States between 1990 and 2015 showed that in spite of substantial increases in the provision of treatment, primarily in the form of antidepressant medication, rates of major depressive disorder (MDD) and anxiety disorders did not decrease (Jorm, Patten, Brugha, & Mojtabai, 2017).  Jorm et al. (2017) attribute this lack of progress to a failure to provide quality care to those in greatest need, as well as a lack of attention to preventive strategies. They report no evidence suggesting an increase in risk factors or the effects of greater public awareness. Given the implications of psychological disorders in terms of lost functionality and quality of life, improving preventive and treatment strategies should be given high priority. Psychological treatments, or psychotherapiespsychotherapiesA treatment designed to improve symptoms of psychological disorder through conversation between the therapist and the patient or client.            psychotherapies        A treatment designed to improve symptoms of psychological disorder through conversation between the therapist and the patient or client.            , which generally involve a conversation between the professional providing the therapy and the person seeking help, are often combined with biological treatments, such as the medications described in more detail in later sections of this chapter. For example, a person who is terrified of being part of a crowd might benefit from a combination of psychotherapy and medication for anxiety. As will be shown in this chapter, different combinations of methods can be tailored to different types of problems and to the individuals being treated. Biological approaches to treatment have their roots in our understanding of the biological factors that contribute to psychological disorders. Along with the overall burst of scientific and medical knowledge beginning in the 18th century, scientists began to learn more about psychological disorders and their causes. This knowledge in turn set the stage for the discovery of more effective biological treatments. Improved medical knowledge made psychological disorders less mysterious. When untreated, syphilis (a common type of sexually transmitted disease caused by a bacterial infection) can produce a characteristic set of abnormal behaviors, including hallucinations, delusions, and changes in mood and personality. As noted in Chapter 14, these behaviors are characteristic of a number of psychological disorders, including schizophrenia. The discovery that behaviors like hallucinations could have a biological cause led to the recognition that abnormal behavior can result from infection or other medical conditions. In these cases, the abnormal behaviors should disappear when the underlying medical condition is treated. The use of one biological approach to treatment, medication, revolutionized the care of people with some psychological disorders. In 1949, an Australian psychiatrist named John Cade discovered that lithium salts could produce remarkable improvements in the behavior of patients with bipolar disorder. Cade’s discovery was followed quickly by breakthroughs in the use of medication to treat schizophrenia. Bolstered by these successes with some of the most troubling psychological conditions, the pharmaceutical industry quickly introduced medications for depression and anxiety. These successes not only led to better outcomes for many patients, but also strengthened the credibility of using biological approaches to treat abnormal behaviors. Australian psychiatrist John Cade searched for alternatives to the ECT, lobotomies, and psychoanalysis that were commonly used to treat psychological disorders in the 1940s. After observing that lithium salts had a sedating effect on guinea pigs, he tried lithium on himself to test for side effects. He then tried lithium on his patients and noticed that those with bipolar disorder appeared to respond favorably. Lithium is an element, so it could not be patented, providing little financial incentive for the pharmaceutical industry. However, lithium was approved for use in the United States in 1970. Lithium salts continue to be used extensively to treat bipolar disorder, although we now understand that this treatment has serious side effects. Running parallel to the successes of scientists engaged in biological approaches to disorders were significant contributions from those who focused on the psychological and emotional underpinnings of disorders. During the 19th century, psychologists and psychiatrists were interested in hysteria, a condition that would be diagnosed today as somatic symptom disorder (see Chapter 14). Hysteria did not seem to have any biological basis and did not respond to the medical treatments of the day. Among the first efforts to treat hysteria through nonmedical means was the use of hypnosis by Franz Anton Mesmer (1734–1815), whose name gives us an alternative term for being hypnotized—mesmerized. Mesmer attempted to treat hysteria by inducing a hypnotic state, which was described in Chapter 6. In many cases, people with hysteria no longer experienced their symptoms under hypnosis and, following Mesmer’s suggestions made during hypnosis, continued to be symptom free even after they left the hypnotic state. Franz Anton Mesmer (1734–1815), whose name gives us the term mesmerized, attempted to use hypnosis to treat people with hysteria, which would now fall into the somatic symptom disorders category. Mesmer’s practices inspired Sigmund Freud to use hypnosis, although he eventually abandoned the technique in favor of other approaches. Despite skepticism regarding Mesmer and his techniques, other scientists continued to explore hypnosis, suggestion, and hysteria. When positive outcomes were reported by the noted French neurologist Jean-Martin Charcot (1825–1893), hypnosis and suggestion took on a new legitimacy in the scientific community. Among the admirers of Charcot and his work were Josef Breuer (1842–1925) and his more famous colleague, Sigmund Freud (1856–1939). As will be discussed later in this chapter, Freud eventually abandoned his interest in hypnosis in favor of other approaches, which eventually gave rise to psychoanalysis. Instead of hypnotizing people, Freud encouraged them to speak freely about whatever came to mind. Franz Anton Mesmer’s methods for using hypnosis to treat hysteria were quite theatrical and were denounced as fraudulent by none other than Benjamin Franklin, who was serving as the U. S. ambassador to France at the time. During the 20th century, psychological approaches to treatment reflected the rise of the behavioral and humanistic perspectives, described in Chapters 1, 8, and 12. The behaviorist perspective, unlike the biological and Freudian approaches, focused exclusively on observable behavior and approached the treatment of abnormal behavior using the principles of classical conditioning, operant conditioning, and observational learning, described in Chapter 8. The cognitive revolution of the second half of the 20th century added a recognition of the role of cognitions in abnormal behavior, leading to the development of cognitive behavioral therapy (CBT). Later in this chapter, the use of CBT for disorders such as phobias and depression will be explored. The rise of humanistic psychology in the second half of the 20th century also led to new humanistic therapieshumanistic therapiesA therapy approach patterned after the theories proposed by humanistic psychologists            humanistic therapies        A therapy approach patterned after the theories proposed by humanistic psychologists            . As discussed in Chapter 1, the humanistic psychologists specifically rebelled against psychodynamic thinking and the practice of psychoanalysis. Humanistic therapist Carl Rogers wished to separate his procedures, described in greater detail later in this chapter, from the more traditional Freudian psychotherapies of his time by referring to treatment as counselingcounselingA treatment originally proposed by humanistic therapists that includes a focus on personal strengths and development; the term is now used to refer to treatment for adjustment problems, as opposed to treatment for severe psychological disorders.            counseling        A treatment originally proposed by humanistic therapists that includes a focus on personal strengths and development; the term is now used to refer to treatment for adjustment problems, as opposed to treatment for severe psychological disorders.             rather than psychotherapy or psychoanalysis (Rogers, 1942). Today’s field of counseling psychology mirrors the more positive view of human nature held by the humanistic psychologists by focusing on personal strengths and development. Carl Rogers and the humanistic psychologists rebelled against the practice of psychoanalysis and established new approaches of counseling psychology. The beginning of the 21st century has featured greater demands to tie clinical practice to research on the outcomes of treatment. As shown in Figure 15.3, evidence-based practice (EBP) combines research evidence, clinical expertise, and individual values to provide the best outcome for a patient or client (APA Presidential Task Force on Evidence-based Practice, 2006;  Straus, Richardson, Glasziou, & Haynes, 2005). The movement toward EBP began in medicine, but it has since spread throughout the helping professions, including psychology. Evidence-based practice (EBP) began in medicine and spread to psychotherapy and other helping professions. EBP combines the personal experience of the clinician, the best scientific evidence, and a consideration of patient values and expectations to tailor scientifically valid treatments to the individual. EBP combines five steps (Thyer, 2004). The first is to construct an answerable question, such as Which treatments produce the best outcomes in cases of PTSD? Next, clinicians conduct a search of the most current scientific literature related to the question. In other words, doing something because I’ve always done it this way in my practice is no longer an option. Third, clinicians critically evaluate the relevant literature, using the standards for good science discussed in Chapter 2. This information is then integrated with data regarding particular cases, including the patients’ values and circumstances. Finally, clinicians evaluate their performance based on the patients’ outcomes and make any necessary adjustments. If you think that these EBP steps sound like the problem-solving system introduced in Chapter 10, you’re right. You might have assumed that your health care provider would take this approach automatically, but historically, many clinicians have been convinced that science can do little to assist them with an individual case and that clinical practice was more art than science. Large numbers of clinicians have traditionally valued their clinical expertise over the body of knowledge represented by science (Baker, McFall, & Shoham, 2008). Given the demanding financial realities of health care in the 21st century, however, a logical, scientific evidence–based approach to the delivery of health care services is not just the most effective approach, but also the necessary one. Before therapists can provide any type of useful treatment, they need to understand the problems presented by the person to be treated. The choice of assessment methods is guided by the training and experience of the clinician, but it usually includes the methods described in Chapter 12: interviews, observations, and standardized tests (Groth-Marnat, 2009). A range of professionals, including general medical practitioners, psychiatrists, psychologists, social workers, nurses, and religious leaders, provide counseling and psychotherapy. The licensing of these professionals to provide treatment for psychological disorders is administered by state governments in the United States and provincial governments in Canada. Requirements for licensing can vary widely from state to state and can result in a bewildering array of titles. For example, the simple term therapist is not regulated consistently across the United States. We will use the term psychotherapistpsychotherapistA licensed professional who provides psychotherapy.            psychotherapist        A licensed professional who provides psychotherapy.             to refer to any of the many types of licensed professionals described in this section who provide psychotherapy. Benjamin Rush (1749–1813), the father of American psychiatry, believed that psychological disorders were caused by the irritation of the blood vessels in the brain. He invented the tranquilizer chair for patients and wrote a textbook published in 1812, Medical Inquiries and Observations Upon Diseases of the Mind, which was the standard in the field for the next 50 years. PsychiatristsPsychiatristsA medical doctor who specializes in psychiatry and can use medical procedures, such as the prescribing of medication, to treat psychological disorders.            Psychiatrists        A medical doctor who specializes in psychiatry and can use medical procedures, such as the prescribing of medication, to treat psychological disorders.             first obtain a standard medical doctor (MD) degree and then spend an additional three to four years specializing in psychiatry. Because of their training and licensure as physicians, psychiatrists are allowed to perform the medical procedures reviewed later in this chapter, particularly the prescribing of medication. It is difficult to generalize across the entire profession, but psychiatrists are more likely to look for the causes of disorder in biological factors than in learned or environmental factors and to assume that a biological treatment such as medication will be most effective. In addition, the medical model does not see treatment as taking place within a relationship of equals. Instead, expertise is simply transmitted from the doctor to the patient, and little input is expected from the patient. We will see alternatives to this model in the procedures conducted by other types of therapists. The word psychologist is used to describe professionals who have completed advanced degrees in psychology, including those who become psychotherapists. Most psychologists, including the professors teaching your introductory course, earn the traditional academic doctor of philosophy (PhD) degree in psychology, but not all choose career paths as licensed psychotherapists. Clinical psychology is the path taken by psychology graduate students who wish to study, teach, or practice psychotherapy. In addition to completing the academic requirements for a PhD degree, clinical psychology students must spend an additional year providing treatment in a supervised internship. Beginning in the 1970s, an alternative route to becoming a clinical psychologist arose from suggestions that the practice of psychology could be modeled after dental and medical schools (Norcross & Castle, 2002). The doctor of psychology (PsyD) degree was developed to train students to provide services without undergoing the extensive training in research methods that characterizes the PhD degree. Both PhD and PsyD programs can earn accreditation by the American Psychological Association (APA). PsyD programs are becoming increasingly popular with students (see Figure 15.4). However, the lack of emphasis on research that is characteristic of the PsyD degree has led to concerns about the ability of these graduates to think scientifically about their choice of methods (Baker et al., 2008). For example, without a solid background in science, objective evaluation of and adherence to the EBP model could be quite difficult. Of the faculty members in institutions granting the traditional PhD degree, 65% teach scientifically supported CBT techniques (Norcross & Castle, 2002). In contrast, only about 30% of faculty members in PsyD programs teach CBT techniques. While PhD degrees awarded in psychology have remained fairly constant, the number of PsyD degrees has grown dramatically since 1988. There are now approximately equal numbers of clinical PhD and PsyD degrees awarded each year. How do clinical psychologists and psychiatrists differ, other than that they earn different degrees? One of the traditional distinctions has been psychiatrists’ ability to prescribe medication, but that difference is gradually disappearing. In 2002, New Mexico became the first state to allow select psychologists to prescribe medication, followed by Louisiana in 2004 (Daly, 2006), Illinois in 2014, Iowa in 2016, and Idaho in 2017 (APA, 2017). The requirements for already-licensed clinical psychologists to gain prescription privileges are quite rigorous. Psychologists wishing to prescribe medication in these states must go through additional neuroscience coursework and many hours of clinical practicum and physician-supervised hours of care with patients. In addition to treating people with diagnosed psychological disorders, clinical psychologists often provide marriage and family counseling, substance abuse counseling, vocational counseling, grief counseling, rehabilitation counseling, and prison counseling. However, these types of services are also provided by counselors, who typically do not provide services for psychological disorders such as schizophrenia. In other words, a couple experiencing marital conflict might seek either a counselor or a clinical psychologist, whereas a person experiencing hallucinations and delusions is more likely to be treated by a clinical psychologist or a psychiatrist. Counselors hold master’s or doctoral degrees in psychology and related fields, such as education and social work. Students pursuing undergraduate psychology degrees frequently ask what they can do in the mental health field without a graduate degree. About one third of undergraduates in psychology obtain jobs that are closely related to the field (see Figure 15.5). Students completing an undergraduate degree in psychology often work in group homes and camps for troubled youth or in hospital or outpatient settings under supervision. Many others seek work in people-related professions, including human resources, sales, and teaching (see Chapter 1). More workers with master’s degrees in psychology are employed in degree-relevant fields than are workers with bachelor’s degrees. Closely related fields include professional services, teaching, and research. Somewhat related fields include management, sales, accounting, and employee relations. In recognition of the historical role of religious leaders in providing counseling to members of their congregations, most states allow religious leaders to provide this service without further licensure. The master of divinity (MDiv) degree typically includes human development and counseling courses taught at schools of divinity. Some universities offer MDiv degrees that prepare students to be either religious leaders or licensed counselors in the outside community. Among the least frequently regulated practitioners are hypnotherapists, although 30 states and the District of Columbia now regulate hypnotism in some manner. The National Guild of Hypnotists cautions practitioners who are not also licensed health care professionals to use the title Consulting Hypnotist and to avoid using the terms therapy, depression, anxiety, compulsive, phobia, psychological, medical, clinical, and counseling (National Guild of Hypnotists, 2016). As mentioned in Chapter 6, there are many legitimate uses of hypnosis in therapy, and there are many legitimate therapists who use the technique. But that chapter also recounted the example of Zoe D. Katze, who is a hypnotherapist certified by the National Guild of Hypnotists, the American Board of Hypnotherapy, and the International Medical and Dental Hypnotherapy Association. She is a professional member of the American Association of Professional Hypnotherapists. As you may recall, Zoe also happens to be a cat, owned by a psychotherapist named Steve Eichel, who obtained the credentials for her to make a point about the weaknesses of accreditation in this area (Eichel, 2002). Another unregulated area in the provision of psychological services is the field of life coaching, which requires no licensing. According to the International Coach Federation (ICF, n.d.) coaching is defined as partnering with clients in a thought-provoking and creative process that inspires them to maximize their personal and professional potential. Substitute psychotherapy for coaching in this definition, and few would disagree with the resulting statement. Coaches seem to perform the same duties as psychotherapists and counselors. Some psychotherapists have found that referring to themselves as coaches expands their practices, possibly because of the reduced stigma of seeing a coach rather than a therapist in the minds of some clients. Life coaching also provides the advantage of being more appealing to men, who generally seek more traditional psychotherapy in much smaller numbers than do women (McKelley & Rochlen, 2007). Few studies have attempted to evaluate the effectiveness of coaching, but those that have done so note that clients are satisfied with the services (Grant, 2003) and experience improved coping and self-regulation (Theeboom, Beersma, & van Vianen, 2014). Many studies appear in journals that are essentially trade publications, such as the International Journal of Evidence-Based Coaching & Mentoring, the International Coaching Psychology Review, and Coaching: An International Journal. In spite of the lack of regulation and licensure, life coaching is becoming a big part of the delivery of psychotherapeutic services, even within health care settings (Ammentorp et al., 2013). Corporations spend in the vicinity of $1.5 billion per year on workplace coaching services provided by the approximately 30,000 professional coaches who are active globally (Grant, Passmore, Cavanagh, & Parker, 2010). Although licensure provides significant protection for the public, any consumer of psychological services must thoroughly investigate prospective psychotherapists. Zoe D. Katze may be an extreme example, but we suspect that she is not unique. Credentials are not always what they seem. For example, the author of the 1992 best seller Men Are from Mars, Women Are from Venus lists himself as Dr. John Gray (John Gray’s Professional Bio, 2017), but provides no information on his educational background or licensure. Gray spent nine years as a monk under the leadership of the Beatles’ guru, Maharishi Mahesh Yogi. He earned bachelor’s and master’s degrees in the Science of Creative Intelligence from the unaccredited Maharishi European Research Institute in Switzerland, and a PhD degree from the now-defunct and unaccredited Columbia Pacific University in California, which was described by the state government as a diploma mill. Because of his experience as a monk, the state of California recognizes him as a spiritual counselor, which technically allows him to provide advice. More recently, Gray has been joined by his daughter Lauren, who also provides no educational or licensing material about herself on the website. Donald Asher writes and speaks about career hunting, college applications, and résumé building, topics often presented by life coaches. Although unlicensed, coaches also perform many of the same tasks as psychotherapists and counselors. Licensed psychotherapists have found that referring to themselves as coaches rather than therapists reduces some of the stigma that their clients might feel in using their services. So far, we have focused our discussion on credentialing practices in the United States. Worldwide, an even greater variety of professionals perform psychotherapy and counseling under diverse licensure and oversight practices. The United Kingdom, Australia, and Canada provide systems that are similar to those in the United States for delivering psychotherapy (Priebe, 2006). In these countries, psychotherapy is conducted by a number of professionals, including physicians, psychologists, social workers, nurses, and others in similar professions. In each case, performing psychotherapy requires training beyond that needed to simply obtain the appropriate professional degree. Germany and Russia are particularly strict in their accreditation rules. Because of the prevalence of state-run health systems in Europe, assessments of treatment outcomes are regularly conducted (Priebe, 2006). In the United States, treatment outcomes are a question of interest to academic researchers but are rarely assessed as comprehensively as in many other nations, although that circumstance is likely to change with any further government involvement in health care. Celebrity psychologists like Dr. Phil McGraw are often criticized by the profession for offering overly simple versions of psychological advice to the public. Many people imagining a session of psychotherapy think of a stereotypical Freudian scenario. The patient reclines on a couch, while a bearded man sitting in a chair scribbles copious notes. Although some therapy continues to take place in such a setting (though perhaps the therapist isn’t bearded—or a man), many more options are available. Most psychotherapy occurs in the form of individual therapy involving face-to-face, hourlong meetings between the therapist and the patient or client (as a subsequent section will demonstrate, terminology varies among practitioners). In traditional psychoanalysis, such meetings are quite frequent (four to five times per week) and take place over the course of years. In CBT, described in detail in a later section, sessions might occur once per week for about six weeks. Given rates of $150 or more per hour for individual therapy, insurance providers are pressuring psychotherapists to find faster, cheaper, and more efficient ways to provide treatment. Many insurance plans limit psychotherapy sessions to 10 per calendar year. Brief therapyBrief therapyPsychotherapy provided over a short time frame, usually between three and five sessions.            Brief therapy        Psychotherapy provided over a short time frame, usually between three and five sessions.            , another treatment regime, specifically avoids spending long periods searching for the historical basis of a person’s problems. Instead, therapists using this approach focus on solution-building rather than problem-solving (Iveson, 2002, p. 149). Consequently, practitioners argue that only three to five sessions are necessary. Many brief therapy interventions involve only one session, and no cases require more than eight. Many people thinking about psychotherapy imagine a bearded man scribbling notes next to a client reclining on a couch, but this stereotype is usually far from the truth. In addition to individual sessions, psychotherapy can take place with groups of patients or clients in group therapygroup therapyPsychotherapy conducted with a group of people rather than individually            group therapy        Psychotherapy conducted with a group of people rather than individually            . Group therapy does not imply a particular type of treatment approach. Although some treatment approaches seem better suited for group work than others, therapists of all persuasions are known to use groups in their work. The groups formed by the therapist can be homogeneous, in the sense that the group members share a common problem, or heterogeneous, in which the group members have diverse diagnoses. In most cases, groups are limited to between 6 and 12 individuals. People with severe conditions, such as psychotic, suicidal, or antisocial tendencies, are usually excluded from groups until they progress sufficiently to behave as good group members. People participating in group therapy benefit from reduced feelings of isolation. Seeing the progress of others with the same condition raises hope and provides opportunities to model successful strategies. Although many groups are organized and supervised by a licensed psychotherapist, leaderless self-help groups are also quite popular. Weight Watchers, Alcoholics Anonymous (AA), and other groups modeled after AA have become popular sources of support and self-improvement. These groups often operate with a member–leader who is not a psychotherapist, and they function outside the realm of government licensure and supervision. In addition to the obvious cost-effectiveness of having a group split the therapist’s fee, group therapy offers a number of other advantages (Yalom, 1995). For example, group therapy offers hope to the participants, who benefit from seeing others who have made more progress with their condition. Group members also benefit from knowing that others share their condition, decreasing their sense of isolation. Although practitioners generally believe that group therapy is effective, research on group therapy outcomes is difficult to conduct because of the large number of variables that must be considered. The need to control for such factors as different disorders and previous treatment history makes this area challenging for researchers. Family therapyFamily therapyA type of therapy in which family members participate individually and in combination with other family members.            Family therapy        A type of therapy in which family members participate individually and in combination with other family members.            , in which various family members participate in therapy individually and in combination with other family members, is another alternative to the purely individual provision of treatment. Couples therapyCouples therapyA type of family therapy that focuses on intimate relationships            Couples therapy        A type of family therapy that focuses on intimate relationships             is a type of family therapy that focuses on intimate relationships. Family therapy is frequently used when a child or teen is experiencing problems, because these problems typically affect and are affected by the behavior of other family members. Family therapists face a number of significant challenges. In some cases, the member who seems to need treatment the most refuses to participate. In others, maintaining the confidentiality of all participants can be difficult. A teen might disclose information that will help the parents do their job better, but sharing this information with the parents could interrupt the trust that the therapist has built up with the teen. Family therapy combines individual sessions with sessions that include some combination of family members, like these parents and their young daughter. An emerging delivery system for psychotherapy is online therapy. When online therapy is used as a Google search term, more than 11,600,000 results are returned. The benefits of privacy, engaging in therapy from home, and the ability to choose a suitable therapist regardless of location provide significant advantages for some clients. In its early days, online therapy was conducted through e-mail or chat services. Because the written word conveys less information than a therapist obtains face to face, this method could be less desirable. However, many online therapists have taken advantage of Skype and other voice and video technologies. Online delivery of CBT has been shown to be as effective as face-to-face CBT for a number of psychological conditions (Herbst et al., 2014;  Sijbrandij, Kunovski, & Cuijpers, 2016). Other technologies have influenced the delivery of psychotherapy. Therapists can buy products such as Wiley’s TheraScribe, a computer program that includes more than 2,500 prewritten interventions, goals, objectives, and suggested diagnoses. The program generates reports catering to Medicare and other managed care organizations. The time savings provided by these technologies can be useful in controlling costs, but receiving a prewritten evaluation might seem somewhat impersonal to individuals seeking therapy. If you receive a treatment plan from your campus health center that is nearly identical to a friend’s, you might not feel that the counselor cared about you as an individual. Providing treatment for psychological disorders has changed dramatically since the days of Freud. Originally, therapy was provided by individual professionals in small private practices. However, this model became increasingly rare in the latter part of the 20th century, as psychological treatments became more likely to be managed by large organizations, including insurance companies. Many of the concerns about these new models of managed psychological treatment echo complaints from the medical community regarding contemporary medical treatment. Decisions formerly made by the therapist about the types of disorders that warranted treatment and the length, frequency, and type of treatment to be offered are now more likely to be made by insurance company personnel who lack medical or psychological training and focus more on costs (Baker et al., 2008). In addition to the need to adapt to the managed care of psychological disorders, psychotherapy faces many of the same challenges as medicine in providing scientifically valid treatments. Just as we would expect a physician to provide a scientifically supported treatment for cancer, we should expect a psychotherapist to provide equally well-documented treatments for depression. In the earlier section on EBP, we joined most research clinical psychologists and psychiatrists in endorsing the use of science in selecting, implementing, and evaluating psychological treatments. However, as demonstrated in the discussion of the Rorschach Inkblot Test in Chapter 12, lack of scientific support does not always prevent an approach from being widely used. The benefits of privacy, engaging in therapy from home, frequent 24-7 coverage, video interactions using Skype or similar technologies, and the ability to choose a suitable therapist regardless of location provide significant advantages for some clients. Online delivery of cognitive behavioral therapy has been shown to be as effective as face-to-face delivery (Ruwaard et al., 2011). Frustration over the lack of emphasis on science in psychotherapy has led to considerable debate. The American Psychological Association has the primary responsibility for accrediting programs that train clinical psychologists with either PhD or PsyD degrees in the United States. This system has been challenged by a new accreditation body, the Psychological Clinical Science Accreditation System (PCSAS, 2017), which specifically aims to increase the scientific basis of clinical treatment and research literacy among clinical psychologists. Since 2007, the PCSAS has accredited 32 programs, with several more under review (PCSAS, 2017). Time will tell whether PCSAS will be able to nudge the clinical psychology community into greater reliance on research-based methods. Among the barriers to obtaining counseling and psychotherapy services faced by racial and ethnic minority individuals in the United States are cultural distrust, racial and ethnic norms related to self-reliance and privacy, and concerns about culturally sensitive providers (Hayes, McAleavey, Castonguay, & Locke, 2016). Psychologists have begun to identify interventions that might address these barriers by offering greater cultural competency. Among the recommendations for more culturally competent therapy are using interventions that are consistent with the client’s values, communicating in culturally sensitive ways (such as in the client’s native language), expressing empathy and high regard, and avoiding equating cultural differences with deficits. Armed with these competencies, we should be able to identify which therapists and counselors are actually doing a better job of culturally competent counseling. Unfortunately, many early studies depended on perceptions by the client, therapist, or an observer, none of which seem to actually predict positive outcomes of the therapy for the clients.  Hayes et al. (2016) instead suggest that a culturally competent therapist is one who produces the best outcomes. Pursuing this approach,  Hayes et al. (2016) were able to demonstrate that therapists differed in terms of overall competence, regardless of the race and ethnicity of their clients (see Figure 15.6). Some therapists did seem to produce better outcomes with one racial or ethnic group than with others.  Hayes et al. (2016), however, do note that general competence and cultural sensitivity are both very important. They argue that therapist training should emphasize both areas and that the multiculturally expert therapists in their study could serve as excellent role models for other therapists. Zero in this graph indicates no change in Distress Index scores as a result of treatment. The dotted line indicates the mean amount of change across all therapists. The overall most effective therapists were also much more effective with racial and ethnic minority clients. TitleDegreesSpecialtiesPsychiatristMedical doctor (MD)Treats psychological disordersPrescribes medicationClinical psychologistDoctor of philosophy (PhD) or doctor of psychology (PsyD)Treats psychological disorders and problems with adjustmentIf specially licensed, prescribes medicationCounselorPhD, PsyD, doctor of education (EdD), or one of a number of master’s degreesSpecializes in problems with adjustmentBecause Freud is typically credited with being the first person to use talk therapy in his efforts to improve his patients’ psychological symptoms, we begin our discussion of specific treatment approaches with psychoanalysispsychoanalysisSigmund Freud’s treatment approach based on his psychodynamic theory            psychoanalysis        Sigmund Freud’s treatment approach based on his psychodynamic theory            . A psychotherapist who uses psychoanalysis is typically known as a psychoanalyst. Psychoanalysis has not fared well in scientific studies of its effectiveness, although supporters continue to argue in its favor (Leichsenring, 2005;  Lichtenberg, Lachmann, & Fosshage, 2013). The absence of strong evidence for its efficacy has led to a decline in its use. However, psychoanalysis served as an important step to contemporary treatments. Freud’s study featured not only the famous couch, but also Persian rugs and a sample of Freud’s collection of antiquities. Freud’s approach to psychotherapy reflected several key themes from his theory of personality (see Chapter 12). In particular, Freud emphasized the importance of conflict among the self (ego), the aggressive energy of the id, and the restrictions supplied by society (superego). Freud’s view of the ability of the unconscious mind to affect behavior is also central to his therapeutic approach. Because of Freud’s training as a physician, psychoanalysts take a directive role, which means that they are in charge of the session. Like your physician, the psychoanalyst does not expect contributions from you other than information about your symptoms. Freud called his slips faulty actions. For example, a man texting his partner I wish you were her instead of I wish you were here would have a great deal of explaining to do. Modern cognitive psychologists are more likely to blame slips on conflicts between grammar sequences than between the unconscious and the conscious minds. Freud’s psychoanalysis is a classic example of an insight therapyinsight therapyA therapy that improves symptoms of psychological disorder by building people’s understanding of their situation.            insight therapy        A therapy that improves symptoms of psychological disorder by building people’s understanding of their situation.            . Insight therapies assume that people will improve once they understand their problems. For Freud, many psychological disorders arose from problems that resided in the unconscious mind, out of conscious awareness. Although the person might not be consciously aware of these continuing conflicts, the conflicts exert their influence on behavior and through dreams. For example, a patient might arrive at the therapist’s office complaining about her inability to enjoy a satisfactory sex life with a beloved spouse. Although many factors could contribute to such problems, the psychoanalyst is likely to suspect the influence of some traumatic experience that is relatively unavailable to the patient’s conscious thoughts. Perhaps the patient is the victim of childhood sexual abuse, or she might be experiencing a conflict between the ultrastrict morality that she learned from her parents and her current desire to have a fulfilling relationship with her partner. For the psychoanalyst, the cure lies in uncovering the unconscious contributions to the patient’s problems. Once she understands the source of her problems (the insight), her situation should improve. How does one go about uncovering unconscious material? Although Freud experimented with the use of hypnosis to access the unconscious mind, he became dissatisfied with the results and abandoned the procedure. Instead, Freud decided to use the technique of free associationfree associationThe psychoanalytic technique of encouraging a patient to say whatever comes to mind, without attempting to censor the content.            free association        The psychoanalytic technique of encouraging a patient to say whatever comes to mind, without attempting to censor the content.            . Free association occurs when a person says whatever comes to mind. As we mature, most of us become aware that we do not literally speak our mind out of consideration for our fellow humans. Although small children might tell a person, You’re fat, such comments, although they may be true, are not the type that most adults consider polite. We learn to think before we speak. To engage in free association, this self-censor must be turned off, allowing the patient to verbalize those thoughts that are typically withheld. Without the self-censor, the famous Freudian slips might occur and are assumed to reflect unconscious wishes. Not all psychoanalysts engage in dream analysis, but it is likely that therapists interested in dream analysis are psychoanalysts. For Freud, dreaming represented a reflection of unconscious wishes and, therefore, a potential source of useful insight into the unconscious mind. If patients are undergoing dream analysis, they are encouraged to awaken following a dream to jot some notes that can later be reviewed with the therapist, who will attempt to interpret the dream’s symbolic meanings. In addition to reviewing the content of free association, dreams, and Freudian slips, the analyst looks for signs of transference and resistance. TransferenceTransferenceA psychoanalytic technique in which the therapist uses the responses of the patient to the therapist to understand the patient’s approach to authority figures in general.            Transference        A psychoanalytic technique in which the therapist uses the responses of the patient to the therapist to understand the patient’s approach to authority figures in general.             occurs when a patient uses an earlier relationship, usually with a parent or other authority figure, as a prototype for a current relationship, such as that between the patient and the therapist. If the patient begins yelling at the therapist, the therapist might infer that the patient reacted similarly to parents and others in authority under similar circumstances in the past. ResistanceResistance(a) A psychoanalytic technique in which the patient’s reluctance to accept the interpretations of the therapist indicate that the interpretations are correct. (b) The second stage of the general adaptation syndrome (GAS), characterized by coping with ongoing stress            Resistance        (a) A psychoanalytic technique in which the patient’s reluctance to accept the interpretations of the therapist indicate that the interpretations are correct. (b) The second stage of the general adaptation syndrome (GAS), characterized by coping with ongoing stress             occurs when the patient is getting closer to insight. Freud believed that we have unconscious wishes to maintain infantile fantasies and avoid punishment, and that therefore, a patient will resist efforts to uncover this material. Resistance, for the psychoanalyst, can be expressed in many forms. When a patient changes the subject, disagrees with the analyst’s interpretation, or even misses an appointment because of a flat tire, the analyst might suspect that the patient is getting closer to the uncomfortable truth. The Interpretation of Dreams, by Sigmund Freud, was originally published in German in 1900. The 600 copies of the first edition sold slowly, and the publisher did not issue a second edition until 1909. Nonetheless, this book is considered one of Freud’s most influential. In recent years, psychoanalysis has changed in the length and frequency of treatment. Driven by demands from insurance providers, psychoanalysts have developed new, briefer treatments. These briefer approaches tend to be less past-oriented as well. In other words, the psychoanalyst might ask a person about recent experiences at work, rather than about experiences as a toddler. Does psychoanalysis work? Some published meta-analyses suggest that psychoanalysis produces better results than no treatment and occasionally produces results comparable to more modern approaches to therapy (Leichsenring & Rabung, 2008;  Shedler, 2010). However, many of these studies have been criticized on the basis of their methods, such as including Rorschach Inkblot results as measures of patient improvement (McKay, 2011). As mentioned in Chapter 12, humanistic approaches developed in direct opposition to the theories and techniques of Freud and his followers. Where Freud believed that humans are inherently selfish and aggressive, the humanistic psychologists believed that humans are inherently good, possessing natural drives to grow and improve. Society for Freud had a beneficial effect on behavior by providing controls on our selfish and aggressive tendencies. For the humanistic psychologists, society can interfere with growth by providing judgmental feedback. Although we think that dressing and sitting like your client is not necessary to demonstrate empathy, attempting to see situations through the client’s eyes is an important part of humanistic therapies. Not too surprisingly, this conflict in theory led to an equal opposition in treatment approaches. As mentioned previously in this chapter, the psychoanalyst takes a powerful, directive role reminiscent of an MD. The individual seeking therapy is identified as a patient. As in the medical setting, the therapist identifies the nature of the patient’s problems and prescribes a course of action. Other than supplying information, the patient plays a relatively passive role in the process. In contrast, the humanistic therapist is nondirective and treats the person seeking therapy, called a client instead of a patient, as more of an equal in the process. Clients are expected to take an active role in helping to identify their problems and develop a positive course of action. Primary among the humanistic approaches to treatment is the person-centered therapyperson-centered therapyA humanistic therapy approach developed by Carl Rogers to help clients achieve congruence, or an alignment of the real and ideal selves.            person-centered therapy        A humanistic therapy approach developed by Carl Rogers to help clients achieve congruence, or an alignment of the real and ideal selves.             of Carl  Rogers (1942). In place of the psychoanalytic goal of uncovering unconscious sources of problems, person-centered therapy aims to achieve congruence, which was discussed in Chapter 12. For Rogers, discrepancies between the ideal and the real selves, such as when we fail to live up to our ideals, can lead to psychological disorders. Therapy techniques that bring the selves into alignment should treat the disorders effectively. Among the techniques used to achieve this congruence are empathy, unconditional positive regard, and the reflection and clarification of feelings. Empathic therapists attempt to see the world through the eyes of the client, as opposed to imposing their own worldviews. Allowing the client to feel understood in this manner contributes to the establishment of a safe, supportive environment where natural growth can occur. Unconditional positive regard, discussed in Chapter 12, also contributes to the safety of the therapeutic setting. Rogers believed that much human unhappiness resulted from conditional regard, in which children learn that they have value only when their behavior is pleasing to an authority figure. The nonjudgmental, unconditional positive regard of the therapy setting is designed to compensate for those earlier, potentially damaging experiences. Reflection techniques consist of the therapist restating some of the client’s comments in a way that conveys understanding. As such, reflection contributes to empathy and the client’s sense of being fully understood. For example, a client might state that he felt helpless when his boss reprimanded him for losing an important account. The therapist might simply comment, You sound like you were very angry and frustrated when your boss yelled at you. Clarification allows the therapist to point out important insights that the client might have reached. The therapist might clarify the situation with the boss by pointing out its similarities with the feelings that we have as children when we are corrected by an adult. Reflecting feelings takes practice to avoid sounding silly or reverting to the cliché I hear what you’re saying, but you might give it a try the next time someone confronts you. Reflecting the other person’s feelings instead of focusing on your own helps you stay calm, and it’s hard to carry on a one-sided argument with a calm person. Even when psychotherapists use therapy techniques other than a person-centered approach, they frequently incorporate some or all of Rogers’s communication strategies, such as reflection. These strategies also have become quite popular in programs designed to improve interpersonal communication and group process. For example, it is not uncommon for teens to stomp into the house and declare that they hate their parents. The typical response of the parents at that point is to become hurt and angry. The argument escalates as one of the parents retaliates with a hurtful statement, perhaps I wish you were never born! Rogers suggests that such immature battles are easily avoided. Instead of retaliating, the parent might say, I know it makes you very angry when I restrict your privileges, but we had a deal, you broke it, and that means there will be consequences. Reflecting another’s emotions in this way may increase the chances of defusing needless hostility and of coming to a mutual understanding because people will know more concretely that they are being heard. According to the humanistic psychologists, people who learn they have value only when their behavior is pleasing to authority figures can benefit from a therapy setting that features unconditional positive regard. Like psychoanalysis, humanistic approaches to therapy work better in some instances than in others. Rogers referred to his approach as counseling, and his techniques are better suited to problems of adjustment than cases of serious disorders, such as schizophrenia. Efforts to assess the outcomes of humanistic therapy scientifically have been rejected by its followers. For example, one group of proponents argued that natural science methodology should not be privileged over human science methodology (Bohart, O’Hara, & Leitner, 1998, p. 141). By now, you probably understand that we would reject this suggestion and argue instead that science is science, regardless of the phenomenon being examined. Although a small minority of psychotherapists continue to use Freudian or humanistic approaches, most contemporary therapists use other methods. Freud’s psychodynamic theory, humanistic theory, and behaviorism were among the most dominant influences in psychology during the 20th century. We have seen how psychodynamic and humanistic theories were applied to psychotherapy in the form of psychoanalysis and Rogers’s person-centered therapy, so it should come as no surprise to find that behaviorism has similar applications. Given our formal definition of learning as a relatively permanent change in behavior or the capacity for behavior due to experience, it seems logical to use learning as a way to increase the frequency of desirable behaviors and decrease unwanted behaviors in the therapy setting. Behavior therapiesBehavior therapiesApplications of classical and operant conditioning principles to the treatment of symptoms of psychological disorder and adjustment problems            Behavior therapies        Applications of classical and operant conditioning principles to the treatment of symptoms of psychological disorder and adjustment problems             represent applications of classical and operant conditioning to problems of adjustment and psychological disorders. These techniques have proved helpful in both reducing unwanted behaviors and increasing the frequency of desirable behaviors over a range of psychological conditions, including autism spectrum disorder (ASD) and schizophrenia. The first book to use the term behavior modification in its title was Leonard Ullman and Leonard Krasner’s Case Studies in Behavior Modification, published in 1965. The public initially distrusted behavioral treatment methods because they were based on animal research and assumed that human behavior was relatively simple. The mysticism of the psychoanalysts’ unconscious drives and impulses seemed far more romantic. However, when behavior modification produced successful results in cases that were previously considered untreatable, such as in ASD, respect for the approach grew. Like behaviorism in general, behavior modification, now known as applied behavior analysis (ABA), focuses on what can be observed and makes few assumptions about such unseen factors as the unconscious mind. We can contrast the way that Freud and a therapist using ABA might approach Freud’s case of Little Hans, a 5-year-old boy who had a phobia of horses (Freud, 1909/1976). For Freud, Hans’s phobia resulted from the Oedipus complex issues that were supposedly at their height at his age. Horses, Freud argued, symbolized Hans’s father. The boy had noted that he was especially frightened of horses with black bits in their mouths, which Freud suggested represented the father’s mustache. Freud concluded that Hans’s fear of horses resulted from his unconscious fear of retaliation by the father for sexual fantasies about his mother and would disappear with the resolution of Hans’s Oedipus complex. The subject of Freud’s famous case study of Little Hans was actually named Herbert Graf. Graf grew up to be a successful opera producer at New York’s Metropolitan Opera and the Salzburg Festival in Austria. He is shown here directing the great Marion Anderson as she rehearses for her operatic debut in Giuseppe Verdi’s Masked Ball in 1954. Now it is the behaviorist’s turn. According to classical conditioning theory, phobias can result from conditioned fears. In Hans’s day, horses were the most frequently used means of transportation, especially in large cities. Accidents involving horses were frequent and quite serious. Hans related that he was frightened of horses falling—and with good reason, because it is likely that viewing such accidents would be quite traumatic. Horses could become classically conditioned fear stimuli through their association with accidents and the resulting fear. To reduce the fear, Hans could be gradually exposed to horses in a safe environment (desensitization), rewarded for approaching horses (counterconditioning), or allowed to play with a group of children interacting with a horse (peer modeling). It is not necessary to know why Hans fears the horses to apply behavioral treatment successfully. ABA has been used frequently to reduce unwanted habits. For example, let us suppose that Jennifer would like to stop smoking. The behavior therapist begins by evaluating Jennifer’s current behavior by observing the circumstances in which she smokes. Based on these observations, the therapist might note that Jennifer rarely smokes alone. Instead, her smoking is quite social. If she is at a party or with friends who are smoking, she smokes. The therapist also takes notes of reinforcers or punishers that might be influencing the behavior. Perhaps Jennifer’s friends praise her when she smokes, but her parents get angry when they notice evidence of her smoking. Jennifer’s dentist also might warn her about the poor state of her oral hygiene, which smoking only worsens. Based on these observations, the behavior therapist might attempt a number of techniques that will separate Jennifer from high-risk situations for smoking. She might be encouraged either to gain the support of her friends for her efforts to quit smoking or to spend more time with her nonsmoking friends. Jennifer’s parents might be willing to provide some incentives for her to quit smoking. She could set aside the money that she normally spends on cigarettes for some special treat. Counterconditioning, in the form of asking Jennifer to smoke until she feels sick, is an unpleasant but effective strategy (Lichtenstein, Harris, Birchler, Wahl, & Schmahl, 1973). Behavioral treatments for phobias have been improved with the development of virtual reality for exposing clients to fear-producing stimuli. Researchers at the University of Washington have developed SpiderWorld and successfully demonstrated that participants could reduce their fear of real spiders after undergoing three treatment sessions. The effectiveness of behavior therapies is not restricted to changing negative habits such as tobacco use. As discussed in Chapter 8, token economies, in which reward tokens are later exchanged for preferred rewards, have been shown to increase the frequency of desirable behaviors across a variety of settings, including classrooms, prisons, and institutions (Barkley, 2002; Glynn, 1990). Behavior therapies have helped many people change negative habits. As the name implies, cognitive therapies are based on the notion that the way that we think about our circumstances is essential to our health and adjustment. According to this view, situations do not cause abnormal behavior, but the way that we think about situations might do so. One of the earliest versions of cognitive therapy was developed beginning in the 1940s by Aaron Beck (see Figure 15.7). According to  Beck (1975), people run into psychological trouble when they adopt irrational, self-defeating ways of thinking. For example, it is unrealistic to think that other people will always treat you fairly and kindly, or that you must be perfect to be loved. An important aspect of the therapy process occurs when the therapist attempts to change the irrational beliefs of the client. For example, let’s assume that a therapist is trying to help a group of students who suffer from test anxiety. Each student has an irrational belief that he or she is going to flunk out of school, despite having an adequate grade point average. To help the students realize that this belief is irrational, the therapist might ask each one to provide evidence of the likelihood of flunking out or to identify the worst possible outcome of giving up the belief. Faced with logic, each student begins to substitute a new, rational belief for the old, irrational belief. This substitution of rational for irrational beliefs is termed cognitive restructuringcognitive restructuringA technique used in cognitive therapies in which new, rational beliefs replace earlier, irrational beliefs held by the client.            cognitive restructuring        A technique used in cognitive therapies in which new, rational beliefs replace earlier, irrational beliefs held by the client.            . Increasing interest in CBT is reflected in the number of scholarly articles published on this topic, which has grown substantially over the past few decades. A variation of cognitive therapy emerged from work by Albert Ellis, beginning in the 1950s. Ellis’s version of cognitive therapy is known as rational emotive behavioral therapy (REBT; Ellis, 1975). Compared to many other types of psychotherapy, REBT is somewhat confrontational because the therapist is more openly critical of a client’s thinking. Ellis retained the need to produce cognitive restructuring but deemphasized the supportive role of the therapist. Aaron Beck (1921–), shown here addressing a workshop at the Beck Institute, developed one of the earliest types of cognitive therapies. Beck’s approach featured cognitive restructuring, in which irrational beliefs, such as Everyone must love me or I can’t be happy, are replaced with more rational beliefs, such as I can be happy because I have some people in my life who love me. As cognitive therapy expanded further, therapists began to use the term cognitive behavioral therapy (CBT)cognitive behavioral therapy (CBT)A combination of cognitive restructuring with behavioral treatments that has been shown to be effective in reducing symptoms of many psychological disorders            cognitive behavioral therapy (CBT)        A combination of cognitive restructuring with behavioral treatments that has been shown to be effective in reducing symptoms of many psychological disorders            . In addition to promoting cognitive restructuring, therapists wanted to help clients initiate behavioral changes. For example, a person with major depressive disorder (MDD), as described in Chapter 14, can have irrational beliefs about hopelessness and worthlessness, coupled with behavior that is counterproductive to good mood, such as staying at home in bed watching television. Not only can the irrational beliefs be addressed by CBT, but behavioral methods can be used to encourage the client to take a brisk walk and to provide self-rewards for using more positive, rational patterns of thought. CBT has an excellent record of success, particularly in the treatment of MDD (Dobson & Dobson, 2016). Because CBT typically requires fewer than 16 sessions and can be used for a variety of conditions, it is a favorite among insurance providers. Patient characteristics, the type of problems being addressed, and other variables influence the success of any particular treatment. Consequently, many therapists use the biopsychosocial approachbiopsychosocial approachAn integrated approach to therapy that combines treatments addressing the biological, personal, and social underpinnings of psychological disorders.            biopsychosocial approach        An integrated approach to therapy that combines treatments addressing the biological, personal, and social underpinnings of psychological disorders.            , which combines elements of therapies designed to address the biological, psychological, and social underpinnings of an individual case. The therapist focuses on what works for an individual client, without adopting a single theoretical orientation (Lazarus, 2005). For example, a therapist might find that relaxation training seems to help one client, while role-playing is more effective with another. The major types of biological treatments for psychological disorders include medication, electroconvulsive therapy (ECT), psychosurgery, brain stimulation, and neurofeedback. Medication is the most commonly used of the medical therapies. Particularly in the last 60-odd years, the discovery of medications that effectively treated psychological disorders has dramatically shaped the outcomes of millions of patients. A comprehensive review of psychoactive medications is clearly beyond the scope of this text, but we will discuss the use of medication for some specific disorders later in this chapter. The frequent use of medication is not consistent with patient preference. In a meta-analysis of patient treatment preferences for psychological disorders, 75% of the participants preferred psychological treatment to medication (McHugh, Whitton, Peckham, Welge, & Otto, 2013). This preference was particularly strong in younger patients and in women. Electroconvulsive therapy (ECT)Electroconvulsive therapy (ECT)A biological treatment in which seizures are induced in an anesthetized patient; it is used primarily in the treatment of mood disorders that have not responded to medication or other treatments            Electroconvulsive therapy (ECT)        A biological treatment in which seizures are induced in an anesthetized patient; it is used primarily in the treatment of mood disorders that have not responded to medication or other treatments             is used in some severe cases of depression that do not respond to other treatments. In ECT, the patient is anesthetized and given a muscle relaxant before the induction of general seizures, which are produced by electricity applied through electrodes on the head. Between 6 and 12 treatments, at a rate of 3 treatments per week, are typically given. A patient is shown being prepared for ECT for serious depression. The patient is anesthetized before electricity is applied through electrodes to the head, inducing seizures. ECT is believed to increase responsiveness to dopamine and norepinephrine and to stimulate neural growth. Ladislas Meduna (1896–1964) recommended ECT for patients with schizophrenia after observing that people who had epilepsy had unusually high numbers of glia and people with schizophrenia had low numbers of glia (Kalinowsky, 1986). ECT was first used with patients with schizophrenia in the 1930s, with poor results. However, observations of improved mood in these patients following ECT led to further use with patients diagnosed with major depressive or bipolar disorder. This procedure is used primarily with patients who do not respond to conventional treatments with medication (Medda, Perugi, Zanello, Ciuffa, & Cassano, 2009). Although the exact mode of action of ECT is unknown, most patients experience an increase in responsiveness to dopamine and norepinephrine, along with the desired reduction in subjective feelings of depression. Although many people have a negative view of ECT (perhaps as a result of reading Ken Kesey’s One Flew Over the Cuckoo’s Nest or seeing the film or play of the same name), ECT is considered as safe as any minor surgery conducted under general anesthesia (American Psychiatric Association, 2001). However, approximately one third of patients experience persistent memory loss following the procedure (Rose, Fleischmann, Wykes, Leese, & Bindman, 2003). This memory loss specifically affects autobiographical memory, discussed in Chapter 9, which can be impaired for at least six months following treatment (Sackeim, 2014). As observed in Chapter 1, the drilling of holes in the skull by ancient people might represent the first effort to use psychosurgerypsychosurgeryThe attempt to improve symptoms of psychological disorders through operating on the brain.            psychosurgery        The attempt to improve symptoms of psychological disorders through operating on the brain.            . Although particular functions might be localized to parts of the brain, it is a giant leap of faith from our current understanding of localization and brain networking to the idea that someone can use psychosurgery to treat psychological disorders. Trepanation has made a bizarre modern comeback in the form of an alternative medicine or elective procedure. People are doing this in their own homes, in a possible variation of the self-injurious behavior discussed in Chapter 8. They argue that the procedure improves brain circulation and metabolism, but research support for such claims is lacking (Moskalenko et al., 2008). Historically, one of the most frequent types of psychosurgery aimed at changing behavior was the frontal lobotomy, which was discussed in Chapter 4. After hearing about the calm that resulted following the procedure in chimpanzees, Egas Moniz encouraged his colleagues to try the procedure on psychotic patients in the 1930s (Valenstein, 1986). Between 40,000 and 50,000 patients received the procedure between the 1930s and the 1950s, sometimes for vague problems such as mild depression in housewives or misbehavior in children (Sabatini, 1997). Walter Freeman popularized the procedure in the United States, traveling in a vehicle that he called his lobotomobile. John Fulton, whose research with chimpanzees had formed the basis for Freeman’s operation, was horrified. What are these terrible things I hear about you doing lobotomies in your office with an ice pick? he asked Freeman. Why not use a shotgun? (Kopell, Machado, & Rezai, 2005, p. 192). As we have emphasized in several contexts in this textbook, the frontal lobes participate in some of the highest-order cognitive functions carried out by the human brain. Consequently, the deliberate damage to the frontal lobes and their connections to other parts of the brain involved with the lobotomy procedure has harmful effects on a person’s judgment, personality, initiative, social behavior, and sense of self. Despite the outrage among many in the medical community, frontal lobotomies were performed well into the 1960s, until the use of medications for psychological disorders became an established practice (Sabatini, 1997). Today’s psychosurgery is far removed from the lobotomy procedure. Radiation and precise lesions are used to treat small numbers of people with MDD or OCD (Kopell et al., 2005). Walter Freeman (1895–1972) performed more than 3,400 lobotomies at $25 per procedure, despite having no formal training in surgery. Although this procedure is considered barbaric today, Freeman enjoyed the respect of many of his colleagues, serving as cofounder and president of the American Board of Psychiatry and Neurology from 1946 to 1947. He nominated his mentor, Egas Moniz (1874–1955), for the Nobel Prize in Physiology and Medicine, which Moniz won in 1949. Brain stimulation has a number of advantages over psychosurgery. Most importantly, the effects are usually reversible. In contrast, once the brain has been damaged by psychosurgery, there is no going back. Following observations of mood changes in people undergoing deep brain stimulationdeep brain stimulationElectrical stimulation applied through surgically implanted electrodes that is used to treat some anxiety and mood disorders.            deep brain stimulation        Electrical stimulation applied through surgically implanted electrodes that is used to treat some anxiety and mood disorders.             through surgically implanted electrodes for the relief of Parkinson’s disease, physicians attempted to use stimulation to address OCD and MDD. Microelectrodes are surgically inserted into specific parts of the brain, and in many cases, the patient controls the application of any current (Mayberg et al., 2005). A total of 50% of patients with severe depression who had not responded to more conventional treatments achieved improvements in mood following stimulation of the nucleus accumbens (Bewernick et al., 2010). In Chapter 4, we indicated that the nucleus accumbens was part of a circuit that processes feelings of reward. Stimulating this area improved patients’ mood and led to their engagement in more pleasurable activities. After the successful use of deep brain stimulation for Parkinson’s disease, the method was used to treat people with depression. The image on the right shows where an electrode was implanted in this woman’s brain. Deep brain stimulation requires surgery to implant the microelectrodes. Repeated transcranial magnetic stimulation (rTMS) provides an alternative way to stimulate the brain without surgery. When applied through a handheld device touching the scalp, low-frequency magnetic pulses change activity in underlying cortical regions. This technique has been recommended for use in MDD (Speer, Wassermann, Benson, Herscovitch, & Post, 2014; also see Figure 15.8) and some cases of schizophrenia, but it appears to have a negative effect in cases of OCD (Slotema, Dirk Blom, Hoek, & Sommer, 2010). Compared to participants with depression undergoing a placebo version of rTMS (sham rTMS), participants receiving active rTMS showed significantly reduced scores on the standard Hamilton Depression Rating Scales. Participants showed rather strong placebo effects in the sham condition (note the drop in depression at week 2), which emphasizes the need for placebo-controlled studies in assessing the effectiveness of all types of treatment. Neurofeedback, a type of biofeedback training that focuses on brain activity, has been used to treat ADHD in children. As mentioned in Chapter 4, biofeedback training allows participants to gain some conscious control over some otherwise unattended physical functions. For example, people can be taught to overcome white coat hypertension, or the classically conditioned spike in blood pressure that occurs when a nurse or doctor (in a lab coat) takes a measurement. By learning to relax instead of becoming tense in this situation, people can reduce their blood pressure, which is not a process that we typically try to control. NeurofeedbackNeurofeedbackA type of biofeedback used to treat attention deficit hyperactivity disorder (ADHD) and seizures by teaching the client to keep measures of brain activity within a certain range.            Neurofeedback        A type of biofeedback used to treat attention deficit hyperactivity disorder (ADHD) and seizures by teaching the client to keep measures of brain activity within a certain range.             is a type of biofeedback that concentrates on the activity of the brain. Recordings of brain activity, usually obtained from electroencephalography (EEG) but increasingly from functional magnetic resonance imaging (fMRI), are usually displayed for the participant, who is trained to keep readings in a desired range. In most cases, people are not consciously aware of what they are doing to comply. So far, however, rigorous studies have not provided strong support for the efficacy of EEG neurofeedback in the treatment of ADHD, ASD, OCD, generalized anxiety disorder (GAD), and MDD (Begemann, Florisse, van Lutterveld, Kooyman, & Sommer, 2016). Consequently, few insurance providers will cover this type of treatment. Type of TherapyTheoretical PerspectiveDefining FeaturesPsychoanalysisPsychodynamic theoryDirective therapyFree associationDream analysisAnalysis of resistanceAnalysis of transferenceHumanistic therapyHumanistic theoryNondirective therapyUnconditional positive regardReflectionEmpathyBehavior therapyBehaviorismApplication of classical and operant learning principles in behavior modificationTherapy based only on observed behaviorCognitive therapyCognitionCognitive restructuringCombination of restructuring with applied behavior analysis in CBTBiological treatmentsBiologyMedicationElectroconvulsive therapyPsychosurgeryBrain stimulationNeurofeedbackAs mentioned previously, approaches to treatment have differing success rates when applied to specific types of disorders. This section will outline the typical approaches to the psychological disorders discussed in Chapter 14. All therapy approaches discussed previously in this chapter have been applied to the treatment of children and adolescents. However, special care must be given to the use of biological therapies in childhood and adolescence because of the possible adverse effects of these treatments on the development of the nervous system. Although ASD is accompanied by abnormalities in the neurotransmitters serotonin, GABA, and glutamate, no medications have proved effective in alleviating core symptoms of the disorder (Pardo & Eberhart, 2007). This lack of a beneficial result does not mean that medications are not prescribed in cases of ASD. Two antipsychotic medications have been approved by the U. S. Food and Drug Administration (FDA) for use in ASD and appear helpful in severe cases involving aggression, tantrums, and self-injury (Fung et al., 2016;  Posey, Stigler, Erickson, & McDougle, 2008). Unfortunately, children treated with antipsychotic drugs experience the same side effects as adults, including weight gain, diabetes, motor dysfunction, and immune system problems. Antidepressants and several other types of drugs are commonly prescribed for children with ASD off-label, which means that the drugs have not been officially approved for this purpose on the basis of research demonstrating effectiveness (see Figure 15.9). Although only two medications (antipsychotic medications typically used to treat schizophrenia and bipolar disorder) have been approved by the FDA for use in treating ASD, large-scale studies show that many other medications are routinely prescribed by physicians for children with ASD. Alpha agonists are usually prescribed for high blood pressure and have a sedative effect. In this sample, 35.3% of the children had at least one prescription, and 9% had three or more drugs that they were taking simultaneously. The most commonly used psychological treatment for ASD is applied behavior analysis (ABA), which builds on the principles of operant conditioning outlined in Chapter 8 (Smith & Lovaas, 1998). ABA focuses on observable, socially important behavior such as language and should produce lasting improvements that generalize to settings outside the therapy setting (Baer, Wolf, & Risley, 1968). ABA in the treatment of ASD often involves therapy that takes up most of a child’s waking hours, which is a large investment for families in terms of time and resources. As shown in Figure 15.10, although studies are complicated by the child’s initial level of functioning and many other variables, meta-analyses of the outcome of intensive, early-childhood behavioral interventions show a strong positive effect of treatment on language acquisition, but more moderate effects on social skills and daily functioning (Virués-Ortega, 2010). A meta-analysis demonstrated that early behavioral intervention in ASD produced measurable improvements in IQ scores, language, and daily living. High-intensity treatment (measured by hours per week) seemed to have the greatest effects on IQ, but duration of treatment seemed to have the biggest impact on language. Most clinical psychologists suggest that an effect size of 1 for a treatment means that the treatment is very effective. Parents and teachers of individuals with ASD are anxious to find new methods for improving academic, communication, employment, and leisure outcomes. Subjective observations might lead us to think that computers—or better yet, mobile devices like tablets and smartphones—would be perfect for this purpose. But do the results of scientific research demonstrate true benefits? The answer, happily, appears to be yes (Kagohara et al., 2013). A review of 15 studies carefully selected based on the strength of their methodology demonstrated that mobile technologies have advantages for the person with ASD. The most frequent uses of mobile technology were using the device to access preferred stimuli, such as snacks and toys, which might otherwise be difficult for individuals with language delays. In cases of severe language disability, the devices could generate speech that would allow the individual to request preferred stimuli. Other uses included videos that teach specific academic skills, such as how to use the spell-check function of a word processing program. Video modeling of skills can be useful, given the reluctance to engage in face-to-face instruction that characterizes this disorder. Using an iPad to improve math skills produced mixed results (O’Malley, Lewis, Donehower, & Stone, 2014). However, students in this study who used the math app were more likely to complete their work with fewer prompts on the part of the teacher. Technology also offers potential relief from the high cost of face-to-face applied behavior analysis (ABA) treatment in cases of ASD. Randomized trials are currently underway using an iPad-based behavioral intervention called TOBY (Granich et al., 2016), which is intended to compliment, not replace, conventional therapy. The overall advantages of using these technologies include people’s ability to advance at their own pace with minimal supervision. At the same time, use of the devices should not substitute for face-to-face contact because such interaction is essential to improvements in behavior in individuals with ASD. However, social skills themselves might be the object of mobile apps. The evaluations of apps specifically designed to enhance children’s creative, expressive, and collaborative social activities seem encouraging, but the use of apps for this purpose needs further study (Hourcade, Williams, Miller, Huebner, & Liang, 2013). Research supports the use of tablets and other mobile technologies for improving the skills and behaviors of children with ASD. Because of the severity and lifelong nature of ASD, parents of children with this disorder are especially vulnerable to so-called cures that have little research support. Dietary interventions have become popular in recent years, but these appear to have few benefits. People with ASD show evidence of excess peptides from gluten (found in wheat) and casein (found in dairy products), but controlled studies do not demonstrate benefits from restricting these foods (Millward, Ferriter, Calver, & Connell-Jones, 2008). Chelation treatments designed to remove heavy metals from the body are also popular but lack research support (Weber & Newmark, 2007). Even worse, at least one child died from this type of treatment because a physician substituted a toxic chemical for the correct one with a similar name (Sinha, Silove, & Williams, 2006). Hyperbaric oxygen therapy, usually used to treat the bends in scuba divers, has not been shown to improve ASD symptoms in spite of Internet claims to the contrary (Granpeesheh et al., 2010;  Xiong, Chen, Luo, & Mu, 2016). Among the other treatments that are advertised to parents are sensory integration treatment, massage, music, special diets, and animal-assisted therapies. Many children with a diagnosis of ADHD are treated with medication, either alone or in combination with behavior therapy (Storebø et al., 2016). The use of medications for attention problems resulted from an accidental discovery. Charles Bradley used a stimulant (benzedrine sulfate) to treat a group of children hospitalized for learning and behavior problems (Bradley, 1937). He noted that during the week that the children used the drug, half demonstrated a spectacular improvement in school performance (p. 584). The most commonly prescribed drugs for ADHD are the closely related stimulants methylphenidate (Ritalin), dextroamphetamine (Dexedrine or Dextrostat), and a combination of amphetamine salts (Adderall). These drugs increase the activity of dopamine and norepinephrine. Most patients tolerate these medications well, but side effects of weight loss, sleep disturbance, and higher heart rate and blood pressure can occur. If children take the medications continuously for long periods, growth can also be suppressed. Methylphenidate does produce tolerance, and patients should stop the drug under medical supervision rather than on their own (Ross, Fischhoff, & Davenport, 2002). Although it is chemically quite similar to amphetamine, methylphenidate is not considered an addictive drug, given its slower action on the brain (Volkow, Wang, Fowler, & Ding, 2005). Individuals who do not respond well to the stimulant drugs are often treated with nonstimulant drugs that increase norepinephrine activity (Prasad & Steer, 2008). Charles Bradley observed the behavior of 30 institutionalized children, who had a variety of problem behaviors, after they were administered a type of amphetamine used at the time for asthma and other breathing problems. Bradley’s observations about the children’s spectacular improvement in school performance, as well as their becoming emotionally subdued, led to the modern treatment of ADHD with stimulant medication, including other types of amphetamine. It might appear odd to treat a disorder that often features hyperactivity, or high levels of impulsive behavior, with a stimulant. Wouldn’t it make more sense to try medications that have a calming effect? The rationale for stimulant treatment of ADHD suggests that impulsive hyperactivity occurs if the parts of the brain responsible for planned, thoughtful behavior, such as the basal ganglia and frontal lobes, are not active enough. Raising the activity in these areas theoretically should reduce impulsive behavior. One of the common myths about medications for ADHD, however, is that they are ineffective in people who do not have the disorder. This is not the case, and medications prescribed for ADHD are commonly abused (Arnsten, 2006). The use of medication for ADHD in the United States is five times higher than in any other nation, leading to criticism of the practice (see Figure 15.11). An unusual percentage of published research reports on the efficacy of medication has been sponsored directly by the pharmaceutical industry, leading to concerns about possible bias (Storebø et al., 2016). Approximately 75%–80% of children treated with medication alone experience significant short-term improvement on ratings of social compliance and sustained attention (Barkley, 1995). Research has not demonstrated long-term benefits of stimulant use on later outcomes in adolescence and adulthood (Leo & Feldman, 2007). In other words, children with ADHD treated with medication do not enjoy superior outcomes later in life compared to children with ADHD who do not receive medication. This map of medication use for ADHD from the Centers for Disease Control and Prevention (CDC) shows the same regional variations that were observed in Chapter 14 for the prevalence of ADHD diagnoses. Together, these data suggest that about half of all children diagnosed with ADHD are prescribed medication. The use of medication produces short-term improvements in about 75%–80% of children with ADHD. However, children with ADHD who are treated with medication are no better off in adolescence or adulthood than children with ADHD who were not treated with medication. College students are not the only ones using ADHD medications inappropriately. A survey in the prestigious scientific journal Nature found that 65% of the scientists who responded knew a colleague who used methylphenidate or similar drugs without a prescription for the purpose of improving focus and concentration (Maher, 2008). Although medications are widely prescribed, behavioral methods for treating ADHD can be helpful. In particular, behavioral parent training, which builds the skills of parents of children with ADHD to use operant conditioning methods effectively, and behavioral classroom management, which applies learning principles in the school setting, are especially effective (Pelham & Fabiano, 2008). Another behavioral approach featuring peer-focused treatments in recreational settings also has shown promise (Evans, Owens, & Bunford, 2014;  Pelham & Fabiano, 2008). Unfortunately, it is often difficult to maintain the consistency needed for successful behavioral intervention without a great deal of cooperation among teachers, parents, and others working with the child. Before the 1950s, effective treatment for schizophrenia was virtually nonexistent. Patients were often subjected to odd treatments that had no basis in any scientific understanding of their condition and that were frankly dangerous in some cases. They were forcibly restrained in lukewarm baths with canvas covers that prevented their escape. Insulin shock therapy, in which injections of insulin were used to place patients in daily comas, enjoyed a brief fad status, as did the frontal lobotomy, discussed earlier in this chapter. The discovery of effective medications for schizophrenia was accidental. A French surgeon, Henri Laborit, was impressed by the calming effects of the phenothiazines. He experimented with one of these drugs, chlorpromazine (Thorazine), to reduce stress and shock in his surgical patients. Based on his promising results, Laborit encouraged his psychiatric colleagues to try the drug on their psychotic patients, which they did with dramatic success. A new era of treatment began. Largely as a result of this new type of treatment, institutionalized populations dropped by about 50% in the United States between 1955 and 2000 (see Figure 15.12). People responding well to medication could now return to their communities instead of living in institutions. The percentage of the U. S. population that was institutionalized in asylums because of mental disorders was cut almost in half in the years following the discovery of antipsychotic medications. Initially, nobody understood how phenothiazines reduced psychosis. One clue came from the fact that high doses of the drug produced symptoms similar to those of Parkinson’s disease, such as muscular tremor. Parkinson’s disease was known to result from the degeneration of motor neurons that used dopamine as their primary neurotransmitter. Further research indicated that phenothiazines acted as dopamine antagonists, blocking dopamine at the receptor site. Phenothiazines are not without their share of problems, though. Nearly one quarter of patients with schizophrenia do not respond to these drugs (Kane & Freeman, 1994). The drugs successfully reduce psychotic symptoms, such as hallucinations and delusions, but are often less effective in reducing negative symptoms, such as social withdrawal and emotional disturbance (Buchanan, Breier, & Tamminga, 1995). To address these deficits of phenothiazines, often called typical or first-generation antipsychotic medications, newer atypical or second-generation antipsychotic medications, such as clozapine (Clozaril), were introduced. Approximately half of all contemporary patients with schizophrenia are prescribed the atypical antipsychotics (Meltzer, Bobo, Lee, Cola, & Jayathilake, 2010). These newer drugs are often more effective than phenothiazines in treating the negative symptoms of schizophrenia because they affect a wider range of neurotransmitters, including serotonin. All antipsychotic medications produce significant side effects. Tardive dyskinesiaTardive dyskinesiaA movement syndrome that results from the use of medications used to treat symptoms of schizophrenia.            Tardive dyskinesia        A movement syndrome that results from the use of medications used to treat symptoms of schizophrenia.            , (tardive means slow and dyskinesia means difficulty moving) is a syndrome that results from the use of antipsychotic medications by some patients. The syndrome produces tremors and involuntary movements, especially of the face and tongue. These problems often continue even when medication stops. Although risk of tardive dyskinesia seems lower with the second-generation antipsychotics (Leucht et al., 2013), these drugs have their own set of side effects, including diabetes, weight gain, reductions in white blood cell count, aggravation of existing cancers, and seizure disorders. Medications used for schizophrenia often produce in patients a movement disorder known as tardive dyskinesia. This patient is experiencing the intrusive, involuntary movements and tremors that characterize this condition, which is usually permanent. Newer medications for schizophrenia are somewhat less likely to produce tardive dyskinesia but have their own side effects, including diabetes, weight gain, reductions in immune system functioning, aggravation of existing cancers, and seizure disorders. Most individuals with schizophrenia are treated with medication, but psychological treatments still have much to offer. The World Health Organization (WHO) noted that people with schizophrenia in developing countries, such as Nigeria, India, and Colombia, were recovering more frequently than patients in the United States and Europe (Hopper, Harrison, Janca, & Sartorius, 2007). Further research suggested that improved outcomes occur when patients remained in their homes and were given opportunities to do meaningful work. These observations led to the increased use of psychosocial rehabilitation for patients with schizophrenia (Bustillo, Lauriello, Horan, & Keith, 2001). This approach emphasizes providing opportunities to do meaningful work, social skills training, education about schizophrenia and the importance of medication, affordable housing linked to services, and information about symptom management. The primary method of treating bipolar disorder is the use of medication. The most commonly prescribed medication is a simple salt, lithium carbonate. Not only does lithium prevent recurrences of mania, but it is uniquely preventive of suicidal behavior (Alda, 2015). Lithium’s actions remain poorly understood. In general, lithium appears to protect individuals from neurodegeneration (Alda, 2015). Lithium has antioxidant effects (Jornada et al., 2011) and appears to both promote the birth of new neurons and prevent further loss of neurons (Quiroz, Machado-Vieira, Zarate Jr, & Manji, 2010). Administration of lithium in patients with bipolar disorder results in increased gray matter relative to patients who are not treated with lithium (see Figure 15.13 and Figure 15.14). Newly diagnosed participants with bipolar disorder were randomly assigned to groups treated with lithium or valproic acid, which is also used to treat bipolar disorder. Over the next 16 weeks, these groups were compared to healthy controls using no medications. Gray matter volume increased in the group treated with lithium but did not change in the healthy controls or in the group treated with valproic acid. These results support the ability of lithium to increase neural growth, which might also explain its effectiveness in treating bipolar disorder. These images compared gray matter density between (a) patients with bipolar disorder using lithium treatment and control participants and (b) patients with bipolar disorder who were not using lithium treatment and control participants. As indicated by the colors (blue = little to no difference and red/white = statistically significant difference), the patients taking lithium experienced considerable growth in gray matter density in many areas of the cerebral cortex. Unfortunately, lithium carbonate has a number of unpleasant and potentially dangerous side effects. Toxic levels of lithium produce nausea, vomiting, muscular tremor, coma, and seizures. Because of these risks, patients must have regular blood tests to monitor their lithium levels. The unpleasantness of these side effects also contributes to the poor compliance with their treatment plan that characterizes patients with bipolar disorder. These patients are less likely to follow treatment instructions than any other kind of patient on any other type of medication (Sajatovic, Valenstein, Blow, Ganoczy, & Ignacio, 2007). Other medications, including antipsychotics, are used in treating bipolar disorder but do not appear to be as effective as lithium in reducing hospital admissions (Joas et al., 2017). Psychosocial rehabilitation for schizophrenia includes opportunities to do useful work. Socializing with peers, engaging in healthy outdoor physical activity, and earning a paycheck contribute to the clients’ personal growth. Research demonstrating that bipolar disorder is more common in nations in which consumption of seafood is low, discussed in Chapter 14, has led to suggestions that omega-3 fatty acids in the diet might provide some protection from bipolar disorder (Noaghiul & Hibbeln, 2003). Lower levels of omega-3 fatty acids have been observed in patients with bipolar disorder (Balanzá-Martínez et al., 2011). The use of omega-3 supplements improves symptoms of depression in these patients, but it does not seem to have much effect on the symptoms of mania (Grosso et al., 2014). While not a substitute for treatment with lithium, the benefits of omega-3 fatty acids are obtained without side effects. You have probably noticed that this discussion of treating bipolar disorder has not emphasized psychotherapy. Most individuals with bipolar disorder appear to require medication for the alleviation of their symptoms, and psychotherapy alone is not typically considered an option. However, psychotherapy aimed at assisting individuals and their families with coping with bipolar disorder and adhering to treatment is often beneficial. Even in conditions like bipolar disorder that have strong bases in biology, integrating several perspectives when selecting treatment approaches can produce superior outcomes. Given the range of suspected causal factors in depression, it should not surprise you that treatment approaches are diverse. Treatments range from medical, to cognitive, to behavioral, to simple aerobic exercise, and combinations of these approaches are common. The most popular method for treating MDD today is the use of medication. Antidepressant medicationsAntidepressant medicationsA medication designed to alleviate symptoms of depression, but often prescribed for other types of conditions            Antidepressant medications        A medication designed to alleviate symptoms of depression, but often prescribed for other types of conditions             are the third most common prescription drug taken by Americans (Pratt, Brody, & Gu, 2011). As shown in Figure 15.15, about 10% of Americans over the age of 12 years use antidepressants. Rates of antidepressant use in the United States increased nearly 400% between 1988 and 2008 (Pratt et al., 2011). Keep in mind that antidepressant medications are often prescribed for conditions other than MDD, such as anxiety disorders. This is still a remarkable number of prescriptions and raises the distinct possibility that, like barbiturates and benzodiazepines before them, antidepressants are being overprescribed. In addition, studies have shown that these medications are effective in severe cases of MDD, but they do not show benefits compared to placebos in mild to moderate cases (Khan & Brown, 2015). Antidepressant clinical trials as a whole have small (0.30) effect sizes, even when severely depressed individuals are included in the sample (Khan & Brown, 2015). This means that use of the medication explains a small portion of the variation in patient outcomes, including the extent of improvement. Use of antidepressant medication in the United States is high, with one in ten over the age of 12 years and more than one in five 40-59 year old females using these medications. The most widely prescribed antidepressant medications are the selective serotonin reuptake inhibitors (SSRIs), which include popular brands like Prozac and Zoloft. As their name implies, these medications increase serotonin activity at the synapse by interfering with reuptake of the neurotransmitter, a process discussed in Chapter 4. As a result, more serotonin is available in the synapse (see Figure 15.16). The use of SSRIs is based on a belief that MDD results from unusually low serotonin activity and that therefore, boosting serotonin activity should improve mood. These drugs achieve this effect by interacting with the serotonin transporters located in the membrane of the neuron releasing the serotonin. This interaction slows the rate of reuptake, or the process by which the neuron retrieves and repackages the molecules of serotonin that it has just released. In addition to inhibiting the reuptake of neurotransmitters at the synapse, antidepressants provide further benefit by increasing the number of neurons in the hippocampus—at least in rats (Nasrallah, Hopkins, & Pixley, 2010). Stress and depression both appear to reduce the number of neurons found in the hippocampus, and the administration of antidepressants may not only halt this loss, but also promote the birth of new neurons in this area. Results from experiments with rats showed a 20%–40% increase in neurons in the hippocampus in response to the administration of antidepressant drugs. These medications have their share of risks and side effects too. Increases in thinking about suicide following use of SSRIs, especially among children and youth, have prompted stronger warnings for close supervision by attending physicians (Hall, 2006;  Wessely & Kerwin, 2004). Other side effects of SSRIs include nausea, reduction of rapid eye movement (REM) sleep, headache, weight gain, and sexual dysfunction. Between 30% and 60% of patients treated with SSRIs report problems with sexual activity (Gregorian et al., 2002). SSRIs reduce the activity of nitric oxide (NO), the gaseous neurotransmitter that helps maintain erections in men and genital engorgement in women (Yu et al., 2003). In contrast, erectile dysfunction medications, such as Viagra and Cialis, act by boosting NO activity. When patients fail to respond to medication, other biological treatments might be used to treat MDD, such as ECT, deep brain stimulation, or rTMS (Demitrack, 2007;  Mayberg et al., 2005; O’Reardon et al., 2007). In addition to biological treatment options, a number of psychotherapy alternatives are quite successful in alleviating depression and have the added benefit of producing no adverse side effects. One of the most popular approaches is the use of CBT. As described previously, this approach combines the restructuring of negative cognitions (Nobody loves me becomes Not everybody loves me, but some people do) and behavioral techniques aimed at increasing the person’s activity level and social skills (Beck, Rush, Shaw, & Emery, 1979). Between 40% and 60% of people with MDD treated with CBT not only experience relief from their symptoms, but also appear to be less susceptible to repeated bouts of depression than people who use medication alone (Hollon, Thase, & Markowitz, 2002). People who are depressed also benefit from adding aerobic exercise to their routine (see Figure 15.17). For cases of mild to moderate depression, exercise produces effects comparable to medication and psychotherapy, and it complements these treatments in cases of severe depression. Because people who are depressed face higher risks of cardiovascular disease and type 2 diabetes, exercise is a valuable addition in terms of patients’ physical health as well. Participants with MDD who had not been previously treated were randomly assigned to medication alone, exercise alone, and medication plus exercise groups for a 12-week study. Because the participants were randomly assigned to groups, there were no statistically significant differences in the severity of their depression before treatment. All three treatment groups experienced improvement in their symptoms, as shown by their reduced scores on the Beck Depression Inventory. Although it appears that the medication group might have improved more than the exercise and combination groups, the differences between these three groups were not statistically significant. Therefore, we can conclude that exercise alone was equally effective in reducing symptoms of depression as medication alone or medication in combination with exercise. Although our methods for treating mdd are typically effective, there is a significant risk of recurrence. Some individuals experience low rates of recurrence when continuing to take antidepressant medications, even when their depressive symptoms are gone. However, these medications produce significant side effects, making this approach undesirable for many people. Continuation of CBT after recovery is typically more helpful than continuing to take medication, but many health plans pay for only limited numbers of sessions. Recurrences of depression might occur because people who have experienced episodes of MDD think differently than people with no history of depression when a mildly negative mood state occurs (Teasdale et al., 2000). A technique known as mindfulness-based cognitive therapy (MBCT) combines aspects of CBT and mindfulness-based stress reduction. The idea of MBCT is to teach people to detach from depression-related thoughts and feelings by endorsing statements like Thoughts are not facts and I am not my thoughts. How well does this approach work to prevent repeated episodes of MDD? The Question: Does the addition of MBCT to MDD treatment produce better outcomes than treatment as usual alone? In a study, 145 patients in recovery from MDD were randomly assigned to continue with their treatments as usual or to receive additional MBCT training. Their progress at a 1-year follow-up was assessed. Treatment as usual meant that the patients would seek help as they normally would, perhaps from a family doctor, if they experienced mood difficulties. Participants assigned to receive MBCT training attended eight weekly 2-hour group training sessions and completed daily homework exercises. Key themes included empowerment, freedom of choice, and awareness of experience in the moment. Patients as participants require additional protections. Making treatment as usual available ensures that no participants are left untreated. Referrals to appropriate services should be provided to all participants, and confidentiality should be maintained with great care. Among participants with three or more previous episodes of depression, who made up 77% of the sample, the addition of MBCT reduced recurrence rates by nearly 50% compared to treatment as usual alone. This study indicated that MBCT provides a powerful tool for preventing further episodes of depression—a finding that has since been replicated and supported by a significant amount of research (Khoury et al., 2013;  Warren et al., 2016). The use of MBCT has been extended successfully from the treatment of MDD to the treatment of anxiety and stress (Khoury et al., 2013). Anxiety disorders, including GAD, phobias, and panic disorder, are typically treated by using medication, CBT, or a combination of the two (see Figure 15.18). Medication, exposure therapy (in which the individual is gradually introduced to anxiety-producing situations), and combined medication and exposure therapy appear to produce approximately equal improvement in the functioning of people with social anxiety disorder. Scores of social functioning of people randomly assigned to these treatment conditions were significantly better than scores of the placebo group after 24 weeks of treatment. However, when treatment was discontinued after 24 weeks, the functioning of the groups that received medication and medication combined with exposure therapy deteriorated at the 1-year follow-up assessment, whereas the exposure-alone group continued to improve. These results suggest that patients choosing to use medication should continue treatment to maintain gains in functioning. As discussed in Chapter 14, anxiety is associated with a heightened level of brain activity because of anticipated danger. Consequently, most efforts to medicate anxiety attempt to reduce brain activity, or produce a tranquilizing effect. Because GABA is the major inhibitory neurotransmitter in the brain, a common method for slowing brain activity is to enhance its effects. As described in Chapter 6, many substances, including alcohol, barbiturates, and benzodiazepines, interact with the GABA receptor to produce stronger inhibition in the brain. Each of these types of substances produces a tranquilizing effect on the brain. Because these drugs have additive effects with one another and work at the same receptor site, taking combinations of these drugs is frequently fatal because of a life-threatening level of brain inhibition. Knowing that one of the motivations for alcohol use might be to reduce the anxiety resulting from social events and other stressors can be helpful in avoiding problem drinking. If a person feels like she can’t face a situation without being drunk, it would be more positive to seek professional help than to continue drinking. Barbiturates, developed in 1903, continue to be quite useful in the treatment of seizures, but their use as a treatment for anxiety is no longer considered desirable. They were one of the first of several medications used for psychological treatments to become vastly overprescribed. By the mid-1960s, enough barbiturates were being made to provide every American man, woman, and child with 40 to 50 doses per year (Regan, 2000). The addictive properties of barbiturates, as well as their frequent use in suicides, were definite drawbacks. The discovery of benzodiazepines after World War II changed the treatment of anxiety dramatically, but like the barbiturates, the benzodiazepines became overused. By the 1970s, chlordiazepoxide (Librium) and diazepam (Valium) were the most frequently prescribed drugs of their day. Alprazolam (Xanax) remains a widely used benzodiazepine, typically prescribed for panic disorder, GAD, and premenstrual syndrome. In some cases, alprazolam seems to be helpful in treating MDD, but its mechanism for doing so is not understood. You might think that tranquilizers would be a logical medication for treating anxiety disorders. Although these drugs are used occasionally, the antidepressants discussed in a previous section of this chapter are typically more effective. This result might reflect a common underlying cause for both depression and anxiety. Behavioral and cognitive behavioral techniques are commonly used to treat anxiety disorders and have the advantage over medication of having no side effects. Phobias, for instance, can be treated with exposure therapy, in which the person is gradually exposed to the fear stimulus or to other people interacting harmlessly with the fear stimulus while the person relaxes. Medication and psychological treatments produce similar levels of improvement for cases of anxiety (Baldwin et al., 2014). OCD can be treated with antidepressant medication, CBT, or a combination of both. CBT for OCD involves training the person to anticipate compulsive behavior and then to engage in a competing behavior. As mentioned earlier, a compulsive hand-washer could go outside, away from faucets and soap, to do some gardening. Meta-analyses show that CBT is a highly effective approach to treating OCD (Olatunji, Davis, Powers, & Smits, 2013). As you have seen in this chapter, administering therapies for psychological disorders and problems with adjustment is typically the domain of highly trained professionals. However, there are a few activities that people can do safely on their own to reduce stress and change habits. Chapter 8 outlined a method that you could use to change unwanted habits based on behavioral therapies. This box reviews a progressive relaxation technique that you can use to reduce stress. These same techniques are often implemented to produce relaxation during exposure therapy. To enhance your ability to relax, it is often beneficial to make an audio recording of the instructions. To begin, put on some loose, comfortable clothing and either lie down on your back or sit in a chair with your head supported. Breathe slowly and deeply. Each muscle group is tensed for 5 seconds (silently count one–one thousand at a normal speaking pace), followed by a pause of 20 seconds, during which you relax the muscle group and think of tension draining from that part of the body. The procedure can be repeated for more dramatic relaxation. With practice, you should be able to will yourself into a relaxed state rather easily. Here is the order of muscle groups to tense and relax: Tense the muscles of your head and face. Tense the muscles of your neck (gently). Tense your elbows and biceps for 5 seconds, and then relax and straighten your arms. Clench your fists tightly. Tense the muscles of your chest. Tense the muscles of your stomach. Tense the muscles of your calves. Tense the muscles of your feet and toes (gently, to avoid cramping). If you are successful in achieving deep relaxation, someone should be able to pick up your arm or leg and it will be limp and unresisting. Tensing and then relaxing each muscle group in the body leads to a deep state of relaxation. CBT is particularly helpful in cases of body dysmorphic disorder (Allen & Woolfolk, 2010). Cognitive restructuring can help the individual think more realistically about perceived physical flaws. Many individuals with this disorder, however, insist on continued treatment by cosmetic surgeons and resist suggestions that they should obtain psychotherapy. PTSD is typically treated using exposure therapy with or without SSRI medications (Le, Doctor, Zoellner, & Feeny, 2014). Because many people with PTSD also develop a variety of substance use disorders, additional treatment addressing these disorders is necessary for full recovery (Back et al., 2014). You might expect virtual reality (VR), the presentation of realistic stimuli through a device worn on the head, to be more exciting to video gamers than to therapists. However, this new technology is showing remarkable promise for the treatment of phobias and PTSD. As discussed in this chapter and in Chapter 8, one of the best ways to address these stress- and fear-related problems is to combine relaxation with a controlled exposure to the fear-producing stimuli. This application of classical conditioning was popularized by Joseph Wolpe in the 1950s (Wolpe, 1958). For example, people who are afraid of heights could imagine progressively more stressful situations (climbing a ladder or visiting the Empire State Building) while remaining relaxed. Eventually, the fear of heights should be reduced. Imagination is a wonderful capability of the mind, but it has its limitations. This is where VR comes in. Instead of asking a client to imagine a situation, the scenario can be presented in a realistic way through the use of VR technology. As our height-phobic client maneuvers through elevators, construction sites, and other high places, wind, vibration, and sound can be added to make the visual experience even more realistic. In addition to treating phobias, VR is being used to help combat veterans recover from PTSD (Spira et al., 2006). In these cases, odors from diesel fuel and gunpowder can be added to enhance the reality of the virtual experience. The U. S. Army is testing Virtual Iraq, a 360-degree, interactive, computer-generated environment, as a treatment for soldiers with PTSD. Soldiers are able to relate their traumatic experiences to the therapist, who can then program the environment to reproduce weather conditions, terrain, helicopter flyovers, types of attacks, and sound. Soldiers treated with VR are being compared with soldiers treated with other traditional psychotherapy approaches and a control group of soldiers on the waiting list for treatment. Research comparing the effectiveness of VR therapies, alone or in conjunction with traditional CBT, suggests that VR therapy alone is about as effective as CBT alone in treating phobias, but the former might have an advantage in certain situations, such as fear of flying (Krijn et al., 2007). VR also performed well in comparisons with CBT in cases of PTSD (Baños et al., 2011). In particular, VR therapy appealed to people with PTSD who were unwilling to undergo more traditional treatments. People who grew up playing video games might find the VR approach more effective and comfortable. The treatment of dissociative identity disorder can be quite controversial, as is the diagnosis of this condition, explained in Chapter 14. Some psychotherapists work on the assumption that dissociative identity disorder is a valid diagnostic category that often results from serious trauma (childhood sexual abuse in particular). Treatment is aimed toward integrating the various personalities by first identifying and working through the traumatic memories related to the disorder (International Society for the Study of Trauma and Dissociation, 2015). Hypnosis is commonly used by these therapists to assist in the recovery and resolution of traumatic memories, but this practice is not supported by scientific research. Under hypnosis, a suggestible person might be vulnerable to the manufacture of false memories, and individuals identified as having dissociative identity disorder appear to be far more suggestible than the general public (Frischholz, Lipman, Braun, & Sachs, 1992). In one notable case study, a woman sought therapy to cope with a traumatic event involving her daughter. Her psychiatrist employed hypnosis and convinced her that she had experienced horrific abuse as a child and now had more than 120 separate personalities (including one that was a duck), thereby illustrating the influence that therapists can have on the memories that they help recover (Loftus, 1997). A particularly damaging finding regarding the state of treatment for dissociative identity disorder is the suggestion that individuals labeled with the disorder subsequently show even worse symptoms (Piper & Merskey, 2004). Critics of the dissociative identity disorder classification suggest that psychotherapists should restrict their actions to resolving the stress, anxiety, and depression of the individual, without referring to dissociation. Both biological and psychological treatments are commonly used to treat somatic symptom disorders, which involve physical symptoms such as pain that have no apparent physical basis. Among the biological treatments for somatic symptom disorders are medication with antidepressants and increased physical activity. Psychological treatments usually take the form of CBT (Allen & Woolfolk, 2010). Complicating the treatment of these disorders, most of these patients are seen by their primary physicians, and relatively few are willing to pursue psychotherapy. Between 50% and 80% of patients referred for psychological treatment by their physicians do not comply, possibly because of their continued belief that they have a real medical condition that has not yet been appropriately diagnosed (Allen & Woolfolk, 2010). British painter Kim Noble has been diagnosed with dissociative identity disorder. This disorder is characterized by the experience of two or more personality states. Noble paints quite differently when she feels that she is in a particular personality state as opposed to others and believes that she has improved her well-being by expressing her perceived personalities through her art. Finding effective treatments for antisocial personality disorder (ASPD) has a high priority, given the extent of harm that individuals in this category can inflict on the rest of society. Unfortunately, effective treatments remain elusive (Bateman, Gunderson, & Mulder, 2015). No known medications specifically reduce antisocial behavior, although many incarcerated individuals are treated with tranquilizing drugs, including antipsychotic and anticonvulsant medications (such as barbiturates), to make their aggressive, violent behavior more manageable. Even if effective medications are eventually discovered, individuals with ASPD tend to view themselves as OK and not in need of treatment. Compliance with effective treatments is likely to remain a problem. Psychological treatments for ASPD include learning models that emphasize anger control, social skills, and moral reasoning (Frick, Ray, Thornton, & Kahn, 2014). These models appear to be modestly effective in reducing antisocial behavior in children and youth, but they do not appear to have much benefit for individuals who show evidence of true psychopathy, or lack of guilt and empathy (Hornsveld, Nijman, Hollin, & Kraaimaat, 2008). Individuals with ASPD might benefit from training in character formation (Salmon, 2004). Addressing the selfishness inherent in people with ASPD, therapists can attempt to convince the person that doing the right thing will produce more favorable outcomes (e.g., staying out of prison), even if the person fails to grasp the underlying value of respecting the rights of others. Despite these efforts, many nations consider this condition untreatable and rely on incarceration of criminals with ASPD to protect the public. Narcissistic personality disorder (NPD), while potentially disabling, is often accompanied by high overall levels of functioning (Ronningstam, 2014). In addition, many traits associated with this disorder (hypersensitivity, defensiveness, inability to see one’s own role in problems) might reduce the likelihood that a person with it will seek treatment. Finally, lower-functioning individuals with NPD are likely to have other serious psychological problems that motivate them to seek treatment, such as depression, substance abuse, or suicidality, which take precedence over the narcissistic behaviors themselves. As a result, the scientific literature provides fewer details about treating NPD compared to some of the other personality disorders. Therapists generally employ CBT with an emphasis on carefully building an alliance with the client (Kealy, Goodman, Rasmussen, Weideman, & Ogrodniczuk, 2017). Psychotherapy for NPD is likely to be most effective when the therapist is aware of the traits that might interfere with the process, such as defensiveness. Clarifying the difference between pathological and protective behaviors and reviewing situations and experiences that trigger narcissistic reactions can be particularly helpful (Ronningstam, 2014). No medications are currently approved specifically for the treatment of borderline personality disorder, but medications are frequently prescribed. Among the commonly prescribed medications are antidepressants, antipsychotic medications, mood stabilizers (e.g., lithium), antianxiety medications (e.g., benzodiazepines), and anticonvulsants. Because of the potential for self-destructive behavior and serious depression, individuals with borderline personality disorder are occasionally hospitalized for a few days for their own safety. Psychotherapy for borderline personality disorder often takes the form of CBT. In this case, the individual learns to manage stress, emotions, and relationships. In particular, psychotherapists emphasize skills aimed at reducing the individual’s potential for suicidal thinking and behavior. An approach specifically designed for individuals with borderline personality disorder, dialectical behavior therapy (DBT), attempts to address symptoms in the order of their importance: reducing suicidal thoughts, reducing behaviors that interfere with therapy, and finally reducing behaviors that interfere with the quality of life (Linehan, Armstrong, Suarez, Allmon, & Heard, 1991). Outcomes using DBT for borderline personality disorder appear superior to those produced by more conventional treatments with CBT (Linehan et al., 2006). Perhaps after reading about the variations in both the biological and psychological treatments available for specific types of disorders, you will better appreciate why people who want to be therapists spend such a long time in school perfecting their skills and learning about their options. Although some disorders seem to defy the available treatment methods, we do not want to leave you with the impression that therapy is somehow ineffective. Quite the contrary is true. Compared to the state of available therapies as recently as 60 years ago, psychologists and psychiatrists have made great strides in their ability to alleviate the distress of many millions of people. As we learn more about the causal factors underlying these disorders, as we surely will by pursuing integrative approaches, our abilities to help will expand. At the same time, we are making significant progress in our understanding about the prevention of many of these disorders, which we hope will make these problems a thing of the past. The use of DBT for borderline personality disorder has been simplified by the development of a smartphone app, which takes the place of keeping diary cards of emotions, events, and problem behaviors. Treatment based on sound psychological science is the goal not only of those working with individuals who have diagnosed psychological disorders, but also counselors working with people who simply want to improve their relationships and well-being. Decades of careful research has informed the recommendations of John and Julie Gottman (Gottman & Gottman, 2017). Their research method of assessing correlations between physiological arousal and behaviors as couples discuss areas of personal conflict now guides their approach to counseling (see Figure 15.19). These methods have predicted which couples will divorce within 5 years. Research by John and Julie Gottman has identified a number of principles that they believe should guide marriage counseling, which they incorporate into their Sound Relationship House model. The Gottmans are fond of observing couples in states of conflict, which they consider to be normal regardless of the couples’ reported happiness. Most couples get along during the good times—it’s often the tough times that test the quality of the relationship. The Gottmans equip couples with simple measures of heart rate as they discuss areas of conflict. This approach is based on their observations that unhappy couples experience much higher levels of sympathetic arousal than happy couples do during conflict. If the measures get too high, the couple is instructed to take a break in the discussion. The general idea is that you can’t solve problems if your levels of arousal are too high, and continuing conflict under these circumstances is destructive. The first step asks the couple to postpone any efforts at persuasion until each person has clarified his or her position. The focus is on self-disclosure, not attack–defend. A second step involves reprocessing past emotional injuries. Couples are guided through five steps with regard to processing past hurts: feelings they had, subjective realities, triggers, taking responsibility and apologizing, and constructive plans. Finally, in the Gottmans’ lab work, they realized that couples raised the same conflicts at 3-year, 6-year, and 9-year intervals! To integrate this concept with their counseling, they worked with couples to accept each partner’s personality (often the source of long-term conflict) and the ideal solutions imagined by each partner. While not removing the problem, the couples could learn to live with it. The Gottmans’ work in the lab has provided them with unique, scientific tools for helping people build more effective relationships with their partners, and even their children. Observations a few months after a wedding predicted the couple’s happiness with their first-born. Data from the last trimester of pregnancy predicted the frequency of a baby’s laughing, smiling, or crying. A 10-hour seminar over 2 days reversed relationship dissatisfaction in 77% of last-trimester couples, an effect replicated now in 24 countries. It is hard to imagine a better use of psychological science than to intervene positively in the happiness of these couples and their children. In chapter 14, we described cyberbullies as being somewhat different from traditional bullies, in that the traditional bullies evidenced Dark Triad traits (Machiavellianism, narcissicim, and psychopathy), but these traits were not predictive of cyberbullying. Instead, cyberbullies were characterized by higher levels of sadism, or taking pleasure in the pain of others. Prior to 1994, sadistic personality disorder was included in the Diagnostic and Statistical Manual of Mental Disorders (DSM), but it is no longer. As a result, relatively little recent psychological research has focused on this behavior, and the research that does exist focuses on sexual sadism. We can assume that the basic features of sadism have been incorporated into the DSM category of ASPD, yet van Geel et al. (2017) were able to distinguish between psychopathy (closely related to ASPD) and sadism. Sadism is described as having theoretical roots completely divorced from the Dark Triad of personality (Paulhus, Jones, Boyle, Saklofske, & Matthews, 2015, p. 582), yet it also is part of what the authors refer to as social toxic personalities, along with sensational interests and amoralism. The Varieties of Sadistic Tendencies (VAST) measure used by van Geel et al. (2017) to study bullies shows independence from Dark Triad traits in statistical analyses and predicts undergraduates’ willingness to crunch bugs in a machine or apply white noise blasts to innocent victims. The instrument also identifies a person’s willingness to engage in vicarious sadism, which, of course, is relevant to the bystanders of cyberbulling and those who share the content produced by their friends. Because of the relative neglect of the topic of nonsexual sadism in the scientific literature, even less is known about interventions that might prove useful. As noted earlier in this chapter in the section on treating ASPD, this type of problem can be highly resistant to treatment. We might hypothesize that the type of sadism seen in cyberbullies also would be resistant to change. Because personality disorders are not diagnosed until adulthood, it is possible that empathy interventions might be somewhat effective with younger cyberbullies. If empathy cannot be established, it is possible that the behavior can be managed using simple learning principles, as described in Chapter 8. Researchers have looked at the consequences in place for cyberbullying and the cyberbullies’ perception of the likelihood of experiencing those consequences (see Figure 15.20). Many states have passed or updated laws in an effort to combat cyberbullying, and police are becoming more likely to be involved. Surveys of middle school students, however, report that they are more deterred by the threat of punishment from their parents and school than the threat of punishment by police (Patchin & Hinduja, 2016). A total of 75% of a group of middle school students believed that the cyberbullies described in six scenarios would face some type of consequence for their behavior (Pettalia, Levin, & Dickinson, 2013). At the same time, students rated the harm done by cyberbullying as much higher than the likelihood of consequences for the perpetrator. Encouraging students to report incidents of cyberbullying, which they may not because they think that nothing will happen and because they fear retaliation, and publicizing the administration of consequences might help reduce the prevalence of cyberbullying. A more reliable solution, however, would be to undermine whatever gains in social status cyberbullies enjoy that continue to positively reinforce their behavior. Middle school children believe that most traditional bullying and cyberbullying will result in some type of consequence. They are more likely to believe that the school will act in response to traditional bullying than cyberbullying, but that parents are about equally likely to act in either case. The children do not seem to think that police responses are likely, but they see cyberbullying as more likely than traditional bullying to produce a police response. DisorderPsychological TreatmentBiological TreatmentASDApplied behavior analysisAntipsychotic medications for self-injuryADHDBehavior therapyStimulant medicationsSchizophreniaPsychosocial rehabilitationEvaluation of expressed emotionMedication with antipsychotic drugsBipolar disorderSupportive CBTMedication with lithium carbonateECT for depressionMDDCBTMedication with antidepressant drugsECTExerciseAnxietyCBTMedication with antidepressant or antianxiety drugsPersonality disordersLearning models for ASPDCBT for NPDDBT for borderline personality disorderNo effective biological treatments, although medications are sometimes prescribedBe sure that you can define these terms and use them correctly.`;\n","export const text = `The adrenal glands are the source of several important hormones that are released into the bloodstream at times of stress. We have all had days like the woman in this photo. Whether it comes in relatively minor forms (losing your keys, getting a traffic ticket, or doing poorly on an exam) or in large forms (death of a loved one or serious illness), stress is a fact of life. We don’t know what kind of stress is producing this woman’s reactions, but we know a great deal about what is happening to her physically and psychologically in response to her stress. Zooming in, we look at one of her adrenal glands, shown in the larger image. Your adrenal glands are located on top of your kidneys in your lower back, and they are the source of hormones that are dumped into your bloodstream at times of stress. How does feeling stressed get translated into activity by a gland? Here is how the system works. First, the woman must identify something as a threat to her. Making that cognitive appraisal, or judgment, of her situation begins a cascade of processes designed to mobilize her to either run (flight) or fight. Messages from her amygdala to her hypothalamus result in the activation of hormones that tell the adrenal glands to release a hormone called cortisol. In Chapter 6, we noted that cortisol levels are naturally high in the morning. In other words, circulating cortisol wakes you up. In response to the cortisol released by the adrenal glands, the body is mobilized for action. We also have had enough stress in our lives to understand that too much is not a good thing. Again, if we zoom in, we can understand that our bodies are usually protected from too much cortisol by a feedback loop that reduces cortisol release once circulating levels reach a certain point. It is likely that this feedback loop malfunctions in stress-related disorders, such as depression (see Chapter 14). While still at the microscopic level, we might also see that one person’s genetic makeup might predispose that individual to have a more or less dramatic response to perceived stressors. By now, you can probably anticipate that we have to zoom back out again to fully understand this situation. These genetic differences among individuals interact with developmental and social factors, such as the possibility that this woman was maltreated as a child or now enjoys a strong social network of family and friends. Professional athletes like Lydia Ko of New Zealand have learned to cope with stress to perform their best. We can’t make your stress go away, and it is probably not in your best interests for us to do so. Some stress, such as the stress associated with exercise or even surviving a traumatic experience, actually can have quite beneficial effects on your overall health. Instead, we can acquaint you with research findings on coping and resilience that might help you deal with life’s inevitable challenges. College students rarely need much introduction to the concept of stress. They are no strangers to the stress that accompanies worrying about assignments and grades, money matters, world affairs, and relationships. StressStressAn unpleasant emotional state that results from the perception of danger.            Stress        An unpleasant emotional state that results from the perception of danger.             is an unpleasant emotional state that results from the perception of danger. The source of stress is called a stressorstressorA stimulus that serves as a source of stress.            stressor        A stimulus that serves as a source of stress.            . The key to our definition is the word perception. No one set of stressors reliably produces stress in everybody. People’s stressors are highly individual and idiosyncratic. The object of one person’s phobia may be another person’s beloved (although scaly and slithery) pet. Regardless of the stressor responsible for feelings of stress, once people perceive that they are in some kind of danger, a common and predictable set of responses to the stressor is set in motion. Psychologists have identified positive outcomes of our responses to stress (Selye, 1975). Stress, by its nature, is a powerful adaptive response that mobilizes the body’s resources to enhance survival in dangerous situations (Meaney, 2010). Although sometimes unpleasant, stress can motivate us to perform well and makes us healthier in the long run (Crum, Salovey, & Achor, 2013). The exercise we do at the gym is undeniably stressful, but it produces significant benefits down the road. Many of us do some of our best work under time pressure and are not too much the worse for the wear as a result. Stress might have additional positive impacts on groups of people, not just individuals. People experiencing stress are actually more prosocial, at least with people they know well (Buchanan & Preston, 2014;  Margittai et al., 2015). Stress seems to increase our ability to empathize with others in pain (Tomova et al., 2016). While personally unpleasant, these advantages of stress for our social networks could produce enormous benefits. However, in other cases, stress can be counterproductive and interfere with our performance and well-being. In extreme cases, as observed in Chapter 14, a diathesis–stress model predicts that stress can contribute to the development of major depressive disorder (MDD), schizophrenia, posttraumatic stress disorder (PTSD), and other serious conditions. These negative outcomes are more likely when we are unable to cope with or adapt to a stressor. We will be focusing most of our discussion on these negative situations. Walter Cannon, whom we met in Chapter 7, demonstrated the ability of a number of stressors to activate the sympathetic division of the autonomic nervous system, described in Chapters 4 and 7.  Cannon (1929) reported that extreme cold, lack of oxygen, and emotional experiences all had the capacity to initiate a fight-or-flight response. During such a response, heart rate, blood pressure, and respiration all increase, while nonessential functions, like digesting food, are inhibited. Stored energy is released, and blood is shunted from the surface of the body to the muscles needed for exertion. These physical responses reflect a process that has been finely tuned through evolution to maximize our survival in emergencies. Use the scale in Table 16.1 to respond to the items listed. Select the number that best describes you in the space provided. To obtain your overall mindset score, add up your scores for items 1, 3, 5, and 7 and subtract this amount from 16,add your scores for items 2, 4, 6, and 8,add this total to the final number that you produced in step 1, and finallydivide the total by 8 to produce your mean Stress Mindset Measure (SMM) score. The higher the score, the more likely you are to believe that the effects of stress are enhancing. Among the college students participating in the study by  Crum et al. (2013), the mean SMM was 1.6, or on the Stress is debilitating side of the scale. How does that mean compare with your score and your classmates’ scores? What factors in your life do you think influence your score? The effects of stress are negative and should be avoided.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeExperiencing stress facilitates my learning and growth.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeExperiencing stress depletes my health and vitality.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeExperiencing stress enhances my performance and productivity.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeExperiencing stress inhibits my learning and growth.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeExperiencing stress improves my health and vitality.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeExperiencing stress debilitates my performance and productivity.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeThe effects of stress are positive and should be utilized.0Strongly Disagree1Disagree2Neither Agree nor Disagree3Agree4Strongly AgreeHans Selye identified a general adaptation syndrome (GAS), which is our characteristic set of responses to stressors. Hans Selye, in a research career lasting more than 40 years, extended Cannon’s findings by studying the effects of stronger, longer-lasting stressors. Selye worked primarily with rats, exposing them to a variety of stressors and measuring the amount of time that they could subsequently swim before giving up. (They were then rescued.) Cold water, restraint, electric shock, surgery, and having their whiskers cut off were some of Selye’s stressors that greatly reduced the amount of time the rats would swim before giving up. Regardless of the nature of the stressor,  Selye (1946) found that the rats responded with a consistent pattern of behavior, which he labeled the general adaptation syndrome (GAS)general adaptation syndrome (GAS)Hans Selye’s three-stage model for an organism’s response to stressors            general adaptation syndrome (GAS)        Hans Selye’s three-stage model for an organism’s response to stressors            . The GAS occurs in three stages (see Figure 16.1). An alarm reaction is initiated when a stressor is first perceived and identified. Selye’s alarm reactionalarm reactionThe first stage of the general adaptation syndrome (GAS), characterized by sympathetic arousal and mental clarity            alarm reaction        The first stage of the general adaptation syndrome (GAS), characterized by sympathetic arousal and mental clarity             is essentially the same process as Cannon’s fight-or-flight response. All possible resources are deployed to survive the danger, and all nonessential systems are inhibited. It is likely that you have had at least one close call while driving a car, and you can probably remember how that felt. Your heart pounds, you breathe rapidly, you feel unusually mentally alert and focused, and your hands may be sweaty. This is precisely the way that your autonomic nervous system is supposed to react in an emergency. Hans Selye’s GAS has three stages: alarm, resistance, and exhaustion. During the alarm stage, all resources are mobilized for fight or flight. If stress is prolonged, we enter the resistance stage, where we adapt and cope as well as possible. Finally, resources are depleted when we reach the exhaustion stage. During extended periods of stress, judgment can suffer. You might have had the experience of looking back at the decisions that you made during a stressful time, scratching your head, and asking, What was I thinking? Some sources of danger do not go away as quickly as your close call on the freeway. Many students think, Oh, if only I survive this term—everything else will be easy, only to find themselves facing the same problems next term, along with some new ones. One of the challenges of modern living that our ancestors did not face is the need to worry about many future events. For the hunter–gatherer, the challenge was usually surviving that day. In contrast, today’s college student may be concerned not only with fairly immediate problems, such as paying the rent and passing a midterm, but also more distant problems of future job markets, world events, climate change, and the future of Social Security. If these uncertainties are perceived as threatening or dangerous, they will act as stressors with the ability to initiate the GAS. When stressors are prolonged, Selye suggests that we enter a stage of resistanceresistance(a) A psychoanalytic technique in which the patient’s reluctance to accept the interpretations of the therapist indicate that the interpretations are correct. (b) The second stage of the general adaptation syndrome (GAS), characterized by coping with ongoing stress            resistance        (a) A psychoanalytic technique in which the patient’s reluctance to accept the interpretations of the therapist indicate that the interpretations are correct. (b) The second stage of the general adaptation syndrome (GAS), characterized by coping with ongoing stress            . During this stage, we continue to experience ongoing stress, which requires us to adapt and cope as well as possible. Although resistance is not as dramatic as the briefer and more intense alarm reaction, it can still take its toll. Under normal circumstances, we alternate between periods of calm and periods of relative arousal. You might calmly enjoy your lunch on the lawn between classes but feel aroused while taking a quiz in a later class. During the calm periods, we have an opportunity to store nutrients and rest and repair the body. During periods of arousal, such as during the quiz, we expend energy instead of storing it. In the resistance phase of the GAS, we attempt to take care of both arousal and resting functions simultaneously. Neither function operates as smoothly under these circumstances as when it is operating alone. You might get an upset stomach when digesting your lunch is combined with the excitement of giving an oral presentation to your classmates. In contrast to the mental clarity that typically accompanies the brief alarm reaction, judgment during the longer periods of resistance may not be as good. If stressors are severe and last long enough, a person might reach Selye’s exhaustionexhaustionThe third and last stage of the general adaptation syndrome (GAS), characterized by depletion of physical and psychological resources            exhaustion        The third and last stage of the general adaptation syndrome (GAS), characterized by depletion of physical and psychological resources             stage, where strength and energy drop to very low levels. As mentioned in Chapter 14, stress is a risk factor for MDD, and the exhaustion stage has much in common with the criteria for this disorder. Exhaustion can even lead to death. Forced marches during war produce higher rates of death than the rate that might be expected to occur because of injuries and lack of food. Similar observations of stress, exhaustion, and death have been made among baboons in Kenya (Sapolsky, 2001). Under normal circumstances, lower-status baboons avoid higher-status baboons whenever possible. Unfortunately, because of fears of losing their crops, the Kenyan villagers caged the local baboons one year. While caged, the lower-status baboons had no way to escape the higher-status animals, and many died as a result. They did not die, as you might have guessed, from battle wounds resulting from fights with the higher-status animals. Instead, the baboons died from stress-related medical conditions, such as cardiovascular disease. Stressors exist in the eye of the beholder. Stressing over an exam might seem reasonable to many students, but this behavior might look silly to people stressed by war, poverty, and disease, who would probably be thrilled to have the opportunity to attend college. Cognitive appraisal models help us to predict when a particular stimulus or event is likely to be a stressor for an individual person (Lazarus, 1966). According to this approach, we make appraisals, or rapid initial assessments, of potential stressors to determine whether they are irrelevant or harmless, positive or negative. Because people appraise situations from the vantage point of their own strengths and experiences, we would expect that a single potential stressor might produce different amounts and intensity of stress for different people. Despite this variability in response to stressors, we can identify several types of events that are likely to produce significant stress in most people (see Figure 16.2). Natural and human-made disasters, such as the terrorist attacks of 9/11, the Haiti earthquake of 2010, the 2011 Japanese earthquake and tsunami, and the 2016 shooting at the Pulse nightclub in Orlando, Florida, typically produce stress in large numbers of people. PTSD in New York following 9/11 was estimated to affect 130,000 people in that city alone. In the three to five days following the attacks, 90% of Americans reported feeling unusually stressed (Schuster, 2002). Approximately 30,000 people with and without PTSD were asked to identify the worst trauma they had ever experienced. Among those who did not have PTSD, the unexpected death, illness, or injury of someone close and indirect exposure (television and other news reports) to the 9/11 terrorist attacks were the most commonly named worst traumas. Among those with PTSD, unexpected death, illness, or injury were also commonly noted traumatic experiences, but people with PTSD were more likely than healthy controls to identify sexual assault, intimate partner violence, and combat as their worst traumatic experiences. Older zoos featured cages that often contributed to stress and illness among the animals. Modern zoos go to great lengths to provide more natural, less stressful habitats for their animals, in recognition of the stress that often accompanies captivity. Stress because of disasters can produce long-range and cross-generational effects. Researchers identified women who witnessed the 9/11 attacks in New York while pregnant and who subsequently developed PTSD (Yehuda et al., 2005). One year later, the women and their infants showed indications of long-term, chronic stress. As preschoolers, the children born to the women who were diagnosed with both PTSD and MDD following 9/11 showed evidence of higher reactivity to stimuli and more aggressive behavior than did children of terrorism-exposed mothers who did not develop these disorders (Chemtob et al., 2010). The ability of cortisol and other stress hormones to cross the placenta probably accounts for the effects of stress beginning in the prenatal environment (Weinstock, 2005). As mentioned in Chapter 14, children are especially susceptible to PTSD. Three weeks after a 2004 tsunami, prevalence of PTSD among children in areas of Sri Lanka ranged between 14% and 39% and could be predicted by variables such as family loss and severity of exposure (Neuner, Schauer, Catani, Ruf, & Elbert, 2006). To put these very high numbers of childhood cases into perspective, we would expect about 10% of the general population to experience PTSD after most large disasters (Kessler, Sonnega, Bromet, Hughes, & Nelson, 1996). Automobile accidents are a major source of PTSD in childhood, with about 25% of children developing the disorder after being injured in an accident (de Vries et al., 1999). Although disasters are dramatic, they are not the only sources of stress that we face. Changes, including some changes for the better, can also trigger stress. Holmes and Rahe (1967) compiled a list of life events (good and bad) that they believed might be correlated with stress and then surveyed participants about the amount of adjustment that each event required. Subsequently, researchers used the Holmes and Rahe scales to predict vulnerability to physical illness and psychological disorder because of different stressors (Derogatis & Coons, 1993;  Scully, Tosi, & Banning, 2000). According to the scales, Christmas is more stressful than minor violations of the law. Getting married is more stressful than being fired from your job. Research results from studies using the Holmes and Rahe scales suggest that we should consider spreading out controllable changes over time. In other words, it may not be the best strategy to graduate from college, get married, move to a new city, and start a new job, all at the same time. This man is obviously stressed. Is somebody being injured? Is he in danger? No—he is witnessing a tense moment during a baseball game. Cognitive appraisal theories of stress suggest that different people are likely to respond to stimuli with varying levels of stress based on their individual appraisals of a situation. Not all psychologists are convinced that so-called good life events are as stressful as bad ones, nor are they convinced that change per se is a reliable predictor of stress. Critics of the Holmes and Rahe approach argue that most items on their list are quite negative and that these negative items are responsible for more stress (McLean & Link, 1994). It is likely that most of the stress associated with good changes occurs when the anticipated event does not live up to the person’s expectations. Vacations and holidays may not be inherently stressful, but trying to fulfill unrealistic fantasies about what vacations and holidays should be like is probably stressful indeed. Relatively insignificant sources of stress, often called hassles, also can contribute to a person’s overall level of stress. Waiting in long lines, losing your keys, getting a parking ticket, and oversleeping on the day of an important exam are not life-threatening events. However, if enough of these hassles occur within a short period, people begin to react as if something larger had occurred. The number of hassles that people report experiencing predicts psychological symptoms of stress, even when the impact of major life events is factored out (Kanner, Coyne, Schaefer, & Lazarus, 1981). You might hear advice to avoid sweating the little stuff because enough little stuff happening at the same time can still affect your well-being. According to Holmes and Rahe (1967), positive life events, such as weddings and holidays, can be equally or even more stressful than some negative events, such as flunking an important assignment. Some individuals, such as those serving in the military, have to deal with major stressors as part of their job. Concerns regarding stress and consequent mental health problems in the military have grown since the wars in Iraq and Afghanistan. Soldiers serving in these conflicts have experienced especially high rates of mental health disorders and suicide (Ramsawh et al., 2014). In a representative sample of 5,927 U. S. soldiers on active duty in 2008,  Ramsawh et al. (2014) found that 34% had probable MDD, 13% had probable PTSD, and 6% reported past-year suicidal ideation These numbers are consistent with other reports of stress and psychological disorders among soldiers (Hoge et al., 2004;  Tanielian & Jaycox, 2008). As in studies of civilians, research on military personnel suggests that feeling socially isolated (loneliness) may be an early warning sign of, if not a contributing factor to, these mental health disturbances (Solomon, Mikulincer, & Hobfoll, 1986). Combat experiences do not lead directly to suicidal ideation and behavior, but they might do so indirectly by producing symptoms of PTSD (see Chapter 14) and negative states including loneliness, sadness, anger, and frustration (Griffith, 2012,  2015). Soldiers who committed suicide had previously reported being lonelier and more depressed and tended to think in more catastrophic terms than soldiers who did not commit suicide (Cacioppo et al., 2015;  Lester, Harms, Bulling, Herian, & Spain, 2011). Regardless of the identity of a stressor, once you appraise a stimulus as a danger, you initiate Selye’s GAS. The first stage, the alarm reaction, is accompanied by a coordinated reaction including physical, cognitive, and behavioral responses to perceived danger. Imagine for a moment that one of your ancient ancestors was out hunting and suddenly found himself face to face with a hungry lion. Physically, the autonomic nervous system prepared him for fight or flight. The brainstem, described in Chapter 4, initiated the release of the neurotransmitter norepinephrine, which increased vigilance and fear. Cognitively, your ancestor accessed his memory for information about lions, which he hoped would include ideas about how similar situations were handled in the past. Behaviorally, your ancestor carried out his plan for escape. We assume that given your presence today, your ancestor was successful. Feeling socially isolated makes us feel unsafe, which triggers stress responses. Loneliness makes people behave in ways that make them difficult to be around, such as displaying greater pessimism, anxiety, and hostility, beginning a loop of self-fulfilling prophesies and more loneliness. We can trace the neural pathways that coordinate these responses to potentially hazardous stimuli. Sensory pathways provide information to higher cognitive centers in the cerebral cortex, letting our hunter know that the object in front of him is a lion. Memories of lion behavior, including eating habits, contribute to accurately identifying the lion as a danger, and hence a stressor. Simultaneously, sensory input travels from the thalamus to the amygdala, which plays an important role in the identification of dangerous stimuli, as discussed in Chapters 4 and 7. The amygdala participates in a fear circuit that provides a rapid assessment of a stimulus or situation as potentially dangerous (LeDoux, 2000,  2014). If the amygdala is lesioned, animals no longer respond with conditioned fear to previously learned classically conditioned associations between a stimulus (perhaps a tone or light) and an electric shock, an example of classical conditioning discussed in Chapter 8 (see Figure 16.3). Animals with lesions in the amygdala are also unable to learn to respond appropriately to unfamiliar dangerous stimuli (Wilensky, Schafe, Kristensen, & LeDoux, 2006). The amygdala participates in a fear circuit that provides a rapid response to a potentially dangerous stimulus. After rats learn a classically conditioned association of a tone (a conditioned stimulus, or CS) that signals the arrival of an electric shock (an unconditioned stimulus, or UCS), they freeze whenever they hear the tone. If the amygdala is lesioned, the rat no longer freezes when it hears the tone, and it is unable to learn about new signals for shocks. Because sensory information can reach the amygdala along routes that are separate from the pathways for information going to the cortex, we might find ourselves frightened by stimuli that we don’t immediately understand or consciously view as dangerous (Knight, Nguyen, & Bandettini, 2003). As mentioned in Chapter 4, a man who was blind because of damage to his visual cortex still showed normal response in his amygdala when shown faces expressing fear (Pegna, Khateb, Lazeyras, & Seghier, 2005). You might have found yourself feeling anxious while returning to your car after an evening class without knowing why you are reacting this way, but it is probably a good idea to listen to your amygdala until further information becomes available. Once the amygdala has identified a stimulus as potentially dangerous, it communicates with the hypothalamus. As mentioned in Chapter 4, the hypothalamus most directly commands the autonomic nervous system, particularly the sympathetic division. This part of the autonomic nervous system is responsible for our fight-or-flight response to danger. Perceiving a potential source of danger mobilizes the body’s resources using two systems—the sympathetic adrenal–medullary (SAM) systemsympathetic adrenal–medullary (SAM) systemA circuit that responds to perceived stressors by initiating the release of epinephrine and norepinephrine into the bloodstream.            sympathetic adrenal–medullary (SAM) system        A circuit that responds to perceived stressors by initiating the release of epinephrine and norepinephrine into the bloodstream.             and the hypothalamic–pituitary–adrenal (HPA) axishypothalamic–pituitary–adrenal (HPA) axisA circuit that responds to perceived stressors by initiating the release of cortisol into the bloodstream.            hypothalamic–pituitary–adrenal (HPA) axis        A circuit that responds to perceived stressors by initiating the release of cortisol into the bloodstream.            . The SAM system initiates the release of adrenaline (also known as epinephrine) and norepinephrine into the bloodstream from the adrenal glands, located above the kidneys in your lower back. These chemical messengers circulate to many organs, including the brain, producing many of the immediate, short-lived, fight-or-flight responses to stress, such as a pounding heart and rapid breathing (see Figure 16.4). In response to the appraisal of a stimulus as a stressor, two systems mobilize the body’s resources. The sympathetic adrenal–medullary (SAM) system responds quickly by initiating the release of epinephrine (adrenaline) and norepinephrine into the bloodstream by the adrenal glands. These chemical messengers initiate many immediate, short-lived responses, such as a pounding heart and rapid breathing. Simultaneously, activation of the hypothalamic–pituitary–adrenal (HPA) axis results in the release of cortisol into the bloodstream by the adrenal glands. Cortisol acts to boost the energy available for dealing with the stress. Cortisol receptors in the hippocampus serve as a feedback loop to keep the system from overreacting. The effects of HPA axis activity are slower to develop and last longer than those of the SAM response. HPA axis activity is associated with more of the effects of chronic stress, such as suppression of the immune system. Simultaneously, activation of the HPA axis sets an entirely different system in motion. Here’s how the circuit works. The hypothalamus (H) communicates with the pituitary gland (P), located just above the roof of your mouth, which in turn tells the adrenal glands (A) to release a hormone known as cortisolcortisolA hormone released into the bloodstream from the adrenal glands            cortisol        A hormone released into the bloodstream from the adrenal glands             into the bloodstream. Circulating cortisol boosts the energy available for dealing with a stressor. The HPA axis response to stress can continue longer than the SAM response, which explains many of the outcomes of chronic stress. One of the possible outcomes of chronic stress is prolonged high levels of circulating cortisol. Long-term exposure to cortisol can produce a number of harmful effects, including the death of neurons. When rats received daily injections of the rat equivalent of cortisol, neural death began to occur in just a few weeks (Stein-Behrens, Mattson, Chang, Yeh, & Sapolsky, 1994). Identical amounts of neural death occurred if the rats were stressed daily instead of receiving the injections, suggesting that the action of cortisol is responsible for most of the neural damage observed to result from stress. Cushing’s disease, a medical condition that causes unusually high cortisol levels, results in reduced hippocampus volume, memory problems, abnormal sleep patterns, and depression (Langenecker et al., 2012). Not only can high levels of cortisol damage neurons, but these same levels appear to inhibit neurogenesis, or the birth of neurons, which might help to offset the damage (Cowen, 2010). Normally, activity of the HPA axis is regulated by a feedback loop involving the hippocampus (Stokes, 1995). The hippocampus works like a rev limiter in an automobile engine that prevents the driver from going over a certain speed. The hippocampus contains large numbers of receptors for cortisol and other stress hormones. When the hippocampus detects high levels of these hormones, it signals the hypothalamus, which in turn tells the adrenal glands to reduce the release of cortisol, and arousal dissipates. The role of the hippocampus as the rev limiter of the HPA axis might be the bridge between extreme stress and MDD. As observed in Chapter 14, many cases of MDD are preceded by unusually stressful events. Consistently elevated levels of cortisol because of stress can overwhelm the hippocampus’s feedback loop, leading instead to the continuous release of cortisol and constant arousal. Without the regulation of cortisol usually provided by the hippocampus acting as a rev limiter, a person can begin to experience depression (Stokes, 1995). People who are treated with cortisol and similar stress hormones for medical conditions such as rheumatoid arthritis are often troubled by deep depression, reinforcing the role of excess cortisol in depressed mood. Not only can the hippocampus fail to regulate cortisol release under conditions of extreme stress, but continued stress can damage the hippocampus further. The stressed baboons mentioned earlier in this chapter experienced neural death, particularly in the hippocampus, in addition to their other medical problems (Sapolsky, 2001). People with PTSD, discussed in Chapter 14, also show evidence of having a smaller than average hippocampus (Ahmed-Leitao, Spies, van den Heuvel, & Seedat, 2016;  Bossini et al., 2008;  Bremner, 1995). However, it remains unclear whether reduced hippocampal volume precedes the onset of PTSD, results from PTSD, or both (Hayes et al., 2017). As this family faces the stress of the father’s overseas deployment, the mother responds with a tend-and-befriend approach by comforting her daughter. Although psychologists typically discuss fight or flight as the common response to stressors, an alternative response has been suggested that might be more typical of women’s responses to stressors. Noting that from an evolutionary standpoint, a mother with small children is unlikely to find either fight or flight easy to do,  Taylor (2006) suggested that women are more likely to tend and befriendtend and befriendAn alternative to fight or flight as a response to stressors, characterized by soothing and building social connections, which is possibly more characteristic of females.            tend and befriend        An alternative to fight or flight as a response to stressors, characterized by soothing and building social connections, which is possibly more characteristic of females.             in response to stressors. Soothing frightened children, hiding, and forming social alliances for further protection might be more effective strategies. Research support for tend and befriend includes a study showing that after being stressed, women, but not men, demonstrated an increase in caretaking motivation in response to videos showing crying infants (Probst et al., 2017). Instead of the traditional hormones associated with fight and flight, a tend-and-befriend response is more closely associated with the release of oxytocin, a hormone related to social bonding discussed in Chapters 4 and 7. Beginning with initial observations that death rates were negatively correlated with increases in occupational grades among British civil servants (Marmot, Rose, Shipley, & Hamilton, 1978;  Marmot, Shipley, & Rose, 1984), scientists have become aware that health disparities exist worldwide as a function of wealth. People with lower socioeconomic status (SES) experience worse health. Absolute wealth is not the only factor here. People’s subjective estimation of their own status in the community also predicts susceptibility to disease (Cohen, Alper, Adler, Treanor, & Turner, 2008). What processes can explain SES effects on health disparities? The poor experience more noise, more toxins, more carcinogens, more violence, fewer resources, less health care, higher levels of drugs and alcohol abuse, less exercise, more anger, less control, and less trust (Adler, 2013). This long list has something in common—all these factors are associated with high stress. This stress accelerates the aging process, including the shortening of telomeres, the caps found at the end of chromosomes (Needham et al., 2013). Premature aging, of course, is also going to lead to earlier death. In Chapter 3, we discussed the significant role played by the environment in determining whether and when a particular gene is expressed. We defined epigenetics as the influence on traits by factors that determine how genes perform; the field of epigenetics explores gene–environment interactions. For example, we observed how both rats and human children who were well nurtured by their mothers showed more resilience to stress later in life. The nurture received from the mother had influenced how the genes responsible for producing and reacting to stress hormones behaved during later stressful experiences (Champagne, Francis, Mar, & Meaney, 2003;  Neigh, Gillespie, & Nemeroff, 2009). Severe stress early in life produces lasting challenges, including increased HPA axis responses to stress, hyperactivity of the norepinephrine system, reduced volume in the hippocampus, and heightened responses by the amygdala to threat stimuli, such as negative facial expressions (Osório et al., 2016). Genes and life stress also interact to produce MDD. As observed in Chapter 14, MDD is often accompanied by dysfunction in systems using the neurotransmitter serotonin. Life stress affects people differently depending on whether they possess a short form or a long form of a gene related to serotonin function. Having the long or short version of the gene interacts with life stresses to produce different levels of activity in the amygdala and hippocampus, differences in the pathways connecting the amygdala and hippocampus with other regions of the brain, differences in gray matter, and different levels of rumination, the repetitive rethinking of problems that is particularly characteristic of depressed people (Canli et al., 2006; Osório et al., 2016). These gene–stress interactions help explain why some people are more vulnerable to life stress, while others seem relatively well protected against it. The first 1,000 days after conception (until about age 2 years) strongly influences individual vulnerability to chronic disease and life expectancy. While low birth weight indicates stress during pregnancy, the 280 days or so between conception and birth are only a fraction of the first 1,000 days. Having an asymmetrical lower face, however, appears to be a reliable biomarker of stress during this early stage of childhood. Health disparities, or the differences between the health of wealthy and that of poor individuals, can result from the greater exposure of poor people to stressors. Our stress response was forged by the challenges that our hunter–gatherer ancestors were most likely to face. The result is a system that has beneficial effects in the short term but can lead to negative effects when stress becomes chronic (Cacioppo & Berntson, 2011). After all, it would not take your ancestors hours, days, months, or years to figure out what to do to escape a hungry lion. For better or for worse, the outcome would be decided quickly. In contrast, many of the stressors that we face in modern living are not brief, and as a result, we are fairly constantly experiencing Selye’s resistance phase, which combines stress and coping. You might be worried about finding a job after you graduate from college, which might be years away. Any long-term, chronic stress has the potential to affect health in negative ways. Not only does responding to stress require large amounts of energy, but while in the resistance stage, you do a less efficient job of storing nutrients and giving your body the rest and repair that it needs. Improved understanding of the relationships between stress and health can guide our quest for physical and psychological well-being. Short-term bursts of stress can have a beneficial effect on many biological systems, including the immune systemimmune systemThe body system that defends against infection and cancer            immune system        The body system that defends against infection and cancer            , your body’s frontline defense against infection and cancer. Participating in a stressful memory task was correlated with an improvement in the response of the immune system, as measured by markers of immune system activity in the saliva of volunteers (Bosch, De Geus, & Kelder, 2001). However, the immune system does not perform as well in the face of long-term, chronic sources of stress (see Figure 16.5). When responding to an emergency, our stress response system prioritizes body functions. Those that are not necessary for handling the immediate emergency are taken offline. Unfortunately for those experiencing chronic, ongoing stress, one of those expendable systems is the immune system (Thornton, Andersen, Crespin, & Carson, 2007). As a result, stress can lead to greater frequency and severity of illnesses, as you may have noticed immediately following final exam periods (Wadee, Kuschke, Kometz, & Berk, 2001). Students with high anxiety, but not those with low anxiety, experienced a drop in immune function in anticipation of an exam coming up in two weeks. Immune system function had returned to normal two weeks following the exam. Numerous studies indicate that people experiencing chronic stress are more vulnerable to infectious diseases, like colds and the flu (Cohen, Tyrrell, & Smith, 1991). White blood cells, or lymphocyteslymphocytesWhite blood cells that are products of the immune system.            lymphocytes        White blood cells that are products of the immune system.            , protect us from invading organisms. Unfortunately, stress hormones directly suppress the activity of lymphocytes (Calcagni & Elenkov, 2006). Flare-ups of both oral and genital herpes are most likely to occur when a person is experiencing unusual stress (Cohen & Herbert, 1996). The progression of HIV infection to AIDS is influenced by the person’s level of stress (Harper et al., 2006). Stressed students show greater vulnerability to the virus responsible for mononucleosis, which normally is kept in check by a robust immune system (Cacioppo & Berntson, 2011). Unfortunately, knowledge of these relationships can place an even greater burden on sick people, who may be led to believe that their illness wouldn’t be so bad if they were somehow better at managing stress. Many of the dangers faced by these San hunters in South Africa are likely to arise and resolve quickly. Our responses to stress are better suited for short-term stressors than for many of the ongoing, chronic sources of stress that we face in industrialized settings. Stress related to our social relationships seems to be especially harmful to our ability to stay healthy. A meta-analysis of almost 300 studies and about 20,000 participants demonstrated that chronic stressors affecting people’s social roles (e.g., death of a loved one or divorce) produce the greatest suppression of the immune system (Segerstrom & Miller, 2004). In particular, stressors that are outside a person’s sense of control and lead to little hope for improvement have the most damaging effects. Those who are older and already ill are especially susceptible to adverse consequences of stress. Meyer Friedman and Ray  Rosenman (1974) stimulated a large quantity of research by suggesting that highly competitive workaholics, whom they called Type A personalitiesType A personalitiesA competitive, workaholic, and in some cases hostile personality type.            Type A personalities        A competitive, workaholic, and in some cases hostile personality type.            , were more susceptible to heart disease than the mellower, laid-back people they called Type B personalitiesType B personalitiesA mellow, laid-back personality type.            Type B personalities        A mellow, laid-back personality type.            . Further study, however, showed that simply being competitive had little to do with increased risk of heart disease. Instead, the Type As who did develop heart problems exhibited a pattern of interpersonal hostility. The Type As most at risk for heart disease were frequently suspicious, angry, and resentful of other people. It is possible that their own hostile behavior provoked more hostile situations and reactions from others, confirming their worldview and maintaining their negative behavior. In addition, hostile people are not likely to build up networks of friends and loved ones, who are often our best protectors from the untoward effects of stress (Jackson, Kubzansky, Cohen, Jacobs, & Wright, 2007). In middle-aged adults, frequent conflict with members of a person’s social network raised the risk of death 200% to 300% (Lund, Christensen, Nilsson, Kriegbaum, & Rod, 2014). Stress puts the cardiovascular system at risk by affecting the ability of blood vessels to expand when necessary. People whose arteries are already stiff or clogged because of age, poor fitness, or disease often suffer from high blood pressure and might be especially susceptible to heart attacks following stress. In a heart attack, interruptions of blood flow to the heart trigger death of a part of the cardiac muscle. Flexible blood vessels maintain blood flow and blood pressure and lessen the chances of a heart attack. To test the effects of stress on blood flow, healthy participants were given a standardized mental stress task in which colored buttons were to be pushed as quickly as possible in response to flashing lights of the same color (Spieker et al., 2002). For the following 45 minutes, the ability of the participants’ blood vessels to expand was reduced by a factor of 50%. A participant’s change in blood pressure in response to a stressful situation was predictive of a later diagnosis with hypertension, or high blood pressure, which can lead to heart attacks or stroke (Spieker et al., 2002). Research shows that in middle-aged adults, frequent conflict with members of a person’s social network increased the risk of death by 200% to 300%. In this textbook, we have explored a number of examples of interactions between people’s genetic makeup and their life experiences that predict psychological and physical health. We have also seen how some early life experiences, such as child maltreatment, seem to have especially pronounced effects on people’s outcomes throughout the rest of their life. What is there about experiences early in life that is so important? How does the experience of stress early in life affect health later? To examine these questions, researchers evaluated the effects of early stress on genetic pathways that shape the immune system (Cole et al., 2012). The Question: How does adverse early life experience affect later health? In Chapter 11, we reviewed work by Harry Harlow in which infant rhesus monkeys were raised by cloth and wire surrogate mothers to investigate questions about attachment. Steven Cole and his colleagues (2012) also exposed infant rhesus monkeys to surrogate mothers, other peers, or both instead of their real mothers. The researchers then conducted analyses of genetic changes in the infant monkeys’ immune systems. Some would object to the removal of an infant monkey from its parent because we know from other research that this can have long-term negative effects. The researchers would need to demonstrate necessity, which means that they cannot answer their research question with other methods and that the importance of their research question justifies the harm being done to the animals. When the monkeys were only 4 months old, the researchers identified changes in the expression of genes in the monkeys’ developing immune systems. Genes that were related to inflammation and activation of the immune system were more active in monkeys raised by peers or by peers plus a surrogate mother than in monkeys raised by their biological mothers. Genes involved in basic defenses against bacteria and viruses were less active in these monkeys than in the monkeys raised by their biological mothers. The researchers believed that sympathetic nervous system responses to the stress of being raised by peers or a surrogate mother and peers changed the basic way in which genes important to the immune system were expressed. Their results were similar to genetic studies of adults who had experienced adverse childhood events. These changes help to explain why people with early social challenges have a more difficult time responding to diseases caused by bacteria or viruses. Rhesus infants raised by surrogate mothers or peers experienced changes in the expression of genes related to their immune systems. These results help explain the connection between adverse childhood experience and poor adult health outcomes. As discussed in Chapters 4 and 14, mood, sleep, and appetite are closely intertwined. A change in one of these behaviors usually results in changes in the others. Long-term, chronic stress can begin a cascade of changes in mood, sleep, and appetite that compromise well-being. Disruptions of sleep because of stress are particularly hazardous to health because we need sleep to restore our bodies after the challenges of the day. Both sleep quantity and sleep quality are associated with both overall health and cognitive outcomes (Hawkley & Cacioppo, 2010). Stress frequently serves as a trigger for a depressed mood. Self-reports of daily stressors, like having an argument with a friend, were correlated with the participants’ mood (Stader & Hokanson, 1998). The ability of stress to alter levels of circulating cortisol, discussed earlier in this chapter, might form the basis for this connection between stress and depressed mood. As observed in Chapter 14, many people diagnosed with MDD show signs of abnormal cortisol function (Aihara et al., 2007). Cortisol is not released just in response to perceived stressors. As observed in Chapter 6, it also plays a role in maintaining our natural cycles of sleep and waking. Cortisol is released in large quantities early in the morning, contributing to wakefulness. As the day progresses, cortisol levels drop off, reducing wakefulness and setting the stage for sleep. If you experience a big jolt of cortisol because of a stressor late in the evening, getting to sleep is going to be difficult, even when you’re tired. Even when you do get to sleep, high levels of cortisol interfere with good sleep quality (Van Cauter, Leproult, & Plat, 2000). Both high levels of stress and MDD can produce sleep disturbances, especially the experience of waking frequently throughout the night (Koenigsberg et al., 2004). Long before vaccinations and antibiotics were available, death rates because of infectious diseases began to drop in the United States. Improved wages and nutrition, coupled with more rest time away from work, allowed people to recover from their daily stresses (Cacioppo & Berntson, 2011). Mood and sleep can both affect appetite. Among the criteria for MDD are changes in appetite. Some people who are depressed lose weight without dieting, while others gain weight. By now, it shouldn’t surprise you to learn that stress, along with depression, also can contribute to obesity. In response to stress-related hormones, fat cells behave differently, growing in both size and number (Kuo et al., 2009). Stressed mice gained more weight than mice that were not stressed, even when both were fed the same high-fat, high-sugar diet. In addition to this direct effect of stress on appetite, stress can produce indirect effects on obesity by interfering with sleep. Dieters who enjoyed a full night’s sleep lost the same amount of weight as dieters who slept less, but there was a difference in the kind of weight that was lost (Nedeltcheva, Kilkus, Imperial, Schoeller, & Penev, 2010). Dieters who slept well lost a healthy amount of fat, while sleep-deprived dieters lost only half as much fat as the sleeping dieters, and three quarters of their weight loss consisted of precious bone and muscle tissue instead of fat (see Figure 16.6). Participants who slept 8.5 hours per night lost about the same amount of weight during a treatment program as participants who slept 5.5 hours per night, but the two groups differed in the amount of body fat and lean body mass that was lost. The group that slept 8.5 hours per night lost more body fat and less lean body mass, which is characteristic of desirable weight loss, than the group that slept 5.5 hours per night. These results remind us that many of our health habits are intimately interconnected, and that changing one can have a large effect on others. We have reviewed a number of studies in this section that demonstrate the adverse effects that chronic, prolonged stress can have on our health. Popular wisdom suggests that people with the highest levels of stress should therefore experience the worst health. However, we have already seen one example, the healthy Type As, in which this was not the case. In another study, 5,000 men reported their stress levels, which were then compared 20 years later to their medical records. Surprisingly, the men who had reported the highest levels of stress had experienced the least amount of heart disease (Macleod et al., 2002). How can we explain these apparently contradictory results? One approach to reconciling the good health enjoyed by some stressed people with data demonstrating the harmful effects of stress in others is to step back and look at the whole person within her environment over the entire life span. Health can be described as not just the absence of disease, but also as the ability to respond to the challenges of being alive (Juster, McEwen, & Lupien, 2010). Personal factors, including your genes, interact with the parental care that you receive, as discussed previously in this chapter. Other protective factors, such as your social networks and sense of meaning or purpose in life, contribute to your resilience in the face of the negative and cumulative effects of a lifetime of responding to stressors (Cacioppo & Berntson, 2011). These protective factors occur at critical periods, such as the impact of parenting in early childhood, so a complete understanding of protective mechanisms should be examined within the context of life-span development. Once we fully understand these protective processes, psychologists should be in the exciting position of being able to design effective interventions for improving health and well-being (see Figure 16.7). Individuals do not experience stress in a vacuum. Personal factors, like genes and everyday experiences, influence outcomes within an environment consisting of ever-larger circles of social connectivity. The relationships between social environment and personal factors are reciprocal. Individual actions influence the social environment, which in turn influences individual factors. We don’t need to tell you that being in college can be stressful. For many students, college can mean new environments, new rules and routines, new social interactions, less sleep, not-so-great eating habits, and more sitting than exercising. Can the application of psychological research make the transition into college easier and less stressful? We can’t make many of these stressors go away, but a simple, one-time intervention at the beginning of the first year of college made a large difference in the academic performance and health of a group of students in one study (Walton & Cohen, 2011). Incoming first-year students at Stanford University were randomly assigned to a control group or a social-belonging group. The social-belonging group read what they thought were the results of a survey of older students, who said that they had worried about fitting in during their first year but had grown more confident over time. Participants were asked to write an essay comparing their own first-year experiences with the survey results and then deliver a speech based on their essay to a video camera for use with future students. At no time were the participants aware that they were actually experiencing a treatment designed to make them see the social stressors of college life as typical of all students and of short duration. Over the next three years, the participants’ grade point averages (GPAs) and health were monitored. The belongingness intervention had little impact on the health and grades of White students, but it had significant and positive effects on African American students. Compared to African American students in the control group, the students who had received the one-time reassurance that their concerns about fitting in were normal and were not permanent had higher grades, fewer visits to the doctor, and higher subjective happiness (see Figure 16.8). A one-time treatment aimed at framing the stress of fitting in to the college experience as common to most students and limited in duration significantly improved the grade point averages, happiness ratings, and health of African American students at Stanford University three years later. White students did not experience the same treatment-related differences, with both treated and nontreated White students experiencing the same health and subjective happiness as the treated African American students. This result suggests that fitting in was already framed by White students in positive ways. While further research is necessary to know how far these results can be generalized to other students, the results emphasize the importance to your overall performance, health, and well-being of how you interpret your situation and whether you feel like you belong. Stress is inevitable. That simple statement does not imply that there is nothing you can do about stress. Psychologists have identified a number of strategies that people can use to effectively reduce the negative impacts of stress on their happiness and health. The stress of middle school, such as attending school dances, is real to students at the time, but as we get older and more experienced, our capacity to manage stress improves. With age, people report feeling less stress, even though stressors tend to become more complicated and numerous at the same time. One of the most positive aspects of stress is that we adapt to it over time. Take a minute to jot down the things that you found most stressful five years ago and then make a list of the stressors that you face today. Would you trade today’s list for the one from five years ago? Most people would do so in a heartbeat. Life gets more complicated and often more stressful, but our skills for coping improve at the same time. People tend to report less stress as they get older (Shields, 2004). Earlier in this chapter, we described the role of cognitive appraisal in identifying a stimulus or event as a stressor. Appraisal plays an important role in coping with identified stressors as well. Once a stressor has been identified, a second set of appraisals occurs that guides our coping responses (Lazarus, 1966). What harm has already occurred? What threats remain? What resources do we need to overcome the challenge? Taking conscious control of our appraisals can contribute to our ability to cope. Even when a stressor takes us by surprise, we can regain a sense of control. Viktor Frankl (1959, p. 161), a Holocaust survivor, developed what he described as tragic optimism for coping with unforeseen disasters. Instead of asking why these disasters should happen to us, Frankl recommends that people exert a sense of control by asking, How can I face this disaster with courage and responsibility? An obvious starting place for keeping your levels of stress low is to ask whether stressors can be eliminated. If you feel stressed about having too much to do and too little time to do it, it might be possible to reduce your workload or practice better time management skills. If money is tight, a visit to your campus’s financial aid office might provide you with solutions that you had not considered. The worst possible approach is to withdraw and avoid stressors in the vain hope that they will go away. Your latest credit card statement is not going to get better magically if you ignore it. In fact, your emotional response to seeing the statement on your desk every day is likely to get a lot worse over time. A major variable that predicts our response to stress is the sense of control. Feeling surprised by life or out of control can lead to significant stress. Some of the worst stressors are those that seem to strike randomly, such as being diagnosed with lung cancer when you never smoked (Pietrzak, Goldstein, Southwick, & Grant, 2011). These seemingly random events undercut our sense of control, countering the belief that If I don’t smoke, I am ensuring that my lungs will stay healthy. Many stressors are out of our control, however. We can’t wish our cancer away or magically ensure that enough money appears in our bank accounts to pay this month’s bills. However, we can respond to our stressors in ways that reduce our overall stress. This approach is consistent with the elements of positive psychology, which we review later in this chapter. Even if you are struggling with a life-threatening disease, stress can be reduced by educating yourself as much as possible about your condition and participating fully in decisions about your treatment. Residents of nursing homes who have more control over their choice of daily activities live longer than those who are not given such choices (Rodin, 1986). We observed in Chapter 12 that stressors have less effect on people who experience an internal locus of control, which means that they believe that most of their outcomes result from their personal efforts, as opposed to luck, chance, opportunities, or other external forces. Patients who educate themselves about their conditions and participate fully in treatment decisions experience less stress. Because of the inevitability of stress, people cope best when they keep themselves as healthy as possible. If you follow good health habits, stress might still deplete your reserves and inhibit your immune system, but a healthy body can take more abuse than an unhealthy one. Students are more likely to get sick during final exams if they have neglected good eating and sleeping habits all term long. Regular aerobic exercise appears to be especially helpful. In one experiment, people with heart disease were observed as they engaged in exercise as part of their rehabilitation programs. The exercise reduced the overall stress levels of all participants (Milani & Lavie, 2009). Exercise seemed particularly beneficial for the participants who also scored high on psychosocial stress. None of the high-stress participants who showed the greatest physical benefits from exercise (such as increased oxygen uptake) died in the follow-up period, compared to 19% of the high-stress participants who showed relatively low physical benefits from exercise (see Figure 16.9). Students would be wise to engage in some level of activity to offset the effects of increased stress. People with cardiovascular disease were assessed for psychosocial stress levels during a rehabilitation program that included exercise. A control group did not have a formal exercise program. Based on their improvement in physical measures following the exercise program, the study participants were divided into low and high exercise change groups. In a follow-up five years later, high levels of physical improvement because of exercise did not affect the mortality of the low psychosocial stress group but significantly reduced mortality in the high-stress group. In Chapter 15, we discussed the use of mindfulness in the treatment of anxiety and MDD. Mindfulness, or the trained awareness of our current thoughts, emotions, and actions, has also been shown to be an effective method for reducing stress (Tang & Posner, 2009). In Chapter 6, we explored the effects of the altered state of consciousness produced through meditation. Although many people meditate within a religious context, a large meta-analysis indicated that meditation is an effective strategy for reducing psychological stress (Goyal et al., 2014). One of the most powerful antidotes to unhealthy effects of stress is social connectedness and support. Social support from friends and family and belief systems that allow a person to make sense of the world provide powerful buffers against stress (Montpetit, Bergeman, Deboeck, Tiberio, & Boker, 2010). As we have argued on many occasions in this textbook, people with good social relationships and support are far less vulnerable to adverse health and psychological conditions than are people who are lonely and isolated (Cacioppo, Cacioppo, Capitanio, & Cole, 2015;  Hostinar, Sullivan, & Gunnar, 2013). In times of distress, people frequently turn to others in an effort to cope. According to a study of responses to the terrorist attacks of 9/11, nearly 100% of the participants reported sharing their thoughts and feelings about the attacks with other people (Schuster, 2002). More than 90% reported either praying or engaging in spiritual contemplation appropriate to their faith. More than 60% engaged in relevant public activities. Nearly 40% focused on the needs of others by donating money or blood to relief efforts. Understanding the importance of social support for coping helps to explain why people living in poor neighborhoods often experience more stress and have higher rates of MDD. Living in a neighborhood where few people know one another or help one another with tasks like child care increases the risk of developing MDD (Russell & Cutrona, 2010). These socially disconnected neighborhoods become more prevalent as the income of the residents decreases, making people living below the poverty line especially vulnerable to stress and depression. Specialist Lawrence Shipman and Sergeant First Class Jonathan Zeke, a combat stress–relief dog, received an award for their work in reducing the stress of soldiers deployed in Iraq. Shipman notes that Zeke acts as an icebreaker, encouraging soldiers to talk to the behavioral health counselors. Although it is ideal to receive your social support from other people, having a pet can also provide benefits to your health. Stockbrokers with high blood pressure who lived alone were selected to receive a cat or dog from a local shelter (Allen, 2003). When stressed, the pet owners experienced a much lower increase in blood pressure than their petless counterparts. The effect was most obvious among participants who listed few social connections with other people. Many people find that having religious beliefs helps them cope with stress. People who report having religious beliefs appear to withstand the challenges of unemployment, low income, and widowhood better than people who report no religious beliefs (Diener & Seligman, 2004). Across a number of nations practicing different forms of religion, a higher rate of belief in a god predicts higher ratings of life satisfaction and lower rates of suicide (Helliwell, 2003). Coping with a stressor can take three forms: problem-focused, emotion-focused, and relationship-focused (O’Brien & DeLongis, 1996). In each case, positive and negative versions of each style can be observed. Problem-focused copingProblem-focused copingA response to stress designed to address specific problems by finding solutions.            Problem-focused coping        A response to stress designed to address specific problems by finding solutions.             is designed to address an issue head-on. If you just flunked an important exam, positive problem-focused coping strategies might include making an appointment with your professor to discuss your options, hiring a tutor, taking a study skills seminar, or joining a study group formed by your classmates. Negative problem-focused coping can include escape and avoidance. Instead of confronting the problem, a student might stop going to class. Problem-focused coping is more frequently used for work-related stressors, which would include schoolwork, than for interpersonal stressors like the breakup of an important relationship (Terry, 1994). Emotion-focused copingEmotion-focused copingA combination of arousal, physical sensations, and subjective feelings that occurs spontaneously in response to environmental stimuli.            Emotion-focused coping        A combination of arousal, physical sensations, and subjective feelings that occurs spontaneously in response to environmental stimuli.             helps you deal with the negative emotions associated with a stressor. Flunking an exam might make you feel sad, discouraged, and depressed. Positive coping with these negative emotions might involve sharing your concerns with your friends or family, taking a break from your studies to enjoy a movie or go for a jog, or, when the negative emotions seem overwhelming, visiting your campus’s counseling center for advice. Negative emotion-focused coping, such as eating food that you do not need or using alcohol to dull your stress, might simply add to your problems and increase your overall levels of stress. Relationship-focused copingRelationship-focused copingA response to stress designed to maintain and protect social relationships.            Relationship-focused coping        A response to stress designed to maintain and protect social relationships.             helps you maintain and protect social relationships in response to stress (O’Brien & DeLongis, 1996). This type of coping has both positive and negative aspects. For example, people caring for Alzheimer’s patients, which can be stressful, exhibited both positive coping, including empathy, support, and compromise; and negative coping, including confronting, ignoring, blaming, and withdrawal (Kramer, 1993). As we have seen so frequently in our discussions of human behavior, the choice of a coping approach results from interactions between people and the situation in question (Aldwin, 1994). For example, people who score high on the Big Five trait of neuroticism (discussed in Chapter 12) tend to experience more distress than people with low neuroticism scores (Bolger & Schilling, 1991). When responding to stressors in the work situation, people high in neuroticism are more likely to engage in negative problem-focused coping, such as escaping the situation (O’Brien & DeLongis, 1996). When dealing with interpersonal stress, however, people with high neuroticism use the negative relationship-focused coping strategy of confrontation. People with high neuroticism also cope differently with stress involving close relationships (significant others, family members, and close friends) than with stress involving distant relationships (coworkers and neighbors). They are more likely to use a positive relationship-focused coping method, empathy, when dealing with distant relationships than when dealing with close relationships (O’Brien & DeLongis, 1996). At times of stress, we often engage in three types of coping. (a) Problem-focused coping is usually used to solve problems at work. (b) Emotion-focused coping helps us deal with the negative emotions of stress. The use of strategies like overeating, however, can lead to more problems. (c) Relationship-focused coping helps us maintain and strengthen social networks at times of stress. You might have observed that under the same stressful circumstances, such as an upcoming exam, some students seem calm and others appear to be in a state of panic. What determines these individual differences in coping? Austrian psychiatrist Viktor Frankl (1905–1997) was sent to the dreaded Nazi concentration camp Auschwitz, where he lost his pregnant wife, parents, and brother. Frankl used his experiences in developing his theories of logotherapy, after logos, the Greek word for meaning. The ability to find meaning even in the depths of despair by recalling pleasant memories or helping a suicidal inmate helped Frankl survive the stress and horror surrounding him. We have already seen how epigenetic interactions between genes and nurturing can influence later responses to stress. In addition, people differ in resilienceresilienceThe ability to adapt to life’s challenges in positive ways.            resilience        The ability to adapt to life’s challenges in positive ways.            , or the ability to adapt to life’s challenges in positive ways. Resilient people do not ignore feelings of sadness or stress, but like Holocaust survivor Viktor Frankl, they harness their inner strengths to enable them to remain optimistic and get on with the business of life. In contrast, when people lack resilience, they feel overwhelmed, helpless, and victimized. They become more vulnerable to the use of negative coping strategies, including alcoholism and drug abuse, to escape their problems. Early research in resilience focused on children’s ability to thrive even in conditions of poverty, neglect, and abuse (Masten et al., 1999). People who write scripts for how their life should happen, such as thinking that everyone should enjoy a set schedule of activities over a holiday, often experience high stress levels when events take an unexpected turn. Maintaining a flexible approach to meeting your life’s goals is far less likely to create stress. Some psychologists refer to individual differences in the ability to cope with stress as resulting from a personality trait of hardiness (Hystad, Eid, Laberg, Johnsen, & Bartone, 2009;  Maddi, 2013). As mentioned in Chapter 12, a personality trait is likely to be stable and enduring over time. Compared to less hardy people, people with high hardiness experience less threat or disruption in response to the normal stressors of life. Hardiness combines commitment, control, and challenge (Bartone, 2000). People with high commitment see the world as interesting and seek involvement rather than withdrawal. Control refers to individuals’ belief in their ability to influence events. Challenge is a state of mind that sees change and new experiences not as negative stressors, but as opportunities for learning and personal growth. Among college students, hardiness did not predict grades, but it was negatively correlated with health complaints and stress related to academic work (Hystad et al., 2009; also see Figure 16.10). The personality trait of hardiness, which combines commitment, a sense of control, and the appraisal of stressors as challenges, seems to protect college students from the health consequences of high academic stress. Other protective factors contributing to individual differences in resilience are cognitive skills, social skills, and flexibility in response to new situations (Garmezy, 1991). Individual differences in resilience might also have their roots in emotion. People who are generally more positive in mood tend to build the resources that they need, including strong social networks, to sustain them at difficult times (Cohn, Fredrickson, Brown, Mikels, & Conway, 2009). StageWhat’s Happening? Alarm Mobilization of resources for fight or flightResistance Continued coping with chronic stressExhaustion Systems begin to failYou might be surprised to see a section about health psychologyhealth psychologyA branch of psychology that investigates the relationships between psychological variables and health.            health psychology        A branch of psychology that investigates the relationships between psychological variables and health.             in your psychology textbook. Isn’t health the exclusive domain of medical personnel rather than of psychologists? It all depends on how you define health. For most of its history, the field of medicine viewed health as a lack of disease (Juster et al., 2010). Treatments were designed to make sick people feel better. In more recent years, however, health has been more commonly defined as the attainment of a positive state of well-being rather than simply the absence of disease. Medicine today is more likely to use a biopsychosocial modelbiopsychosocial modelA model that sees health as the result of biological, psychological, and social factors            biopsychosocial model        A model that sees health as the result of biological, psychological, and social factors             of health, in which health is viewed as the sum of biological (such as genetics, infection, and injury), psychological (such as lifestyle, stress, and health beliefs), and social factors (such as culture, family, and social support;  McEwen & Gianaros, 2011) (see Figure 16.11). Health today is viewed as the product of interactions among individual characteristics, life experiences, cognitions, and behavior. In 1948, the World Health Organization (WHO) proposed a definition that reflects this more contemporary model of health: a state of complete physical, mental, and social well-being and not merely the absence of disease or infirmity (WHO, 2003, para. 1). In addition to making sick people feel better, which is an important function of medicine, this new view of health seeks to improve the well-being of people who are not currently sick. One reason for this transition from the biomedical model to the biopsychosocial model is the changing face of health and medicine over the last 100 years or so. Because of improved sanitation and vaccination, the threats to health faced today are quite different from those faced by earlier generations. In 1900, the leading causes of death in the United States were pneumonia, influenza, and tuberculosis, but by 2005, the leading causes of death were heart disease, cancer, lung disease, and stroke (see Figure 16.12). In other words, we have gone from facing major threats from infectious, short-duration conditions to noninfectious, long-term conditions. In the last few decades, considerable progress was made in the treatment of heart disease, cancer, and stroke. This improvement in turn has resulted in a larger number of Americans who are living long enough to be diagnosed with age-related conditions such as Alzheimer’s disease and Parkinson’s disease. Over the last 100 years, infectious diseases have been replaced as the major causes of death in the United States by heart disease, cancer, lung disease, and stroke. As medical progress decreases deaths because of these causes, age-related diseases such as Alzheimer’s disease have become more common causes of death. The contemporary leading causes of death just listed (heart disease, cancer, lung disease, and stroke) form significant interactions with behavior because their risk factors include obesity, smoking, lack of exercise, and alcohol consumption. Even when we consider infectious diseases, we see evidence of strong behavioral components, such as the amount of stress that you’re experiencing (Cohen et al., 1991) or the likelihood that you practice prevention by washing your hands during flu season. In addition, lifestyle factors and adherence to treatment plans can be extremely important to achieving and maintaining wellness. Health psychologists are interested in helping people to better prevent and recover from disease and to cope with the stress of chronic health problems and pain. When we discuss the contributions of behavior to health, by no means do we want to blame the victims of poor health. People who have never smoked still get lung cancer, and thin, fit people get heart disease. The relationships between lifestyle factors and health are derived from the statistical analyses of large populations, not single individuals. At the same time, understanding that most people who engage in particular behaviors have certain health outcomes can provide us with clues for maintaining the healthiest lifestyles possible. Consumer Reports conducted a survey of 1,000 Americans to see how many engaged in everyday prevention behaviors (ConsumerReports.org, 2009). As shown in Table 16.2, the respondents did rather well in a number of categories (91% read the warnings that come with a prescription drug, and 87% report not drinking beer while using a power tool or mower) but neglected some obvious preventive steps (58% never wear a bike helmet, 24% report being in a car without using a seat belt, and 27% do not use sunscreen when outdoors for an extended period). We do worry about those 13% who are drinking beer while using power tools or mowing the lawn, but the point is that many of our choices contribute to the prevention of accidents and disease. BehaviorYesNoUse cotton swabs to clean ears73%26%Let your kids play on a trampoline43%56%Eat raw dough when making cookies39%61%Use the top step of a ladder31%69%Fail to use the car’s seat belt24%75%Drink beer while using a power tool or mower13%87%Have a rubber mat in the shower39%61%Have a carbon-monoxide detector in home51%48%Eat burgers only well done67%32%Change batteries in the smoke alarm annually79%21%Clean lint trap in the dryer after each use81%18%Read warnings on prescription medicines91%9%Wear a bike helmet42%58%Wear sunscreen when outside for an extended time73%27%Behaviors that contribute to our major causes of death include smoking, poor nutrition, alcohol use, lack of exercise, and loneliness (see Figure 16.13), which are discussed in more detail in the following sections. Following four simple health habits—never smoking, exercising 30 minutes per day, maintaining a nonobese weight, and eating a healthy diet including fruits and vegetables—reduced overall risk of chronic disease by 78%. Unfortunately, only 9.1% of a sample of more than 20,000 adults between 35 and 65 years of age had all four habits. Most people reported following only one to three of the four health habits. Cigarette smoking is considered to be the leading preventable cause of death in the United States, with a direct responsibility in one of five deaths each year (CDC, 2014). On average, smokers die 13–14 years earlier than nonsmokers. Tobacco use can lead to additional health problems. Nicotine is a well- established gateway drug, capable of producing epigenetic changes that increase the likelihood of addiction to cocaine (Li et al., 2014). Tobacco’s effects are not restricted to its users either. In the United States, exposure to secondhand smoke leads to 3,400 lung cancer deaths, 46,000 heart disease deaths, 430 cases of sudden infant death syndrome (SIDS), 24,500 low-birth-weight babies, 71,900 preterm deliveries, and 200,000 episodes of childhood asthma each year (WHO, 2008). Cigarette smokers in the United States dropped from 21% of adults in 2005 to just 15% in 2015 (Jamal et al., 2016) (see Figure 16.14). Worldwide, however, rates of smoking are higher than those found in the United States and other developed countries. According to the WHO (2008), two thirds of current smokers live in developing countries, with 30% of smokers residing in China and 10% in India. Gender, race, ethnicity, education, mental health, and income are strong determinants of the likelihood of smoking. In the United States, men are more likely to smoke than women (16.7%, as opposed to 13.6%). American Indians or Alaska Natives reported the highest incidence of smoking (21.9%), followed by non-Hispanic multirace individuals (20.2%), non-Hispanic Whites (16.6%), non-Hispanic Blacks (16.7%), Hispanics (10.1%), and Asians (7.0 %). Lesbian/gay/bisexual individuals were more likely to smoke cigarettes (20.6%) than heterosexual individuals (14.9%). Smoking drops with increasing education, from 34.1% of individuals with a general equivalency diploma (GED) certificate to 3.6% of adults with a graduate degree. Poverty plays a strong role in smoking; 26.1% of adults living below the poverty line compared to 13.9% living at or above the poverty line report smoking. Between 2005 and 2015, rates of smoking among adults in the United States dropped from 20.9% to 15.1%. At the same time, however, use of e-cigarettes increased. It is unclear how much of the drop in cigarette smoking is related to the use of e-cigarettes. You might be wondering how trends regarding e-cigarettes (i.e., vaping) might influence these numbers. In 2015, 3.5% of U. S. adults used e-cigarettes, and 58% of these are also current cigarette smokers (CDC, 2016). Among e-cigarette users aged 18–45, 40% have never used regular cigarettes. It is unclear how much of the overall decline in cigarette use is related to e-cigarette use. E-cigarettes often contain levels of nicotine capable of producing addiction, but their overall, long-term health implications remain relatively unknown (Harrell, Simmons, Correa, Padhya, & Brandon, 2014). Secondhand exposure to the vapor from e-cigarettes presents less risk than to conventional cigarette smoke, but it nevertheless has the potential to produce adverse health outcomes (Hess, Lachireddy, & Capon, 2016). Given the carnage produced by tobacco, why do people start smoking and then continue? An understanding of the initiation of tobacco use requires combining the perspectives discussed in this textbook—development, biological psychology, learning, cognitive, clinical, and social/personality psychology. One of every three cigarettes lit each day is smoked by a person in China, where it is a conventional courtesy for one businessperson to greet another by offering a cigarette. The 2011 ban on smoking in bars, in restaurants, and on transportation does not apply to offices and factories. The ban was required when China signed the World Health Organization Framework Convention on Tobacco Control. Public health officials fear that the lack of an awareness program and penalties will encourage the population to ignore the ban. Most tobacco users begin smoking in childhood or early adolescence, long before decision-making abilities are mature. Because of the addictive nature of nicotine, about half of those who experiment with tobacco continue to use it. In addition to its action on synapses where the neurotransmitter acetylcholine is released, as discussed in Chapter 6, nicotine has the ability to stimulate the dopamine reward circuits of the brain and to produce a particularly unpleasant set of withdrawal symptoms. As discussed in Chapter 11, adolescence is also a time when people try out new roles and learn important skills for getting along with peers, frequently making them susceptible to peer pressure. Teens often overestimate how many other people are using tobacco, so they might begin using it to avoid looking different (IOM, 1994). Finally, most smokers have friends and parents who also smoke, suggesting a role for social learning (Biglan, Duncan, Ary, & Smolkowski, 1995). A troubling aspect of the initiation of smoking is the prevalence of tobacco use among people with diagnosed psychological disorders (see Figure 16.15). Of the approximately 20% of American adults diagnosed with any mental disorder (see Chapter 14), 40.6% were current smokers—a much higher rate than that found in the general population (Jamal et al., 2016). Adults diagnosed with mental disorders smoke 31% of all cigarettes consumed by adults in the United States (CDC, 2013b). Tobacco use among people with schizophrenia and other severe mental illnesses is about 70% in the United States, more than three times as high as the approximately 20% of users in the general population, as reported by the CDC (Dixon et al., 2007). Individuals identified as having high levels of psychological distress, including being diagnosed with severe psychological disorders such as schizophrenia, are more than three times as likely to smoke cigarettes as people with low or no psychological distress. This might represent efforts to self-medicate, but a causal role for tobacco in a number of psychological disorders has not been ruled out. In addition, the severity of smoking among these individuals, which includes such factors as how many cigarettes are smoked per day, appears to be much higher among people with psychological disorders than in the general population. In a large-scale study of more than 50,000 adults who did not live in mental institutions, individuals with serious psychological distress (a global measure of psychological disorder) were more likely to smoke currently and heavily and were less likely to quit than individuals without serious psychological distress (Sung, Prochaska, Ong, Shi, & Max, 2011). Although many smokers with psychological disorders report using cigarettes to feel better, most began smoking long before their symptoms emerged and they were diagnosed with a disorder (Sacco, Termine, & Seyal, 2005). People do quit smoking, although many find it difficult. Again, the social nature of our species plays a role in this process. A person’s chances of successfully quitting are reduced by 67% if a spouse smokes, 25% if a sibling smokes, 36% if a friend smokes, and 34% if a coworker smokes (Schroeder, 2008). We are not advocating that a prospective quitter abandon the essential social support needed at a difficult time; rather, we are suggesting that people trying to quit physically separate themselves when friends and family light up. As noted in Chapter 8, being exposed to conditioned stimuli, such as the smell of tobacco, can initiate a variety of conditioned behaviors that might make refusing a cigarette difficult for the person trying to quit. In the United States, ex-smokers now outnumber smokers (47 million to 46 million; Chapman & MacKenzie, 2010). Smokers use a variety of techniques to quit, including simply stopping all use abruptly (cold turkey), gradually reducing intake, using nicotine patches and gum or other nicotine replacement tools, participating in counseling and support groups, or some combination of these. Smoking cessation programs also can take advantage of the self-reference effect, discussed in Chapter 12. This effect explains the superior recall for information relevant to the self by suggesting that the self serves as an important schema for organizing information. Participants experiencing interventions for smoking that were tailored to their own lives, needs, interests, and obstacles not only were more successful at quitting smoking, but also showed brain activity in parts of the prefrontal cortex that are believed to participate in thinking about the self (Chua et al., 2011). The benefits of quitting smoking appear within minutes to hours as the body begins to repair itself. After 5 years of abstention from smoking, risk of stroke is the same for ex-smokers and nonsmokers. After 10 years of abstention, the risk of lung cancer is cut in half. If the smoker quits before the age of 30 years, life expectancy remains the same as for someone who has never smoked (Doll, Peto, Boreham, & Sutherland, 2004). Nutrition plays a significant part in overall physical development, including brain development, and is believed to be responsible for many differences in psychological and health outcomes related to SES (Rosales, Reznick, & Zeisel, 2009). People need the right amount and quality of nutrients to support optimum health and brain functioning, and being either underweight or obese is associated with reduced health. Obesity increases the rates of many chronic conditions, including heart disease, stroke, diabetes, arthritis, and breast and colon cancers. As noted previously, maintaining a body mass index (BMI) below 30 (nonobese) was one of the four protective factors associated with much lower risk of disease. Returning the U. S. population to normal weight (a BMI of between 18.5 and 24.9) would produce about the same improvement in overall life expectancy as the elimination of smoking (Stewart et al., 2009). One of the challenges that we face today is the high cost of eating a healthy diet. If you have $1.50 in your pocket, you can buy about 1,100 calories at McDonald’s (breakfast of pancakes and sausage) or 250 calories of fresh apples. We can guess which choice financially struggling parents with hungry children are likely to make. As mentioned in Chapter 7, not only do we face problems in the form of eating disorders, such as anorexia nervosa and bulimia nervosa, but the world also has experienced an unprecedented obesity epidemic over the last 25 years (see Figure 16.16). In the United States, the percentage of obese adults rose from 12% in 1991 to 35% in 2012 (Ogden, Carroll, Kit, & Flegal, 2014). It is likely that multiple factors have contributed to this change, including our sedentary lifestyle, increases in caloric intake, changes in the types of food that we eat, changes in sleep patterns, and social factors. WHO (2006) describes low- and middle-income countries as facing a double burden of disease because undernutrition and obesity occur at the same time, often in the same households. A combination of inadequate nutrition prenatally and in childhood, followed by exposure to high-calorie but nutrient-poor foods, sets the stage for a lifetime of poor health, possibly because of epigenetic factors (Haemer, Huang, & Daniels, 2009). In 1960, fewer than 15% of American adults were obese (body mass index, or BMI, > 30), but today about one third are obese. The number of people who are extremely or morbidly obese (BMI > 40) also has climbed during this period. The reasons for the obesity epidemic are not well understood and are likely to be complex. As was the case with smoking, we can see evidence of social factors that maintain our eating habits and weight. Our recent history of eating cheap, fatty, and sugary foods interacts with poverty. Nutritionists have raised the alarm that eating healthy foods is becoming more expensive than ever. Also, we seem to use the people around us as a measure of how we’re doing in terms of our weight. Having an obese spouse increases your risk of obesity by 37%, and having obese friends increases your risk by 57% (Christakis & Fowler, 2007). As observed in Chapter 7, there are no quick fixes for obesity and poor nutrition. Many of the same challenges face people who want to lose weight that we observed among people attempting to quit smoking. One advantage that would-be ex-smokers enjoy, however, is the option of complete abstinence. In contrast, we cannot abstain from eating; instead, we must choose to eat differently. Research identifying the contagious aspects of smoking and obesity might provide a hopeful note. If people in your social circle begin to eat healthier diets and maintain healthier weights, perhaps it will become easier for you to do so as well. Although losing weight can seem difficult, small changes in behavior are helpful. People using smaller plates unconsciously ate less (Wansink, 2006). In addition to addressing concerns about obesity, current research in nutrition is focusing on the specific nutrients needed for healthy development and psychological well-being. As mentioned in Chapter 14, prevalence of bipolar disorder is much lower in countries consuming large amounts of seafood than in countries where seafood consumption is rare (Noaghiul & Hibbeln, 2003). Although much remains to be explored in this area, a further understanding of the consequences of changes in our modern diet should help us achieve a healthier lifestyle. Alcohol is widely used in the United States and in many other countries around the world (see Figure 16.17). According to the National Institute on Alcohol Abuse and Alcoholism (NIAAA, 2017), 86.4% of American adults consumed alcohol at some point in their lifetime, 70.1% consumed alcohol in the past year, and 56.0% consumed alcohol in the past month. About 27% of the adult population engaged in binge drinking (five or more alcoholic beverages on the same occasion) during the past month, and 7% reported heavy drinking (five or more alcoholic beverages on one occasion on each of five or more days in the past month); 6% of adults and 2.5% of adolescents meet criteria for alcohol use disorder. Alcohol use is the fourth-leading preventable cause of death in the United States. Globally, about one quarter of total deaths of individuals between the ages of 20 and 39 occur due to alcohol (NIAAA, 2017). The World Health Organization (WHO) reports that worldwide, alcohol is third (behind only childhood starvation and unsafe sex that leads to HIV/AIDS) as a cause of loss of years of healthy life because of disability and poor health. The largest threats to health from alcohol use are to the liver, which is responsible for clearing the body of toxins, but alcohol use also contributes to lower life spans because of stroke, high blood pressure, and some cancers (breast, digestive, and liver). In the United States alone, excessive alcohol consumption leads to approximately 88,000 deaths annually, at a cost of $249 billion (NIAAA, 2017). Alcohol-impaired driving accounts for 31% of all driving fatalities in the United States. In addition, alcohol poses a risk of abuse and dependence for many people. Psychologists define alcohol abuse as repeated use despite adverse consequences and alcohol dependence as alcohol abuse accompanied by tolerance, withdrawal, and a compulsive urge to drink more (APA, 2013). As observed in Chapter 4, tolerance is defined as the need to administer greater quantities of a drug to maintain the same subjective effect, and withdrawal refers to symptoms that occur when a habitually used drug is no longer used. As we have seen in so many other domains of human behavior, complex interactions between biological and environmental factors lead to alcohol abuse and dependence. Genes related to the body’s ability to break down alcohol in the liver are not evenly distributed across the world’s populations. Certain types of genes found primarily in Asians are correlated with lower rates of alcohol dependence (Eng, Luczak, & Wall, 2007). To illustrate the need to consider more than simply genetic predisposition, however, see the example of Native Americans, who have much higher rates of alcohol dependence than do many other ethnic groups in the United States, despite a high prevalence of the supposedly protective genes (Ehlers, 2007). Many of the same factors that encourage teens to begin smoking also encourage drinking. Most people who go on to abuse alcohol are drinking heavily by late adolescence, and most cases of alcohol dependence are well established by the age of 30 years (Enoch, 2006). This timeline provides insight into when prevention programs are likely to be most effective. Peer pressure, the need to fit in or look cool, overestimates of Everybody’s doing it, modeling the behavior of parents and other family members, and the teen’s characteristic sense of being immune to harm from risky behavior can contribute to decisions to begin drinking. Because alcohol reduces anxiety, socially anxious teens are especially at risk for problem drinking. If teens feel more socially competent while drinking, they are likely to continue this practice because of operant conditioning, as described in Chapter 8. Reduced feelings of anxiety are rewarding, so a person is likely to repeat the behavior (alcohol consumption, in this case) associated with this reward. Among the environmental influences on problem drinking is the age at which people take their first drink. Individuals exposed to alcohol at earlier ages show a much higher rate of alcohol dependence later in life. Again, we find that age of first drink interacts with a person’s genetic predisposition to alcohol dependence (Agrawal et al., 2009). In yet another example of the epigenetic mechanisms discussed in Chapter 3, early exposure to alcohol might affect the expression of genes related to problem drinking. A similar interaction is found between the maltreatment and neglect of a child and the genetic vulnerability to alcohol dependence. Not all children who are maltreated go on to become dependent on alcohol, although many do (Shin, Edwards, Heeren, & Amodeo, 2009;  Shin, Edwards, & Heeren, 2009). The maltreated children who do not develop drinking problems are likely to be protected by combinations of genetic predisposition and the presence of peer and parental support (Enoch, 2006). People who are exposed to alcohol at younger ages have the highest risk for alcohol abuse. Treatment for very heavy drinkers often requires medical supervision because withdrawal from alcohol can produce life-threatening seizures. Recall from Chapter 4 that alcohol boosts the inhibition produced by GABA, leading to alcohol’s classification as a central nervous system depressant. Because withdrawal symptoms are usually the opposite of drug effects, withdrawing from a depressant produces excitation, and too much excitation leads to seizures. One of the remaining controversies in the treatment of alcohol dependence is whether a person can safely return to moderate drinking. Most psychologists do not believe that it is possible for recovered problem drinkers to resume alcohol consumption without relapse. In one 60-year longitudinal study of men with problem drinking, few returned to moderate drinking without experiencing a relapse (Vaillant, 2003). In contrast to previous generations, who simply played, it is not uncommon today to find children as young as 3 to 5 years of age involved with adult-organized formal sports. Further research is necessary to determine the long-term impacts of this social trend on adult patterns of play and exercising. Exercise was not an option for our hunter–gatherer ancestors, for whom physical work meant the difference between death and survival. In today’s sedentary lifestyle, however, in which many workers spend hours commuting in cars to sit for more hours in front of computers, exercise becomes something that we must consciously remember to do. Children in previous generations would dash from school to play outdoors, but social changes, including lack of supervision at home and safety concerns, have led to much less spontaneous outdoor activity. Parents might prefer that their children stay indoors playing video games rather than ride their bicycles in the neighborhood. For many middle-aged and older adults, loneliness might lead to less physical activity (Hawkley & Cacioppo, 2010;  Hawkley, Thisted, & Cacioppo, 2009). Lack of exercise and sitting are related but different issues. Children and adults in the United States spend an average of 55% of their day sitting while riding in cars, watching television, doing work or attending school, and playing video games or doing other computer work (Matthews et al., 2008). A person’s amount of sitting time is correlated with risk of death (see Figure 16.18). However, high levels of moderately intense activity (60–75 minutes per day) seems to offset some of this risk (Ekelund et al., 2016). Although the highest risk of death occurs in obese individuals who spend nearly all their time sitting, the amount of time spent sitting also predicts mortality within a group of active individuals (Katzmarzyk, Church, Craig, & Bouchard, 2009). Data showing decreased survival as a function of the amount of your day spent sitting have led to a market for standing desks. If you find these desks comfortable, by all means use them, but recall that again, the sitting data are correlational, and we cannot conclude that sitting causes death. We are not randomly assigning people to sitting groups. People who sit all day are likely to be obese or have other health problems that contribute to their higher risk of mortality. Exercise not only benefits our bodies by keeping our muscles and cardiovascular systems in good shape, but also benefits our psychological well-being. A brisk 30-minute walk has emerged as an effective way to treat MDD, as discussed in Chapter 15 (Blumenthal et al., 1999). In addition to improving mood, exercise increases cognitive performance (Hogan, Mata, & Carstensen, 2013). One mechanism for this improvement could be the increased delivery of oxygen to the brain resulting from a fit cardiovascular system. Other research suggests that exercise also has the capacity to boost neurogenesis, particularly in the hippocampus of the brain (Gibbons et al., 2014). As you learned in Chapter 4, the hippocampus plays important roles in learning and memory. We mentioned in an earlier section that stress can be detrimental to the functioning of the immune system. Exercise can be stressful as well; so what effect could it have? Not too surprisingly, we find complex relationships between the amount and type of exercise that a person performs and the response of the immune system. Compared to sedentary people, those who engage in regular moderate exercise enjoy a lower rate of infection (Gleeson, 2007). However, too much of a good thing, in the form of continuous, prolonged, and high-intensity exercise such as preparation for and competition in marathons and triathlons, can produce a temporary decrease in immune system function for about a day. Elite athletes, because of their constant and intense training schedules, often experience more minor illnesses, such as sniffles and colds, but the long-term benefits of exercise far outweigh these small vulnerabilities. Loneliness, or a perceived deficit in social connectivity, has long been understood as a risk factor for psychological disorders, but now it is understood to contribute to mortality in older adults as well (Cacioppo & Cacioppo, 2017). Even when other factors are controlled, loneliness produces a 26% increase in the odds of early death. Given the growing number of older adults and high prevalence of loneliness among them (20%–60% report being lonely some of the time, and 5%–10% feel lonely frequently or always), this presents a significant risk to health and well-being. Loneliness produces risk for death along a number of pathways that are familiar to you by now: poor sleep quality, increased activity in the HPA axis, increased sympathetic activity relative to parasympathetic activity, altered gene expression in immune system cells, decreased immunity to viruses, increased inflammation, decreased impulse control, and increased symptoms of depression (Cacioppo & Cacioppo, 2017). While the amount of harm from changes in each of these pathways might be small, the combined and cumulative changes due to loneliness can produce significant damage to health and well-being (see Figure 16.19). Loneliness, or perceived social isolation, does not directly reduce life expectancy. Instead, loneliness exerts its influence on mortality through complex interactions between genetics and environment, which in turn affect important health factors like inflammation. Many health-related variables, including poverty, education, access to medical care, nutrition, and substance abuse, vary widely from country to country and within ethnic groups living in the United States. The United States, despite its high standard of living, ranks 36th among countries of the world in terms of life expectancy, suggesting that wealth alone is insufficient to guarantee good health (United Nations, 2006; also see Figure 16.20). Many health-related variables, including poverty, education, access to medical care, nutrition, and substance abuse, vary widely from country to country and interact to determine average life expectancy. In the United States, death rates for all ethnic groups have declined over the last decade, but ethnic differences remain. Risk of death is lowest among Asians and Pacific Islanders and highest among the non-Hispanic Black population (Kochanek, Murphy, Xu, & Tejada-Vera, 2017). Although life expectancy for White and Black Americans differs, the gap continues to narrow. In 2000, life expectancy for Whites was 5.5 years more than life expectancy for Blacks, but this gap was reduced to 3.8 years by 2010. Increases in life expectancy across all ethnic groups resulted from reductions in the rate of death from leading causes, including heart disease, cancer, chronic respiratory diseases, and stroke. Compared to white Americans, African Americans, Hispanics, and Native Americans have been as much as two to three times less likely to have health insurance (Derksen, 2013). Not only do these discrepancies burden minorities with increased disability, but they contribute to continued poverty and violence affecting society as a whole. A shortage of physicians willing to treat racial and ethnic minorities and non-English-speaking patients contributes to health disparities as well (Marrast, Zallman, Woolhandler, Bor, & McCormick, 2013). In addition to ethnic and cultural differences in overall health and health habits, discrepancies occur in mental health. Minorities living in the United States have less access to mental health services, are less likely to receive needed services, receive a poorer quality of care, and are underrepresented in mental health research (U. S. Department of Health and Human Services; U. S. Public Health Service, 2007). Among possible variables accounting for increased mental health problems among minorities are poverty, discrimination, and violence. Poverty in particular is an important risk factor for psychological disorders, with people living at the lowest levels of the socioeconomic scale experiencing two to three times greater rates of psychological disorder (U. S. Department of Health and Human Services, U. S. Public Health Service, 2007). The exact prevalence remains unknown because many people living in poverty do not have access to care. Stress from poverty, discrimination, and violence might contribute to higher rates of stress-related health problems and disorders. Distrust of clinicians who might seem to be disrespectful or whose approach is culturally insensitive also might deter people from obtaining the treatment that they need. Culture contributes to risk for suicide as well. In 2015, the highest rates of suicide in the United States among individuals aged 10 years or older occurred among Whites (15.1%), followed by American Indians/Alaska Natives (12.6%), Asian or Pacific Islander groups (6.4%), Hispanics (5.8%), and non-Hispanic Blacks (5.6%;  Curtin, Warner, & Hedegaard, 2016). These data suggest the presence of protective mechanisms within some cultures that offset the negative effects of minority status. If you haven’t already noticed during our discussions of stress, sleep, nutrition, smoking, alcohol use, and exercise, all these variables interact. Stress can lead to poor sleep, additional consumption of calories, more smoking, more drinking, and less exercise. People who eat poorly are unlikely to sleep well or feel like going to the gym. Smoking reduces weight (naming a cigarette marketed to women Virginia Slims was no accident), but of course, it has negative effects on health that far outweigh that advantage. Problem drinkers either cut back on nutritious food or become obese (Breslow & Smothers, 2005). College students engage in drunkorexia, or the deliberate restriction of calories before planned alcohol consumption for the purpose of avoiding weight gain (Burke, Cremeens, Vail-Smith, & Woolsey, 2010). Most of if not all these behaviors are affected by our social context. We eat, drink, and smoke but also exercise more in the company of our fellow humans. Poverty is a significant risk factor for many types of psychopathology. These interactions in health behaviors remind us of the need to zoom out to look at the whole picture of individuals within their social contexts when we attempt to apply interventions leading to better health (see Figure 16.21). We have amassed a significant amount of data on why people engage in a particular problem behavior, whether that is smoking, drinking, or overeating, and we are now in a better position to identify the overlapping patterns of epigenesis, stress, child maltreatment, peer pressure, and motivation that underlie these behaviors. An example of contemporary research that uses an integrated model of health is the ACE Study, being undertaken by researchers affiliated with the Centers for Disease Control and Prevention (CDC). The study plans to address identified gaps in our understanding about the relationships between adverse childhood experiences, such as abuse and neglect, and later social, emotional, and cognitive impairments, which in turn could influence the adoption of risky health behaviors. DomainCorrelatesTobacco Use Smoking is the leading preventable cause of death in the United States. Nutrition Obesity increases rates of many chronic diseases, including heart disease, stroke, diabetes, arthritis, and some cancers. Specific nutrients, not just calories, are necessary for optimum functioning. Alcohol Alcohol’s main effect on life span is its contribution to accidents, but use is also correlated with liver disease, stroke, high blood pressure, and some cancers. Exercise Exercise improves overall health, mood, and cognition. Loneliness Loneliness produces indirect effects on health by affecting sleep, immunity, HPA axis function, sympathetic nervous system function, gene expression, inflammation, impulse control, and depression. Although helping people with problems is an important part of psychology, some psychologists have pointed out that there are differences between making people less uncomfortable and helping them live happier, more fulfilling lives. The interventions explored in Chapter 15 are designed to alleviate psychological pain, but these treatments are not intended to make people who do not have psychological disorders happier. At the same time, we know a great deal about people with psychological problems, whom we described in Chapter 14 as representing a minority of the population. What about most people, who lead relatively untroubled, typical lives? As observed in Chapter 10, we quite possibly know the least about people with unusual strengths, such as talent and genius. The field of positive psychologypositive psychologyAn approach to psychology that emphasizes normal behavior and human strengths.            positive psychology        An approach to psychology that emphasizes normal behavior and human strengths.             focuses on these relatively neglected areas of behavior and mental processes by using scientific methods to understand positive human experiences and adjustment. A new set of interventions should emerge from this understanding that will guide individuals, families, and communities in their attempts to maximize their potential (Seligman & Csikszentmihalyi, 2000). William James foreshadowed positive psychology in his 1902 book The Varieties of Religious Experience. In this book, James discussed people who achieve healthy mindedness as having an understanding of the goodness of life and a soul (a word often used by people in James’s era as synonymous with mind) with a sky-blue tint (James, 1905, pp. 79–80). James goes further to speculate about the origins of healthy mindedness. For the lucky few, healthy mindedness occurs naturally and effortlessly. For others, it must be achieved through effort. Despite James’s efforts, psychology continued its fascination with the abnormal and the unhealthy into the 20th century and did not direct the same energy toward understanding the normal and healthy. Voices of dissent arose from among the humanistic psychologists, including Abraham Maslow, discussed in Chapters 7 and 12. Maslow argued that psychologists could learn more about human behavior by studying outstanding individuals than by studying people with severe problems. As part of Maslow’s thinking about self-actualization, the pinnacle of social motivation in his theory, he took detailed notes on people he admired in a GHB (Good Human Being) notebook that he kept between 1945 and 1949 (Lowry, 1973). In addition to making observations of living people whom he admired, Maslow combed through biographies and autobiographies of famous people in search of common features of healthy mindedness. Running parallel to the humanistic psycholgists’ rejection of a negative, sickness model of human behavior were innovations among cognitive and behavioral approaches. As mentioned in Chapter 15, Albert Ellis and Aaron Beck were achieving excellent results with cognitive restructuring around the same time that the humanistic psychologists began reconsidering Freud. Ellis and Beck were able to help their clients with MDD see a glass of water as half full as opposed to half empty, improving their mood and optimism in the process. In his book The Varieties of Religious Experience, William James foreshadowed the development of positive psychology by considering what makes some people healthy minded and what gives others a sick soul. One of the most significant voices leading to contemporary positive psychology approaches is that of Martin Seligman. (We discussed his concept of learned helplessness in Chapter 14.) If people’s outcomes do not seem connected to their behaviors, the resulting feelings of helplessness and lack of control could result in depression. Based on his investigations of learned helplessness, Seligman proposed that optimismoptimismThinking positively about the future            optimism        Thinking positively about the future            , or thinking positively about the future, can be learned, just as helplessness can (Seligman, 1990). Positive psychology embraces humanistic emphases on human growth and fulfillment, yet applies the scientific reasoning and research characteristic of cognitive and behavioral psychology. Positive psychology has focused on the study of positive emotions, positive traits, and positive institutions. Positive emotions and the experiences that produce them, including happiness, love, gratitude, contentment, and hope, contribute to our well-being. For our social species, it is not an accident that many of the situations that produce these positive feelings, such as promotions at work, marriage, and other successes, involve people who are important to us. Martin Seligman (right) and the Dalai Lama (left), spiritual leader of Tibetan Buddhism, discussed the important role of positive emotions at a Mind and Its Potential conference in 2009. We all know happiness when we experience it, and most people prefer the state of happiness to the alternatives of unhappiness and depression. Psychologists approach happiness from two different perspectives (Ryan & Deci, 2001). A hedonic approach focuses on obtaining pleasure and avoiding pain. A eudaimonic approach focuses on meaningfulness and self-realization. Happiness often seems fleeting or transient. The primary reason for this elusiveness is that happiness is typically relative (see Figure 16.23). We quickly adapt to our current circumstances, and it is only change in those circumstances that provokes feelings of happiness or unhappiness. In a study of life satisfaction, health, wealth, and education were the three most important predictors of happiness. The five happiest countries were Denmark, Switzerland, Austria, Iceland, and the Bahamas, with the United States ranked 23, China ranked 82, Japan ranked 90, and Russia ranked 167. Research with identical twins suggests that we have a happiness set point that is largely influenced by genetics (Lykken & Tellegen, 1996). Changes in circumstances might raise or lower happiness from the set point, but the effect would be temporary. However, having a genetic predisposition to being happy or to being cranky does not mean that a person’s behavior has no influence. Instead, people can do a great deal to improve their happiness. Rather than looking for external things to boost happiness, such as a new car or a new nose, looking within is more effective. Happiness can be improved by thinking carefully about what things interfere with our happiness, such as allowing ourselves to stay mad at a partner following an argument, and working to avoid these things. Like most of american psychology, positive psychology has been criticized as being overly influenced by the values of the white majority. To what extent are concepts like happiness the same or different depending on race, ethnicity, or culture? To what extent are such values simply human? Optimism, or a positive expectation for the future, is linked with both physical and psychological well-being (Carver, Scheier, & Segerstrom, 2010). Previous studies have shown little variability in the level of optimism across cultures, although some differences seem to be associated with the individualistic–collectivistic dimension of cultures (Jeglic et al., 2016). While Asian and Asian American individuals report similar levels of optimism compared to White Americans, they report higher levels of pessimism at the same time (Chang, 1996; see Figure 16.22). Comparisons between American and Ghanaian college students found that the Ghanaian students demonstrated higher optimism and hope, while expressing less suicidal ideation (Eshun, 1999). While more research is needed regarding racial and ethnic differences in optimism within the United States, lack of hope was shown to be a risk factor for suicidal ideation for Hispanic and African American students, but not for White students (Chang, Elizabeth, Kahle, Jeglic, & Hirsch, 2013;  Davidson, Wingate, Slish, & Rasmus, 2010). White and Asian college students demonstrate similar levels of optimism, but the Asian students show much higher levels of pessimism. These perspectives appear to have different implications for White and Asian students, however. High levels of pessimism were associated with better problem-solving performance in Asian students, but worse performance in White students. It is likely that concepts that are as value-oriented as those studied by positive psychologists will show nuanced variations across different groups, and our understanding of these concepts would be enriched by further investigation into these similarities and differences. One critical factor in people’s happiness is the strength of their interpersonal relationships. When participants are asked, What is necessary for your happiness? or Tell me what makes your life meaningful, nearly everybody talks about their close relationships first—family, friends, and romantic partners (Berscheid & Peplau, 1983). This finding is consistent with one of the ongoing themes of this text: social environment plays a central role in human behavior. Among interpersonal relationships, the institution of marriage is particularly likely to contribute to happiness (see Figure 16.24). In a study of more than 42,000 Americans beginning in 1972, 40% of married adults, but only 22% of never-married adults, report that they are very happy (Davis, Smith, & Marsden, 2006). Also, 18% of divorced adults report being very happy, and remarriage increases happiness only in a minority of cases. In a large-scale study that followed 13,000 adults for five years, people who remained married experienced higher well-being than those who separated or divorced (Waite, Luo, & Lewin, 2009). Among the benefits of marriage are reduced infidelity, longer-lasting relationships, and longer life (Waite, & Gallagher, 2001). Many studies have pointed to a strong relationship between being married and subjective well-being. People who are married and living with their spouse enjoy a significantly lower death rate than those who are unmarried. People who have never married, particularly males, experience an especially high risk for premature death (Kaplan & Kronick, 2006). Married individuals have lower cortisol levels compared to never married or previously married individuals (Chin, Murphy, Janicki-Deverts, & Cohen, 2017). As we observed earlier, cortisol release is correlated with stress. Why would marriage make people happy? One possibility is that the relationship between marriage and happiness is a false one because of the likelihood that happy people produce happy marriages instead of the other way around. However, careful research that controls for premarital happiness does not support this hypothesis (Horwitz, White, & Howell-White, 1997). Regardless of how happy people are before marrying, on average being married makes them happier. If you’re thinking of cases that you know in which married couples are miserable, time together seems to help. Among the unhappiest couples, only 12% report being unhappy five years later, and 70% of the formerly unhappy partners report being very or quite happy (Horwitz et al., 1997). What is the source of this marital happiness? Traditionally, psychologists have suggested that marital happiness results from the perceptions that one partner has of the other. But more sophisticated analyses have shown that the factors associated with marital happiness are more complicated (Luo, Zhang, Watson, & Snider, 2010). While it’s true that perceptions of your partner influence how satisfied you are in a relationship (I’m with a great person), it is also true that your overall satisfaction with the relationship affects how you see your partner (I’m happy, so I must be with a great person). Then, what is the source of that satisfaction? A major factor in satisfaction with a relationship is how you think your partner sees you (My partner thinks I’m terrific, which makes me happy, and because I’m happy, I must be with a great person). Another common-sense notion about happiness is that wealth can make you happy. After all, we see frequent images of ecstatic lottery winners and smiling sports superstars who have signed megabuck contracts. At both national and individual levels, being poor does appear to be correlated with being unhappy. However, once basic needs are met, additional money does not guarantee happiness (see Figure 16.25). Interviews with the 100 richest people in the United States indicated that they were only slightly happier than average (Diener, Horwitz, & Emmons, 1985). In addition, how people use their wealth can influence happiness. People who are less materialistic report being happier than people who are materialistic (Tsang, Carpenter, Roberts, Frisch, & Carlisle, 2014). When adjusted for inflation, Americans’ after-tax disposable income grew dramatically between 1946 and 1989, but measures of subjective well-being remained flat. Increases in income above a certain point where basic needs can be met tend to have mild, if any, long-term impact on happiness. In chapter 7, we noted that an update of Abraham Maslow’s hierarchy of needs pyramid featured parenting at the top to represent this role’s importance from an evolutionary perspective (Kenrick, Griskevicius, Neuberg, & Schaller, 2010). Parenting in the 21st century, however, probably shares little similarity with the way parenting was for our hunter–gatherer ancestors. The goal of contemporary marriages has less to do today with financial survival or raising families than it has with meeting partners’ emotional needs (Dew & Wilcox, 2011). A major predictor of marital satisfaction is time spent with a partner, so where does that need fit with the challenges of meeting the demands of a family of young children? Some research suggests that parenting results in reductions in marital satisfaction, which increase with the number of children (Twenge, Campbell, & Foster, 2003). This result is most pronounced among younger, higher-income mothers of infants. Even in the 21st century, the arrival of the first child has a tendency to traditionalize the gender roles of the new parents. Fathers begin to take on more of the income-earning responsibilities and mothers take on more of the childcare and housework. Her changing roles might make the new mother less happy with her circumstances (Dew & Wilcox, 2011). Other researchers disagree. In another set of studies, parents evaluated their lives more positively than nonparents, felt better than nonparents from day to day, and found more satisfaction in caring for their children than in other daily activities (Nelson, Kushlev, Dunn, & Lyubomirsky, 2014;  Nelson, Kushlev, English, Dunn, & Lyubomirsky, 2013). Parenting contributed to more positive emotions on a daily basis and to the sense of having a meaningful life—concepts consistent with the approach to happiness held by some positive psychologists. What might be the source of these discrepancies? One possibility is that parents as a group are highly diverse. Parenting under some circumstances can be more stressful than others. Young, unpartnered parents, not too surprisingly, reported less happiness and satisfaction than their nonparent age peers, yet they still indicated that they felt their lives were meaningful (Nelson et al., 2013). Parents may experience differing levels of support from governments, employers, communities, and families. Globally, institutional policies such as paid time off and child-care subsidies for parents are associated with less unhappiness on the part of parents (Glass, Simon, & Andersson, 2016). Evolutionary theories view parenting as an important motivation for human behavior, yet contemporary psychologists debate whether parents are truly happier than nonparents. Now we return to the idea that happiness occurs because of changing circumstances. Perhaps the 100 richest Americans had simply adapted to their wealthy lifestyles. What about people who suddenly become wealthy? The Camelot Group, which manages the national lottery in the United Kingdom, reported that winning the lottery did have an impact on happiness (Camelot Group, 2004). According to these findings, 65% of winners reported that they were happier following the win because of financial security and increased freedom; 35% said that they were about as happy after winning as before; and none reported feeling less happy than before the win. Although the winners traded up in terms of lifestyle (most had bought new cars and new homes), 92% of those who were married when they won were still married to the same spouse. Lottery winners experience an increase in happiness, but they tend to be cautious about changing their lifestyles. Most continue working and attempt to maintain as normal a life as possible. The correlation between happiness and income has been assumed to reflect the effect of money on happiness. However, longitudinal research suggests that happiness may influence income (Cacioppo et al., 2008). As in prior studies, happiness and income in middle-age and older adults were correlated. However, happiness predicted bigger increases in income, whereas higher income did not predict increases in happiness. Happy people form better relationships, and these better relationships contribute to larger increases in income, perhaps because of improved job performance, better performance reviews and promotions, and better networking opportunities that lead in financially productive directions. Results from the Gallup World Poll, the first representative sample of all people on the planet, showed that financial wealth and social psychological wealth predict different types of well-being across many cultures (Diener, Ng, Harter, & Arora, 2010). Social psychological wealth is measured by the respect that you receive; closeness with family and friends; the opportunities to learn, do what you do best, and choose how to spend your time; and the need to work long hours. A single happiest nation measure does not seem supported by the data. Central American and some African nations have greater social psychological wealth than financial wealth, while nations previously in the Soviet bloc experience much higher financial wealth than social psychological wealth. The United States ranks first in financial wealth and 19th in social psychological wealth. Financial wealth predicted global measures of life satisfaction, but social psychological wealth made much stronger predictions of the presence of positive emotions like happiness. Some psychologists have moved past research identifying the causes and correlates of happiness to make recommendations for how to increase individual happiness. Among the steps recommended for improving happiness are practicing time management, exercising regularly, sleeping well, finding interesting work to do, nurturing close relationships, and following a chosen religious faith (Myers, 1993). Methods for increasing happiness can be tailored to three types of happiness: the pleasant life (hedonics) and two sources of eudaimonic happiness—the good life and the meaningful life (Seligman, 2002;  Seligman & Csikszentmihalyi, 2000). The pleasant life describes the pleasures that we enjoy from moment to moment. Enjoying a great meal or a beautiful sunset enhances our daily experiences. The good life occurs when we are maximizing our strengths in our work, hobbies, and other activities, making time subjectively slow down. The meaningful life describes our ability to focus away from the self to serve others or participate in something more permanent and larger than ourselves. In general, people taking tests designed to assess their involvement with these three types of happiness are happiest overall if either their good life or their meaningful life score is high. If the score in one of these aspects is high, then a high score on the pleasant life can raise overall happiness, like having a cherry on top of an already-delicious ice cream sundae. Unfortunately, those who score highest on the pleasant life, without comparable scoring in either the good or the meaningful life category or both, tend to be less happy (Seligman, 2002). Two reasons help explain why isolated pleasant life scores fail to greatly affect overall happiness. First, these scores tend to be highly heritable, which means that you can’t do much to improve your sense of pleasure. Some people seem to respond more intensely than others to pleasurable experiences. Second, these feelings do not last long and habituate quickly. Think about the last time you had a delicious dessert. The first few bites are fantastic, but the last ones are usually less so. A short time later, you might have forgotten how delicious the dessert even was. Researchers studying happiness identify three types. The pleasant life consists of pleasures, like ice cream, that are enjoyed from moment to moment. The good life features flow and a sense that time is standing still. The meaningful life describes the positive emotions obtained by helping others. Even if heritability affects our ability to experience pleasure, positive psychology has some suggestions for enhancing our experiences. How often have you eaten your meals while watching television, reading, or working on your computer? If you try to multitask in this way, it is unlikely that you will derive much pleasure or satisfaction from your food, which can lead to overeating. Learning to savor pleasurable experiences by paying more attention to them can make them more memorable and enjoyable (Bryant & Veroff, 2007). The good life is somewhat easier to modify than the pleasant life. To enhance your happiness from the good life, all you have to do is identify your strengths and find ways to use them in your important work and hobby activities. During activities that contribute to the good life, people experience flowflowA state characterized by complete absorption in a current activity, such as work, problem solving, or creativity            flow        A state characterized by complete absorption in a current activity, such as work, problem solving, or creativity            , in which they are absorbed in their current activity, usually related to work, problem solving, or creativity (Csikszentmihalyi, 1990,  1996). The word flow was chosen to describe this experience on the basis of interviews in which people spontaneously used the metaphor to explain their own experiences. These experiences may qualify as altered states of consciousness and are usually perceived as highly positive. Time appears to stand still for people who are immersed in flow. We are not suggesting that happiness can be found by spending all your time playing video games; instead, we recommend that you seek this type of flow in the work and hobbies that you enjoy. People who do not play video games are often astonished by the nearly compulsive attractiveness of the activity for those who do play. However, video games meet many of the criteria for a flow experience (Csikszentmihalyi, 1990). The games have clear goals (reaching the next level or beating a dungeon boss), they provide the opportunity to focus, they encourage a merging of action and awareness (as opposed to self-consciousness), and players generally experience a distortion of subjective time. In addition, games provide instant and precise feedback (you succeed or not), a balance between ability and difficulty (gamers reject games that are either too easy or too challenging), and a sense of personal control (you’re the one making the moves). Finally, gaming, like other flow activities, is intrinsically reinforcing. Many a parent has diabolically used access to gaming as a reinforcer for other desired behaviors, such as chores or homework. It is likely that video games, at least for some people, tap into a predisposition to find activities that meet these flow criteria to be highly reinforcing. In an effort to further understand the sources of well-being, positive psychologists have built upon the trait theories discussed in Chapter 12. Among the traits that contribute to well-being are hope, resilience, spirituality, and gratitude. Hope refers to an expectation that your goals will be met in the future. You are probably hopeful that you will complete your college education and find a good job. The trait of hope is tightly linked to feelings of optimism. Hope in turn is determined by motivation and planning (Snyder, 1994). Your expectations for a college degree are based on beliefs that you can achieve a degree through hard work and that following your plans will give you the results that you desire. Being a hopeful person provides a number of benefits to well-being. Hopeful people can withstand more pain and stress because they believe that the future will be better (Snyder et al., 2005). Hopeful people are more capable problem solvers because they are likely to consider alternative pathways for meeting their goals instead of giving up at the first obstacle. Finally, hopeful people usually enjoy strong social networks because other people find hope attractive (Snyder, Rand, & Sigmon, 2002). According to Mihaly Csikszentmihalyi, video games can be compelling because they meet many of the criteria required of a flow-inducing experience, such as clear goals, the opportunity to focus, the merging of action and awareness, and the experience of distortion of time for the player. We discussed resilience in an earlier section of this chapter. Resilience not only serves as a buffer for stress but can lead to growth following a traumatic experience (Davis & Nolen-Hoeksema, 2009). None of us likes being traumatized, but when we look back on difficult episodes of our lives, we often feel like a traumatic experience led to growth and improved strength. Among the positive changes associated with posttraumatic growth are perceptions of the self as a survivor rather than a victim, increased self-confidence and self-efficacy, and greater appreciation for the fragile nature of life. Many people respond to prior trauma with increased compassion for others, closer bonds with family and friends, and enhanced willingness to share emotions. Trauma also can lead people to reduce the value that they place on material aspects of life, such as money and possessions, while increasing their sense of what is truly meaningful about life. As we mentioned in a previous section, the experience of a meaningful life enhances well-being and happiness. Although finding meaning simply encompasses recognizing things that are bigger than one’s personal existence, some people achieve meaningfulness through religious practice (Myers, 1993). Religious beliefs and practice also have been associated with a long list of health benefits because most major world religions promote healthier lifestyles (Emmons, Barrett, & Schnitker, 2008). Many people find religious prohibitions to be a helpful tool for resisting the temptation to engage in risky, unhealthy behaviors. Although participation in religious or spiritual activities is a behavior, it is strongly predicted by a person’s need for deeper meaning, or trait spirituality (Pargament & Krumrei, 2009). Hopeful people are capable problem solvers who believe that their goals can be met in the future. The idea of individual traits related to well-being has been extended to include shared social values and experiences, resulting in values or virtues. A value can be defined as an enduring belief that a specific mode of conduct or end state of existence is personally or socially preferable to an opposite or converse mode of conduct or end state of existence (Rokeach, 1968–1969, p. 5). The voluntary nature of values is stressed by many psychologists, including Gordon Allport, who defines a value as a belief upon which a man acts by preference (Allport, 1961, p. 454). One effort to identify values combined those listed by major world religions and such organizations as the Boy Scouts, Rotary International, and Alcoholics Anonymous with data from major personality tests (Franken, 2002). The resulting values are social, cheerful, peaceful, tolerant, kind, generous, trusting, assertive, self-control, self-confidence, communication, leading, and autonomous. Additional lists have added some interesting items, such as awe and forgiving (Peterson & Seligman, 2004). Nobody wants to experience trauma, but some survivors report experiencing posttraumatic growth. Mike Atherton lost both legs and his left arm in a boating accident, but he has returned to performing with the Tampa Bay Water Ski Show Team and reports feeling optimistic about his future. One interesting difference between the positive psychologist’s discussion of values and the personality psychologist’s discussion of traits is the judgmental approach to values. In discussing personality traits, we don’t want to think of traits as necessarily good or bad. A variety of personalities, such as the introverted computer programmer and the extroverted salesperson, helps meet the needs of society. In contrast, the values discussed by positive psychologists are good, and to be without them is not so good. This deviation from being nonjudgmental represents a significant departure by the positive psychologists from the humanistic psychologists, who argue strongly against any type of outward judgment of others. Do values matter in everyday life? As we saw in Chapter 13, attitudes and behavior may or may not correlate. However, possessing certain values seems to correlate with a number of positive outcomes. Participants’ self-ratings on the 13 values mentioned earlier are positively correlated with measures of self-esteem (Franken, 2002). In addition, explosiveness, or a propensity to violence, was negatively correlated with a person’s values ratings. As ratings on the values increased, a person’s potential for violent behavior decreased. People with high values scores were the least likely to experience alcohol or tobacco addictions or to experience anxiety and depression. Organizations such as Rotary International often reinforce the values of their members, such as service above self. Positive psychologists study the roles of institutions such as workplaces, schools, and families in promoting well-being in the community. What characteristics should we strive for in our institutions to raise the likelihood that members of the community will lead satisfying lives? Positive institutions share several features (Peterson, 2006). They share a purpose in the form of institutional goals. Their rules and consequences are fair. They feature a reciprocal caring by the institution for its members and by the members for the institution. A positive institution provides safety for its members and treats them with respect. One of your goals in completing a college education is probably related to the type of work you’ll be able to do as a result. Your future working conditions will have significant influence on your overall well-being. Positive psychologists have identified three ways that people feel about their work (Wrzesniewski, McCauley, Rozin, & Schwartz, 1997). Some people just have jobs. A job is viewed as a way of supporting your family, and workers with jobs emphasize this role without asking more of their work life. A career not only pays the bills but also fulfills workers’ needs for status and achievement. Finally, a calling satisfies both personal and cooperative goals. The worker pays the bills while gaining both personal achievement and a sense of contributing to the community. When workers view their employment as a calling, they are less concerned with work conditions, salary, benefits, and status. Positive workplaces facilitate the workers’ abilities to meet their personal work-related goals. Forbes rankings of best employers for 2016 cover a wide variety of industries. Positive psychologists attempt to find common features of organizations that make them positive institutions.1. Marathon Petroleum6. SAS2. Google7. University of Iowa Hospitals & Clinics3. Costco Wholesale8. JetBlue Airways4. Wegmans Food Markets9. Mayo Clinic5. United Services Auto Association10. Seattle Children’s HospitalSchools from kindergarten through college are particularly important to the health and future of a community. In a manner similar to psychology’s emphasis on disorders, most of the research in education has focused on what goes wrong in schools rather than on what schools are doing well. Positive schools promote growth and produce a sense of satisfaction in students. Although this area is sparsely studied, student satisfaction with schools predicts a number of positive outcomes, including higher grades and fewer problem behaviors, even as early as kindergarten (Ladd, Buhs, & Seid, 2000). Significant challenges face today’s families, and identifying what works could provide guidance for many. Family-centered positive psychology (FCPP) focuses on the characteristics of positive families (Sheridan & Burt, 2009). Consistent with the general approach of positive psychology, FCPP identifies the strengths of families rather than emphasizing their failures and weaknesses. Positive psychology is not without its critics. Among the weaknesses seen in positive psychology are its unintended negative effects on people who face medical and psychological challenges (Held, 2004). Although research indicates that optimism and health are linked, this finding might give sick people the unfortunate impression that all they have to do to cure their illness is to maintain a positive attitude. A second criticism is related to the integration theme that we have featured in this textbook. Positive strengths are valuable, but they are best studied within the entire context of individuals interacting in environments. Positive and negative emotions can coexist, and we cannot fully understand one type of experience without considering the other (Cacioppo, Berntson, Norris, & Gollan, 2011). It is unlikely that psychology will ever abandon its goal of helping troubled people feel better, but positive psychology’s emphasis on what goes right is also illuminating. We do not know at this time whether the label of positive psychology will drop out of favor, like Wilhelm Wundt’s structuralism, or enjoy the fate of William James’s functionalism by becoming so integrated into mainstream psychology that a label is no longer necessary. By applying the same evidence-based criteria that we have described for evaluating interventions for negative behaviors to interventions based on positive psychology, we will figure out the role that positive psychology should play in our continued search for understanding the human mind. We have studied relationships from the vantage points of 15 psychological perspectives so far in this textbook, and we end the journey with an analysis of the role of a positive emotion—gratitude—on relationship building. Psychologists traditionally have focused on how an individual invests in relationships and report that individuals’ perceptions of how much they have already invested (time, energy, and other resources) influences how committed they feel. However, researchers know relatively little about how our perceptions of our partner’s investment in the relationship influence our commitment. When a partner invests in a relationship, the typical response is the experience of the positive emotion of gratitude. Gratitude includes the feelings of thankfulness and appreciation felt for the partner, which in turn affects the perception of who the partner is as a person (Joel, Gordon, Impett, MacDonald, & Keltner, 2013). When people experience gratitude for a partner, it stimulates further commitment. The amount of commitment that an individual experiences is linked to the perceived investment in the relationship on the part of the partner. What types of investment promote more gratitude, and through those feelings, greater commitment? You might guess that financial investments would be important, but research has shown that intangible investments, like emotional involvement, have a greater impact on commitment (Goodfriend & Agnew, 2008). This is reminiscent of the finding discussed earlier that it is not wealth that makes us happy, but rather our social connectivity. It’s often the little things that people do for their partners that result in gratitude and a stronger commitment to the relationship. In sections in the previous chapters, we have focused more on the perpetrators of cyberbullying than the victims. In considering policies at the school and community levels, preventing and providing consequences for cyberbullying are key tools. At the same time, realistically, interventions are unlikely to prevent bullying altogether. This raises a question about how we can equip young people to cope with any cyberbullying that comes their way. Earlier in this chapter, we defined resilience as the ability to adapt to life’s challenges in positive ways. Resilience recognizes that bad things happen, and yet we can manage to adapt and buffer ourselves against adversity. Online resilience has been defined as the ability to respond to a negative online experience actively, as opposed to passively, using problem-based coping to avoid further harm (Vandoninck, d’Haenens, & Roe, 2013). Among the factors that seem to help youth cope with being cyberbullied are confiding in adults, discussing the experience with another person, being digitally literate, and having parents who are knowledgeable about their children’s online environment (Papatraianou, Levine, & West, 2014). There is a theme here—the importance of social involvement that has been featured so frequently throughout this textbook. Youth who have strong social connections might still experience cyberbullying, but they are in a better position to weather the storm. Efforts to reduce cyberbullying through policies and awareness are useful, but it is unlikely that bullying will be completely stopped. Helping children to be resilient in the face of cyberbullying can help them weather the inevitable storms. Source of HappinessHow to Get ItExamplesThe pleasant life Savor your experience without trying to multitask. Avoid distractions from technology (television, smartphone, etc.) when you’re trying to enjoy a meal. The good life Find ways to incorporate your key strengths into your activities. Choose an occupation that you truly enjoy, not just one that pays the bills. The meaningful life Focus on something bigger than yourself. Volunteer for a cause that’s important to you. Be sure that you can define these terms and use them correctly.`;\n","import React from \"react\";\nimport \"./App.css\";\nimport Header from \"../Header/Header\";\nimport SideBar from \"../SideBar/SideBar\";\nimport TextView from \"../TextView/TextView\";\nimport {\n  colors,\n  NUM_SENTENCES,\n  Sentence,\n  SentenceScore,\n} from \"../../constants\";\nimport {\n  calculateScore,\n  findAllChapMatches,\n  getChapterSentences,\n  getChapterText,\n  indexOfMax,\n  isValidReplacement,\n} from \"../../utils\";\n\ninterface StateTypes {\n  wordObjs: any;\n  chapter: number;\n  sentences: Array<Sentence>;\n  currSentence: number;\n  bestChap: number;\n  chapMatches: Array<number>;\n}\n\nclass App extends React.Component {\n  state: StateTypes = {\n    sentences: [],\n    wordObjs: {},\n    chapter: 0,\n    currSentence: -1,\n    bestChap: 1,\n    chapMatches: [],\n  };\n  componentDidMount(): void {\n    this.setState({ sentences: getChapterSentences(1) });\n  }\n\n  updateWords(searchWords: Array<string>) {\n    let wordObjs: any = {};\n\n    searchWords.forEach((word: string, index: number) => {\n      let newWord = word.toLowerCase().replace(/[^\\w\\s]/g, \"\");\n      wordObjs[newWord] = {\n        color: colors[index % colors.length],\n        word: newWord,\n        enabled: true,\n      };\n    });\n\n    this.setState({ wordObjs });\n\n    this.runSearch();\n  }\n\n  runSearch(chapterNum?: number) {\n    let { wordObjs, chapter } = this.state;\n\n    if (Object.keys(wordObjs).length === 0) {\n      if (chapterNum !== undefined && chapterNum !== chapter) {\n        this.setState({ sentences: getChapterSentences(chapterNum) });\n      }\n      return;\n    }\n\n    chapter =\n      chapterNum !== undefined && chapterNum >= 0 ? chapterNum : chapter;\n\n    let chapMatches = findAllChapMatches(wordObjs);\n    let bestChap = indexOfMax(chapMatches) + 1;\n    if (chapter === 0) {\n      chapter = bestChap;\n    }\n\n    let sentCount: Array<Sentence> = getChapterText(chapter).map(\n      (value: string, sentIndex: number) => {\n        let currScore: SentenceScore = {\n          count: 0,\n          uniqueCount: 0,\n          length: value.length,\n        };\n        let uniqueWords: any = {};\n\n        // Loop through words in each sentence\n        let valueArr = value.split(\" \").map((word: string) => {\n          let wordComp = word.toLowerCase().replace(/[^\\w\\s]/g, \"\");\n          let replacement = isValidReplacement(wordObjs, wordComp);\n          if (replacement !== \"\") {\n            //Handle sentence scoring\n            if (!uniqueWords[replacement]) {\n              currScore.uniqueCount++;\n              uniqueWords[replacement] = true;\n            }\n            currScore.count++;\n\n            // Return a formatted string to make it easier to spot words in TextView\n            return `<strong style=\"color: white; background-color: ${wordObjs[replacement].color}\">${word}</strong>`;\n          }\n          return word;\n        });\n\n        //Current scoring function\n        let score = calculateScore(currScore);\n\n        return {\n          text: valueArr.join(\" \"),\n          index: sentIndex,\n          score,\n        };\n      }\n    );\n\n    this.setState({ sentences: sentCount, chapMatches, bestChap });\n  }\n\n  getTopSentences(numSent: number) {\n    const { sentences } = this.state;\n    const newSents = [...sentences];\n\n    return newSents\n      .sort((sent1: Sentence, sent2: Sentence) => sent2.score - sent1.score)\n      .slice(0, numSent)\n      .filter((sent) => sent.score > 0);\n  }\n\n  scrollTo(sentNumber: number) {\n    let { currSentence } = this.state;\n    if (currSentence >= 0) {\n      let span = document.getElementById(`sentence_${currSentence}`);\n      if (span != null) {\n        span.style.border = \"none\";\n      }\n    }\n    let span = document.getElementById(`sentence_${sentNumber}`);\n    let parent = document.getElementsByClassName(\"TextView_Container\")[0];\n    if (span === null) return false;\n    // get parent position\n    let parentRect = parent.getBoundingClientRect();\n    let parentViewableArea = {\n      height: parent.clientHeight,\n      width: parent.clientWidth,\n    };\n\n    // Where is the span\n    let spanRect = span.getBoundingClientRect();\n    // Is the span viewable?\n    let isViewable =\n      spanRect.top >= parentRect.top &&\n      spanRect.bottom <= parentRect.top + parentViewableArea.height;\n\n    // if you can't see the span try to scroll parent\n    if (!isViewable) {\n      // scroll by offset relative to parent\n      parent.scrollTop =\n        spanRect.top +\n        parent.scrollTop -\n        parentRect.top / 2 -\n        parentViewableArea.height / 2;\n    }\n\n    span.style.border = \"2px solid red\";\n\n    this.setState({ currSentence: sentNumber });\n  }\n\n  onChapterChange(index: number) {\n    this.setState({ chapter: index });\n    this.runSearch(index);\n  }\n\n  toggleEnable(wordObj: any): void {\n    let { wordObjs } = this.state;\n    wordObjs[wordObj.word].enabled = !wordObjs[wordObj.word].enabled;\n\n    this.setState({ wordObjs });\n\n    this.runSearch();\n  }\n\n  render() {\n    const { wordObjs, sentences, bestChap, chapMatches, chapter } = this.state;\n    return (\n      <div className=\"App_Container\">\n        <Header\n          bestChap={bestChap}\n          chapMatches={chapMatches}\n          onWordChange={this.updateWords.bind(this)}\n          onChapterChange={this.onChapterChange.bind(this)}\n        />\n        <div className=\"App_Content\">\n          <SideBar\n            wordObjs={wordObjs}\n            topSentences={this.getTopSentences(NUM_SENTENCES)}\n            toggleEnable={this.toggleEnable.bind(this)}\n            scrollTo={this.scrollTo.bind(this)}\n          />\n          <TextView\n            sentences={sentences}\n            chapter={chapter}\n            bestChapter={bestChap}\n          />\n        </div>\n      </div>\n    );\n  }\n}\n\nexport default App;\n","// This optional code is used to register a service worker.\n// register() is not called by default.\n\n// This lets the app load faster on subsequent visits in production, and gives\n// it offline capabilities. However, it also means that developers (and users)\n// will only see deployed updates on subsequent visits to a page, after all the\n// existing tabs open on the page have been closed, since previously cached\n// resources are updated in the background.\n\n// To learn more about the benefits of this model and instructions on how to\n// opt-in, read https://bit.ly/CRA-PWA\n\nconst isLocalhost = Boolean(\n  window.location.hostname === 'localhost' ||\n    // [::1] is the IPv6 localhost address.\n    window.location.hostname === '[::1]' ||\n    // 127.0.0.0/8 are considered localhost for IPv4.\n    window.location.hostname.match(\n      /^127(?:\\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/\n    )\n);\n\ntype Config = {\n  onSuccess?: (registration: ServiceWorkerRegistration) => void;\n  onUpdate?: (registration: ServiceWorkerRegistration) => void;\n};\n\nexport function register(config?: Config) {\n  if (process.env.NODE_ENV === 'production' && 'serviceWorker' in navigator) {\n    // The URL constructor is available in all browsers that support SW.\n    const publicUrl = new URL(\n      process.env.PUBLIC_URL,\n      window.location.href\n    );\n    if (publicUrl.origin !== window.location.origin) {\n      // Our service worker won't work if PUBLIC_URL is on a different origin\n      // from what our page is served on. This might happen if a CDN is used to\n      // serve assets; see https://github.com/facebook/create-react-app/issues/2374\n      return;\n    }\n\n    window.addEventListener('load', () => {\n      const swUrl = `${process.env.PUBLIC_URL}/service-worker.js`;\n\n      if (isLocalhost) {\n        // This is running on localhost. Let's check if a service worker still exists or not.\n        checkValidServiceWorker(swUrl, config);\n\n        // Add some additional logging to localhost, pointing developers to the\n        // service worker/PWA documentation.\n        navigator.serviceWorker.ready.then(() => {\n          console.log(\n            'This web app is being served cache-first by a service ' +\n              'worker. To learn more, visit https://bit.ly/CRA-PWA'\n          );\n        });\n      } else {\n        // Is not localhost. Just register service worker\n        registerValidSW(swUrl, config);\n      }\n    });\n  }\n}\n\nfunction registerValidSW(swUrl: string, config?: Config) {\n  navigator.serviceWorker\n    .register(swUrl)\n    .then(registration => {\n      registration.onupdatefound = () => {\n        const installingWorker = registration.installing;\n        if (installingWorker == null) {\n          return;\n        }\n        installingWorker.onstatechange = () => {\n          if (installingWorker.state === 'installed') {\n            if (navigator.serviceWorker.controller) {\n              // At this point, the updated precached content has been fetched,\n              // but the previous service worker will still serve the older\n              // content until all client tabs are closed.\n              console.log(\n                'New content is available and will be used when all ' +\n                  'tabs for this page are closed. See https://bit.ly/CRA-PWA.'\n              );\n\n              // Execute callback\n              if (config && config.onUpdate) {\n                config.onUpdate(registration);\n              }\n            } else {\n              // At this point, everything has been precached.\n              // It's the perfect time to display a\n              // \"Content is cached for offline use.\" message.\n              console.log('Content is cached for offline use.');\n\n              // Execute callback\n              if (config && config.onSuccess) {\n                config.onSuccess(registration);\n              }\n            }\n          }\n        };\n      };\n    })\n    .catch(error => {\n      console.error('Error during service worker registration:', error);\n    });\n}\n\nfunction checkValidServiceWorker(swUrl: string, config?: Config) {\n  // Check if the service worker can be found. If it can't reload the page.\n  fetch(swUrl, {\n    headers: { 'Service-Worker': 'script' }\n  })\n    .then(response => {\n      // Ensure service worker exists, and that we really are getting a JS file.\n      const contentType = response.headers.get('content-type');\n      if (\n        response.status === 404 ||\n        (contentType != null && contentType.indexOf('javascript') === -1)\n      ) {\n        // No service worker found. Probably a different app. Reload the page.\n        navigator.serviceWorker.ready.then(registration => {\n          registration.unregister().then(() => {\n            window.location.reload();\n          });\n        });\n      } else {\n        // Service worker found. Proceed as normal.\n        registerValidSW(swUrl, config);\n      }\n    })\n    .catch(() => {\n      console.log(\n        'No internet connection found. App is running in offline mode.'\n      );\n    });\n}\n\nexport function unregister() {\n  if ('serviceWorker' in navigator) {\n    navigator.serviceWorker.ready\n      .then(registration => {\n        registration.unregister();\n      })\n      .catch(error => {\n        console.error(error.message);\n      });\n  }\n}\n","import React from 'react';\nimport ReactDOM from 'react-dom';\nimport './index.css';\nimport App from './components/App/App';\nimport * as serviceWorker from './serviceWorker';\n\nReactDOM.render(\n  <React.StrictMode>\n    <App />\n  </React.StrictMode>,\n  document.getElementById('root')\n);\n\n// If you want your app to work offline and load faster, you can change\n// unregister() to register() below. Note this comes with some pitfalls.\n// Learn more about service workers: https://bit.ly/CRA-PWA\nserviceWorker.register();\n"],"sourceRoot":""}