export const text = `The rods and cones in the retina begin the process of interpreting the light energy that enters the eye. We like to think we understand reality. After all, we can see, hear, touch, smell, and taste it. We don’t live in some science fiction universe where things are not how they appear. Or do we?The human eye can see many different colors, but what does it mean to  see  a color? Is color something that is a fixed quality of an object? Is the sky really blue? Is an apple really red? Or does the human mind construct these colors from the light reflected from these objects into the eye? Consider the image of the blue/black or white/gold dress that became an Internet sensation in February 2015. A friend of a Scottish bride posted the dress worn by the bride’s mother on her Tumblr blog, leading to a discussion that engaged everyone from Justin Bieber to esteemed neuroscientists. Why do people see this photo so differently? The Dress  became an Internet phenomenon as people debated its true colors. Do you see it as black and blue? White and gold? Something else?Neuroscientists disagree about why the dress produced such different responses. The Journal of Vision prepared an entire issue ( A Dress Rehearsal for Vision Science ) devoted to explaining the dress phenomenon. A survey of 1,400 people found that 57% described the dress as blue/black (which is correct), 30% as white/gold, 11% as blue/brown, and 2% as something else (Lafer-Sousa, Hermann, & Conway, 2015). Older individuals and women were more likely to choose white/gold. These researchers believe that people choose dress colors based on their expectations regarding the lighting. If you think the dress is seen in daylight, you make different conclusions than if you think the dress is seen under artificial light. Other scientists believe there is something special about the color blue due to our considerable experience with natural lighting (Winkler, Spillmann, Werner, & Webster, 2015). Because indirect lighting and shadows are usually blue, participants are more likely to confuse blue objects with blue lighting. If you assume the light falling on the dress is somewhat blue, you will probably see it as white. Would having a color deficiency change the way a person sees the dress? See for yourself. We can reconstruct how the image would look to a person with a rare type of blue-yellow color deficiency (see Figure 5.1). Surprisingly, this has little effect, although the blue looks somewhat gray. Is the reality seen by a person with a color deficiency different from your reality? We’re going to argue that it is not reality that changes but rather the way the brain views that reality. Tritanopes  have a rare type of blue-yellow color deficiency. We can simulate how the mysterious dress would look to a tritanope. The differences are surprisingly subtle, which explains why we use the term  color deficiency  rather than  colorblindness.  This example also reminds us that a single reality (the dress) can be sensed and perceived very differently by individual minds. Humans see only a small part of the electromagnetic energy emitted from the Sun. Some animals see even less. Dogs apparently do fine seeing blues, yellows, and grays, whereas humans have evolved to see a more colorful world. The dog’s view of the world is simulated in the photo on the right. As you’ll see in this chapter, we construct models of reality from the information obtained through our senses. We like to think that we are aware of the world around us, and it is unsettling to realize that the world might be different from the representations of reality formed by the human mind. You will learn how the models built by the human mind have promoted our survival over many generations. Our models of reality are distinct from those built by the minds of other animals, whose survival depends on obtaining different types of information from their environments.Our bodies are bombarded with information during wakefulness and sleep. This information takes many forms, from the electromagnetic energy of the sun to vibrations in the air to molecules dissolved in saliva on our tongues. The process of sensationsensationThe process of detecting environmental stimuli or stimuli arising from the body.            sensation        The process of detecting environmental stimuli or stimuli arising from the body.             brings information to the brain that arises in the reality outside our bodies, like a beautiful sunset, or originates from within, like an upset stomach. Sensory systems have been shaped by natural selection, described in Chapter 3, to provide information that enhances survival within a particular niche. We sense a uniquely human reality, and one that is not shared by other animals. Your dog howls seconds before you hear the siren from an approaching ambulance because the dog’s hearing is better than yours for high-pitched sounds. Horses bolt at the slightest provocation, but they may be reacting to the vibration of an approaching car or an animal that they sense through their front hooves, a source of information that is not available to the rider. Some animals sense light energy outside the human visible spectrum. Insects can see ultraviolet light, and some snakes use infrared energy to detect their prey. Differences in sensation do occur from person to person, such as the need to wear corrective glasses or not, but they are relatively subtle. However, once we move from the process of sensation to that of perceptionperceptionThe process of interpreting sensory information.            perception        The process of interpreting sensory information.            , or the interpretation of sensory input, individual differences become more evident. For example, friends voting for different presidential candidates will come to different conclusions about who won a debate. Everyone watching the exchange sensed similar information, but each person’s perceptions are unique. Some types of snakes (vipers, boas, and pythons) can sense prey using infrared energy.Sensation begins with the interaction between a physical stimulus and our biological sensory systems. A stimulus is anything that elicits a reaction from our sensory systems. For example, we react to light energy that falls within our visual range, as we will see later in this chapter, but we cannot see light energy that falls outside that range, such as the microwaves that cook our dinner or the ultraviolet waves that harm our skin (see Figure 5.1). Before you can use information from your senses, it must be translated into a form the nervous system can understand. This process of translation from stimulus to neural signal is known as transductiontransductionThe translation of incoming sensory information into neural signals.            transduction        The translation of incoming sensory information into neural signals.            . You might think of sensory transduction as being similar to the processing of information by your computer. Modern computers transduce a variety of inputs, including voice, keyboard, mouse clicks, and touch, into a programming language for further processing. We have all had the experience of watching events with others (sensation) and then being shocked by the different interpretations we hear of what just happened (perception).Once information from the sensory systems has been transduced into neural signals and sent to the brain, the process of perception, or the interpretation of the sensory information, begins. Perception allows us to organize, recognize, and use the information provided by the senses. If you think about the most memorable advertisements you have seen lately on television or online, it is likely that they share the features of attention-getting stimuli: novelty (we don’t see talking geckos every day), change (rapid movement, use of changing colors, and the dreaded pop-up), and intensity (the sound is often louder than the program you’re watching). An important gateway to perception is the process of attention, defined as a narrow focus of consciousness. As we discuss in Chapters 6, 9, and 10, attention often determines which features of the environment influence our subsequent thoughts and behaviors. Which stimuli are likely to grab our attention? Unfamiliar, changing, or high-intensity stimuli often affect our survival and have a high priority for our attention. Unfamiliar stimuli in our ancestors’ environment might have meant a new source of danger (an unknown predator) or a new source of food (an unfamiliar fruit) that warranted additional investigation. Our sensory systems are particularly sensitive to change in the environment. Notice how you pay attention to the sound of your heating system cycling on or off but pay little attention to the noise it makes while running. This reduced response to an unchanging stimulus is known as sensory adaptationsensory adaptationThe tendency to pay less attention to a nonchanging source of stimulation.            sensory adaptation        The tendency to pay less attention to a nonchanging source of stimulation.            . High-intensity stimuli, such as bright lights and loud noises, draw our attention because the situations that produce these stimuli, such as a nearby explosion, can have obvious consequences for our safety. We rarely have the luxury of paying attention to any single stimulus. In most cases, we experience divided attention, in which we attempt to process multiple sources of sensory information. Students walk to class without getting run over by a car while texting. These divided attention abilities are limited. We simply cannot process all the information converging simultaneously on our sensory systems. To prioritize input, we use selective attention or the ability to focus on a subset of available information and exclude the rest. These abilities may be disrupted in cases of attention deficit hyperactivity disorder (ADHD;  Wimmer et al., 2015; also see Chapter 14). We refer to the brain’s use of incoming signals to construct perceptions as bottom-up processingbottom-up processingPerception based on building simple input into more complex perceptions            bottom-up processing        Perception based on building simple input into more complex perceptions            . For example, we construct our visual reality from information about light that is sent from the eye to the brain. However, the brain also imposes a structure on the incoming information, a type of processing known as top-down. In top-down processingtop-down processingA perceptual process in which memory and other cognitive processes are required for interpreting incoming sensory information            top-down processing        A perceptual process in which memory and other cognitive processes are required for interpreting incoming sensory information            , we use knowledge gained from prior experience with stimuli to perceive them. For example, a skilled reader has no trouble reading the following sentences, even though the words are jumbled:All you hvae to do to mkae a snetnece raedalbe is to mkae srue taht the fisrt and lsat letrtes of ecah wrod saty the smae. Wtih prcatcie, tihs porcses becoems mcuh fsater and esaeir. Divided attention abilities are limited. Some people believe that heads-up displays for cars assist drivers with divided attention, while others believe the displays are too distracting. How can we explain our ability to read these sentences? First, we require bottom-up processing to bring the sensations of the letter shapes to our brain. From there, however, we use knowledge and experience to recognize individual words. Many students have learned the hard way that term papers must be proofread carefully. As in our example, if the brain expects to see a particular word, you are likely to see that word, even if it is misspelled, a mistake that is unlikely to be made by the literal, bottom-up processing of a computer spell-checker. Selective attention, or our focus on a subset of input, prioritizes incoming information. However, we can sometimes be so focused that we miss important information. An astonishing 20 out of 24 expert radiologists completely missed the image of a gorilla superimposed on scans of lungs while scanning for signs of cancer. Can we predict when the mind will use bottom-up or top-down processing? There are no hard and fast rules. Obviously, we always use bottom-up processing, or the information would not be perceived. It is possible that bottom-up processing alone allows us to respond appropriately to simple stimuli, like indicating whether you saw a flash of light. As stimuli become more complicated, like reading a sentence or recognizing a friend in a crowd, we are more likely to engage in top-down processing in addition to bottom-up processing. Gustav Fechner (1801–1887) developed methods, which he called psychophysicspsychophysicsThe study of relationships between the physical qualities of stimuli and the subjective responses they produce.            psychophysics        The study of relationships between the physical qualities of stimuli and the subjective responses they produce.            , for studying the relationships between stimuli (the physics part) and perception of those stimuli (the psyche or mind part) (see Figure 5.2). Fechner’s careful methods not only contributed to the establishment of psychology as a true science but are still used in research today. Golden  rectangles, named for their proportions rather than color, appear in art and architecture dating back to ancient Greece, but why are they attractive? Gustav Fechner (1801–1887) made many attempts to link physical realities with human psychological responses. He asked people to choose which rectangles are most pleasing or least pleasing. His results indicated that the most pleasing rectangle was fourth from the right. This rectangle is the closest to having golden proportions (1:1.618). Its sides have a ratio of 13:21. The methods of psychophysics allow us to establish the limits of awareness, or thresholds, for each of our sensory systems. The smallest possible stimulus that can be detected at least 50% of the time is known as the absolute thresholdabsolute thresholdThe smallest amount of stimulus that can be detected.            absolute threshold        The smallest amount of stimulus that can be detected.            . Under ideal circumstances, our senses are surprisingly sensitive (see Figure 5.3). For example, you can see the equivalent of a candle flame 30 miles (about 48 kilometers) away on a moonless night. We can also establish a difference thresholddifference thresholdThe smallest detectable difference between two stimuli.            difference threshold        The smallest detectable difference between two stimuli.            , or the smallest difference between two stimuli that can be detected at least 50% of the time. The amount of difference that can be detected depends on the size of the stimuli being compared. As stimuli get larger, differences must also become larger to be detected by an observer. An absolute threshold is the smallest amount of sensation that can be processed by our sensory systems under ideal conditions. Moving from left to right in this image, we see that the absolute threshold for touch is the equivalent of feeling the wing of a fly fall on your cheek from a distance of 0.4 inch (about 1 centimeter), the absolute threshold for olfaction is a drop of perfume in the air filling a six-room apartment, the absolute threshold for sweetness is the equivalent of one teaspoon (about 5 grams) of sugar in two gallons (about 7.5 liters) of water (the absolute threshold for bitter tastes is even more sensitive), the absolute threshold for hearing is the equivalent of the sound of a mosquito 10 feet (about 3 meters) away, and the absolute threshold for vision is seeing a candle flame 30 miles (about 48 kilometers) away on a dark, clear night. Another example of signal detection is a jury’s decision about whether a person is guilty. Based on frequently uncertain and conflicting evidence, jurors must weigh their concerns about convicting an innocent person (false alarm) or letting a real criminal go (miss). Many perceptions involve some uncertainty. Perhaps you’re driving rather fast, and you think a distant car behind you might be a police officer. Do you slow down right away? Or do you wait until the car is close enough that you know for sure it’s a police officer? How do your personal feelings about making mistakes affect your decision? Would the cost of a ticket ruin your budget?This type of decision making can have serious implications, such as in the case of decisions made by radiologists examining the results of mammograms for signs of cancer or by intelligence officers assessing the possibility of an attack. Is there reason for concern or not? This situation is different from the thresholds described earlier because it adds the cognitive process of decision making to the process of sensation. In other words, signal detectionsignal detectionThe analysis of sensory and decision-making processes in the detection of faint, uncertain stimuli.            signal detection        The analysis of sensory and decision-making processes in the detection of faint, uncertain stimuli.             is a two-step process involving the actual intensity of the stimulus, which influences the observer’s belief that the stimulus did occur, andthe individual observer’s criteria for deciding whether the stimulus occurred. Experiments on signal detection provide insight into this type of decision making. In these experiments, trials with a single, faint stimulus and trials with no stimulus are presented randomly. The participant states whether a stimulus was present on each trial. The possible outcomes of this experiment are shown in Table 5.1. In the case of reading mammograms, we can use such experiments to help us understand why two people might respond differently, even if they were sensing the same information. Ideally, a radiologist would identify 100% of all tumors without any false alarms, but mammograms are not that easy to evaluate. A radiologist afraid of missing a tumor might identify anything that looks remotely like a tumor as the basis for more testing. Few cases of cancer would be missed (high hit rate), but many healthy patients would go through unnecessary procedures (high false alarm rate). In contrast, another radiologist might need a higher level of certainty about the presence of a tumor before asking for further tests. This would reduce the number of false alarms, but it would also run a higher risk of overlooking tumors (high miss rate). According to Fechner’s work on the difference threshold, British Olympian Zoe Smith would be more likely to notice the difference between 2- and 4-pound (between 1 to 2 kilograms) weights than the difference between her new record of 121 kilograms (266.76 pounds) in the clean and jerk event and the former record of 120 kilograms (264.56 pounds). Does this mammogram indicate the woman has cancer or not? Many decisions we make are based on ambiguous stimuli. Signal detection theory helps us understand how an individual doctor balances the risks of missing a cancer and those of alarming a healthy patient. Participant ResponseStimulus PresentStimulus AbsentYesHitFalse alarmNoMissCorrect rejectionConceptDefinitionExampleAbsolute thresholdThe smallest amount of stimulation that is detectable. Seeing light from a candle flame 30 miles away on a dark night. Difference thresholdThe smallest difference between two stimuli that can be detected. Being able to detect the difference between two different weights. Signal detectionCorrectly identifying when a faint stimulus is or is not present. A radiologist correctly detecting cancer from a mammogram.VisionVisionThe sense that allows us to process reflected light.            Vision        The sense that allows us to process reflected light.            , the processing of light reflected from objects, is one of the most important sensory systems in humans. Approximately 50% of our cerebral cortex processes visual information, in comparison to only 3% for hearing and 11% for touch and pain (Kandel & Wurtz, 2000;  Sereno & Tootell, 2005). We will begin our exploration of vision with a description of the visual stimulus, and then we will follow the processing of that stimulus by the mind into a meaningful perception.Visible light, or the energy within the electromagnetic spectrum to which our visual systems respond, is a type of radiation emitted by the sun, other stars, and artificial sources such as a lightbulb. As shown in Figure 5.4, light energy moves in waves, like the waves in the ocean. Wavelength, or the distance between successive peaks of waves, is decoded by our visual system as color or shades of gray. The height, or amplitude, of the waves is translated by the visual system into brightness. Large-amplitude waves appear bright, and low-amplitude waves appear dim. The distance between two peaks in a light wave (wavelength) is decoded by the visual system as color and the height, or amplitude, of the wave as brightness. The human visual world involves only a small part of this light spectrum (review Figure 5.1). Gamma rays, x-rays, ultraviolet rays, infrared rays, microwaves, and radio waves lie outside the capacities of the human eye.Human vision begins with the eye. The eye is roughly sphere shaped and about the size of a ping-pong ball. Its hard outer covering helps the fluid-filled eyeball retain its shape. Toward the front of the eye, the outer covering becomes clear and forms the corneacorneaThe clear surface at the front of the eye that begins the process of directing light to the retina.            cornea        The clear surface at the front of the eye that begins the process of directing light to the retina.            . The cornea begins the process of bending light to form an image on the back of the eye. Traveling light next enters the pupilpupilAn opening formed by the iris.            pupil        An opening formed by the iris.            , which is actually an opening formed by the muscles of the irisirisThe brightly colored circular muscle surrounding the pupil of the eye.            iris        The brightly colored circular muscle surrounding the pupil of the eye.             (see Figure 5.5). The iris, which means  rainbow  in Greek, adjusts the opening of the pupil in response to the amount of light present in the environment and to signals from the autonomic nervous system, described in Chapter 4. Arousal is associated with dilated pupils, while relaxation is associated with more constricted pupils. Light entering the eye travels through the cornea, the pupil, and the lens before reaching the retina. Among the landmarks on the retina are the fovea, which is specialized for seeing fine detail, and the optic disk, where blood vessels enter the eye and the optic nerve exits the eye. Directly behind the pupil and iris is the main optical instrument of the eye, the lenslensThe clear structure behind the pupil that bends light toward the retina.            lens        The clear structure behind the pupil that bends light toward the retina.            . Muscles attached to the lens can change its shape, allowing us to accommodate, or adjust our focus to see near or distant objects. Behind the lens is the main chamber of the eye, and located on the rear surface of this chamber is the retinaretinaLayers of visual processing cells in the back of the eye            retina        Layers of visual processing cells in the back of the eye            , a thin but complex network of neurons specialized for the processing of light. Located in the deepest layer of the retina are specialized receptors, the rods and cones, that transduce the light information. However, before light reaches these receptors, it must pass through layers of blood vessels and neurons. We normally do not see the blood vessels and neural layers because of sensory adaptation. As we mentioned previously in this chapter, adaptation occurs when sensory systems tune out stimuli that never change. Because the blood vessels and neural layers are always in the same place, we see them only under unusual circumstances, such as during certain ophthalmology (eye) tests. We can identify several landmarks on the surface of the retina. The blood vessels serving the eye and the axons that leave the retina to form the optic nerve exit at the optic disk. Because there are no rods and cones in the optic disk, each eye has a blind spot. Normally, we are unaware of our blind spots because perception fills in the missing details. However, if you follow the directions in Figure 5.6, you should be able to experience your own blind spot. Toward the middle of the retina is the foveafoveaAn area of the retina that is specialized for highly detailed vision.            fovea        An area of the retina that is specialized for highly detailed vision.            , which is specialized for seeing fine detail. When we stare directly at an object, the image of that object is projected onto the fovea. The fovea is responsible for central vision, as opposed to peripheral vision, which is the ability to see objects off to the side while looking straight ahead. There are no photoreceptors in the optic disk, producing a blind spot in each eye. We do not see our blind spots because our brain fills in the hole. You can demonstrate your blind spot by holding your textbook at arm’s length, closing one eye, focusing your other eye on the dot, and moving the book toward you until the stack of money disappears. The image projected on the retina is upside down and reversed relative to the actual orientation of the object being viewed (see Figure 5.7). You can duplicate this process by looking at both sides of a shiny spoon. In the convex (or outwardly curving) side, you see your image normally. In the concave (or inwardly curving) side, you see your image as your retina sees it. Fortunately, the visual system easily decodes this image and provides realistic perceptions of the actual orientations of objects. The image projected on the retina is upside down and reversed, but the brain is able to interpret the image to perceive the correct orientation of an object. RodsRodsA photoreceptor specialized to detect dim light            Rods        A photoreceptor specialized to detect dim light             and conesconesA photoreceptor in the retina that processes color and fine detail.            cones        A photoreceptor in the retina that processes color and fine detail.             are named after their shapes. The human eye contains about 90 million rods and between 4 million and 5 million cones. Rods and cones are responsible for different aspects of vision. Rods are more sensitive to light than cones, and they excel at seeing dim light. As we observed previously, under ideal circumstances, the absolute threshold for human vision is the equivalent of a single candle flame from a distance of 30 miles (about 48 kilometers; see  Hecht, Shlaer, & Pirenne, 1942). Rods become more common as we move from the fovea to the periphery of the retina, so your peripheral vision does a better job of viewing dim light than your central vision does (see Figure 5.8). Before the development of night goggles, soldiers patrolling in the dark were trained to look to the side of a suspected enemy position rather than directly at their target. In humans, cones, indicated by red, blue, and green dots, become less frequent as you move from the fovea to the periphery of the retina. The colors of the dots representing cones indicate the colors to which each shows a maximum response (see Figure 5.12). Rods (light brown dots) and cones are named according to their shapes. This extraordinary sensitivity of rods has costs. Rods do not provide information about color, nor do they provide clear, sharp images. Under starlight, normal human vision is 20/200 rather than the normal daylight 20/20. In other words, an object seen at night from a distance of 20 feet would have the same clarity as an object seen in bright sunlight from a distance of 200 feet. Cones function best under bright light and provide the ability to see both sharp images and color. The rods and cones are the only true receptors of the visual system. When they absorb light, they trigger responses in four additional layers of neurons within the retina. Axons from the final layer of cells, the ganglion cells, leave the back of the eye to form the optic nerveoptic nerveThe nerve exiting the retina of the eye.            optic nerve        The nerve exiting the retina of the eye.            . The optic nerves cross the midline at the optic chiasm (named after its X shape, or the Greek letter chi). At the optic chiasm, the axons closest to the nose cross over to the other hemisphere, while the axons to the outside proceed to the same hemisphere. This partial crossing means that if you focus straight ahead, everything to the left of center in the visual field is processed by the right hemisphere, while everything to the right of center is processed by the left hemisphere. This organization provides us with significant advantages when sensing depth, which we discuss later in the chapter. Beyond the optic chiasm, the visual pathways are known as optic tractsoptic tractsNerve pathways traveling from the optic chiasm to the thalamus, hypothalamus, and midbrain.            optic tracts        Nerve pathways traveling from the optic chiasm to the thalamus, hypothalamus, and midbrain.             (see Figure 5.9). About 90% of the axons in the optic tracts synapse in the thalamus. The thalamus sends information about vision to the amygdala and the primary visual cortex in the occipital lobe. The amygdala uses visual information to make quick emotional judgments, especially about potentially harmful stimuli. The remaining optic tract fibers connect with the hypothalamus, where their input provides information about light needed to regulate sleep–wake cycles, discussed in Chapter 6, or with the superior colliculi of the midbrain, which manage a number of visually guided reflexes, such as changing the size of the pupil in response to light conditions. Visual information from the retina travels to the thalamus and then to the primary visual cortex in the occipital lobe. The primary visual cortex begins, but by no means finishes, the processing of visual input. The primary visual cortex responds to object shape, location, movement, and color (Hubel & Livingstone, 1987;  Hubel & Wiesel, 1959;  Livingstone & Hubel, 1984). Two major pathways radiating from the occipital cortex into the adjacent temporal and parietal lobes continue the analysis of visual input. The parietal pathway helps us process movement in the visual environment. The temporal pathway responds to shape and color and contributes to our ability to recognize objects and faces.To see something requires the brain to interpret the information gathered by the eyes. How do you know your sweater is red or green, based on the information sent from the retina to the brain? How do you recognize your grandmother at your front door?Most of us think about colors in terms of the paints and crayons we used in elementary school. Any kindergartner can tell you that mixtures of red and yellow make orange, red and blue make purple, and yellow and blue make green. Mixing them all together produces a lovely muddy brown. Colored lights, however, work somewhat differently (see Figure 5.10). The primary colors of light are red, green, and blue, and mixing them together produces white light, like sunlight. If you have ever adjusted the color on your computer monitor or television, you know that these devices also use red, green, and blue as primary colors. Observations supporting the existence of three primary colors of light gave rise to a trichromatic theorytrichromatic theoryA theory of color vision based on the existence of different types of cones for the detection of short, medium, and long wavelengths            trichromatic theory        A theory of color vision based on the existence of different types of cones for the detection of short, medium, and long wavelengths             of color vision. The primary colors of paint might be red, yellow, and blue, but in the world of light, the primary colors are red, green, and blue. Trichromatic theory is consistent with the existence of three types of cones in the retina that respond best to short (blue), medium (green), or long (red) wavelengths. Our ultimate experience of color comes not from the response of one type of cone but from comparisons among the responses of all three types of cones (see Figure 5.11). Our perception of color results from a comparison of the responses of the red, green, and blue cones to light. A 550-nanometer light is perceived as yellow and produces a strong response in green cones, a moderate response in red cones, and little response in blue cones. Color deficiency occurs when a person has fewer than the typical three types of cones. We no longer use the term colorblind, as this is not accurate. Most people with color deficiencies see color differently than someone with all three cone types. Very rarely, individuals have either one type of cone or none. To these people, the world appears to be black, white, and gray. Trichromatic theory does a good job of explaining color deficiency, but it is less successful in accounting for other color vision phenomena, such as color afterimages. For example, if you stare at the yellow, green, and black flag in Figure 5.12 and then focus on the dot within the white rectangle to the right, you will see an afterimage of the American flag in its more traditional colors of red, white, and blue. If you stare at the dot in the center of the yellow, green, and black flag for a minute and then shift your gaze to the dot in the white space on the right, you should see the flag in its traditional red, white, and blue. An opponent process theoryopponent process theoryA theory of color vision that suggests we have a red-green color channel and a blue-yellow color channel in which activation of one color in each pair inhibits the other color            opponent process theory        A theory of color vision that suggests we have a red-green color channel and a blue-yellow color channel in which activation of one color in each pair inhibits the other color             of color vision does a better job than the trichromatic theory in explaining these color afterimages. This theory proposes the existence of color channels: a red–green channel and a blue–yellow channel. We cannot see a color like reddish green or bluish yellow because the two colors share the same channel. The channels are  opponent  or competing. Activity in one color group in a channel reduces activity in the other color group. Returning to our green, yellow, and black flag, how can we use opponent process theory to explain our experience of the red, white, and blue afterimage? By staring at the flag, you fatigue some of your visual neurons. Because the color channels compete, reducing activity in one color group in a channel, such as green, increases activity in the other group, which is red. Fatiguing green, black, and yellow causes a rebound effect in each color channel, and your afterimage looks red, white, and blue. (Black and white also share a channel.) If you stare at an image of a real red, white, and blue flag and then look at a white piece of paper, your afterimage looks like our green, black, and yellow illustration. Which of these two theories of color vision, trichromatic theory or opponent process theory, is correct? The trichromatic theory provides a helpful framework for the functioning of the three types of cones in the retina. However, as we move from the retina to higher levels of visual analysis, the opponent process theory seems to fit observed phenomena neatly. Both theories help us understand color vision but at different levels of the visual system. Monty Roberts, known as the Horse Whisperer, attributes his abilities to observe horse behavior to his complete lack of color vision. This condition is quite rare, occurring in only 1 person out of every 30,000. Now that you have an understanding of color perception, we can consider one of the practical problems associated with individual differences in color vision. Between 7% and 10% of males and about 0.4% of females have a form of red–green color deficiency (see Figure 5.13). Males are more affected than females because the genes for the pigments used by red and green cones are located on the X chromosome, making red–green color deficiency a sex-linked condition (see Chapter 3). Smaller numbers of people lack blue cones (0.0011%) or cones altogether (0.00001%). Given the frequency of color deficiency, making visual materials accessible to people with all types of color vision is a serious concern. The Ishihara Color Test, designed by Shinobu Ishihara in 1917, is a standard method for detecting color deficiency. The test is printed on special paper, so the recreated image here would not be considered a valid basis for diagnosing color deficiency. Color can be an effective tool for designing exciting and engaging websites, but many graphic web designers, who typically have excellent vision, fail to consider how the site might look to a person with a color deficiency. One clue for designing an accessible site can be found in other systems based on color, such as traffic lights. Although most of us rely on the color information from the red, yellow, or green lights, the lights also vary in location. In other words, color should never be the only basis for extracting meaning. A second major concern is contrast, which we discuss in the next section. The strong contrast between black letters on the white pages makes text easy to read for most people. Colored text against a colored background might add interest, but it runs the risk of being harder to read, especially when reds and greens are used. As shown in Figure 5.14, online resources simulate how a web page looks to a person with color deficiency, which helps designers maximize accessibility. Web designers have found colors that work for people with typical color vision and people who have color deficiency. This set of colors shows how different shades would be seen by people with typical vision and by people with three of the most common forms of color deficiency. Even though these colors are seen differently by the three groups, nobody mistakes one shade for another. We asked earlier how your brain uses incoming visual signals to recognize your grandmother. A bottom-up approach assumes that as information moves from the retina to higher levels of visual processing, more complicated responses are built from simpler input. In this hierarchical model, the result would be a hypothetical  grandmother cell,  or a single cell that could combine all previous input and processing to tell you that your grandmother is at the door. Although the hierarchical model is attractive in many ways, it does not fit perfectly with what we know about the visual system. First, we would need a large number of single neurons to respond to all the objects and the events that we can recognize visually. In addition, the hierarchical model is unable to account for top-down processing. Figure 5.15a may appear to be a random pattern of black dots on a white background. Figure 5.15b may not appear to be a recognizable object at all. The sensations produced by these stimuli lead to no meaningful perceptions. However, once we tell you that the first image is a Dalmatian dog and the second image is a cow, you can instantly pick out their shapes. Now that you know what the images are, you will probably never see them again the way you did initially. Recognizing these objects requires knowledge and memory of what Dalmatians and cows look like. It is unlikely that a single cortical cell acting as a Dalmatian or cow detector could incorporate such complex inputs from memory.(a) This might look like a splattering of black dots on a white page until you learn that it represents a Dalmatian dog. (b) Top-down processing ensures that once you know this is a photo of a cow, you can pick out its features easily. If we don’t use single cells to recognize objects, how can we accomplish this task? The visual system might perform a mathematical analysis of the visual field (De Valois & De Valois, 1980). While the hierarchical model implies a reality built out of individual bars and edges, the mathematical approach suggests that we analyze patterns of lines. The simplest patterns of lines are gratings, as shown in Figure 5.16. Gratings can vary along two dimensions: frequency and contrast. High-frequency gratings have many bars in a given distance and provide fine detail, while low-frequency gratings have relatively few bars. High-contrast gratings have large differences in intensity between adjacent bars, like black next to white. The print you are reading in is an example of high contrast because the black letters are quite different from the white background. Low-contrast gratings have subtler differences in intensity between bars, such as dark gray next to black. An alternative to the hierarchical model suggests that the visual system analyzes the visual environment as a collection of patterns, like these gratings. Gratings vary in frequency (number of bars in a given distance) and contrast (the difference in light intensity from one bar to the next). Observing responses to gratings gives us a window into the visual capacities of other species. At a certain point of contrast and frequency, gratings look plain gray. Animals can be trained to make a distinction between gratings and gray circles. For example, if a bird is rewarded with food for pecking at a disk with a grating but not for pecking a uniform gray disk, any performance that is better than 50-50, or chance, indicates that the bird can see the difference between the grating and the gray. We can graph the range of gratings that are visible to the observer as a function of their contrast and frequency. Figure 5.17 illustrates the visible ranges for human adults and cats. Compared to human adults, cats see less detail. However, cats see large (low-frequency), low-contrast objects better than humans do. Large, low-contrast shadows on the wall may get kitty’s attention but not yours. You will think kitty is chasing ghosts again. Using gratings, we get a window into the visual world of the cat. By comparing gratings to a uniform gray disk, we can learn when a grating with a certain contrast and frequency simply looks gray to humans or cats. We can see better detail than kitty, but she sees large shadows that we don’t even notice. As we observed in Chapter 1, a group of German researchers known as the Gestalt psychologists tackled visual perception with a number of ingenious observations. The word Gestalt is derived from the German word for  shape.  These psychologists objected to efforts by Wilhelm Wundt and the structuralists to reduce human experience to its building blocks, or elements. Instead, the Gestalt psychologists argued that some experiences lose information and value when divided into parts. The main thesis of the Gestalt psychologists, as stated by Kurt Koffka, maintains,  It is more correct to say that the whole is something else than the sum of its parts  (Koffka, 1935, p. 176). According to the Gestalt psychologists, we are born with built-in tendencies to organize incoming sensory information in certain ways. This natural ability to organize simplifies the problem of recognizing objects (Biederman, 1987). One organizing principle suggests that we spontaneously divide a scene into a main figure and ground. We frequently assume that the figure stands in front of most of the ground, and it seems to have more substance and shape. It is possible to construct ambiguous images, like the vase on this page, in which the parts of the image seem to switch roles as figure or ground. The Gestalt psychologists believe we naturally see the difference between objects and their background, but this figure is designed to make us switch back and forth from the vase to the background faces. This vase was designed to commemorate an anniversary of Queen Elizabeth II of England (face on the right) and her husband, Prince Philip (face on the left). A second Gestalt principle is proximity (see Figure 5.18). Objects that are close together tend to be grouped together. The dots that make up our Dalmatian in Figure 5.17a are close together, suggesting they belong to the same object. The principle of similarity states that similar stimuli are grouped together. On a close examination of the dog image, the dots that make up the dog are similar to one another and slightly different (more rounded perhaps) than the dots making up the remainder of the image. The set of dots in (a) do not appear to have any particular relationship with one another, but when we color rows in (b), we suddenly see the dots in columns or rows. Moving two columns or rows slightly closer to each other in (c) makes us see the array differently too. The principle of continuity suggests that we assume that points that form smooth lines when connected probably belong together (see Figure 5.19). In our dog picture, continuity helps us see the border of the curb or sidewalk and the ring of shadow around the base of the tree. Continuity is perhaps a little less useful in identifying the dog, although we can pick out the lines forming the legs. The Gestalt principle of continuity says that we perceive points forming a smooth line as belonging to the same object. If you follow this knot, you can see that it is formed by two objects, but our initial perception is of a single form. Closure occurs when people see a complete, unbroken image even when there are gaps in the lines forming the image (see Figure 5.20). We use this approach in viewing the dog in Figure 5.15a when we  fill in the blanks  formed by the white parts of its body. Because of the principle of closure, we  fill in the blanks  to see a single object, the World Wildlife Fund logo, although it is made up of several objects. Finally, the Gestalt psychologists believed in the principle of simplicity, which suggests that we will use the simplest solution to a perceptual problem. This principle may help explain the fun in pictures like that of our Dalmatian dog. It is simpler to assume that this is a random splash of black dots on white background. Finding a hidden picture within the dots is not the simplest solution, which may account for our surprise. An image projected onto the retina is two dimensional, as flat as the sheet of paper or screen on which these words appear. Somehow, the brain manages to construct a three-dimensional (3D) image from these data. Adelbert Ames constructed a room that was named in his honor, the Ames Room, which illustrated vulnerabilities in our depth perceptiondepth perceptionThe ability to use the two-dimensional image projected on the retina to perceive three dimensions            depth perception        The ability to use the two-dimensional image projected on the retina to perceive three dimensions             (Ittleson, 1952). When viewed directly from the front, the room appears to be a rectangle. People within the room, shown in Figure 5.21, seem to be larger or smaller than normal. This distortion of perceived size results from the room’s ability to confuse our judgment of distance. Many distance cues, such as the apparently rectangular windows, conspire to make these two people look different. The person on the right is much closer to us than the person on the left. The diagram shows the actual layout of the Ames Room. To construct a 3D image, we use both monocular cuesmonocular cuesA depth cue that requires the use of only one eye            monocular cues        A depth cue that requires the use of only one eye             (one eye) and binocular cuesbinocular cuesA depth cue that requires the use of both eyes            binocular cues        A depth cue that requires the use of both eyes             (two eyes). Many monocular cues are found in paintings because the artists attempt to provide an illusion of depth in their two-dimensional pieces. The use of linear perspective, or the apparent convergence of parallel lines at the horizon, by Italian artists during the 15th century provided a realism unknown in earlier works. Linear perspective revolutionized the video game and movie industries, beginning humbly with Sega’s Zaxxon in 1982 and advancing to the ever more realistic environments of Halo, Pixar’s animated films, and the 2009 film Avatar. Other monocular cues include texture gradients and shading. We can see more texture in objects that are close to us, while the texture of distant objects is relatively blurry. Shading and the use of highlights can be used to suggest curved surfaces. Among the most powerful monocular depth cues is occlusion, or the blocking of images of distant objects by closer objects. We also use relative size to judge the distance of objects, although this method requires you to be familiar with the real size of an object. We know how big people are. When the retinal image of a person is small, we infer that the person is farther from us than when the retinal image of a person is larger. Several illusions result from our use of monocular cues to judge depth. Relative size helps to explain the moon illusion. You may have noticed that the moon appears to be larger when it is just above the hills on the horizon than when it is straight overhead. The moon maintains a steady orbit 239,000 miles (385,000 km) above the Earth. How can we account for the discrepancy in its apparent size? When viewed overhead, the moon is seen without intervening objects, such as trees and hills, that might provide cues about its size and distance. However, when viewed near the horizon, we see the moon against a backdrop of familiar objects whose sizes we know well. We expect trees and hills to be smaller at the horizon than when they are close to us, and if we group the moon with those objects, we adjust its apparent size as well. The next time you are viewing the full moon as it rises over the hills, form a peephole with your hand, and you will see the moon in its normal small size. Although some researchers argue that atmospheric differences between the two viewpoints may contribute to the illusion, viewing the moon through your hand should demonstrate that most of the effect arises from your use of other objects to judge distance. In the Müller–Lyer illusion, shown in Figure 5.22, we see the line with outward-pointing arrowheads as being farther from our position, even though the main lines project images of equal length on the retina. The Ponzo illusion, shown in Figure 5.23, confounds size and distance judgments in a similar fashion. The parallel lines signal depth, leading us to believe that the upper horizontal line is farther away than the lower line. If both lines project the same image on the retina, the more distant line must be longer. You might find it hard to believe that the two red vertical lines are actually the same length. We perceive depth because of linear perspective, which in turn make us see the upper horizontal bar as more distant than the lower bar. Even though they are the same length, the bar perceived as more distant looks longer. So far, we have discussed monocular cues that involve a person and a scene that is not moving. The introduction of motion can heighten the impression of depth. As you ride in a car, focus your gaze at a distant point. The objects you pass will appear to be moving in the opposite direction of your car, with closer objects appearing to move faster than distant objects. Next, focus on a point about midway between you and the horizon. Now, the closer objects will continue to move in the opposite direction, but more distant objects appear to be traveling with you. This motion parallax has been used to enhance the 3D feeling in video games. One of our most effective depth cues is retinal disparityretinal disparityThe difference between the images projected onto each eye            retinal disparity        The difference between the images projected onto each eye            . Because this cue requires the use of both eyes, we refer to retinal disparity as a binocular cue. Predator species, including ourselves, have eyes placed in the front of the head facing forward. As a result of this configuration, the visual scenes observed by the two eyes are different and overlapping, as shown in Figure 5.24. The differences between the images projected onto each eye are called disparities. These disparities do not tell us how far away an object is. Instead, they provide information about the relative distance between two objects in the visual field. As the distance between the objects increases, disparity increases. To illustrate the sensitivity of this system, you can identify an object as being 1 millimeter (about .04 inch) closer than another at a distance of 1 meter (about 3.3 feet) from your body, or a difference of 0.1% (Blake & Sekuler, 2006). The right and left eyes see slightly overlapping versions of the visual scene in front of us. We can use the retinal disparity, or discrepancy between the locations of two objects on the two retinas, as a sensitive depth cue. Why would this binocular depth system be an advantage to predators? Most prey species do an excellent job of hiding, often aided by an appearance that blends into the nearby environment. However, retinal disparity allows us to spot tiny variations in the depths of objects in the visual field. This might make an animal stand out against its background, even though it is well camouflaged. Retinal disparity has been used to identify camouflaged military equipment and counterfeit currency. Retinal disparity is imitated by cameras used to film 3D movies, which have two lenses separated by about the same distance as our two eyes.Although human infants can’t report what they see, we can take advantage of their longer gazing at patterns than at uniform stimuli, like a patch of a single color. This allows us to construct graphs of the contrasts and the frequencies to which children respond, similar to those we saw previously for cats. Based on these analyses, we know that human infants see everything human adults see but with less detail. To see well, the infant also needs more contrast than the adult. These findings help explain children’s preferences for large, high-contrast objects. Video games, such as Minecraft, incorporate many monocular cues to provide an experience of depth. The standard-sized blocks used to build structures in the game are separated by lines, which when placed in a row provide linear perspective. Texture gradients, shading, and relative size (players understand the size of the blocks well) also contribute to perceived depth. The two lenses of 3D cameras are separated by about 2 inches (about 50 millimeters), mimicking the distance between two human eyes that makes retinal disparity possible. The photographs shown below provide insight into the visual world of the infant. Frequencies that cannot be seen by the infant have been removed from each photograph. Other research shows that infants as young as 4 months not only show binocular disparity, but also show normal adult responses to color (Bornstein, Kessen, & Weiskopf, 1976). Other depth cues discussed previously develop early too. Infants as young as 2 months understand occlusion (Johnson & Aslin, 1995), and the use of the relative size of objects to judge depth appears between the ages of 5 and 7 months (Granrud, Haake, & Yonas, 1985). Infants’ abilities to perceive faces also develop quite rapidly, as 2-day-old newborns spend more time gazing at their mothers’ faces than at a stranger’s face (Bushnell, 2001; also see Chapter 11). Predictable changes occur in other aspects of human vision as we grow older. Accommodation of the lens, which allows us to change focus from near to far objects, becomes slower beginning in middle adulthood. Older adults respond more slowly to changes in brightness, such as leaving a dark theater into the sunlight. The muscles of the iris lose their elasticity, so pupils remain smaller, further reducing vision by limiting the amount of light that enters the eye. The lens of the eye begins to yellow, which protects the eye from ultraviolet radiation but affects the perception of color. At any age, individual differences shape what people see. In addition to the color deficiencies discussed previously, people differ in their abilities to see near and far objects. Those who deviate from the average often wear corrective lenses or undergo laser surgery to reshape the cornea. The most common visual problems result from eyeball length, with elongated eyeballs interfering with a person’s vision for distant objects (nearsightedness) and shortened eyeballs interfering with vision for close-up objects (farsightedness), as in reading. Vision is also affected by astigmatism, which means that the surface of the cornea is uneven. You can test yourself for astigmatism by looking at Figure 5.25. We can filter out the frequencies and contrast that a baby cannot see to simulate what the world looks like to an infant. If you have astigmatism, which results from an uneven surface of your corneas, some spokes of this figure will appear darker than others. In addition to deficits in social skills and language (see Chapter 14), individuals with autism spectrum disorder (ASD) often demonstrate unusual responses to stimuli. In some cases, children seem to barely notice things, but in others, children seem to be hypersensitive to stimulation. Psychologists agree that diagnosing autism as early as possible leads to the best treatment outcomes. Identifying differences in sensation and perception at early ages might lead to better diagnoses (Gliga et al., 2015). The Question: Do superior visual search skills predict autism?Eighty-two infants with older siblings diagnosed with ASD (high-risk) and 27 control infants with no relatives with ASD took part in a visual search task and measures of ASD risk (response to name, eye contact, social reciprocity, and imitation). Eye-tracking was used to measure the child’s attending to the  odd  character within a visual array (Figure 5.26). Children were assessed at 9 months, 15 months, and 2 years of age. Eye-tracking showed that 9-month-old infants at risk for autism spectrum disorder (ASD) were able to find the  odd  character within each display faster than children at low risk for ASD. Performance on this visual search task predicted the severity of autism symptoms at the ages of 15 months and 2 years. Children enjoy extra protection when they serve as research participants. Researchers must present parents with a thorough informed consent form, and great care must be taken to avoid coercion through rewards for participation. Procedures must be suited to the young participants to avoid fatigue. Parents whose children show evidence of risk for ASD should be referred to services. Superior performance on the visual search task at 9 months was correlated with indicators of ASD symptoms at 15 months and 2 years of age. Assessments of visual search might prove useful in identifying infants who may be diagnosed with autism spectrum disorder. The results also emphasize the role of attention and perception in the development of ASD. Previously, much research about ASD has focused on social deficits, but these might in fact emerge from more basic differences in information processing. Psychologists have discovered a number of differences in the ways that East Asians (e.g., people from China, Korea, and Japan) and European/North Americans process information. In general, Eastern cultures process information holistically, which means processing is based on context and relationships. In contrast, Western cultures process information analytically, focusing on salient objects and forming categories. To illustrate this difference, consider the following images (Chiu, 1972). Of the three images in each box, think about which go together, and ask yourself why you think this is correct. Children from East Asian cultures were most likely to group the woman shape with the baby shape and the cow with the grass, whereas children from Western cultures grouped the man and woman shape and the cow and chicken shape. The East Asian children reported grouping the woman and the baby together because the mother takes care of the baby (relationship), whereas the Western children grouped the man and woman together because they shared a category as adults. East Asian children grouped the cow and grass together (context: the cow eats grass), whereas the Western children grouped the chicken and cow together (both are animals). Further research demonstrated that all children begin by using holistic reasoning regardless of culture, but Western children begin to ignore context and focus more on objects beginning around the age of 5 years. What possible aspects of culture might shape thinking in this way? It is possible that child-rearing practices, such as the emphasis on appropriate social behavior in East Asian cultures versus the emphasis on labeling objects in Western culture, could lead to a divergent emphasis on context or object. The roots of this type of thinking might go even deeper. East Asians and Westerners have been found to use different eye movement strategies while scanning faces (Kelly, Miellet, & Caldara, 2010). Eye-tracking shows that Westerners focus on the eye and mouth regions of a face, while East Asians focus on the nose area (Figure 5.27a and b). This difference might arise from social norms. Eye contact is considered rude in East Asian cultures but is expected in Western cultures. However,  Kelly et al. (2010) found the same cultural differences when participants looked at the faces of sheep and at artificial figures known as Greebles.(a) Which Ones Go Together? Western children group the man and woman shapes and the cow and chicken shapes using categorical reasoning. East Asian children group the woman and baby shapes and the cow and grass together based on relationships and context. (b) Western and East Asian Participants Use Different Face Scanning Approaches Eye-tracking shows that Western participants focus on the eyes and the mouth of a face, whereas East Asian participants focus on the nose. To control for possible social norm influences (eye contact is considered rude in some East Asian cultures), the researchers investigated scanning approaches using sheep and make-believe stimuli known as Greebles. The same principles held for these alternate stimuli, possibly reflecting cultural differences in the emphasis on objects and context. While researchers often attribute these processing differences to the effects of living within an individualistic (Western) versus collectivistic (Eastern) culture, we cannot rule out the effects of genetics and biology without further research. Repeating these studies with second or third generation Asian Americans, for example, might provide insight into the extent of the cultural influences. Anyone can tell, simply by looking, whether two tabletops are the same shape, right? For instance, most people would agree that the shapes and sizes of the two tabletops depicted in Figure 5.28 are different. One tabletop appears to be rectangular, while the other appears to be more square. As much as our perceptions may tell us otherwise, these tabletops are identical. To verify this, trace one of the tabletops, and then rotate it to place it above the other. You will be able to prove that these tables have identical tops. It is not our eyes (sensory receptors) that deceive us; it is our brains. You will probably want to use a ruler to prove to yourself that the lines marked  a  are the same and that the lines marked  b  are the same. Roger Shepard combined a profound scientific curiosity with a love of mischief. Shepard developed a number of creative visual illusions, including  Turning the Tables,  which he illustrated himself (Shepard, 1990). The visual illusion produced in this illustration results from our use of a visual system designed to cope with the three dimensions of the physical world on stimuli that have only two dimensions. In Roger Shepard’s words:The drawings … achieve their effects by means of various visual tricks. But to call them tricks is not to imply that they are without psychological significance. The tricks work by taking advantage of fundamental perceptual principles that have been shaped by natural selection in a three-dimensional world. Our ability to make pictures, which emerged only recently on an evolutionary time scale, enables us to present the eyes with visual patterns that systematically depart from the patterns that we and our ancestors experienced in nature. In considering the ways pictures can trick the eye, we can gain insight into the nature and ultimate source of the principles of visual perception. (Shepard, 1990, p. 121)More generally, examples like this underscore the importance of relying on scientific investigation and evidence to unveil how sensation and perception work. FeatureSignificanceCorneaBends light toward the retina. PupilForms an opening in the iris. LensFocuses light onto the retina. RetinaContains rods, cones, and other visual neurons in its layer of cells. Fovea (area of the retina)Processes detailed vision. ThalamusActs as the target for most axons forming the optic tracts. Primary visual cortex (area in the occipital lobe)Receives visual input from the thalamus and performs initial analysis of input.We have spent a considerable amount of time on the sense of vision, which might be considered a dominant source of information for humans. However, when Helen Keller, who was both blind and deaf, was asked which disability affected her the most, she replied that blindness separated her from things, while deafness separated her from people. AuditionAuditionThe sense of hearing.            Audition        The sense of hearing.            , our sense of hearing, not only allows us to identify objects in the distance but also plays an especially important role in our ability to communicate with others through language.Sound begins with the movement of an object, setting off waves of vibration in the form of miniature collisions between adjacent molecules in air, liquid, or solids. Because sound waves require this jostling of molecules, sound cannot occur in the vacuum of space, which contains no matter. Those explosions we enjoy in Star Wars films are great entertainment but not good science. Earlier in this chapter, we described light energy as waves with different amplitudes and frequencies. Sound waves possess the same dimensions. However, in the case of sound, the height or amplitude of the wave is encoded as loudness or intensity and the frequency of the wave is encoded as pitch. High-amplitude waves are perceived as loud, and low-amplitude waves are perceived as soft. High-frequency waves (many cycles per unit of time) are perceived as high pitched, whereas low-frequency sounds are low pitched. In sound, amplitude is measured in units called decibels (dB), and frequency is measured in cycles per second, or hertz (Hz; see Figure 5.29). Like the light energy we see, sound waves are characterized by frequency and amplitude. We perceive frequency as the pitch of the sound (high or low), measured in hertz (Hz), and we perceive amplitude as the loudness of the sound, measured in decibels (dB). In addition to dizziness and nausea, being exposed to infrasound makes people report feelings of chills down the spine, fear, and revulsion, even though they cannot consciously detect the sound. Some scientists believe that infrasound produced in certain places leads people to conclude the places are haunted. As we observed in the case of the light spectrum, parts of the auditory spectrum are outside the range of human hearing. Ultrasound stimuli occur at frequencies above the range of human hearing, beginning around 20,000 Hz (see Figure 5.30). Ultrasound can be used to clean jewelry or your teeth or to produce noninvasive medical images. Infrasound refers to frequencies below the range of human hearing, or less than 20 Hz. Many animals, including elephants and marine mammals, use infrasound for communication. Infrasound is particularly effective in water because it allows sound to travel long distances. Ultrasounds are above the range of human hearing, and infrasounds are below the range of human hearing.Human audition begins with an ear located on either side of the head. The components that make up the ear are divided into three parts: the outer ear, the middle ear, and the inner ear (see Figure 5.31). The human ear is divided into the outer, middle, and inner ear. The fetus has no bubble of air in the middle ear, having never been exposed to air. Because fluids do a better job than air of transmitting sound waves, there is good evidence that the fetus can hear outside sounds, such as mother’s voice, quite well during the final trimester of pregnancy. The outer ear consists of the structures that are visible outside the body. The pinna, the outer visible structure of the ear, collects and focuses sounds, like a funnel. In addition, the pinna helps us localize sounds as being above or below the head. Sounds collected by the pinna are channeled through the auditory canal, which ends at the tympanic membrane, or eardrum, at the boundary between the outer and the middle ear. The boundary between the middle and the inner ear is formed by another membrane, the oval window. The gap between these two membranes is bridged by a series of tiny bones. The purpose of these bones is to transfer sound energy from the air of the outer and middle ear to the fluid found in the inner ear. Sound waves are weakened as they move from air to water. When you try to talk to friends underwater, the result is rather garbled. Without the adjustments provided by these small bones, we would lose a large amount of sound energy as the sound waves moved from air to liquid. The inner ear contains two sets of fluid-filled cavities embedded in the bone of the skull. One set is part of the vestibular system, which we will discuss later in this chapter. The other set is the cochleacochleaThe structure in the inner ear that contains auditory receptors.            cochlea        The structure in the inner ear that contains auditory receptors.            , from the Greek word for  snail.  When rolled up like a snail shell, the human cochlea is about the size of a pea. It contains specialized receptor cells that respond to vibrations transmitted to the inner ear. The cochlea is a complex structure, which is better understood if we pretend to unroll it (see Figure 5.32). The cochlea may be divided into three parallel chambers divided from one another by membranes. Two of these chambers, the vestibular canal and the tympanic canal, are connected at the apex of the cochlea, or the point farthest from the oval window. Vibrations transmitted by the bones of the middle ear to the oval window produce waves in the fluid of the vestibular canal that travel around the apex and back through the tympanic canal. Lying between the vestibular and the tympanic canals is the cochlear duct. The cochlear duct is separated from the tympanic canal by the basilar membranebasilar membraneMembrane in the cochlea on which the organ of Corti is located.            basilar membrane        Membrane in the cochlea on which the organ of Corti is located.            . Resting on top of the basilar membrane is the organ of Cortiorgan of CortiA structure located on the basilar membrane that contains auditory receptors.            organ of Corti        A structure located on the basilar membrane that contains auditory receptors.            , which contains many rows of hair cells that transduce sound energy into neural signals. Each human ear has about 15,500 of these hair cells. Sound waves produce peak responses on the basilar membrane according to their frequencies. Like the strings on a musical instrument, high tones produce the greatest response at the narrow, stiff base of the basilar membrane, while low tones produce the greatest response at the wide, floppy part of the basilar membrane near the apex. Sound waves travel through the cochlea from the oval window, around the apex, and back to the round window. The waves cause movement of tiny hair cells in the cochlear duct, which we perceive as sound. As waves travel through the cochlea, the basilar membrane responds with a wavelike motion, similar to the crack of a whip. The movement of the basilar membrane causes the hair cells of the organ of Corti to move back and forth within the fluid of the cochlear duct. Bending the hair cells stimulates the release of neurotransmitters onto the cells of the auditory nerveauditory nerveThe nerve carrying sound information from the cochlea to the brain.            auditory nerve        The nerve carrying sound information from the cochlea to the brain.            . The basilar membrane needs to move very little before the hair cells are stimulated. If the hairlike structures extending from the top of the hair cells were the size of the Eiffel Tower in Paris, the movement required to produce a neural response would be the equivalent of 1 centimeter, about 0.4 inch (Hudspeth, 1983). As we mentioned earlier, hair cells stimulate axons forming the auditory nerve. One branch of each auditory nerve cell makes contact with the hair cells, while the other branch proceeds to the medulla of the brainstem. From the medulla, sound information is sent to the midbrain, which manages reflexive responses to sound, such as turning toward the source of a loud noise. In addition, the midbrain participates in sound localization, or the identification of a source of sound. The movement of tiny hair cells in the inner ear produces neural signals that travel to the brain. The midbrain passes information to the thalamus, which in turn sends sound information to the primary auditory cortex, located in the temporal lobe. The primary auditory cortex conducts the first basic analysis of the wavelengths and amplitudes of incoming information (see Figure 5.33). Surrounding the primary auditory cortex are areas of secondary auditory cortex that respond to complex types of stimuli, like clicks, noise, and sounds with particular patterns. The auditory cortex is located in the temporal lobe. The primary auditory cortex processes basic features of sound while the surrounding secondary auditory cortex processes more complex sounds, such as clicks and general noise.Now that we have an understanding of the structures and the pathways used to process the sensations that lead to the perception of sound, we turn our attention to the brain’s interpretation and organization of these sounds in terms of pitch, loudness, and spatial localization. Perception of pitch begins with the basilar membrane of the cochlea (see Figure 5.32). Place theory suggests that the frequency of a sound is correlated with the part of the basilar membrane showing a peak response. The base of the basilar membrane, closest to the oval window, is narrow and stiff. In contrast, at its farthest point near the apex, the basilar membrane is wide and flexible. If you are familiar with stringed instruments like guitars, you know that high tones are produced by striking the taut, small strings, and low tones are produced by striking the floppy, wide strings. The same principle holds for the basilar membrane. High-frequency tones produce the maximum movement of the basilar membrane near the base, while low-frequency tones produce maximum movement near the apex. The hair cells riding above these areas of peak movement show a maximum response. Place theory works well for sounds above 4,000 Hz, which is about the frequency produced by striking the highest key on a piano, C8. Below frequencies of 4,000 Hz, the response of the basilar membrane does not allow precise localization. In these cases, we appear to use another principle described as temporal theory, in which the patterns of neural firing match the frequency of a sound. Humans can perceive sounds that vary in intensity by a factor of more than 10 billion, from the softest sound we can detect up to the sound made by a jet engine at takeoff, which causes pain and structural damage to the ear. Table 5.2 identifies the intensity levels of many common stimuli, measured in the logarithmic decibel scale. Our perception of loudness does not change at the same rate as actual intensity. When the intensity of a sound stimulus is 10 times greater than before, we perceive it as being only twice as loud (Stevens, 1960). Source of SoundIntensity (measured in decibels, or dB)Threshold of hearing0 dBRustling leaves10 dBWhisper20 dBNormal conversation60 dBBusy street traffic70 dBVacuum cleaner80 dBWater at the foot of Niagara Falls90 dBiPod with standard earbuds100 dBFront rows of a rock concert110 dBPropeller plane at takeoff120 dBThreshold of pain/machine gun fire130 dBMilitary jet takeoff140 dBInstant perforation of the eardrum160 dBThe frequency of a sound interacts with our perception of its loudness. Humans are maximally sensitive to sounds that normally fall within the range of speech, or between 80 and 10,000 Hz (see Figure 5.34). Sounds falling outside the range of speech must have higher intensity before we hear them as well. One feature that distinguishes an expensive sound system from a cheaper model is its ability to boost frequencies that fall outside our most sensitive range. These functions plot the results of allowing participants to adjust the intensity of different tones until they sound equally loud. Each curve represents the intensity (dB) at which tones of each frequency match the perceived loudness of a model 1000 Hz tone. The stars indicate that a 100 Hz tone at 60 dB sounds about as loud as a 1000 Hz tone at 40 dB because they fall on the same line. Low frequencies are usually perceived as quieter than high frequencies at the same level of intensity. We are especially sensitive to frequencies found in speech. The pinna helps us localize sounds in the vertical plane, or in space above or below the head. Our primary method for localizing sound in the horizontal plane (in front, behind, and to the side) is to compare the arrival time of sound at each ear. As illustrated in Figure 5.35, the differences in arrival times are quite small, between 0 milliseconds for sounds that are directly in front of or behind us to 0.6 millisecond for sounds coming from a source perpendicular to the head on either side. Because arrival times for sounds coming from directly in front of or behind us are identical, it is difficult to distinguish these sources without further information. In addition to arrival times, we judge the differences in intensity of sounds reaching each ear. Because the head blocks some sound waves, a sound  shadow  is cast on the ear farthest from the source of sound. As a result, a weaker signal is received by this ear. We localize sound to the left and right by comparing the differences between the arrival times of the sounds to our two ears. Just as our visual systems can be fooled by certain types of input, our ability to localize sounds is influenced by interactions between vision and audition. Even before the invention of surround sound, which provides many effective sound localization cues, moviegoers perceived sound as originating from the actors’ lips, even though the speakers producing the sound are located above and to the sides of the screen. Our willingness to believe that the sound is coming from the actors’ lips probably results from our everyday experiences of watching people speak. The McGurk effect is an auditory illusion that occurs when we combine vision and hearing. In this demonstration, hearing  ba-ba  at the same time you see a person’s lips making  ga-ga  results in your perceiving  da-da. In our previous discussion of visual perception, we reviewed the grouping principles developed by Gestalt psychologists. Similar types of groupings occur in audition. Sounds from one location are grouped together because we assume they have the same source, whereas sounds identified as coming from different locations are assumed to have different sources. Sounds that start and stop at the same time are perceived as having the same source, while sounds with different starting and stopping times usually arise from separate sources, such as two voices in a conversation. Grouping plays an especially significant role in the perception of music and speech. In these cases, we see evidence of top-down processing as well, because our expectations for the next note or word influence our perceptions (Pearce, Ruiz, Kapasi, Wiggins, & Bhattacharya, 2010). Similarities between the processing of music and language have led researchers to argue for more music instruction in school to assist children with language learning (Strait, Kraus, Parbery-Clark, & Ashley, 2010).Hearing begins before birth and develops rapidly in human infants. Newborns as young as 2 days show evidence of recognizing their mother’s voice (DeCasper & Fifer, 1980) and respond preferentially to their native language (Moon, Cooper, & Fifer, 1993). Infants younger than 3 months show strong startle reactions to noise. By the age of 6 months, infants turn their heads in the direction of a loud or interesting sound. It is likely that their thresholds for sounds are nearly at adult levels by this age (Olsho, Koch, Halpin, & Carter, 1987). By the age of 1 year, children should reliably turn around when their name is called. An important developmental change in audition is age-related hearing loss. Hearing loss occurs first at higher frequencies. After the age of 30, most people cannot hear sounds above 15,000 Hz. After the age of 50, most people cannot hear above 12,000 Hz, and people older than 70 years have difficulty with sounds above 6,000 Hz. Because speech normally ranges up to 8,000 to 10,000 Hz, older adults might begin to have difficulty understanding the speech of others. Among individual differences in hearing is having perfect pitch, which means that you can name a musical tone that you hear. The brains of individuals with perfect pitch are structurally different from those of people who do not have this ability. Areas of the left hemisphere are larger in musicians with perfect pitch (Schlaug, Jancke, Huang, & Steinmetz, 1995). At the same time, extensive early musical training can shape the structure of the brain (Schlaug et al., 2009).Human culture and social life often provide a framework for the interpretation of stimuli. A dramatic example of this type of influence is our reaction to sine wave speech. To produce this stimulus, scientists artificially alter recordings of speech to resemble regular, repeating sine waves, as shown in Figure 5.36 (Davis, 2007). When people hear these artificial sounds without further instructions, they describe them as tweeting birds or other nonlanguage stimuli. However, if people are told the sounds represent speech, they suddenly  hear  language elements (Remez, Rubin, Pisoni, & Carell, 1981). Sine waves are regular and repetitive waveforms, such as the ones we included earlier to show how the height and frequency of light and sound waves are interpreted by the mind. Researchers can record speech sounds and transform the recordings into artificial sine waves, such as those in this image. If the sounds are played without information about their source, most people interpret the sounds as tweeting birds. However, if people are told that the recordings are language, they report  hearing  language, another example of top-down cognitive influences on perception. Sine wave speech shows us how culture in the form of experience with language can shape perception, but in other instances, perception can shape culture. For many people with hearing loss and for their families and friends, being deaf means something other than having a disability. Instead, deafness is viewed as a culture, complete with its own set of attitudes, language, and norms. American Sign Language (ASL) is viewed as being quite distinct from signed English and is difficult for signing people in Great Britain and Australia to understand (Mindess, 2006).SomatosensationSomatosensationThe body senses, including body position, touch, skin temperature, and pain.            Somatosensation        The body senses, including body position, touch, skin temperature, and pain.             (soma comes from the Greek word for  body ) provides us with information about the position and movement of our bodies, along with touch, skin temperature, and pain. Although these senses may not seem as glamorous as vision and hearing, we are severely disabled by their loss. You might think it would be a blessing to be born without a sense of pain, but people who have impaired pain reception often die prematurely because of their inability to respond to injury. Although unpleasant, pain tells us to stop and assess our circumstances, which might have promoted the survival of our ancestors.Unlike the visual and auditory stimuli we have discussed so far in this chapter, somatosensory stimuli arise from within the body or make contact with its surface. As a result, these stimuli provide an organism little time to react. We can deal with a predator seen or heard from a distance using strategies different from those we use for one that is touching us. Nonetheless, the somatosenses provide essential feedback needed for movement, speech, and safety. The vestibular system helps us maintain a steady view of the world, even when riding the most extreme roller coaster.The transition from walking on four legs to walking on two placed selective pressure on the evolution of primate vision and, to some extent, audition. By standing up on two legs, primates distanced themselves from many sources of information, such as smell. If you don’t believe us, try getting down on your hands and knees and smelling your carpet. This transition did not place the same evolutionary pressure on the human somatosenses, which work about the same way in us as they do in other animals. To begin our exploration of the somatosensory systems, we return to the inner ear. Adjacent to the structures responsible for encoding sound, we find the structures of the vestibular systemvestibular systemThe system in the inner ear that provides information about body position and movement.            vestibular system        The system in the inner ear that provides information about body position and movement.            , which provide us with information about body position and movement. The proximity of these structures to the middle ear, which can become congested because of a head cold, is often responsible for those rather unpleasant feelings of dizziness that accompany an illness. The receptors of the vestibular system provide information about the position of the head relative to the ground, linear acceleration, and rotational movements of the head. We sense linear acceleration when our rate of movement changes, such as when our airplane takes off. Like the cochlea, the vestibular receptors contain sensitive hair cells that are bent back and forth within their surrounding fluid when the head moves. When extensive movement stops suddenly, perhaps at the end of an amusement park ride, these fluids reverse course. You may have the odd sensation that your head is now moving in the opposite direction, even though you are sitting or standing still. The movement of these hair cells results in the production of signals in the auditory nerve, the same nerve that carries information about sound. These axons form connections in the medulla and in the cerebellum. You may recall from Chapter 4 that the cerebellum participates in balance and motor coordination, functions that depend on feedback about movement. In turn, the medulla receives input from the visual system, the cerebellum, and other somatosenses. This arrangement provides an opportunity to coordinate input from the vestibular system with other relevant information. The medulla forms connections directly with the spinal cord, allowing us to adjust our posture to keep our balance. Vestibular information travels from the medulla to the thalamus, the primary somatosensory cortex of the parietal lobe, and then the primary motor cortex in the frontal lobe. This pathway allows vestibular information to guide voluntary movement. In humans particularly, information from the vestibular system is tightly integrated with visual processing. As we move, it is essential that we maintain a stable view of our surroundings. To accomplish this task, rotation of the head results in a reflexive movement of the eyes in the opposite direction. This action should allow you to maintain a steady view of the world, even on the most extreme roller coaster. Touch provides a wealth of information about the objects around us. By simply exploring an object with touch, we can determine features such as size, shape, texture, and consistency. These judgments confirm and expand the information we obtain about objects through visual exploration. Touch is not only a means of exploring the environment. Particularly in humans, touch plays a significant role in social communication. Infants who are touched regularly sleep better, remain more alert while awake, and reach cognitive milestones at earlier ages (Ackerman, 1990). We hug our friends and loved ones to provide comfort, pat others on the back for a job well done, and shake hands to greet a colleague or conclude a deal. The contributions of the sense of touch to human sexuality are obvious. Our sense of touch begins with skin, the largest and heaviest organ in the human body. Embedded within the skin are several types of specialized neurons that produce action potentials whenever they are physically bent or stretched. Different types of receptors respond to certain features of a touch stimulus, such as pressure, vibration, or stretch (see Figure 5.37). In addition to their locations in the skin, receptors are located in blood vessels, joints, and internal organs. Unpleasant sensations from a headache or a too-full stomach or bladder originate from some of these receptors. Some receptor fibers wrap around hair follicles and respond whenever a hair is pulled or bent. Others, as we will see later in this section, participate in our senses of pain and skin temperature. Different receptors in the skin help us sense pressure, vibration, stretch, or pain. Information about touch travels from the skin to the spinal cord. Once inside the spinal cord, touch pathways proceed to the thalamus, along with input from the cranial nerves originating in the touch receptors in the skin of the face, the mouth, and the tongue. The thalamus transmits touch information to the primary somatosensory cortex, located in the parietal lobe. A map of the body’s representation in the primary somatosensory cortex, or a sensory homunculus ( little man ), is shown in the statue to the right. This odd figure demonstrates how areas of the body are represented based on their sensitivity rather than their size. Different species show different patterns of cortical organization for touch. Humans need sensitive feedback from the lips and the hands to speak and make skilled hand movements for tool use and other tasks. Rats devote a great deal of cortical real estate to whiskers, whereas lips have a high priority in squirrels and rabbits. A notable area that is missing from the homunculus is the brain, which has neither touch receptors nor pain receptors. We can only assume that for much of evolutionary history, intrusion into the brain was likely to be fatal. Consequently, there would be no advantage to  feeling  your brain. Because of the lack of somatosensation in the brain, neurosurgeons can work with an alert patient using local anesthesia for the skull and tissues overlying the brain. The surgery produces no sensations of pressure or pain. The representation of touch in the primary sensory cortex is plastic, which means that it changes in response to increases or decreases in input from a body part. Many individuals who lose a body part experience a phenomenon known as phantom limb, a term first used by a Civil War physician to describe his patients’ experience of pain from a missing limb. Phantom sensations can result from the reorganization of the somatosensory cortex following the loss of a body part (Borsook et al., 1998). In one case study, touching different parts of a patient’s face produced  feeling  from the patient’s missing hand (Ramachandran & Rogers-Ramachandran, 2000). When his cheek was touched, he reported feeling his missing thumb, along with the expected cheek, while touching his lip elicited feeling from the missing index finger, along with the normal lip sensations. In an even more bizarre example, a patient was embarrassed to report that he experienced a sensation of orgasm in his missing foot. The sensory homunculus illustrates the amount of representation each part of the body has in the sensory cortex. The human homunculus emphasizes the hands and face. Increased input also changes the organization of the somatosensory cortex. When monkeys were trained to use specific fingers to discriminate among surface textures to obtain food rewards, the areas of the cortex responding to the trained fingertips expanded (Merzenich & Jenkins, 1993). A similar reorganization occurs when blind individuals learn to read Braille (Pascual-Leone & Torres, 1993) or when people train extensively on stringed musical instruments (Elbert, Pantev, Weinbruch, Rockstroh, & Taub, 1995). Using your thumbs for text messaging will probably result in adaptations in cortical representation not seen in older generations (Wilton, 2002). Advances in robotics combined with better understanding of how touch is processed in the brain are leading to the development of prosthetics that can feel. Using fMRI, researchers were able to map areas of the sensory cortex that reacted when a participant imagined something touching different parts of the hand. With electrodes implanted in the relevant areas, the participant could then respond accurately to touch applied to the prosthetic hand, even when blindfolded. With this more natural feedback, the prosthetic hand should be able to manage delicate tasks, like picking up an egg. Individuals with autism spectrum disorder (ASD) experience a very different sensory world (see Chapter 14). Many individuals with ASD are oversensitive to touch, leading to rejection of hugs and cuddling. In addition, brain responses to touch of self or others differ between individuals with ASD and healthy controls (Deschrijver, Wiersema, & Brass, 2017). The extent of the differences correlated with the individuals’ reports of sensory and social difficulties. The representation of body parts in the primary sensory cortex changes in response to the amount of input from a body part. Children who study stringed instruments show more space in the sensory cortex devoted to fingers. Given the anguish experienced by patients with chronic pain, it is tempting to think that not having a sense of pain would be wonderful. However, as mentioned earlier, we need pain to remind us to stop when we are injured, to assess the situation before proceeding, and to allow the body time to heal. Free nerve endings that respond to pain are triggered by a number of stimuli associated with tissue damage. Some pain receptors respond to mechanical damage, such as that caused by a sharp object, while others respond to temperature or chemicals. Among the chemicals that stimulate pain receptors is capsaicin, an ingredient found in hot peppers (Caterina et al., 1997). Information about pain is carried centrally to the brain by two types of fibers. Fast, myelinated axons are responsible for that sharp  ouch  sensation that often accompanies an injury. Slower, unmyelinated axons are responsible for dull, aching sensations. Pain fibers from the body form synapses with cells in the spinal cord, which in turn sends pain messages to the thalamus. This information takes a relatively direct route, with only one synapse in the spinal cord separating the periphery of the body and the thalamus in the forebrain. This arrangement ensures that pain messages are received by the brain with great speed. From the thalamus, pain information is sent to the anterior cingulate cortex and the insula, which manage the emotional qualities of pain, and to the somatosensory cortex in the parietal lobe, which manages information about the location and intensity of pain (Wiech, 2016). Ashlyn Blocker was born with a rare condition preventing her from feeling pain. Without complaint, she went several days with a broken ankle after falling off her bicycle. Pain messages traveling to the brain may be modified by competing incoming sensory signals. Many of us spontaneously rub our elbow after bumping it painfully. The gate theorygate theoryThe theory that suggests that input from touch fibers competes with input from pain receptors, possibly preventing pain messages from reaching the brain.            gate theory        The theory that suggests that input from touch fibers competes with input from pain receptors, possibly preventing pain messages from reaching the brain.             of pain accounts for this phenomenon (Melzack & Wall, 1965). According to this model, input from touch fibers (reacting to rubbing your elbow) competes with input from pain receptors for activation of cells in the spinal cord (see Figure 5.38). Activation of the touch fibers effectively dilutes the amount of pain information reaching the brain. According to the gate theory, incoming pain messages can be influenced by factors such as chronic stress (opening the gate wider and producing a greater sensation of pain) or rubbing an injured body part (closing the gate and reducing the sensation of pain). The perception of pain is affected by the descending influence of higher brain centers. Many forebrain structures form connections with the periaqueductal gray of the midbrain. As we observed in Chapter 4, this area is rich in receptors for our natural opioids, the endorphins. The periaqueductal gray is a major target for opioid painkillers, such as morphine. Electrical stimulation of the periaqueductal gray produces a significant reduction in the experience of pain. In one of the most dramatic examples of how stress can interfere with the perception of pain, Guy Gertsch unknowingly ran the final 19 miles (about 30 km) of the 1982 Boston Marathon on a broken leg. Gertsch finished the race with a highly respectable time of 2 hours and 47 minutes. Pain is an actively constructed experience that involves our expectations and past experiences (Wiech, 2016). The power of expectation can be seen in placebo effects, which occur when people experience pain reduction, even though they have been exposed to an ineffective substance or treatment, such as a sugar pill instead of an aspirin tablet. Traditionally, scientists thought placebo effects were due to the ability of people’s belief that they are being treated for pain to initiate a real decrease in pain sensation. However, even when people are told they are receiving a placebo, pain relief can occur as long as they are also told that placebo effects can be powerful (Carvalho et al., 2016).No other sensory modality is as dramatically affected by culture, context, and experience as our sense of pain. The connection between culture and experience of pain is vividly illustrated by the hook-swinging ritual practiced in India (Melzack & Wall, 1983). This ritual, designed to promote the health of children and crops, involves hanging a male volunteer from steel hooks embedded into the skin and muscles of his back. Instead of suffering excruciating pain, as Westerners might expect, the volunteers appear to be in a state of exaltation. Women who have participated in prepared childbirth classes report less pain than women who are uninformed regarding the birth process. Although athletes and nonathletes share similar pain thresholds, these groups are quite different in their tolerance of pain (Scott & Gijsbers, 1981). Compared to nonathletes, athletes in contact sports such as boxing, rugby, and football tolerate higher levels of pain before identifying a stimulus as painful. Patients who are allowed to self-administer morphine for pain require less medication than patients who receive injections from hospital staff (Bennett et al., 1982). The sense of control may reduce anxiety and the need for pain medication.The famous philosopher Immanuel Kant (1798/1978, p. 46) considered olfactionolfactionSee also Chemical senses            olfaction        See also Chemical senses            , or our sense of smell, to be the  most dispensable  sense. Other species rely more heavily on olfaction and gustationgustationSee also Chemical senses            gustation        See also Chemical senses            , or the sense of taste, than humans do. Nonetheless, our chemical senses provide warning of danger, such as smelling smoke from a fire or tasting spoiled food. The chemical senses also contribute a richness to our emotional and social experiences. The smell of perfume or the taste of chocolate may be accompanied by strong emotional reactions. Contrary to Kant’s view, people who have lost their sense of smell because of head injury often experience profound depression (Zuscho, 1983). Sharing a meal has a strong effect on bonding for humans and other primates (Brosnan, 2010;  Wobber, Wrangham, & Hare, 2010). Culture, context, and experience can shape our perception of pain. During a festival dedicated to penance and atonement, Tamil Hindus walked through the streets carrying devices called kavadis that hold hooks that are pierced through the skin. Without this cultural context, it is likely that most people would find this experience excruciatingly painful.Our chemical senses begin with molecules suspended in the air in the case of olfaction and dissolved in saliva in the case of gustation. Olfaction provides more information from a distance, like vision and audition, whereas gustation, like the somatosenses, involves information from contact with the body.Like the somatosenses, the chemical senses are quite ancient in terms of evolution and have undergone little change over time. However, our sense of smell has been influenced by walking on two feet instead of four. Most olfactory stimuli are relatively heavy and tend to fall to the ground. Consider how your dog puts its nose to the ground when tracking something interesting. Air containing olfactory stimuli is taken in through the nostrils and circulated within the nasal cavities connected to the nostrils, where it interacts with olfactory receptors (see Figure 5.39). The receptors are located in a thin layer of cells within the nasal cavity. Unlike most neurons, the olfactory receptors regularly die and are replaced by new receptor cells in cycles lasting 4 to 6 weeks. Cells at the base of the receptors are responsible for producing the mucus surrounding the receptors. One branch of each receptor interacts with molecules dissolved in the mucus. The other branch carries information back to the central nervous system as part of the olfactory nerveolfactory nerveA nerve carrying olfactory information from the olfactory receptors to the olfactory bulbs.            olfactory nerve        A nerve carrying olfactory information from the olfactory receptors to the olfactory bulbs.            . The olfactory nerve fibers synapse in one of the two olfactory bulbsolfactory bulbsOne of two structures below the frontal lobes of the brain that receive input from the olfactory receptors in the nose.            olfactory bulbs        One of two structures below the frontal lobes of the brain that receive input from the olfactory receptors in the nose.            , located just below the mass of the frontal lobes. Although we often hear that human olfaction is not as good as olfaction in other species, human olfactory bulbs have about the same number of neurons as in 24 other mammalian species (McGann, 2017). Receptors in the nose interact with airborne chemicals to begin the sensing of odor. Unlike most other sensory input to the brain, olfactory pathways do not make direct connections with the thalamus before the information reaches the cerebral cortex. Instead, fibers from the olfactory bulbs proceed to the olfactory cortex, located in the lower portions of the frontal lobe extending into the temporal lobe, and to the amygdala. Because of the role these areas of the brain play in emotion, which we described in Chapter 4, these pathways may account for the significant emotional reactions we experience (disgust or pleasure) in response to odor. The olfactory cortex makes connections with the thalamus, which in turn sends information to the orbitofrontal cortex. It is likely that this pathway contributes to the conscious awareness of odors. The most likely original purpose of our sense of gustation, or taste, was to protect us from eating poisonous or spoiled food and to attract us to foods that boost our chances of survival. Although we seem biased toward detecting negative stimuli (Cacioppo & Gardner, 1999), our attraction to certain tastes also reflects our historical past. Because most of our ancestors were constantly facing the threat of famine, we find fatty and sugary foods to be especially tasty. Unfortunately, given the current availability of safe and palatable foods, our sense of taste may drive us to eat more than we need. Most of us are familiar with four major categories of taste: sweet, sour, salty, and bitter. A fifth type of taste has been proposed, known by the Japanese term umami, which roughly translated means  savory  or  meaty (Chaudhari, Landlin, & Roper, 2000). In addition, the tongue and the mouth contain receptors for carbohydrates (Turner, Byblow, Stinear, & Gant, 2014) and capsaicin, an active ingredient in hot peppers. Mice lacking capsaicin receptors happily consumed water containing capsaicin at levels that were rejected by normal mice (Caterina et al., 2000). Taste receptors are located on the tongue and in other parts of the mouth (see Figure 5.40). Contrary to a popular myth (usually accompanied by an equally mythological map of  taste centers  on the tongue), receptors sensitive to all types of taste are equally distributed across the tongue. You are probably aware of the bumpy texture of your tongue, which results from the presence of papillaepapillaeSmall bumps on the tongue that contain taste buds.            papillae        Small bumps on the tongue that contain taste buds.            . Most papillae contain somewhere between 1 and 100 taste budstaste budsA structure found in papillae that contains taste receptor cells.            taste buds        A structure found in papillae that contains taste receptor cells.            . Each taste bud contains between 50 and 150 receptor cells, which extend tiny hairlike cilia into the saliva that interact with dissolved taste stimuli and transduce the resulting information into neural signals. Like olfactory receptors, taste buds have a limited life before they are replaced. If you burn your tongue by drinking hot liquid, your taste is affected for a day or two. However, when the taste buds are replaced, taste should be back to normal. Taste buds are located in the bumps, or papillae, located on the tongue. Information about taste travels from the mouth and the tongue to the medulla. The medulla in turn communicates with the thalamus, which sends taste information to the insula, lower somatosensory cortex of the parietal lobe, and to the orbitofrontal cortex, where the emotional pleasantness or unpleasantness of particular stimuli is processed (Kobayakawa et al., 2005). As we will see in Chapters 7 and 8, taste information interacts with motivation and learning.Olfaction and gustation share three interesting perceptual themes: We can easily identify a number of complex stimuli combining many types of molecules, such as the aroma of coffee;we can detect small differences among similar smells and tastes; andour experience often shapes our perception of an olfactory or gustatory stimulus (Goldstein, 2010). Humans can distinguish among at least 1 trillion odors (Bushdid, Magnasco, Vosshall, & Keller, 2014). An example of the impact of experience and top-down processing on olfaction is the effect of labeling an odor on people’s rating of its pleasantness. If participants smell an onion stimulus labeled  pizza,  they rate the odor as more pleasant than if the identical stimulus is labeled  body odor  (Herz, 2003). About 25% of the population are supertasters, or people who are extrasensitive to taste; 25% are nontasters, or people who are relatively insensitive to taste; and the remaining 50% fall between these two extremes. You can use the following exercises to determine your taste category. Place a mint Life Saver on your tongue, and allow it to dissolve (no chewing please). Rate the following qualities of the Life Saver on a scale of 1 to 5, with 1 being  very intense  and 5 being  not intense :Sweetness12345Smell12345Coolness12345 Rush 12345How to interpret your results: Mint tasters fall into four groups:Group 1: Mint is mild, no rush. Group 2: Mint is moderate, no rush. Group 3: Mint is moderate, rush. Group 4: Mint is intense, rush. Table 5.3 shows further characteristics of these taste groups. Mildly Sensitive Tasters (Group 1)Moderately Sensitive Tasters (Groups 2 and 3)Supertasters (Group 4)Weak to undetectable sensation from mint. Moderate to strong sensation from mint. Very strong sensation from mint. Flavor of food is not that important. Flavor of food is important. Flavor of food is important. Many foods liked, few foods disliked, and not passionate about food. Many foods liked, few foods disliked, and often passionate about food. Great variation in the number of foods liked and often passionate about food. You need a gummed reinforcer (sticky white ring for notebooks), a swab, blue food coloring, and a mirror. Place one reinforcer on the front of your tongue just to the side of midline. Use a swab to apply blue food coloring to the part of your tongue that shows through the center of the reinforcer. The blue food coloring should make your papillae (bumps) more obvious. Count the number of papillae you see in the ring. More than 25 papillae within the reinforcer ring means you’re a supertaster. The number of papillae of mildly or moderately sensitive tasters will be less than 25, but these two groups cannot be distinguished based on this factor. Rate the tastes of the following foods and drinks using a 1-to-5 scale, with 1 being  dislike strongly  and 5 being  like a great deal :Broccoli12345Grapefruit12345Coffee (black)12345Dark chocolate12345As a child, were you ever described by a parent, teacher, or other adult as a  picky eater ? (circle one)YESNOCan you easily tell the difference between the fat content of milk, for example, between whole and 2% milk or between 1% and 2% milk? (circle one)YESNOPlacing blue food coloring on the tongue makes the papillae easier to see. Supertasters have many more papillae, and thus more taste buds, than other people. Supertasters tend to dislike bitter foods; they have many low numbers. Supertasters also tend to be picky eaters as children and are better at detecting differences in fats in foods. The chemical senses interact to provide the perception of flavor. You have probably noticed that food doesn’t taste good when your sense of smell is decreased by a bad cold. If you close your eyes and hold your nose, you are unable to distinguish between a slice of apple and a slice of raw potato. The orbitofrontal cortex plays an important role in the perception of flavor because the pathways serving olfaction and gustation converge in this part of the brain (Rolls, 2000).Young children are notorious for putting things in their mouths that adults would quickly reject based on taste, including poisonous substances such as drain cleaner. However, this propensity does not mean that children lack a sense of taste. Using facial expressions, researchers have demonstrated that newborns differentiate among sweet, bitter, and sour tastes but seem relatively oblivious to salty tastes (Rosenstein & Oster, 1988). As we get older, the overall number of taste buds decreases, reducing the intensity of many tastes and providing a possible explanation for why some strong flavors, such as that of broccoli, are enjoyed more by adults than by children. As we age, our sensitivity to smell also decreases (Cain & Gent, 1991). Because olfaction and taste interact to form the flavor of foods, decreased sensitivity in both senses might affect overall appetite as we age. Like the other sensory modalities discussed in this chapter, the chemical senses vary from person to person. Females are generally more sensitive to smell than are males (Dorries, 1992;  Koelega & Koster, 1974;  Ship & Weiffenbach, 1993). The average person has approximately 6,000 taste buds, but this number may vary widely. Supertasters have unusually high numbers of papillae and, therefore, have more taste buds (Bartoshuk, 2000). Disturbances in the chemical senses are correlated with several psychological disorders. Olfaction and the experience of posttraumatic stress disorder (PTSD) (see Chapter 14) appear to interact in combat veterans. PTSD is often characterized by intrusive, disturbing flashbacks in which the patient essentially relives the traumatic experience. Given the close association between olfaction and memory, researchers hypothesized that some PTSD flashbacks could be initiated by relevant smells. When compared to combat veterans who did not have PTSD, combat veterans with the disorder experienced marked anxiety when exposed to the smell of diesel, accompanied by changes in the activity of the amygdala (Vermetten, Schmahl, Southwick, & Bremner, 2007).The sense of smell might seem to play a secondary role to vision and audition in humans, but people have manipulated scent for religious, medicinal, and personal purposes since ancient times. We can speculate that once people learned to control fire, a recognition that some burning things smelled better than others could not have been far behind, possibly leading to the use of incense in religious rituals. Use of natural materials for medicine and self-adornment provides the historical roots for large, contemporary industries that manufacture scent for a host of consumer products, including perfume, air fresheners,  new car smell  products, and detergents. We mentioned earlier that of the senses we discussed in this chapter, pain was particularly influenced by cognition and context. Can being in a close relationship affect the way you feel pain?The answer appears to be  yes.  Physical contact with a loved one can affect how the brain processes pain. Women who were expecting an electric shock showed reduced activity in parts of the brain associated with the emotional and arousing aspects of pain when they held their husbands’ hands (Coan, Schaefer, & Davidson, 2006). In addition, the amount of reduction of activity in these pain areas of the brain correlated with the quality of the marriage—happily married women experienced greater decreases in activity associated with pain than less happily married women. Perhaps this buffering effect is why you may reach for your partner when frightened during a scary movie. Knowing that such intimacy could reduce a loved one’s pain might compel you to accompany your partner to a doctor’s appointment or visit a friend in the hospital. Our understanding of pain in connection with our interpersonal relationships can help us in tangible ways to create and maintain stronger, healthier relationships. Research shows that physical contact with loved ones reduces the sensation of pain. Although olfaction often seems to run in the background of our other cognitive processes, it is not immune to the effects of culture and experience. Americans spend millions on products that remove or mask body odor, whereas other cultures do not find such odors offensive. One study compared the categorization of odors by French, American, and Vietnamese participants (Chrea et al., 2004). Although the participants sorted odors similarly into broad categories of floral, sweet, bad, and natural, they differed along subtler dimensions. The French and American participants quickly sorted odors into fruit or flower categories, but this separation had little relevance to the Vietnamese participants. French and French-Canadian participants rated the pleasantness of wintergreen quite differently (Ferdenzi et al., 2016). Wintergreen is used in candy in Canada, but in medicine in France. Different cultures can prefer different foods. It is unlikely that you will find fruit bat pie, a delicacy in Palau, in many American restaurants. As we have seen in this chapter, different people watching the same event can reach different conclusions about what just happened. If researchers are to truly understand a problem such as cyberbullying, they must have methods for identifying perceptions of the experience from the perspectives of those involved. There are two major approaches for collecting information about cyberbullying: self-report and peer-nomination (Pellegrini, 2001). Each provides a unique perspective. Self-report asks youth to identify the frequency and the degree of cyberbullying that they have experienced or perpetrated personally, along with their emotional responses to these instances. Because cyberbullying often takes place where adult supervision is scarce, self-report can be more useful than observation. At the same time, self-report can be biased. Peer nomination provides insight into the larger group’s perception of the individuals being bullied or of the aggressors (Figure 5.41). Again, this method does a good job of providing information that is not typically accessible to adults. To conduct a peer nomination, students are provided with photos or a roster of their group, usually their class. Youth are asked to identify which classmates are picked on frequently and which do the bullying. Peer nominations generally show high correlations among the participants. Peer Nomination. The peer nomination method provides information about how individuals are viewed by their peers. This information usually shows high levels of consensus and provides insights about the group that are not usually observable by outsiders such as teachers and parents. Self-report and peer nomination results can be complimentary, providing perceptions of cyberbullying from both the perspective of the individuals involved and of the peers observing the cyberbullying. In both cases, these methods can provide insight into the phenomenon often unavailable to observation by parents, teachers, researchers, and other adults. Experience clearly plays a role in developing an individual’s taste preferences. The effects of experience on taste begin in the prenatal environment. Infants whose mothers consumed carrot juice during pregnancy showed stronger preferences for carrot flavor (Mennella & Beauchamp, 1996;  Mennella, Jagnow, & Beauchamp, 2001). In terms of survival, this result makes perfect sense. Infants are born with a predisposition to like the safe foods available in their environment. Because the food supply historically has varied widely from place to place, delicacies such as fruit bat pie are appreciated in Palau but not necessarily in the United States. StructureFunctionPinnaCollects sound and identifies its location as coming from above or below the head. Tympanic membraneBegins the process of transduction of sound waves to neural signals when movement occurs. CochleaContains auditory receptors. ThalamusReceives auditory input from the brainstem and connects to the primary auditory cortex. Primary auditory cortex (area in the temporal lobe)Receives and performs an initial analysis of auditory input from the thalamus.`;
