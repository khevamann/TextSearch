export const text = `Learning something new produces structural changes in neurons. Although you may not have had an opportunity to learn to surf, we’re assuming that in your role as a student, you are familiar with the process of learning. But knowing how to learn is different from understanding how and why learning occurs. What is going on in the minds of the people in the photo on this page as they learn to do something new? A behavior like learning to surf is complicated, so scientists interested in learning often begin their examinations using animals that are simpler than humans, observing as they learn to do behaviors simpler than surfing. Scientists have zoomed in to observe changes that happen in single neurons, like the image on the left, found in simple animals like the Aplysia californica sea slug. The sea slug is capable of learning to anticipate an electric shock that happens every time it is touched, and it demonstrates this knowledge by protectively withdrawing its body in response to touch before the shock is administered. Because the slug has such a simple nervous system compared to ours, scientists have been able to identify which neurons are involved in this learning and to describe the changes in a neuron’s functioning that make new behaviors—like withdrawing prior to shock—possible. Is it possible that human learning has anything in common with learning in slugs? The answer to this question is yes and no. Zooming out, we will see that some types of learning are common across animals with vastly different evolutionary histories and complexities, like humans and sea slugs. In other cases, we see interactions between nature and nurture. Experience, or nurture, frequently interacts with the nature of the organism exposed to the experience. You can talk to your kitten and to an infant, but only the infant will respond to this experience by learning to understand what you’re saying and how to reply. Each species brings unique and innate building blocks of learning, accumulated over many generations, to any given situation. Animals respond to the environment with reflexes, instinctive behaviors, and learned behaviors. Animals, like this ferocious kitten, use a reflex called piloerection to make themselves look bigger. The evolutionary purpose of this behavior is to appear so menacing that they can avoid conflict. Zooming out still more, we will see how learning in groups can often be different from learning as an individual. Even the lowly sea slug learns differently when alone than when in a group of fellow slugs. Slugs in isolation seem to have a terrible time learning to stop trying to eat food that is too tough to swallow (Schwarz & Susswein, 1992), but they learn this task easily when in the presence of other slugs (Susswein, Schwarz, & Feldman, 1986). Apparently, slugs can communicate with one another using pheromones, and the presence or absence of these chemicals has a powerful effect on their ability to learn (Susswein, Schwarz, & Feldman, 1986). The people on the previous page, who are learning to surf in a group, might have had a different experience if they were taking lessons individually. Animals, including ourselves, behave in response to their environments. Behavior can take the form of either externally observable actions or internal processes, such as emotions, thoughts, and physiological responses. These behaviors fall into three broad categories: reflexes, instincts, and learned behaviors. Each type of behavior plays a role in helping us survive, but each differs dramatically in its ability to adapt to a changing world. ReflexesReflexesAn inevitable, involuntary response to stimuli.            Reflexes        An inevitable, involuntary response to stimuli.             are inevitable, involuntary responses to stimuli. In casual conversation, we sometimes attribute a baseball player’s high batting average or our ability to step on the brake in time to avoid an automobile accident to great reflexes, but in fact, these examples involve learned behaviors that have become fast and automatic as a result of lots of practice. Nobody is born knowing how to hit baseballs or use the brakes of a car, so these behaviors do not meet this definition of a reflex. Goose bumps are what is left of piloerection in humans. These signals of arousal are remnants of a time when we had enough hair to make this an easily noticeable response. In humans, most reflexes are controlled by nervous system circuits located in the spinal cord and brainstem, as described in Chapter 4. Your physician checks one of these reflexes by tapping your knee with a hammer. The tap stretches your leg muscles, and the stretch is sensed by neurons in the spinal cord. Motor neurons in the spinal cord tell your thigh muscle to contract to compensate for the stretching, and your foot kicks out. No experience with knee-tapping is necessary to produce this behavior, nor can you voluntarily prevent it. By the time your brain realizes your knee was tapped, you have already reacted. Other reflexes pull our bodies away from painful stimuli, such as when we step on a tack or piece of glass or touch a hot stove, turn our heads in the direction of loud sounds, and help us stand upright and walk. Although some scientists disagree, contagious yawning might represent an instinct for a number of species, including our own. In one study, puppies over the age of 7 months were susceptible to contagious yawning in response to seeing humans yawn. Reflexes produce fast, reliable responses that promote your welfare. If you’ve ever touched a hot stovetop and found yourself pulling your hand back seemingly before you even knew what you had done, you know at least one benefit of reflexes. Reflexes have the disadvantage of being inflexible. For example, we respond to stress or cold by forming Goose bumps (bumps on the skin). This reflex appears to be left over from when our species had more body hair. Goose bumps raise each strand of hair, which in times of stress makes an individual look larger, scaring off predators or competitors, and, in response to cold, traps more insulating air near the skin. As humans lost most of their body hair over time, the advantages of this reflex decreased, but we still retain the behavior. InstinctsInstinctsAn inborn pattern of behavior elicited by environmental stimuli; also known as a fixed action pattern.            Instincts        An inborn pattern of behavior elicited by environmental stimuli; also known as a fixed action pattern.            , also called fixed action patterns, are inborn patterns of behavior elicited by environmental stimuli. Once they begin, instinctive behaviors run until completion. Instincts share reflexes’ reliability and lack of dependence on experience, but the resulting behaviors are more complex, requiring many more neurons than the number involved in a reflexive kick of your foot. Instinctive behaviors occur in the mating and parenting behaviors of many species (Tinbergen, 1951). For example, a mother dog instinctively licks clean her first litter of pups immediately after birth. An example of a human instinct is yawning. Once a yawn is initiated, it is difficult to stop. Psychologists define learninglearningA relatively permanent change in behavior or the capacity for behavior due to experience.            learning        A relatively permanent change in behavior or the capacity for behavior due to experience.             as a relatively permanent change in behavior (or in the capacity for behavior) due to experience. The core of this definition is the phrase change in behavior. After learning, we can do something new that we couldn’t do before, providing us with enormous advantages in surviving a changing world. Not all changes in behavior take place because of learning, however. Our behavior changes as we mature from infancy through adulthood, as we will see in Chapter 11. Behavior can be changed by brain damage or by having a psychological disorder. So our definition of learning limits the changes we consider to be learned to those that result from experience. The other qualification in this definition, relatively permanent, prevents the labeling of brief or unstable changes—such as when we experience different moods or suffer from an illness—as learning. The extent to which learning is really permanent will be discussed in Chapter 9. In the 20th century, psychology was dominated by the beliefs that, compared to other animals, humans have relatively few reflexes and instincts and that most human behavior results from learning. William James argued that humans have more instincts than other animals, although we are usually unaware of them (James, 1887). He believed that our behavior simply appears more complex and thoughtful because we often face the need to choose among competing instincts. Animals with fewer instincts experience fewer conflicts, so their behavior appears to be more automatic and less thoughtful. James’s approach to instinct and learning is echoed in the writings of contemporary evolutionary psychologists, who argue for an innate learning instinct that prepares humans to learn certain things in particular ways based on our evolutionary history (Cosmides & Tooby, 1997). Cognitive psychologists also revive the flavor of James by suggesting that learned behavior resulting from experience can look automatic and instinctive (Bargh & Chartrand, 1999). For example, prejudice toward a group of people requires learning, but prejudiced behavior often occurs without much conscious awareness, as explored further in Chapter 13. People who consciously believe that they are without prejudice toward members of a minority group nonetheless sit farther from an individual from that group than from members of the majority (Dovidio & Gaertner, 2005). Interactions between instinct and learning provide an explanation for another observation: experience has different effects at different times in an organism’s life span, as discussed further in Chapter 11. We do not attempt to teach philosophy to 2-year-old children because they are not yet capable of learning this material. Imprinting, the tendency of young animals to bond with and follow an adult, depends on timing. Birds that see a human immediately upon hatching follow that person everywhere, but birds that first see a human after having been hooded for their first few days out of the shell attempt to flee in terror (James, 1887). Imprinting, in which a young organism bonds with adults, provides an example of how experience has different effects at different times in the life span. These baby swans are following their parents because the parents were the first things they saw when they hatched. After a delay of a day or two, exposure to the parents would not lead to following. If the baby swans saw a human experimenter instead of their parents upon hatching, they would follow the human instead. Organisms usually show imprinting by the time they are mobile, which for swans means the first day of life, but for human children means closer to 1 year. Because these children have imprinted on their primary caregivers instead of their preschool teacher, the teacher needs a bit of technological help to encourage his charges to follow him on a walk. Learning is traditionally divided into three categories: associative, nonassociative, and observational. More than one type of learning can operate simultaneously in the same situation. Associative learningAssociative learningThe formation of associations, or connections, among stimuli and behaviors.            Associative learning        The formation of associations, or connections, among stimuli and behaviors.             occurs when we form associations, or connections, among stimuli, behaviors, or both. In other words, if A happens, then B is likely to follow. This type of learning helps us to predict the future based on past experience. The ability to anticipate the future provides enormous survival advantages because through it, animals gain time to prepare. Two important types of associative learning are classical conditioning and operant conditioning. In classical conditioningclassical conditioningA type of learning in which associations are formed between two stimuli that occur sequentially in time.            classical conditioning        A type of learning in which associations are formed between two stimuli that occur sequentially in time.            , we form associations between pairs of stimuli that occur sequentially in time. If a child sees a bee for the first time and then gets stung, the child forms a connection between seeing bees and the pain of being stung. The next time a bee flies by, the child is likely to feel quite frightened. In operant conditioningoperant conditioningA type of learning in which associations are formed between behaviors and their outcomes.            operant conditioning        A type of learning in which associations are formed between behaviors and their outcomes.            , we form associations between behaviors and their consequences. If you study hard, you will get good grades. We will discuss these forms of associative learning in more detail in later sections of this chapter. Nonassociative learningNonassociative learningLearning that involves changes in the magnitude of responses to stimuli.            Nonassociative learning        Learning that involves changes in the magnitude of responses to stimuli.             involves changes in the magnitude of responses to a single stimulus rather than the formation of connections between stimuli. Two important types of nonassociative learning are habituation and sensitization. HabituationHabituationA simple form of learning in which reactions to repeated stimuli that are unchanging and harmless decrease.            Habituation        A simple form of learning in which reactions to repeated stimuli that are unchanging and harmless decrease.             reduces our reactions to repeated experiences that have already been evaluated and found to be unchanging and harmless. For example, you might sleep better the second night than the first in the same hotel because you have adjusted to the unfamiliar noises in that environment. Sometimes we habituate to things that we should, ideally, still be noticing. A major concern about exposing children to violent media is the possibility that their emotional responses to violent images will habituate, leading to higher tolerance for violent behavior (Grizzard et al., 2015). It is likely that the first time this dog’s owner attempted to dress it up, the dog was a bit upset. After repeated experiences of being dressed up, however, the dog probably has habituated, which means that it has learned that no harm results from the process. Now it remains calm. In contrast to habituation, sensitizationsensitizationAn increased reaction to many stimuli following exposure to one strong stimulus.            sensitization        An increased reaction to many stimuli following exposure to one strong stimulus.             increases our reactions to a range of stimuli following exposure to one strong stimulus. Following an earthquake, people experience exaggerated responses to movement, light, or noise. If you are awakened by a loud crash, even if you figure out it’s just your roommate coming home late at night, it might be harder to get back to sleep because of your suddenly increased state of arousal. Every little sound now seems magnified. Many universities offer special programs for students who are the first in their extended families to attend college in recognition of the need to level the playing field with students who enter college having already learned a great deal about college life from observing their college-educated family members. Why would we show habituation to some stimuli and sensitization to others? In general, habituation occurs in response to milder stimuli, whereas sensitization occurs in response to stronger stimuli. Habituation ensures that we do not waste precious resources monitoring low-priority stimuli. On the other hand, sensitization is useful in dangerous situations. After detecting one harmful stimulus, raising our overall level of responsiveness should improve reaction time should other dangers arise. Observational learningObservational learningLearning that occurs when one organism watches the actions of another organism; also known as social learning or modeling.            Observational learning        Learning that occurs when one organism watches the actions of another organism; also known as social learning or modeling.            , also known as social learning or modeling, occurs when one organism learns by watching the actions of another organism. If your knowledge of table manners does not extend to the many forks, knives, and spoons at a fancy dinner, you might want to watch what others do before diving into your food. Observational learning provides the advantage of transmitting information across generations within families and cultures. Following an earthquake, this little boy is likely to be extra jumpy for a while in response to other stimuli, like loud noises, because of sensitization. Watching others is an efficient way to learn new skills, like dancing. Imagine for a moment how difficult it would be to write a description of how this dance should be performed. Watching others is also a useful way to learn what is harmful to do. Associative learningClassical conditioning Signal → Important eventSnakes → SnakebiteOperant conditioning Behavior → ConsequencesPractice → Successfully riding a wave in surfingNonassociative learningHabituation Responses to repeated, unchanging, irrelevant stimuliReduced response to neighbor’s loud television every eveningSensitization Responses to many stimuliJumpiness in response to many stimuli following an earthquakeObservational learningWatch → Imitate Copy new dance moves from your favorite music videoWatch → Avoid imitating Watch friend get sick from alcohol—don’t drink as muchAs discussed in Chapter 1, Ivan Petrovich Pavlov (1849–1936) is so tightly connected to the study of classical conditioning that the phenomenon is frequently called Pavlovian conditioning. Pavlov switched his interests from the study of digestion to the study of learning after noticing that his dogs had learned to anticipate the arrival of food. Instead of salivating when presented with food, his dogs began to salivate as soon as the lab assistant retrieved them from the kennel or strapped them into their experimental harnesses in the laboratory. Most people would probably not have noticed the differences in the dogs’ behavior. Pavlov not only noticed but realized the full significance of his observations: the dogs had formed an association between the stimuli preceding the food and the arrival of the food. In other words, the dogs had learned that certain stimuli served as signals for the eventual appearance of food (see Figure 8.1).(1) and (2) Before conditioning, food (unconditioned stimulus, or UCS) reliably produces salivation (unconditioned response, or UCR), and the sound of the metronome produces no reliable responding. (3) During conditioning, the sound of the metronome is followed by the food (UCS), which again produces salivation (UCR). (4) After conditioning, the sound of the metronome (conditioned stimulus, or CS) by itself is sufficient to produce salivation (conditioned response, or CR). Learning has occurred. If you are like most people reading about classical conditioning for the first time, you are probably wondering why psychologists spend so much time and effort discussing salivating dogs. If classical conditioning were that limited in its scope, it probably wouldn’t warrant more than a small footnote in the history of psychology. Instead, classical conditioning explains many of our learned emotional responses to our environment. It forms the basis for many practical applications, from prepared childbirth methods to the treatment of drug addiction and unrealistic fears. In describing the process of classical conditioning, Pavlov distinguished between conditioned and unconditioned stimuli and responses. Conditioned refers to something that must be learned, while unconditioned refers to factors that are reflexive or that occur without learning. Therefore, a conditioned stimulus (CS)conditioned stimulus (CS)An environmental event whose significance is learned through classical conditioning.            conditioned stimulus (CS)        An environmental event whose significance is learned through classical conditioning.             refers to an environmental event whose significance is learned, while an unconditioned stimulus (UCS)unconditioned stimulus (UCS)A stimulus that elicits a response without prior experience.            unconditioned stimulus (UCS)        A stimulus that elicits a response without prior experience.             has innate meaning to the organism. In a typical experiment by Pavlov, dogs heard the sound of a ticking metronome just before food appeared through a small door. Dogs do not have an innate response to the sound of ticking metronomes, but they generally are born knowing what to do with food. By turning on the metronome before delivering food, Pavlov established the ticking sound as the CS, while the food was the UCS. A UCS may be pleasant, like food, but it may also be unpleasant, like an electric shock. The important features of a UCS are its innate biological significance and its reliable ability to elicit a response without prior exposure. Just as dogs don’t require training to salivate in response to meat powder, few of us need experience with electric shock before we respond with fear and other negative emotions. Conditioned responses (CRs)Conditioned responses (CRs)A response learned through classical conditioning.            Conditioned responses (CRs)        A response learned through classical conditioning.             are learned reactions, while unconditioned responses (UCRs)unconditioned responses (UCRs)A response to an unconditioned stimulus that requires no previous experience.            unconditioned responses (UCRs)        A response to an unconditioned stimulus that requires no previous experience.             don’t need to be learned; they appear without prior experience with a stimulus. Salivating when food is put in your mouth is a UCR because we do this reflexively, without prior experience, but salivating to ticking metronomes occurs only as a result of experience. Our definition of learning requires behavior to change, so the appearance of a CR tells us that learning has occurred. Once learning has taken place, the organism now responds to CSs that reliably predict the arrival of the UCS. As this section will show, understanding classical conditioning illuminates a range of behaviors that you might have seen in yourself or others—from the avoidance of foods that you associate with feeling sick, to the butterflies you feel in your stomach before a big performance, to the development of social prejudices. Pavlov and those following in his footsteps have extended the usefulness of classical conditioning by exploring its features and development in more detail. AcquisitionAcquisitionThe development of a learned response.            Acquisition        The development of a learned response.             refers to the development of a CR. Pavlov argued that acquisition requires contiguity, or proximity in time between the CS and the UCS. If the CS occurs long before the UCS, the organism may not view the two stimuli as related. A CS that occurs simultaneously with a UCS or, worse yet, following a UCS is not a useful signal. A dinner bell sounded after food has been served is not helpful (see Figure 8.2). With each pairing of the conditioned stimulus and unconditioned stimulus, conditioned responses become more likely. Acquisition also requires contingency, or a correlation between the CS and the UCS. Learning about a reliable signal is easier than learning about a signal that occurs only sometimes. To demonstrate the contingency factor, Robert  Rescorla (1968) exposed rats to sound followed by a mild electric shock, which quickly produced fear of the sound. For some rats, a shock was administered only after a sound, while for other rats, shocks were administered following the sound on some occasions and without any sound on others. All the rats had the same number of contiguous sound–shock pairings. They differed in the correlation between sound and shock. Learning was faster for rats experiencing signaled shocks 100% of the time. As the percentage of signaled shocks decreased, learning about the signal slowed. If your migraine headaches are always preceded by exposure to bright light, you are more likely to fear bright lights in the future than if your headaches follow exposure to bright lights only once in a while (see Figure 8.3). Imagine the experience of the two dogs in this experiment. Food (unconditioned stimulus) always follows hearing the bell (conditioned stimulus) for both dogs, which demonstrates contiguity. However, the first dog receives food only after the bell, while the food is not always signaled by the bell for the second dog. The dog in the first case would learn the association between bell and food faster than the dog in the second one, because the first dog’s bell is a more reliable signal for food (contingency). CRs disappear, or undergo extinctionextinctionThe reduction of a learned response. In classical conditioning, extinction occurs when the unconditioned stimulus (UCS) no longer follows the conditioned stimulus (CS). In operant conditioning, extinction occurs when the consequence no longer follows the learned behavior.            extinction        The reduction of a learned response. In classical conditioning, extinction occurs when the unconditioned stimulus (UCS) no longer follows the conditioned stimulus (CS). In operant conditioning, extinction occurs when the consequence no longer follows the learned behavior.            , if the association between the CS and the UCS is broken. When Pavlov continued to expose his dogs to the ticking of the metronome without providing food, the dogs eventually stopped salivating in response to the sound of the metronome. Pavlov believed that extinction is not the same thing as forgetting; rather, it is actually new learning that overrides old learning. As evidence for this belief, Pavlov pointed to the occurrence of spontaneous recoveryspontaneous recoveryDuring extinction training, the reappearance of conditioned responses (CRs) after periods of rest.            spontaneous recovery        During extinction training, the reappearance of conditioned responses (CRs) after periods of rest.            , or the reappearance of CRs following periods of rest between sessions of extinction training. Even if a dog has stopped salivating to the sound of the metronome by the end of an extinction session, conditioned salivation reappears at the beginning of the next session. In other words, the CR decreases during a session of extinction training not because the dog is forgetting the relationship between ticking and food, but because the dog is now learning that ticking no longer predicts food, and it may take several sessions for this new learning to replace the old (see Figure 8.4). Addiction often involves the association of conditioned stimuli (CSs), such as the syringe, with the unconditioned stimulus (UCS) of a drug. Recovering addicts can be exposed to extinction—viewing the syringe (CS) without receiving the drug (UCS). This process reduces conditioned responses (CRs) that might be contributing to cravings for the drug. However, extinction typically requires multiple sessions. When the recovering addict returns for the next extinction training session after a period of rest, that person is likely to show CRs again. Eventually, with enough training, extinction is complete, and no further spontaneous recovery is observed. Without the ability to extinguish CRs, adjusting to further changes in the environment would be difficult, if not impossible. We would not be able to learn to enjoy dogs again after being bitten by one. An addict whose associations between needles and the effects of using heroin never extinguished would have an even harder time overcoming addiction. So far, we have been discussing examples of excitatory classical conditioning, in which the organism learns that a CS predicts the occurrence of a UCS. Pavlov was also quite interested in the classical conditioning of inhibitioninhibitionA feature of classical conditioning in which a conditioned stimulus (CS) predicts the nonoccurrence of an unconditioned stimulus (UCS).            inhibition        A feature of classical conditioning in which a conditioned stimulus (CS) predicts the nonoccurrence of an unconditioned stimulus (UCS).            , in which a CS predicts the nonoccurrence of a UCS (Pavlov, 1927). To demonstrate inhibition, we can begin by establishing excitatory conditioning by pairing a signal—a light—with shock. After some experience with this pairing, a rat learns to fear the light. Now we continue to present light–shock pairings, but we add other training trials that include the inhibitory CS—a sound—by presenting the light and sound together, followed by no shock. Even though the light is present, the rat learns that it is not going to be shocked in the presence of the sound, and it shows no fear. For a vulnerable animal in the wild, it is important to know not only that a predator at a water hole is in hunting mode (the sight of a prowling predator is an excitatory CS eliciting fear), but also that a predator relaxing after a recent kill is unlikely to kill again soon (the sight of a predator calmly drinking water is an inhibitory CS that tells the animal that the predator is unlikely to attack soon and therefore it is safe). For drug addicts, establishing inhibitory CSs associated with the lack of an expected drug effect might provide a more powerful method for rehabilitation (Kearns, Weiss, Schindler, & Panlilio, 2005). If an addict learns that drugs are never available in the presence of a certain signal, turning the signal on whenever the addict’s resolve is weak might help prevent a relapse. Inhibition learning helps organisms behave adaptively when they’ve learned that something important will not occur. These zebras may have learned that when lions act a certain way, they are already full and unlikely to hunt again. These inhibitory signals tell the zebras that they can drink in safety—at least for a little while. Once a CR is successfully acquired, organisms often show a tendency to respond to stimuli that are similar to the CS. For example, the child who learned to be afraid of bees after being stung might also begin to fear wasps and yellow jackets, a process that Pavlov called generalizationgeneralizationThe tendency to respond to stimuli that are similar to an original conditioned stimulus (CS).            generalization        The tendency to respond to stimuli that are similar to an original conditioned stimulus (CS).            . Generalization has obvious survival value. If our ancestors had one bad experience with a lion, it would make sense to avoid all lions, as well as other animals with lionlike characteristics. Unfortunately, our tendency to generalize can also have negative outcomes. For example, a soldier traumatized in combat might react with unnecessary fear to sounds that are similar to gunfire on the battlefield, such as the backfiring of a car back at home. Counteracting our tendency to generalize is another learning process known as discriminationdiscriminationA learned ability to distinguish between stimuli.            discrimination        A learned ability to distinguish between stimuli.            , which allows us to make fine distinctions between the implications of stimuli. In the laboratory, if you present food following a high tone but never following a low tone, a dog initially learns to salivate following both tones because of generalization. As learning progresses, the dog eventually learns to discriminate, or differentiate, between the abilities of the two stimuli to predict food. As a result, salivation to the high tone continues, but salivation to the low tone stops (see Figure 8.5). If generalization had led the soldier to react with fear to the sound of a backfiring car, further experience with the sound would help distinguish it from real gunfire in combat. Because the sound of a backfiring car is not followed by any fear-producing UCSs, it eventually loses its ability to elicit fear. The dog is receiving food after the high tone, but nothing after the low tone. Early in training, the dog salivates after both tones because of generalization. Further along in training, however, the dog learns to discriminate between the abilities of the two tones to signal food and salivates only after the high tone. We have seen how CRs spread to similar stimuli through generalization. In addition, CRs can occur in response to stimuli that predict the CS, a process known as higher-order conditioninghigher-order conditioningLearning in which stimuli associated with a conditioned stimulus (CS) also elicit conditioned responses (CRs).            higher-order conditioning        Learning in which stimuli associated with a conditioned stimulus (CS) also elicit conditioned responses (CRs).            . Higher-order conditioning allows us to make even more distant predictions about the occurrence of significant events. A person who was bitten by a dog might show fear the next time the dog is seen because the sight of the dog (CS) is now associated with the pain of the bite (UCS). Subsequently, the sight of the dog (CS) might begin to act more like a UCS, producing fear in response to other stimuli (seeing the dog’s yard or doghouse or hearing the dog bark) that might signal the appearance of the dog. Higher-order conditioning occurs when stimuli associated with a conditioned stimulus (CS) gain the ability to elicit conditioned responses (CRs) on their own. If a child has learned to fear dogs (CS) because of a previous bite (unconditioned stimulus, or UCS), anything that signals dog might now produce fear too, including the sight of a doghouse, a dog’s feeding bowl, or a chew toy. Pavlov’s dogs had probably never heard a metronome before participating in his experiments. What happens when you are already familiar with a CS? The answer is that you will take a longer amount of time learning to respond to it. It takes more time to learn about a familiar CS than about an unfamiliar CS, a phenomenon known as latent inhibitionlatent inhibitionThe slower learning that occurs when a conditioned stimulus (CS) is already familiar compared to when the CS is unfamiliar.            latent inhibition        The slower learning that occurs when a conditioned stimulus (CS) is already familiar compared to when the CS is unfamiliar.             (Lubow & Moore, 1959). The phenomenon is latent in the sense that its effects are not seen right away (when the stimuli are first presented), but emerge later when the rate of learning is examined. The inhibition part of the term refers to the relatively poor learning that occurs in response to familiar stimuli. If you have eaten lots of pizzas over time (familiar CS) but get sick after eating one, you are unlikely to associate the pizza with feeling ill. In contrast, if you get sick the first time you eat chocolate-covered ants (unfamiliar CS), you’ll quickly associate eating ants with feeling ill. Early behaviorists concentrated their study of learning on external behaviors that they could observe directly. As new technologies became available, such as brain imaging methods and more powerful computers used to model thinking and reasoning, psychologists interested in learning began to explore internal processes, leading to revolutionary advances in our understanding of cognition and biology. Early behaviorists also limited most of their studies to simple animals rather than humans. This restriction resulted from strong beliefs that behavior followed the same rules in all organisms, which meant that it was safe to apply experimental results from studies using rats to the behavior of humans and that having more control over your experimental subjects (food, housing, etc.) led to better science. Although the latter may be true, psychologists discovered that treating the learning animal as some kind of interchangeable black box was overly simplistic. Some learning processes have clearly been conserved over the course of evolution, allowing us to make conclusions relevant to humans about the changes at a synapse that accompany classical conditioning in the Aplysia californica sea slug (Carew & Kandel, 1973). In other cases, species bring their unique biology into the learning situation, as we will see in a later section on taste aversion learning. Psychologists have learned to be cautious about generalizing their conclusions across species. In our earlier discussion of the acquisition of classical CRs, we talked about the contributions of contiguity (closeness in time) and contingency (the correlation between the CS and the UCS). It should be easy to learn about a signal that both precedes and is predictive of an important event. The bell on your microwave both precedes and predicts the availability of food, and we would not be surprised if your mouth starts watering a bit whenever you hear it. What happens, however, if you already possess one good signal but add another one that also precedes and predicts a UCS? Based on past experience, you have learned that if your computer monitor suddenly goes dark (CS), something terrible (UCS) has happened to your computer. Whenever you see a dark screen, you feel extremely stressed (CR). Let’s assume that the next time a dark screen appears, the computer also emits a funny sound. Even though the sound both precedes and predicts a hard drive crash, it’s unlikely that you will learn much about the sound and its relationship with hard drive crashes. We don’t bother to learn much about new signals that provide no additional information, even if they meet our requirements for contiguity and contingency (Kamin, 1968,  1969). Because of latent inhibition, classical conditioning proceeds more slowly when a new conditioned stimulus is familiar than when it is unfamiliar. If you got sick after eating a familiar food (perhaps pizza), you are less likely to associate your illness with the food than if you got sick after eating an unfamiliar food, like this scorpion lollipop that one of your textbook authors is about to try. How can we account for this failure to learn under circumstances that should produce strong learning? Rescorla and Wagner (1972) proposed a model of classical conditioning in which learning occurs as a function of how surprising the association between the CS and the UCS appears. Early in training, more learning takes place because the relationship between the CS and the UCS is relatively unexpected. Later in training, less learning takes place with each exposure to the now-familiar relationship between the CS and the UCS (see Figure 8.6). According to the Rescorla–Wagner model, the rate of learning about a conditioned stimulus (CS) depends on how new or surprising the association between the CS and the unconditioned stimulus (UCS) appears to be. Early in training, learning proceeds rapidly (25 units per block or set of trials) because the association is new and surprising. Later in training, gains in conditioning strength (measured by how often a conditioned response occurs) level off because the association between the CS and the UCS is now familiar and no longer surprising (4 units per block of trials). We can’t imagine early behaviorists using terms like expect, surprise, and predict, because these represent internal states they believed were impossible to investigate scientifically. But today, it seems reasonable to discuss some aspects of learning using this vocabulary. We have already used some examples of classical conditioning involving food that you dislike because you got sick after eating it. These classically conditioned taste aversions result when the sight, smell, or flavor of the food (CS) has been paired in the past with illness (UCS). Taste aversion, or dislike of the food, is the resulting CR. Taste aversion isn’t just another interesting example of classical conditioning. Its demonstration led to a substantial rethinking not only of classical conditioning, but of behaviorism in general. Pavlov believed that stimuli that met the CS or UCS criteria could be successfully paired to produce classical conditioning. He made no provisions in his theory for special interactions between particular types of stimuli. John Garcia, who had a background not only in psychology but also in biology, did not believe that stimuli were so interchangeable. In what became known as a classic taste aversion study, Garcia and Koelling (1966) demonstrated that the types of stimuli used as CSs and UCSs matter, and that some combinations are learned faster than others. Garcia and Koelling presented groups of rats with either saccharin-flavored water (tasty water) or plain water. When the rats consumed the plain water, their drinking triggered a light and a clicking sound, which the researchers called bright-noisy water. After drinking either tasty or bright-noisy water, half the rats were given an injection of lithium chloride, which produces strong sensations of nausea, and the other half received an electric shock (see Table 8.1). Conditioned stimulusUnconditioned stimulusBright-noisy waterTasty waterLithium chloride (LiCl)Weak learningStrong learningShockStrong learningWeak learningThis type of experiment should look familiar to you by now as an example of classical conditioning. Tasty water or bright-noisy water served as CSs, while shock or lithium chloride served as UCSs. Garcia and Koelling subsequently presented tasty and bright-noisy water to see whether either would be avoided, with avoidance serving as an indication of the CR of disgust with or dislike of the water. Rats immediately learned associations between tasty water and subsequent illness but had difficulty learning to use the bright-noisy water as a signal for illness. Conversely, bright-noisy water, but not tasty water, became an effective signal for shock. After all, if you feel sick, you are more likely to decide that your illness was a result of eating mystery leftovers for breakfast rather than the flickering of the fluorescent lights in your classroom. However, if you receive an electric shock, you might be more suspicious of the flickering lights than of your breakfast. This experiment had far-ranging implications. Not only did these findings challenge Pavlov’s views of the relative interchangeability of stimuli, but they also prompted a renewed interest in the biological predispositions of organisms, or their preparedness to learn certain things. Although rats readily formed associations between taste and illness, but not between visual stimuli and illness, birds easily formed associations between visual stimuli and illness, but not between taste and illness. Rats see poorly, so they usually depend on taste and smell for identifying food. Birds have excellent vision and typically identify food sources using visual cues, such as the markings of particular species of butterfly. John Garcia and Robert Koelling’s work on taste aversion explains why the monarch butterfly (on the top) is often rejected by birds as food. The milkweed on which the monarch caterpillar feeds contains cardiac glycosides, which are maintained in the mature insects’ bodies. Ingested glycosides make the birds ill, so they avoid feeding on monarchs, as would be predicted by classical conditioning. Some scientists believe that by looking similar to the monarch, the viceroy butterfly (on the bottom) might escape being eaten too. Recall that visual stimuli (bright—noisy water) were not very effective signals for illness in rats, but apparently they work just fine for birds. The natural capacities of a species prepare it to learn some things better than others. The willingness of behaviorists to treat organisms as interchangeable black boxes with irrelevant internal features was severely challenged by this work, paving the way to an abandonment of the rigid behaviorism that had dominated psychology for most of the 20th century. Many learning theorists closed down their rat and pigeon labs and turned their attention to emerging cognitive, biological, and evolutionary approaches. Although strict behaviorism no longer dominates research in psychology, it still provides powerful explanations of human behavior and effective therapeutic tools for producing change. The pervasiveness of classical conditioning in everyday life is quite remarkable. When your palms get sweaty and you feel butterflies in your stomach before a big exam, awaken just before your alarm clock goes off, or feel more awake just because you smell coffee brewing, you can blame your responses on classical conditioning. If you are a clinician working with a traumatized combat veteran who is frightened by the smell of diesel fuel or a coach working with an athlete to overcome choking in big games, classical conditioning gives you some answers that you need to produce positive change. Psychologists have long been curious about what happened to Albert after he left Watson and Raynor’s laboratory. One hypothesis suggests that Albert was Douglas Merritte, the child of one of the foster mothers living at the Harriet Lane home in 1920. Merritte died at age 6 of hydrocephalus. Another strong candidate is William Albert Barger, whose name and physical health appear to some to be a better match for the baby shown in Watson’s films of his experiment. Barger died in 2007, at the age of 87. If Barger is indeed the famous Albert, we hope that his strong dislike of animals as an adult was not a remnant of Watson’s experiment. In 1920, John B. Watson and Rosalie Raynor conducted an experiment with a 9-month-old infant named Albert. By today’s standards, this experiment hardly appears well designed or ethical, but the results of the experiment led to research that shed a great deal of light onto human fear. While Albert played with a tame, white laboratory rat (CS), Watson and Raynor made a loud noise (UCS) by hitting a steel bar with a hammer. Albert was quite frightened by this noise (UCR). A week later, Albert was again offered the rat, but this time, he was afraid (CR). His fear generalized to other white, furry objects, including a rabbit, a dog, a fur coat, and a Santa Claus mask. Watson and Raynor had successfully demonstrated that fears could result from classical conditioning. Although Albert left Watson and Raynor’s laboratory without treatment for his fear, one of Watson’s students, Mary Cover Jones, demonstrated how classical conditioning procedures could be used to reduce learned fears. Her experiment featured a 3-year-old named Peter, who had a serious phobia, or intense, unrealistic fear, of rabbits (Jones, 1924). Could classical conditioning provide a way to reduce Peter’s fears? One possible approach would be to use extinction. As mentioned previously, CRs extinguish if the CS is presented alone, without the UCS. Treating phobias by exposing people to fear-producing stimuli in a manner that is safe until they no longer respond (i.e., extinction) is known as exposure therapy or flooding. Although exposure therapy works, it is often traumatic. Being exposed to a stimulus you find frightening until you are no longer afraid is not fun. Imagine forcing a person afraid of heights to bungee jump until the fear of heights has gone. Instead of using exposure therapy, Jones treated Peter with counterconditioning, or the substitution of one CR for another, opposite response. Jones associated food, a new UCS, with the presence of a rabbit (CS). Eventually, Peter was able to stroke the rabbit while eating. While not particularly hygienic, this achievement represented a big improvement in Peter’s life. Understanding classical conditioning provides insight into many situations where our emotional responses seem to be triggered by the environment, as in the case of performance anxiety. Counterconditioning has many useful applications. Aversion therapyAversion therapyAn application of counterconditioning in which a conditioned stimulus (CS) formerly paired with a pleasurable unconditioned stimulus (UCS) is instead paired with an unpleasant UCS.            Aversion therapy        An application of counterconditioning in which a conditioned stimulus (CS) formerly paired with a pleasurable unconditioned stimulus (UCS) is instead paired with an unpleasant UCS.             can be used to replace inappropriate positive reactions to a stimulus with negative reactions. For example, a compound containing silver interacts with nicotine to make a tobacco cigarette taste terrible. Substituting a negative outcome for a positive outcome of smoking helps some smokers quit more easily (Rose, Behm, Murugesan, & McClernon, 2010). A variation of counterconditioning used to treat fear is known as systematic desensitizationsystematic desensitizationA type of counterconditioning in which people relax while being exposed to stimuli that elicit fear.            systematic desensitization        A type of counterconditioning in which people relax while being exposed to stimuli that elicit fear.             (Wolpe, 1958). Associations between a phobic stimulus and fear are replaced by associations between the phobic stimulus and relaxation. The person undergoing treatment is first trained to achieve a state of physical and mental relaxation, usually by tensing and relaxing muscle groups from head to toe. If you would like to try this progressive relaxation technique yourself, we outline the procedure in Chapter 15. Once relaxation is achieved, the fear stimulus is gradually introduced, either in physical form, through guided imagery, in which the person is asked to imagine the stimulus, or by using virtual reality. If relaxation falters at any point, the person retreats to an earlier stage of exposure to the fear stimulus until he or she can relax again. In addition to the influences on addiction discussed in Chapter 6, classical conditioning can contribute to dependence on a drug or behavior. Stimuli associated with drug use often become CSs for the effects of a drug. For example, peak caffeine levels occur about 45 minutes after drinking a cup of coffee (Liguori, Hughes, & Grass, 1997), yet most coffee drinkers report feeling more awake as soon as they take that first sip in the morning (or even smell the coffee brewing). John B. Watson and Rosalie Raynor observe Little Albert’s generalization to a bunny mask worn by Watson. The mask is similar to the original conditioned stimulus in their experiment—a white laboratory rat—which stimulated Albert’s conditioned fear. One of the challenges faced by people recovering from addiction to substances is that environmental cues (i.e., CSs) associated with the effects of substance use (i.e., UCSs) continue to elicit cravings (i.e., CRs) for the drug of choice. Most treatments for substance abuse involve simply abstaining from a drug, as opposed to extinction or counterconditioning. There is nothing about avoiding the use of a drug that substantially weakens the previously formed associations surrounding its use. Consequently, being exposed to established CSs, including the people with whom one did the drugs previously or the context in which one used the drugs, often leads to a former addict’s relapse (Chaudhri, Sahuque, & Janak, 2008). The sight, smell, or taste of a recovering alcoholic’s favorite drink, or even a visit to a favorite bar, is often enough to undermine the person’s abstinence from drinking. Reducing these associations through extinction can help the addict continue to abstain. While still a graduate student, Mary Cover Jones used classical conditioning principles to reduce a small child’s fear of rabbits. As a result, she is often called the Mother of Behavior Therapy. Classical conditioning contributes to the formation and change of attitudes (Cacioppo & Berntson, 2001). After all, advertisers have been using classical conditioning for years to influence consumer attitudes about products. By forming associations between the products and other stimuli we value, like celebrities, advertisers hope that our opinions of their product will improve. Prejudice, which is discussed further in Chapter 13, is a negative attitude about a group of people. Like other attitudes, it is influenced by classical conditioning, although it has many other roots as well. In particular, latent inhibition can contribute to the development of negative attitudes (Cacioppo, Marshall-Goodell, Tassinary, & Petty, 1992). Consider the following. Because of latent inhibition, if children grow up with little exposure to people outside their own race, people of their own race are more familiar, and learning to associate their race with other attributes should be slow. In contrast, the children have had less preexposure to people of other races. Latent inhibition would predict that children exposed to news reports about crime would form stronger associations between people of unfamiliar races and crime than between people of their own race and crime. Product placement in movies is expensive, but marketers hope that audiences will form associations between their products and the positive brand of a character. If the hero drinks Coca-Cola, maybe drinking Coca-Cola will make you feel heroic, too. By reducing the amount of learning that occurs in response to familiar stimuli, latent inhibition helps us focus our energy on novelty and change in our environments. Less latent inhibition is seen in creative people and in people diagnosed with schizophrenia than is seen in general (Baruch, Hemsley, & Gray, 1988a, 1988b;  Lubow, Ingberg-Sachs, Salstein-Orda, & Gewirtz, 1992). This difference means that creative people and people with schizophrenia form new associations with familiar stimuli faster than most people do. They make connections under circumstances in which most of us would not. Reduced latent inhibition might account for the creative person’s ability to see familiar things in new ways, which is a positive outcome, but it also might lead to the tendency of a person with schizophrenia to make odd, inappropriate connections among ideas. Chapter 14 refers to this tendency as a loosening of associations. For example, a person with schizophrenia might suggest that a painting has a headache. This is not the type of association between stimuli that most people would make. While reading about Ivan Petrovich Pavlov’s experiments with salivating dogs, it might be difficult to grasp the full significance of Pavlov’s results, but there were some good reasons that his laboratory continued to receive considerable resources during difficult times of war and revolution (Gantt, 1928). As demonstrated in this chapter, the possible applications of classical conditioning range widely, from treating addictions and phobias to understanding prejudice and schizophrenia. One of the less typical applications was the result of work by John Garcia and one of his graduate students, Carl Gustavson (Gustavson, Kelly, Sweeney, & Garcia, 1976). Gustavson was a child of the rural West, and he had grown up watching the ongoing battles among sheep ranchers, coyotes, and wolves. He asked whether the taste aversion work pioneered by Garcia might provide a useful solution. With encouragement from Garcia, Gustavson set up an outdoor training facility with six coyotes and two wolves. The coyotes were fed rabbit meat laced with lithium chloride (LiCl), while the wolves were fed sheep meat with LiCl. He even experimented with a caged cougar that refused to eat deer meat after a single meal of venison laced with LiCl. Some of the researchers’ observations of the captive animals were especially dramatic. When a lamb was tethered in the wolves’ pen, the wolves approached in a normally threatening manner. The lamb froze, which is also species-typical behavior in this situation. Then, something interesting happened. The wolves sniffed at the lamb and then retreated, showing signs of defensive puppy play (crouching with ears and tail down). In response to these submissive displays, the lamb began to charge the wolves and even chased them around the pen. Use of conditioned taste aversion to provide a nonlethal method of predator control is being explored in efforts to save the endangered Mexican wolf, shown here. Dan Moriarty and his colleague Lowell Nicolaus laced meat with the fungicide thiabendazole. Although the fungicide makes the wolves ill, it causes no permanent damage. If the program is successful in the wild, it could reshape the way that endangered predators are managed worldwide. Would these observations transfer to the real outdoors? The researchers distributed sheep-flavored bait and sheep carcasses laced with LiCl across a 3,000-acre sheep ranch in Washington State. A comparison of the ranch’s loss records demonstrated a 30% to 60% reduction in sheep killed by coyotes following the treatment. A larger-scale test of 10 herds over a 3-year period in Saskatchewan demonstrated a reduction in predator control costs of between 86% and 90% per year. Where does the practice stand today? Unfortunately, when wildlife experts attempted to apply taste aversion to predator control, they made numerous procedural mistakes. They viewed the LiCl as a repellant, as opposed to a UCS. The bait was overdosed with LiCl, which ensured that the predator would link salty tastes with illness instead of the taste of the prey with illness, leading to the discarding of the procedure as ineffective. Contemporary coyote management techniques include propane cannons, horns, sirens, radios, and strobe lights (we are left wondering what these stimuli do to the sheep …), but aversion conditioning is still described by wildlife experts as speculative (Wilbanks, 1995, p. 163). Gustavson’s technique has recently made a comeback in an effort to save the endangered Mexican wolf. When released in the wild, the wolves, like Gustavson’s coyotes, naturally run into conflict with ranchers and farmers. Professors Lowell Nicolaus and Dan Moriarty’s initial results using conditioned taste aversion with captive gray wolves convinced U. S. government wildlife agencies to prepare to replicate the study with captive Mexican wolves (U. S. Fish and Wildlife Service, 2014). A total of 23 Mexican wolves were treated between 2011 and 2013. Two animals treated in 2011 were retested in 2012, and both still showed a conditioned aversion to cattle (U. S. Fish and Wildlife Service, 2014). Classical conditioning phenomenonDescriptionExampleAcquisitionGradual development of conditioned responses (CRs)A dog salivates on a higher percentage of trials as training progresses. ExtinctionReduction of CRs when a conditioned stimulus (CS) is presented without being followed by an unconditioned stimulus (UCS)If the metronome is no longer followed by food, the dog stops salivating to the metronome. Spontaneous recoveryReappearance of CRs following periods of rest between extinction training sessionsThe dog shows no salivation at the end of the day’s extinction training, but after a night of rest in the kennel, the dog salivates at the beginning of the next extinction session. InhibitionA CS’s prediction of the nonoccurrence of a UCSSo long as a gauge is in the green, your equipment will not explode. You do not feel fear. GeneralizationResponses to stimuli that resemble the CSAlbert’s fear of white rats generalized to a Santa Claus beard. DiscriminationResponding to the CS, but not to similar stimuli that have not been paired with the UCSA combat veteran learns to distinguish between the sound of gunfire and the backfire from a car. Higher-order conditioningCRs to stimuli that predict the occurrence of a CSA child who has been bitten by a dog begins to fear the street where the dog lives. Latent inhibitionSlower development of CRs to a familiar CS than to an unfamiliar CSAn American forms a taste aversion faster to fruit bat pie than to hamburgers. The discussion of behaviorism in Chapter 1 introduced you to Edward Thorndike and his law of effect. To recap, Thorndike had observed the learning that took place when a cat tried to escape one of his puzzle boxes. According to Thorndike, the cats learned to escape by repeating actions that produced desirable outcomes and by eliminating behaviors that produced what he called annoying outcomes, or outcomes featuring either no useful effects or negative effects (1913, p. 50). Consequently, the law of effect states that a behavior will be stamped into an organism’s repertoire depending on the consequences of the behavior (Thorndike, 1913, p. 129). The association between a behavior and its consequences is called operant or instrumental conditioning. In this type of learning, organisms operate on their environment, and their behavior is often instrumental in producing an outcome. B. F. Skinner extended Thorndike’s findings using an apparatus that bears his name—the Skinner box, a modified cage containing levers or buttons that can be pressed or pecked by animals (see Figure 8.7). A specially adapted cage called a Skinner box, after behaviorist B. F. Skinner, allows researchers to investigate the results of reinforcement and punishment on the likelihood that the rat will press the bar. Operant conditioning differs from classical conditioning along several dimensions. By definition, classical conditioning is based on an association between two stimuli, whereas operant conditioning occurs when a behavior is associated with its consequences. Classical conditioning generally works best with relatively involuntary behaviors, such as fear or salivation, whereas operant conditioning involves voluntary behaviors, like walking to class or waving to a friend. As we all know from experience, some types of consequences increase behaviors, while others decrease behaviors. Skinner divided consequences into four classes: positive reinforcement, negative reinforcement, positive punishment, and negative punishment. Both types of reinforcement increase their associated behaviors, whereas both types of punishment decrease associated behaviors (see Table 8.2). Add stimulus to environmentRemove stimulus from environmentIncrease behaviorPositive reinforcementNegative reinforcementDecrease behaviorPositive punishmentNegative punishmentWe all have unique sets of effective reinforcers and punishers. You might think that getting an A in a course is reinforcing, making all those extra hours spent studying worthwhile, but top grades may be less meaningful to the student sitting next to you, who came to college for the social life. A parent might spank a child, believing that spanking is an effective form of punishment, only to find that the child’s unwanted behavior is becoming more rather than less frequent. For some children, the reward of getting the parent’s attention overrides the discomfort of the spanking part of the interaction. In other words, the identity of a reinforcer or punisher is defined by its effects on behavior, not by some intrinsic quality of the consequence. The only accurate way to determine the impact of a consequence is to check your results. If you think you’re reinforcing or punishing a behavior but the frequency of the behavior is not changing in the direction you expect, try something else. By definition, a positive reinforcement increases the frequency of its associated behavior by providing a desired outcome. Again, each person has a menu of effective reinforcements. In a common application of operant conditioning, children with autism spectrum disorder are taught language, with candy serving as the positive reinforcement. Benjamin Lahey tells of his experience trying to teach a child with autism spectrum disorder to say the syllable ba to obtain an M&M candy (Lahey, 1995). After 4 hours without progress, Lahey turned to the child’s mother in frustration, asking her what she thought might be the problem. The mother calmly replied that her son didn’t like M&Ms. Lahey switched to the child’s preferred treat, chopped carrots, and the child quickly began saying ba. Chopped carrots are probably not the first reinforcer you would try with a 4-year-old boy, but in this case, they made all the difference. Edward Thorndike’s Law of effect stipulates that behaviors followed by positive consequences are more likely to be repeated in the future, and that behaviors followed by negative consequences are less likely to be repeated. Why, then, do large numbers of people, particularly in adolescence, engage in self-injury, or deliberate physical damage without suicidal intent (Klonsky & Muehlenkamp, 2007)? Up to 25% of teens have tried self-injury at least once (Lovell & Clifford, 2016). Most initiate self-injury while in middle school (grades 6 through 8), and approximately 6% of college students continue to self-injure. As this chapter has detailed, reward and punishment are in the eye of the beholder. The first challenge that we face in our analysis of self-injury is the assumption that pain is always a negative consequence. For most of us, it is. However, adolescents who engage in self-injury report feelings of relief or calm, despite the obvious pain that they inflict on themselves. Such feelings probably reinforce further bouts of self-injury. Self-injury often occurs in response to feelings of anger, anxiety, and frustration, and alleviation of these negative feelings might reward the injurious behavior (Klonsky, 2007;  Klonsky & Muehlenkamp, 2007). Finally, injury is associated with the release of endorphins, our bodies’ natural opioids. The positive feelings associated with endorphin release also might reinforce the behavior. Self-injury frequently occurs in people diagnosed with psychological disorders, such as depression, anxiety disorders, eating disorders, or substance abuse, which are discussed further in Chapters 7 and 14. Others engaging in the behavior have a history of sexual abuse. Observations that captive animals in zoos and laboratories are often prone to self-injury might provide additional insight into the causes of this behavior (Jones & Barraclough, 1978). Treatment usually consists of therapy for any underlying psychological disorders, along with avoidance, in which the person is encouraged to engage in behaviors that are incompatible with self-harm. To assist these individuals further, we need to be able to see reward and punishment from their perspective, not just our own. If the consequences of a behavior influence how likely a person is to repeat the behavior in the future, how can we explain the prevalence of self-injury? Why don’t the painful consequences of the behavior make people stop? In situations like this, operant conditioning tells us that we need to look for possible reinforcers for the behavior that override the painful outcomes. In the case of self-injury, people report feeling calm and relief. To treat such behaviors effectively, psychologists need to understand what advantages they provide from the perspective of the person doing the behavior. The Premack principle can help you maintain good time management. If you prefer socializing to studying, use the opportunity to socialize as a reward for meeting your evening’s study goals. If everyone has a different set of effective reinforcers, how do we know which to use? A simple technique for predicting what a particular animal or person will find reinforcing is the Premack principle, which states that whatever behavior an organism spends the most time and energy doing is likely to be important to that organism (Premack, 1965). It is possible, therefore, to rank people’s free-time activities according to their priorities. If Lahey had been able to observe his young client’s eating habits before starting training, it is unlikely that he would have made the mistake of offering M&Ms as reinforcers. The opportunity to engage in a higher-priority activity is always capable of rewarding a lower-priority activity. Your grandmother may never have heard of Premack, but she knows that telling you to eat your broccoli to get an ice cream generally works. Both Thorndike and Skinner agreed that positive reinforcement is a powerful tool for managing behavior. In our later discussion of punishment, we will argue that the effects of positive reinforcement are more powerful than the effects of punishment. Unfortunately, in Western culture, we tend to provide relatively little positive reinforcement. We are more likely to hear about our mistakes from our boss than all the things we’ve done correctly. It is possible that we feel entitled to good treatment from others, so we feel that we should not have to provide any reward for reasonably expected behaviors. The problem with this approach is that extinction occurs in operant, as well as in classical, conditioning. A behavior that is no longer reinforced drops in frequency. By ignoring other people’s desirable behaviors instead of reinforcing them, perhaps with a simple thank-you, we risk reducing their frequency. According to the Premack principle, a preferred activity can be used to reinforce a less preferred activity. Most children prefer candy over carrots, so rewarding a child with candy for eating carrots often increases carrot consumption. One little boy with autism spectrum disorder, however, preferred carrots to M&Ms, and his training proceeded more smoothly when carrot rewards were substituted for candy rewards. Some reinforcers, known as primary reinforcers, are effective because of their natural roles in survival, such as food. Others must be learned. We are not born valuing money, grades, or gold medals. These are examples of conditioned reinforcersconditioned reinforcersA reinforcer that gains value from being associated with other things that are valued; also known as a secondary reinforcer.            conditioned reinforcers        A reinforcer that gains value from being associated with other things that are valued; also known as a secondary reinforcer.            , also called secondary reinforcers, that gain their value and ability to influence behavior from being associated with other things we value. Here, we see an intersection between classical and operant conditioning. If you always say good dog before you provide your pet with a treat, saying good dog becomes a CS for food (the UCS) that can now be used to reinforce compliance with commands to come, sit, or heel (operant behaviors). Classical conditioning establishes the value of good dog, and operant conditioning describes the use of good dog to reinforce the dog’s voluntary behavior. Serena Williams loves her Wimbledon trophy not for its intrinsic value (you can’t eat it, etc.), but because trophies have become conditioned reinforcers. Many superstitious behaviors, like wearing your lucky socks, can be learned through operant conditioning. Operant conditioning does not require a behavior to cause a positive outcome to be strengthened. All that is required is that a behavior be followed by a positive outcome. Unless you suddenly have a string of bad performances while wearing the lucky socks, you are unlikely to have an opportunity to unlearn your superstition. Humans are capable of generating long chains of conditioned reinforcers extending far into the future. We might ask you why you are studying this textbook right now, at this moment. A psychologist might answer that you are studying now because studying will be reinforced by a good grade at the end of the term, which in turn will be reinforced by a diploma at the end of your college education, which in turn will be reinforced by a good job after graduation, which in turn will be reinforced by a good salary, which will allow you to live in a nice house, drive a nice car, wear nice clothes, eat good food, and provide the same for your family in the coming years. Negative reinforcementNegative reinforcementA method for increasing behaviors that allow an organism to escape or avoid an unpleasant consequence.            Negative reinforcement        A method for increasing behaviors that allow an organism to escape or avoid an unpleasant consequence.            , which sounds contradictory, involves the removal of unpleasant consequences from a situation to increase the frequency of an associated behavior. Negative reinforcement increases the frequency of behaviors that allow an organism to avoid, turn off, or postpone an unpleasant consequence; these are sometimes called escape and avoidance behaviors. Let’s look at a laboratory example of negative reinforcement before tackling real-world examples. If a hungry rat in a Skinner box learns that pressing a bar produces food, a positive consequence, we would expect the frequency of bar pressing to increase. This would be an instance of positive reinforcement. However, if pressing the bar turns off or delays the administration of an electric shock, we would still expect the frequency of bar pressing to increase. This would be an instance of negative reinforcement. Be careful to avoid confusing negative reinforcement with punishment, which is covered in the next section. By definition, a punishment decreases the frequency of the behaviors that it follows, whereas both positive and negative reinforcers increase the frequency of the behaviors that they follow. Returning to our Skinner box example, the rat’s bar pressing increases following both positive reinforcement (food) and negative reinforcement (turning off a shock). If we shocked the rat every time it pressed the bar (punishment), it would stop pressing the bar quickly. Many everyday behaviors are maintained by negative reinforcement. We buckle up in our cars to turn off annoying beeps or bells, open umbrellas to avoid getting wet, scratch an insect bite to relieve the itch, take an aspirin to escape a headache, apply sunscreen to avoid a sunburn or skin cancer, and apologize to avoid further misunderstandings with a friend. In many real-world cases, positive and negative reinforcement act on behavior simultaneously. A heroin addict uses the drug to obtain a state of euphoria (positive reinforcer) but also to eliminate the unpleasant symptoms of withdrawal (negative reinforcer). You might study hard to achieve high grades (positive reinforcers) while also being motivated by the need to avoid low grades (negative reinforcers). A punishmentpunishmentA consequence that eliminates or reduces the frequency of a behavior.            punishment        A consequence that eliminates or reduces the frequency of a behavior.             is any consequence that reduces the frequency of an associated behavior. Positive punishmentPositive punishmentA consequence that eliminates or reduces the frequency of a behavior by applying an aversive stimulus.            Positive punishment        A consequence that eliminates or reduces the frequency of a behavior by applying an aversive stimulus.             refers to applying an aversive consequence that reduces the frequency of or eliminates a behavior. As described previously, we can demonstrate that a rat will quickly stop pressing a bar if each press results in an electric shock. Negative punishmentNegative punishmentA method for reducing behavior by removing something desirable whenever the target behavior occurs.            Negative punishment        A method for reducing behavior by removing something desirable whenever the target behavior occurs.             involves the removal of something desirable. In the Skinner box, we can change the rules for a rat that has learned previously to press a bar for food. Now, food is made available unless the rat presses the bar. Under these conditions, the rat will also quickly stop pressing the bar. Putting up an umbrella to avoid getting wet from the rain is an example of a negatively reinforced behavior. Thorndike and Skinner were in agreement about the relative weakness of punishment as a means of controlling behavior. Part of the weakness of punishment effects arises from the difficulties of applying punishment effectively in real contexts. Three conditions must be met for punishment to have observable effects on behavior: significance, immediacy, and consistency (Schwartz, 1984). As we observed with reinforcement, consequences have to matter to the person or animal receiving them (i.e., significance). If we use a punisher that is too mild for a particular individual, there is little incentive for that person to change behavior. College campuses usually charge a fairly significant amount of money for parking illegally. However, there will typically be some students for whom that particular punishment is not sufficient to ensure that they will park legally. How high would a parking fee have to be to gain complete compliance on the part of the university community? What if you risked the death penalty for parking illegally? We can be fairly certain that most people would leave their cars at home rather than risk that particular consequence. The point is that punishment can work if a sufficiently severe consequence is selected, but using the amount of force needed to produce results is rarely considered practical and ethical. Free societies have long-standing social prohibitions against cruel and unusual punishments, and these conventions are incompatible with using the force that may be needed to change the behavior of some individuals. Immediate punishment is more effective than delayed punishment (i.e., immediacy). For the rat in the Skinner box, delays of just 10 seconds can reduce the effectiveness of electric shock as a punisher. Humans are more capable than rats at bridging long intervals (Kamin, 1959). Nonetheless, the same principle holds true. Delayed punishment is less effective than immediate punishment. We should not be too surprised that the months or years that are required to try and convict a serious criminal may greatly reduce the impact of imprisonment on that person’s subsequent behavior. The likelihood of getting a ticket influences drivers’ behavior. At an intersection with cameras, drivers are unlikely to run a red light, but at other intersections, behavior might be determined by whether a police officer is nearby. Our final requirement for effective punishment is its uniform application (i.e., consistency). College students are a prosocial, law-abiding group as a whole, yet many confess to determining their highway speed based on the presence or absence of a police car in their rearview mirrors. The experience of exceeding the speed limit without consequence weakens the ability of the possibility of tickets and fines to influence behavior. However, at intersections known to be controlled by cameras, compliance is generally quite high. If you are certain that running a red light will result in an expensive ticket, it would be foolish to test the system. To reduce undesirable behaviors, Skinner recommended extinction as an alternative to punishment (Skinner, 1953). In the discussion of classical conditioning earlier in this chapter, the term extinction was used to refer to the disappearance of CRs that occurs when the CS no longer signals the arrival of a UCS. Extinction in operant conditioning has a similar meaning. Learned behaviors stop when they are no longer followed by a reinforcing consequence. Attention is a powerful reinforcer for humans, and ignoring negative behavior should extinguish it. Parents and teachers cannot look the other way when one child is being physically aggressive toward another, but in many other instances, Skinner’s approach is quite successful in reducing the frequency of unwanted behaviors (Brown & Elliot, 1965). Although ignoring a child’s tantrums can be embarrassing for many parents, this can be an effective strategy for reducing their frequency. In this chapter, we have learned that rewards increase and punishments decrease the frequency of associated behaviors. But how do these consequences compare to each other? Do people learn faster when rewarded or punished? Is the relative effectiveness of reward and punishment the same for people of all ages? Participants were asked to choose the correct item from a pair of abstract symbols, and that their overall points (11 for a correct choice and 21 for an incorrect choice) would determine a final monetary prize (Palminteri, Kilford, Coricelli, & Blakemore, 2016). The Question: Do adolescents and adults respond differently to consequences? In this British study, 38 people (20 adults between the ages of 18 and 32 and 18 adolescents between the ages of 12 and 17) participated. They received 5 pounds (about $6.20) for participating, plus anywhere from zero to 10 pounds ($12.40) based on their performance in the task. They viewed pairs of abstract items (the Agathodaimon alphabet). As shown in Figure 8.8, their choices were either rewarded (happy smiley and +1 point) or punished (unhappy smiley and −1 point). Participants viewed these slides in order. First, they fixated on the cross. Next, they see both figures and select one. Finally, they receive feedback in the form of a smiley and point total (in this case, our participant was correct). Although the topic of punishment might make it sound like this study raises more than the typical ethical concerns about research, facing an unhappy smiley and failing to earn 10 pounds ($12.40) are unlikely to cause the participants significant distress. The risks of the study would be spelled out for prospective participants in an informed consent process. The participation of adolescents would require parent or guardian approval plus participant assent. In other words, adolescents can’t be forced to participate by their parents, but they cannot participate without parental approval. The adults learned equally well from punishment and reinforcement. In contrast, the adolescents were more likely to learn from reinforcement than from punishment. In other words, the adolescents were more likely to seek rewards than to try to avoid punishments. This research demonstrated that decision making based on the probabilities of reward and punishment continues to develop through adolescence into young adulthood. Unlike the adults, who were as likely to seek rewards as they were to try to avoid punishments, the adolescents appeared to pay more attention to rewards. Among the many implications of this study, we might conclude that using positive feedback to support learning in adolescents is likely to produce more benefits that using negative feedback. In other words, teens might learn more at school about what they’re doing correctly than what they’re doing incorrectly. College students, in contrast, are likely to benefit from both types of feedback. Reinforcing a behavior every time it occurs is known as continuous reinforcement. Although it is highly desirable to use continuous reinforcement when a new behavior is being learned, it is inconvenient to do so forever. Dog owners want their dogs to walk with a loose leash, but once this skill is learned, the owners don’t want to carry dog treats for reinforcement whenever they walk their dogs. Once we deviate from continuous reinforcement, however, the manner in which we do so may have a dramatic impact on the target behavior. To obtain the results we want, it is helpful to understand what happens when we use partial reinforcementpartial reinforcementThe reinforcement of a desired behavior on some occasions, but not others.            partial reinforcement        The reinforcement of a desired behavior on some occasions, but not others.            , or the reinforcement of the desired behavior on some occasions, but not others. Concerns about the effects of piecework on worker well-being contributed to the Fair Labor Standards Act of 1938, which included a provision for a minimum hourly wage. Psychologists have identified many ways to apply partial reinforcement, but we will concentrate on two variations: ratio schedules and interval schedules. In a ratio schedule of partial reinforcement, reinforcement depends on the number of times a behavior occurs. In an interval schedule of partial reinforcement, reinforcement depends on the passage of a certain amount of time. Either type of schedule, ratio or interval, can be fixed or variable. In fixed schedules, the requirements for reinforcement never vary. In variable schedules, the requirements for reinforcement are allowed to fluctuate from trial to trial, averaging a certain amount over the course of a learning session. A fixed ratio (FR) schedulefixed ratio (FR) scheduleA schedule of reinforcement in which reinforcement occurs following a set number of behaviors.            fixed ratio (FR) schedule        A schedule of reinforcement in which reinforcement occurs following a set number of behaviors.             requires that a behavior occur a set number of times for each reinforcer. Continuous reinforcement, discussed earlier, is equivalent to a FR of 1. If we now raise our requirement to two behaviors per reinforcer, we have a FR schedule of 2, and so on. Using the Skinner box, we can investigate the influence of FR schedules on the rate at which a rat will press a bar for food. To do so, we track cumulative responses as a function of time. FR schedules produce a characteristic pattern of responding. In general, responses are fairly steady, with a significant pause following each reward. As the amount of work for each reward is increased, responding becomes slower (see Figure 8.9). The schedule used to deliver reinforcement has a big impact on the resulting behavior. In general, the variable schedules produce higher rates of responding than do their fixed counterparts. The fixed interval schedule produces a characteristic pattern of low rates of responding at the beginning of the interval and accelerated responding as the end of the interval approaches. Psychologists typically recommend against the use of physical punishment with children, largely due to research showing a relationship between the use of physical punishment and increased aggressiveness on the part of a child. In addition, as you have seen in this chapter, there are many alternative ways to manage behavior successfully. As is the case with many types of psychological research, however, the classic studies on physical punishment and child aggression were conducted with middle-class, white American families. How representative are these samples of families in general? Some researchers believe that they are not representative and that physical punishment effects depend very much on cultural context (Deater-Deckard & Dodge, 1997). These researchers believe that physical punishment in cultures in which it is considered normal has a much less detrimental effect than in cultures where it is considered less normal. Racial and ethnic groups vary in the frequency with which they use physical punishment (Gershoff, Lansford, Sexton, Davis-Kean, & Sameroff, 2012). In a sample of over 11,000 U. S. families with kindergarten-aged children, rates of spanking were generally high (about 80%) across all groups, with 89% of African Americans, 79% of whites, 80% of Hispanics, and 73% of Asians reporting having spanked their child. When asked if they had spanked their child in the previous week, 40% of African Americans, 28% of Hispanics, 24% of whites, and 23% of Asians reported that they had done so. Does spanking have different effects on children’s behavior across the racial and ethnic groups due to these different frequencies?  Gershoff et al. (2012) concluded that it does not. Across all racial and ethnic groups, spanking was associated with higher levels of aggressive behaviors in the child, which in turn leads to more spanking in a coercive cycle of parenting. Psychologists have wondered if spanking had different effects within different racial and ethnic contexts, but it does not. Spanking is associated with higher levels of child aggression, regardless of racial or ethnic context. While many lines of research benefit from considerations of diversity, the message here is not modified by racial or ethnic identity—spanking children not only fails to decrease negative behaviors, but it actually appears to increase them. In early industrial settings, workers were often paid by the piece, a real-world example of the use of an FR schedule. In other words, workers would be paid every time they produced a fixed number of products or parts on an assembly line. Most workers find this system less than ideal. If the equipment malfunctions, the worker cannot earn money. Lunch breaks would also then be viewed as loss of income rather than a helpful time of rest. Some examples of piecework remain today, including the work of most physicians, who get paid by the procedure, and service workers like plumbers or hairstylists, who get paid for finishing a specific task. As in FR schedules, variable ratio (VR) schedulesvariable ratio (VR) schedulesA schedule of reinforcement in which reinforcement occurs following some variable number of behaviors.            variable ratio (VR) schedules        A schedule of reinforcement in which reinforcement occurs following some variable number of behaviors.             also involve counting the number of times that a behavior occurs. However, this time the required number of behaviors is allowed to fluctuate around some average amount. In the Skinner box, we might set our VR schedule to 10 for a 1-hour session. This means that over the course of the session, the rat must press an average of 10 times to receive each food pellet. However, this schedule may mean that only 1 press delivers food on one trial, but 30 presses are required on the next trial. The rat is unable to predict when reinforcement is likely to occur, leading to a high, steady rate of responding in our cumulative record. We do not see the characteristic pausing observed following reinforcement in the FR schedule because the rat cannot predict when the next reward will occur. One of the most dramatic real-world examples of the VR schedule is the programming of slot machines in casinos. Slot machines are human versions of Skinner boxes that use VR schedules. The casino sets the machine to pay off after some average number of plays, but the player doesn’t know whether a payoff will occur after one coin or thousands are inserted. You don’t have to observe the behavior of people playing slot machines long to see a demonstration of the high, steady responding that characterizes the VR schedule. The programming of slot machines can be sophisticated. Slot machines that are located in places where people are unlikely to return (airports and bus stations) pay off less frequently than those in places where people are more likely to play regularly. Unlike ratio schedules, reinforcement in interval schedules depends on the passage of time rather than the number of responses produced. In a fixed interval (FI) schedulefixed interval (FI) scheduleA schedule of reinforcement in which the first response following a specified interval is reinforced.            fixed interval (FI) schedule        A schedule of reinforcement in which the first response following a specified interval is reinforced.            , the time that must pass before reinforcement becomes available following a single response is set to a certain amount. In the Skinner box, a rat’s first bar press starts a timer. Responses that occur before the timer counts down are not reinforced. As soon as the timer counts down to zero, the rat’s next bar press is reinforced, and the timer starts counting down again. In the FI schedule, the interval is the same from trial to trial. Animals and people have a good general sense of the passage of time, leading to a characteristic pattern of responding in FI situations. Reinforcement is followed by a long pause. As the end of the interval is anticipated, responding increases sharply. A graph of the number of bills passed by the U. S. Congress as a function of time looks similar to the rat’s performance on an FI schedule in the Skinner box (Weisberg & Waldrop, 1972). Few bills are passed at the beginning of a session, but many are passed at the end (see Figure 8.10). Workers in the garment industry are often paid by the piece, or with a set amount of money for each finished garment. This compensation system is an example of a fixed ratio (FR) schedule. Because workers cannot make money when their equipment breaks down and they tend to view lunch and other breaks as costing them money, this schedule is not considered to be fair to workers. As the end of a congressional session approaches, the U. S. Congress begins to pass more bills in patterns that look similar to the behavior of rats on FI schedules in Skinner boxes. As you already may have guessed, the variable interval (VI) schedulevariable interval (VI) scheduleA schedule of reinforcement in which the first response following a varying period is reinforced.            variable interval (VI) schedule        A schedule of reinforcement in which the first response following a varying period is reinforced.             is characterized by an interval that is allowed to fluctuate around some average amount over the course of a session. This time, our bar-pressing rat experiences intervals that range around some average amount (say, 2 minutes). On one trial, the rat may obtain reinforcement after only 30 seconds, whereas the next trial may involve an interval of 5 minutes. Over the session, the average of all intervals is 2 minutes. As in the VR situation, we see a high, steady rate of responding. You are probably quite familiar with VI schedules, in the form of pop quizzes administered by your professors. Your professor might tell you that there will be five quizzes given during the term, but the timing of the quizzes remains a surprise. You might have the first two only 1 day apart, followed by a 2-week interval before the next quiz. Your best strategy, like the rat in the Skinner box on a VI schedule, is to emit a high, steady rate of behavior. Many parents have regretted the day that they unintentionally put an unwanted behavior on a partial reinforcement schedule by uttering the words, OK, just this once. Perhaps a parent is strongly opposed to buying candy for a child at the supermarket checkout counter (where, because of John Watson and his applications of psychology to advertising, candy is displayed conveniently at child’s-eye height). Then comes the fateful day when the parent is late coming home from work, the child is hungry because dinner is delayed, and unintentionally, the parent gives in just this once, putting begging for candy on a variable schedule. Subsequently, when the parent returns to the previous refusal to buy candy, a high, steady rate of begging behavior occurs before it finally extinguishes. Most casinos feature a large number of slot machines, which are essentially Skinner boxes for people. The slot machine is programmed on a variable ratio (VR) schedule, which means that the player cannot predict how many plays it will take to win. In response, players exhibit the same high, steady rate of responding that we observe in rats working on VR schedules in the laboratory. In the laboratory once more, we compare the behavior of two rats in Skinner boxes. One is working on a continuous (or FR 1) schedule of reinforcement. The other is working on a partial schedule of reinforcement (perhaps a VR 3 schedule). After several sessions of training, we stop reinforcement for both rats. It may surprise you to learn that the rat working on the continuous schedule will stop pressing long before the rat accustomed to the VR 3 schedule. In other words, extinction occurs more rapidly following continuous reinforcement than following partial schedules. This outcome is known as the partial reinforcement effect in extinctionpartial reinforcement effect in extinctionThe more rapid extinction observed following continuous reinforcement compared to that following partial reinforcement.            partial reinforcement effect in extinction        The more rapid extinction observed following continuous reinforcement compared to that following partial reinforcement.            . The partial reinforcement effect probably occurs because of one of two factors, or a combination of both. First, the transition from continuous reinforcement to extinction is more obvious than the transition from a partial schedule to extinction. If you are accustomed to being paid for a babysitting job every time you work, you will definitely notice any of your employer’s failures to pay. In contrast, if your neighbor typically pays you about once a month for raking his yard each weekend, you might not notice right away that he hasn’t paid you for a while. Second, partial schedules teach organisms to persist in the face of nonreinforcement. In a sense, partial schedules teach us to work through periods in which reinforcement does not occur. Consequently, we might view extinction as just another case where continuing to perform might eventually produce reinforcement. When positive behavior is occurring, such as working on your senior thesis regularly despite a much-delayed grade, persistence is an enormous advantage. However, as shown in our earlier example of begging for candy, placing an undesirable behavior on partial reinforcement makes it more difficult to extinguish. Fishing works according to a variable interval (VI) schedule of reinforcement. Fish (the reinforcers) are caught after periods of waiting for fish to bite that vary in length. As in laboratory demonstrations of the VI schedule, fishing usually produces a steady rate of responding. What happens if you are exposed to two or more schedules of reinforcement at the same time? This scenario is realistic because we face these types of choices every day. Which is a more rewarding use of my time: studying for my midterm or making some extra money by working overtime? In making these types of choices, animals and people follow the matching law, which states that the relative frequency of responding to one alternative will match the relative reinforcement for responses on that alternative (Herrnstein & Heyman, 1979). The law powerfully accounts for the effects on behavior of frequency, magnitude, and delays in reward. Time spent playing online video games provides an interesting example of the effects of simultaneous schedules of reinforcement. The millions of users of massively multiplayer online games, such as League of Legends and Crossfire, spend an average of 22 hours per week on their games (Yee, 2006). What compels these people to make such a lopsided choice between online interactions and real-life social experiences? One clue to this choice is that substantial numbers of players report that the most rewarding or satisfying experience they had over the last 7 or 30 days took place while gaming. We would assume that if the frequency and magnitude of rewards available in gaming were higher than those in real-life socializing, people would choose to spend their time accordingly. So far, this discussion of operant conditioning has centered on increasing or decreasing the frequency of a particular behavior. What happens if you want to increase the frequency of a behavior that rarely or never occurs? Most parents would like to teach their children to use good table manners, but you could wait a long time for the opportunity to reward young children for using the correct utensil to eat their food. Fortunately, we have a method for increasing the frequency of behaviors that never or rarely occur. Using the method of successive approximationsmethod of successive approximationsA method for increasing the frequency of behaviors that never or rarely occur; also known as shaping.            method of successive approximations        A method for increasing the frequency of behaviors that never or rarely occur; also known as shaping.            , or shaping, we begin by reinforcing spontaneous behaviors that are somewhat similar to the target behavior that we want to train. As training continues, we use gradually more stringent requirements for reinforcement until the exact behavior that we want occurs. You can think of shaping as a funnel. Using the table manners example, parents start out with generous criteria for reinforcement (Thank you for picking up the spoon) and gradually narrow the criteria (Thank you for putting the spoon in the food) until they are reinforcing only the target behavior (Thank you for using the spoon to eat your applesauce). One of the most positive features about the shaping process is that behavior doesn’t have to be perfect to produce reinforcement. Whether training imaginary raptors in Jurassic World or real animals, using the method of successive approximations (shaping) can lead to the reliable performance of otherwise low-frequency behaviors. Like Chris Pratt’s character, most trainers use a combination of classical conditioning (clicker sound leads to food) and operant conditioning (approximation of desired behavior leads to clicker sound). The rats in Skinner boxes that have been described in this chapter do not spontaneously start pressing levers. We have to teach them to do so. We begin by making sure the hungry rat understands that food is available in the Skinner box. Using a remote control, we activate the food dispenser a few times. Quickly, the rat forms a classically conditioned association between the sound of the food dispenser (CS) and the arrival of food (UCS) in the cup. However, if we continue to feed the rat in this manner, it is unlikely that it will ever learn to press the bar. Indeed, there is no reason for it to do so because it already is obtaining the food it needs. So, we narrow our criteria for obtaining food from simply existing in the box to standing in the corner of the box that contains the bar. If we press our remote control every time the rat is in the correct corner, it will begin to stay there most of the time. Now we want the rat to rear on its back feet so that it is likely to hit the bar with its front feet on the way down. If we begin to reinforce the rat less frequently for staying in the corner, it will begin to explore. Eventually, the rat is likely to hit the bar with its front feet while exploring, pressing the bar. Now it will begin to press the bar on its own. In the hands of an experienced trainer, this process takes about half an hour. Shaping involves a delicate tightrope walk between too much and too little reinforcement. If we reinforce too generously, learning stops because there is no incentive for change. If your piano teacher always tells you that your performances are perfect, you will stop trying to improve them. However, if we don’t reinforce frequently enough, the learner becomes discouraged. Reinforcement provides important feedback to the learner, so insufficient reinforcement may slow or stop the learning process. Teaching a complex behavior requires chaining, or breaking down the behavior into manageable steps. Chaining can be done in a forward direction, such as teaching the letters of the alphabet from A to Z, or in a backward direction, such as teaching the last step in a sequence, then the next to the last, and so on. Chaining can be useful when teaching new skills, such as working independently on academic projects, to children with special needs (Pelios, MacDuff, & Axelrod, 2003). Backward chaining is used by most trainers of animals used in entertainment. For example, dogs have been taught to perform complex dances like the Macarena (Burch & Bailey, 1999). The trainer uses a verbal, gestural, or clicker cue while shaping the last step in the dance. When the dog performs this last step reliably, the trainer adds the next-to-last step, and so on until the entire complex sequence is mastered. Even the most radical behaviorists, including Skinner, did not deny the existence of cognitive, social, or biological influences on learning (Jensen & Burgess, 1997). Instead, behaviorists believed that internal processes followed the same rules as externally observable behavior. As  Skinner (1953) wrote, We need not suppose that events which take place within an organism’s skin have special properties…. A private event may be distinguished by its limited accessibility but not, so far as we know, by any special nature or structure (p. 257). However, as we saw in the case of classical conditioning, the results of some operant conditioning experiments stimulated greater interest in the cognitive, social, and biological processes involved in learning. One of the important principles of operant conditioning is that consequences are required in order for learning to occur. Edward  Tolman (1948) challenged this notion by allowing his rats to explore mazes without food reinforcement. Subsequently, when food was placed in the goal boxes of the mazes, the previously unreinforced rats performed as well as the rats that had been reinforced all along. Tolman referred to the rats’ ability to learn in the absence of reinforcement as latent learninglatent learningLearning that occurs in the absence of reinforcement.            latent learning        Learning that occurs in the absence of reinforcement.            . He argued that the rats had learned while just exploring, but that they did not demonstrate their learning until motivated by the food reward to do so. We usually judge whether learning has occurred by observing outward behavior. Tolman’s rats remind us that there is a difference between what has been learned and what is performed. Students are all too familiar with the experience of performing poorly on exams despite having learned a great deal about the material. In addition to challenging the role of reinforcement in learning, Tolman disputed traditional behaviorist explanations of the nature of the learning that occurred in mazes. He believed that instead of learning a simple operant turn right for food association, rats learned This is where I can find food (Tolman, 1948,  1959). After training rats to follow a path in a maze to find food, Tolman blocked the path but allowed the rats to choose from a number of additional paths. If the rats had learned a simple turn left–get food response, they should have chosen the paths that were most similar to the training path. Instead, they showed evidence of choosing paths that required them to turn in a different direction from their previously trained path, but one that led them directly to the goal (Tolman, Richie, & Kalish, 1946; see Figure 8.11). Edward Tolman did not believe that rats wandering around a maze learned turn left for food in the way that early behaviorists believed that they did. Instead, Tolman believed that the rats were learning a more cognitive map of where they could find food. He provided evidence for his approach by blocking a learned pathway to food. If the behaviorists were right, the rats should choose the path most similar to the trained one. However, the rats did not do that. They showed evidence of having formed cognitive maps and were willing to turn in a different direction if that path led to food. To account for his results, Tolman suggested that the rats had formed cognitive maps, or mental representations of the mazes. Map formation was viewed as a unique, nonassociative learning process that didn’t follow the previously established rules of associative learning (O’Keefe & Nadel, 1978). For example, in contrast to the gradual acquisition of learning that usually occurs in classical and operant conditioning, cognitive maps are instantly updated when new information becomes available. Chimpanzees show considerable ability to form cognitive maps (Menzel, 1978). After being carried around a circuitous route in their one-acre compound as nine vegetables and nine fruits were placed in 18 locations, chimpanzees were released in the center of the compound. They not only navigated to each food location using the shortest pathways, but, given their preference for fruit over vegetables, they visited the spots containing fruit first. They showed no indication that they were attempting to retrace the pathway over which they were carried as the food was put in place. Just as the work of Garcia and Koelling highlighted the need to consider biological limitations on classical conditioning, biological boundaries in operant conditioning were described by Keller and Marion Breland, two of Skinner’s former students. In their 1961 article titled The Misbehavior of Organisms (a wordplay on Skinner’s classic book, The Behavior of Organisms), the Brelands outlined some challenges they encountered while using operant conditioning to train animals for entertainment. In one instance, the researchers described how they sought to train a pig to pick up large wooden coins and deposit the coins in a large, wooden piggy bank. Initially, all went well. The pig would quickly learn to deposit four or five coins (an example of an FR schedule) for each food reward. Eventually, however, the pig began to work slower and slower, to the point where it couldn’t obtain enough food for the day. Instead of taking the coins to the piggy bank, the pig would repeatedly toss them in the air and sniff around to find them. Raccoons trained with the coins ultimately tried to wash them instead of depositing them in the bank. The animals’ natural approach to food (the rooting by the pigs and the washing by the raccoons) began to interfere with their handling of the coins. You may already have suspected that the coins had become the object of some higher-order classical conditioning because of their relationship with food. The Brelands concluded that these animals are trapped by strong instinctive behaviors, and clearly we have here a demonstration of the prepotency of such behavior patterns over those which have been conditioned. We have termed this phenomenon ‘instinctive drift’ (Breland & Breland, 1961, p. 683). Keller Breland observes the performance of one of the star pupils of the I. Q. Zoo attraction he developed with his wife, Marian. Unfortunately, the Brelands discovered that the animals’ instinctive behaviors often interfered with their training. The Brelands referred to this phenomenon as instinctive drift. So far, this discussion of classical and operant conditioning has focused on the individual in isolation. Learning can take place when people or animals are alone, but it often occurs in the presence of others, especially in a species as social as ours. As we will see in a later section, people are particularly likely to learn by observing others. What do we know about the impact of others on our operant learning? The presence of others may not just promote learning; it also may be necessary for learning. Human infants learn more about language when they are listening to another person face to face than when they are watching a person speak on television (Kuhl, 2007;  Meltzoff, Kuhl, Movellan, & Sejnowski, 2009). Although operant conditioning alone cannot account for language learning, as discussed in Chapters 10 and 11, these results emphasize the importance of social interaction in producing the arousal, focus, and motivation that contribute to effective learning. Experienced whale trainer Dawn Brancheau was killed by one of her favorite killer whales during a 2010 show at Sea World in Orlando, Florida. Animal experts believed that the whale had simply reverted to normal whale behavior, similar to the instinctive drift observed by Keller and Marian Breland. As mentioned previously in the discussion of cognitive maps, learning and the performance of learned behavior are not always identical. Our performance of learned behaviors varies depending on an interaction between the presence of others and the complexity of the learned task. For simple tasks, like pedaling a bicycle or reeling in a fishing line, the presence of others makes us perform faster, a phenomenon known as social facilitation (Triplett, 1898; also see Chapter 13). In complex tasks, such as taking a difficult college entrance exam, the presence of others can make our performance slower and poorer. Again, this effect is not restricted to complex organisms like ourselves; the same results can be observed in the lowly cockroach (Zajonc, 1965). In a straight maze leading to food, cockroaches with an audience of other cockroaches ran quickly. In a more complex maze involving several turns, the cockroaches responded to an audience by running more slowly. Important applications of operant conditioning may be found in contemporary approaches to psychotherapy, education, advertising, politics, and many other domains. Possibly one of the oddest applications of operant conditioning was Skinner’s secret World War II defense project, code-named Project Pigeon. Lagging well behind the Nazis in the area of guided missile technology, the United States invested $25,000 (worth about $400,000 in today’s dollars) in Skinner’s organic homing device (Capshew, 1993). Skinner, who had considerable experience training pigeons to peck at visual stimuli in his laboratory, now trained them to peck at a projected image of a missile’s target. Riding in a chamber within the missile, the pigeon’s pecks would be translated into updated commands for correcting the path of the bomb. Unfortunately for Skinner (but fortunately for his pigeons), Project Pigeon elicited laughter from military officers instead of approval (Skinner, 1960). Although never implemented, Project Pigeon stimulated Skinner and his intellectual descendents to look outside the laboratory for useful extensions of their work on learning. B. F. Skinner’s Project Pigeon was one of the more bizarre applications of operant conditioning research. Pigeons enclosed in this capsule were trained to peck at projected images of bomb targets. Even though Skinner’s device was superior to other World War II missile guidance systems, it was never implemented. A widely used application of operant learning is the token economytoken economyAn application of operant conditioning in which tokens that can be exchanged for other reinforcers are used to increase the frequency of desirable behaviors.            token economy        An application of operant conditioning in which tokens that can be exchanged for other reinforcers are used to increase the frequency of desirable behaviors.            . Money, in the form of coins, bills, bitcoins, or bank statements, is fairly useless. You can’t eat it, wear it, or shelter in it. Nonetheless, people value it because it takes on secondary reinforcing qualities due to its history of association with other things that have intrinsic value. The use of money to buy things of personal value is an example of a token economy. You earn money for doing certain things, and then you have the opportunity to trade the money you earned for items of value to you. This system meets the best practices criteria that have been described in the context of positive reinforcement. Each person can obtain reinforcement that has unique personal value. One friend may spend discretionary money on going out to dinner, while another invests in the stock market. Both find money reinforcing for doing work. Token economies can be effective ways of managing behavior. Tokens, including money, can be traded for a valued reinforcement of the worker’s choice. This woman’s purchase might motivate her work, but another worker might use the same paycheck to buy a motorcycle or go on vacation. An informed approach to compensating employees should include consideration of learning principles. Menu approaches to employee benefits provide an excellent example of this application. Historically, employers offered a set program of health, retirement, and other benefits to their entire workforce regardless of individual needs. We would expect this approach to be minimally reinforcing because it does not match reinforcers to worker priorities. Catering a benefits package to individual needs is more sensible. A young worker in good health might be more motivated by a benefits package that includes childcare, while a more mature worker may worry about long-term care in the event of a disability. By allowing workers to select their benefits from a menu, everybody can find something worth earning. All of us respond positively to token economies, but they are especially useful in educational and institutional settings. Teachers provide frequent rewards in the form of checks, stars, or tickets that can be exchanged later for popcorn parties or a night without homework. The key to an effective token economy is to offer ultimate rewards that are truly valuable to the people you wish to motivate. If students don’t care about popcorn parties, offering them will have little effect. Token economies are equally useful in prison settings and in institutions serving people with an intellectual disability or mental illness. As we will discuss in Chapter 15, learning theories also have been applied successfully to the clinical setting in the form of behavior therapies, also called applied behavior analysis. After all, our formal definition of learning states that it involves a change in behavior, and changing behavior is precisely what therapists usually seek to do. In addition to the extinction and counterconditioning applications of classical conditioning, behavior therapies use operant conditioning concepts such as extinction, reward, and on rare occasions, punishment. An important application of operant conditioning principles is their use in behavior therapies for conditions like autism spectrum disorder. Operant conditioning can be used to increase the frequency of language use and socially appropriate behaviors, like eye contact. Coupled with cognitive methods designed to address the way that people think about their circumstances, these methods comprise the most popular and effective means for treating many types of disorder, from substance abuse to depression. One of the most dramatic applications of behavior therapy is the treatment for autism spectrum disorder pioneered by O. Ivar Lovaas (Lovaas, 1996;  Lovaas et al., 1966). Autism spectrum disorder is characterized by language and social deficits. Although behavior therapy doesn’t cure autism spectrum disorder, behavioral interventions, like the use of chaining described previously, typically improve an individual’s level of functioning. We all have behaviors that could use some improvement. Maybe we eat poorly, drink too much, smoke, or lash out angrily at others. An understanding of the processes of learning provides us with powerful tools for changing behavior. Let’s assume that your eating habits, like those of many students, do not meet the My mom would approve standard. Yet you are learning in your psychology course that good health habits are essential tools for managing stress (see Chapter 16). How do you bring about the necessary changes? Before doing anything to produce change, you need to understand your current behavior. Many people have a poor understanding of what they actually eat during a day, so you could start by keeping a diary. What foods, and how much of them, do you eat? What else is going on when you eat well or poorly? What possible reinforcers or punishers are influencing your eating patterns? For example, let’s say that you observe a tendency to eat high-calorie snacks late at night while studying, even when you are not hungry. Your goal, then, is to eliminate these late-night snacks. Your baseline shows that your snacking is a social behavior. You consume these extra foods only when studying with a group. The social camaraderie and good taste of the food serve as powerful reinforcers for the behavior. Now that you have a better understanding of your problem behavior, you are in a good position to construct a plan. An important part of your plan is to choose appropriate consequences for your behavior. Again, it is essential that we design consequences that are meaningful to individuals. As we have argued in this chapter, positive reinforcement has many advantages over punishment. You might try placing the money you’re saving on junk food in a designated jar to buy a special (nonfood) treat at the end of a successful week, or allow yourself an extra study break each night that you meet your goals. If you are convinced that the only way that you will change is through punishment, you could take an alternative approach. Although some people have successfully stopped smoking through the use of positive punishment, including electric shock (Law & Tang, 1995), this sounds quite unpleasant. Instead, a negative punishment, such as losing texting privileges for the day following a failure to meet one’s goals, might work. As you implement your program, track your progress and make any modifications that seem necessary. In addition to the improvement of your target behavior, a beneficial side effect of applying learning methods to your behavior is the knowledge that given the right tools, you can control your behavior. ScheduleFeaturesIn the labEveryday exampleFixed ratioReinforcement occurs after a set number of responses. A rat presses 3 times for each food pellet. A garment worker is paid for finishing 10 shirts. Variable ratioReinforcement occurs after a variable number of responses, which average a set number for a session. A rat is fed on average after three responses, but the number of responses required for obtaining food varies between 1 and 15. People play slot machines and win, sometimes on the first play and other times after thousands of plays, on a schedule determined by the casino. Fixed intervalEach response begins an interval during which no reinforcement is available. The first response after the interval is reinforced. A rat’s first press after each 1-minute interval has timed out is reinforced. Students study more hours right before finals than at the beginning of the term (although unlike in the rat’s case, this behavior contributes to reinforcement). Variable intervalEach response begins an interval of varying length, with the average length for the session set by the experimenter. A rat’s first press after an interval is reinforced. The interval averages 1 minute over the session, but reinforcement could be obtained after intervals ranging between 10 seconds and 3 minutes. A fisherman trails his line behind the boat, and at various time intervals, a fish is caught. The ability to learn by watching others, known as observational learning, provides considerable advantages, especially in a social species like our own. Learning occurs without personally experiencing negative consequences. This ability to learn from observing others greatly expands our learning capacity, especially when we then generalize from these concrete examples (watching successful students) to produce effective rules (good time management is important to being a successful student). Observational learning also can have a dark side, as will be seen in our later discussion of the pioneering work by Albert Bandura and his colleagues (Bandura, Ross, & Ross, 1963;  Bandura, 1965) on the modeling of aggression by children. Not only do we learn by observing others, but it appears that observational learning can override other influences on behavior, possibly because we are such a social species. Parents learn (often the hard way) that children are more likely to pattern behavior after what they observe their parents doing than after what they hear their parents say. Various behaviors, both positive and negative, appear to be influenced by observation, including aggression, achievement motivation, language development, phobias, cognitive development, moral judgment, and suicidal behavior. People benefit greatly from exposure to positive role models, especially those with whom they can identify. We worry about the relatively small number of women in university math and science faculties, not only because of the possibility of discrimination, but also because seeing women in these positions might inspire young girls to follow in their footsteps. However, the use of performance-enhancing drugs by sports heroes might lead to role modeling of a different sort by young people. Babies learned more Chinese when listening to a person face to face than while watching the same person speak on a television monitor. It is easy to find examples of observational learning in our daily lives. New college students identify successful, more experienced students in their classes and copy their behavior. Stumped by your new computer software, not to mention the manual that came with it, you watch as a tech-savvy friend shows you how to make it work. Young athletes pore over films of superstars to perfect their technique. Popular cooking shows on television teach you to prepare a special meal. Cross-generational cycles of domestic violence persist as children continue the patterns of aggressive behavior they observe in their parents and grandparents. The goal of this section is to identify the circumstances in which this type of learning occurs and the variables that affect its outcomes. Bandura’s work on the observational learning of aggression provides one of the strongest arguments against exposing children to violent media (Bandura, Ross, & Ross, 1963;  Bandura, 1965). He was interested in imitationimitationCopying behavior that is unlikely to occur naturally and spontaneously.            imitation        Copying behavior that is unlikely to occur naturally and spontaneously.            , which is defined as the copying of behavior that is unlikely to occur naturally and spontaneously (Thorpe, 1963). Observational learning can provide a quick and easy way to learn without having to go through individual trial and error. In a series of classic studies of the imitation of aggression in children, Bandura observed children’s interactions with a Bobo doll, an inflatable toy clown with sand in the bottom, which allows it to rock back and forth when pushed. A number of 3- to 5-year-olds watched an adult model physically and verbally assault an unfortunate Bobo doll (Bandura et al., 1963). The adult yelled, Pow, right in the nose! when punching the doll in the face; Sockeroo, stay down! when hitting the doll with a mallet; Fly away! when kicking the doll around the room; and Bang! when throwing a ball at the doll. Subsequently, when the children were allowed an opportunity to play with a Bobo doll, they displayed a significant amount of aggression. In a later study, one group of children saw the adult being rewarded for aggression with candy and soda (Bandura, 1965). A second group of children saw the adult being verbally reprimanded, spanked, and threatened. The third group did not see any positive or negative consequences for the adult’s actions. The group that witnessed the punishment of the adult model showed slightly less aggression than children in the other groups did. After watching an adult model assault the Bobo doll, young children copied the adult’s movements and verbalizations. Bandura identified four necessary cognitive processes in the modeling of others’ behavior: attention, retention, reproduction, and motivation. Models that get our attention are more likely to elicit imitation. A person must retain a memory of what the model did. We must be able to reproduce the behavior. Many of us enjoy watching elite athletes perform, but no matter how long and often we watch Serena Williams or LeBron James, few of us have the talent to duplicate their movements. If you happen to play tennis or basketball, however, you can learn to improve your game if you carefully observe these superstars. Finally, a person must have a motivation for imitating the behavior. Either past or anticipated reinforcement encourages us to model another person’s behavior. In vicarious reinforcement, witnessing somebody else getting reinforced for a behavior raises the likelihood that we will imitate the behavior. At the same time, witnessing the other person getting punished for the behavior should reduce the likelihood that you will copy it. An individual’s learning may serve that person well throughout a lifetime, but the invention of culture provides opportunities to pass the benefits of experience along for many generations. A society is a group of people living together. Culture, in contrast, consists of all the socially transmitted information used by the group of people, including ideas, concepts, and skills. Observational learning in particular provides a powerful tool for transmitting this information over time. Humans are not the only species to show evidence of imitation. Richard  Dawkins (1976) envisioned a way to break culture down into observable parts. He referred to the basic unit of cultural transmission as a meme. Memes, he said, are transmitted by observational learning from one person to another and can take the form of ideas, symbols, or practices. Melodies, religious beliefs, catchphrases, and the technology for building arches are examples of memes. Dawkins viewed memes as the cultural equivalents of genes—they replicate from one person to the next and they respond to selection pressure. Memes that provide an advantage, such as knowledge of the use of fire, are likely to continue. Those that do not confer much advantage, such as some fads, are likely to die out quickly. Still others, such as pagers, are abandoned when more effective replacements (cell phones) emerge. Among the most social of memes are Internet memes, which are inside jokes passed to others using technologies such as social networking sites and e-mail. Special websites that chronicle Internet memes allow viewers to provide updates of their favorite memes, which contributes to their popularity. Richard Dawkins defined meme as the basic unit of cultural transmission passed to others by observational learning. Internet memes are often unpredictable. Chewbacca Mom Candace Payne never imagined that her short video made for friends would become a viral sensation. Knowledge of the way that people learn can improve your social life, and possibly even your love life. Operant conditioning can help you decrease unwanted behaviors and increase desired behaviors toward you by people with whom you interact. Your behavior influences the way that others behave toward you. If you regularly find that you are treated poorly in relationships, understanding the learning perspective provides powerful tools for change. This chapter has recommended an emphasis on regularly noticing and rewarding desired behaviors. It is easy to fall into the trap of feeling entitled to good behavior from the people who are close to us, which can lead to these behaviors being taken for granted and ignored. Without positive reinforcement, these good behaviors might be extinguished. It takes little time and energy to thank people for the nice things they do for us, and this simple courtesy can increase the frequency of positive interactions in the future. Ignoring annoying behavior can be difficult, but you don’t want to reinforce it with attention. When undesirable behaviors inevitably occur, many people turn to punishment. Skinner believed that part of the love affair that we have with punishment is because of the reinforcing properties of punishment to the punisher.  Skinner (1971) stated, We ‘instinctively’ attack anyone whose behavior displeases us—perhaps not in physical assault, but with criticism, disapproval, blame, or ridicule (p. 190). Punishing a partner for bad behavior might make you feel better, but this comes at a significant cost. These punishing behaviors are not endearing, and frequent use of them is likely to end relationships. If punishment is off the table, what then do we recommend you do when you experience negative behavior from a partner? If possible, try to ignore negative behaviors, putting them on extinction. Unfortunately, some people would rather have negative attention from you than no attention and prefer punishment from you to being ignored. This is particularly likely to be the case if you have forgotten to reinforce positive behaviors. If you combine positive reinforcement of good behavior and extinction of negative behavior, you should notice quite an improvement. Some behaviors like aggression cannot be ignored and require either a complete end to the relationship or professional counseling. Thoughtfully observing the way that you treat other people and their reactions to your behavior, using the learning principles described in this chapter, should provide you with the understanding that you need to improve your relationships. In this chapter, we defined observational learning as the ability to learn by watching others. Both prosocial and antisocial behaviors, including aggression, can be learned this way. We also reviewed Albert Bandura’s conditions for observational learning: attention, retention, reproduction, and motivation. To what extent can we use these principles to explain the occurrence of cyberbullying? A study conducted with over 4,000 teens supported the idea that cyberbullying was influenced by observational learning (Hinduja & Patchin, 2013). Having friends who also engaged in cyberbullying was a strong predictor of being a cyberbully (see Figure 8.12). A total of 62% of young people who indicated that all or most of their friends had cyberbullied in the previous six months admitted to cyberbullying themselves, in contrast to only 4% of the participants who had no friends who cyberbullied. Observing your friends engaging in cyberbullying might be similar to seeing Bandura’s adults assaulting the Bobo doll—the behavior is normalized. It is also possible, however, that birds of a feather flock together, or that antisocial teens tend to congregate in peer groups. Still, using the particular outlet of cyberbullying for antisocial tendencies requires the attention and exposure identified as important variables by Bandura. Teens who know that their friends participate in cyberbullying are far more likely to cyberbully themselves than teens who do not report having friends who cyberbully. While this might be yet another example of birds of a feather flocking together, it is also possible that observing others being aggressive leads to acting aggressively yourself, as predicted by Albert Bandura. Hinduja and Patchin (2013) also presented evidence supporting Bandura’s principle of motivation in observational learning. Bandura argued that anticipated rewards and punishments influence the likelihood that we will copy a behavior. The teens who reported a strong likelihood of punishment from either their parents or school for cyberbullying were the least likely to engage in the practice, although this effect was smaller for the teens who associated with other bullies. Among the bullies, rewards related to peer acceptance might outweigh the negative sanctions from adults to some extent. Nonetheless, understanding the nature of the group support enjoyed by cyberbullies and the importance of anticipated sanctions can give schools and communities better tools for reducing cyberbullying. Establishing a peer climate that clearly labels cyberbullying as deviant, undesirable behavior, coupled with reliable, significant sanctions, might put a much-needed dent in the frequency of this behavior. FeatureDescriptionExampleAttentionWe are more likely to model the behavior of people who get our attention. Children wear the jerseys of the best players in the sport. MemoryWe must retain a memory of the behavior to be imitated. A student re-creates from memory a math proof demonstrated earlier that day by a professor. ReproductionWe must have the ability to reproduce the behavior. An athlete works on her technique after watching films of an elite athlete in her sport. MotivationPast or anticipated reinforcement for a behavior motivates us to perform it too. One student received extra credit for participating in an experiment, so his friends also signed up to participate.  `;
