export const text = `Scientific methods, including brain imaging, have allowed researchers to pinpoint structural and functional differences between the brains of fluent speakers and those of people who stutter. Among the many celebrities who stutter is action actor Samuel L. Jackson. At a gala for the American Institute for Stuttering, Jackson shared one of his strategies for avoiding stuttering—uttering a particular swear word under his breath. Stuttering involves disruptions in normal speech production, such as repeating the starting letter of a word (b-b-b-bird), holding a vowel sound for a long time (ah ah ah), or having difficulty initiating speech. About 5% of the population experience stuttering (Månsson, 2000), with males two to five times more likely to stutter than females (Craig & Tran, 2005). Stuttering has been described since the days of the ancient Greeks and occurs across all cultures and ethnicities. Many interesting myths exist regarding its causes and remedies (Kuster, 2005). South African traditions suggest that stuttering results from leaving a baby out in the rain or tickling the infant too much. A Chinese folk remedy for stuttering was to hit the person in the face when the weather was cloudy. In Iceland, people believed that a pregnant woman who drank from a cracked cup was likely to produce a child who stuttered. Many of us might ask, How could anybody believe these things? But how do we know these are merely myths and not facts? Instead of dismissing these efforts to explain and predict, think about what might have happened to lead people to these particular conclusions. What is missing from these conclusions is a system for reaching logical, objective results. Science provides us with this system. What does science have to say about stuttering? Based on the careful evaluation of stuttering using the methods outlined in this chapter, scientists have concluded that there are multiple causes for stuttering (Kang et al., 2010). Many cases seem to have a basis in genetics, which is discussed in Chapter 3 (Raza et al., 2015). Scientists have used brain-imaging technologies to zoom in on brain structures and functions that appear to differ between stutterers and fluent speakers, with stutterers showing more activation of the right hemisphere during speech (Gordon, 2002). As mentioned in the previous chapter, some of the most thorough explanations combine multiple psychological perspectives (Ward, 2013). A complete explanation of stuttering zooms back out to combine a predisposition for the problem resulting from genetics and biology with developmental, emotional, and social factors, like feeling embarrassed or anxious about speaking in front of peers. Although there is no cure for stuttering, carefully tested scientific explanations combining input from various perspectives are leading to more effective treatments. In this chapter, you will learn how science provides a system that allows us to construct increasingly realistic models of the world around us. Science has provided explanations for many natural phenomena, like lightning, that were probably quite frightening for our ancestors.Throughout human history, we have been motivated to understand, predict, and control the world around us. To meet these goals, we need methods for gaining knowledge. We often take contemporary scientific knowledge for granted, but our ancestors did not enjoy the benefits of science while trying to explain and predict their world. Early in history, people attempted to understand natural phenomena by applying human characteristics to nature (Cornford, 1957). Skies could look angry or a lake could be calm. Other explanations involved spirits inhabiting humans and all other objects. Earthquakes and illness were viewed as the actions of spirits, and people attempted to influence these spirits through magical rituals. Later on, people looked to authorities, such as religious leaders and philosophers, for explanations of natural phenomena. People often form strong beliefs about their world based on faith, which literally means trust. Faith is belief that does not depend on logical proof or evidence. We might accept friends’ excuses for being late based on our faith in their honesty, without knowing for certain whether they are telling the truth. In contrast to faith, science requires proof and evidence. The word sciencescienceA method for learning about reality through systematic observation and experimentation            science        A method for learning about reality through systematic observation and experimentation             comes from the Latin scientia, which means knowledge. Science doesn’t refer to just any type of knowledge, but rather to a special way of learning about reality through systematic observation and experimentation. The methods described in this chapter are designed to supply that evidence. Throughout history, people have often turned to authorities, such as religious leaders, instead of to science. The astronomer Galileo Galilei was interrogated as part of the Roman Inquisition for believing that the Earth was not the center of the universe.Not all observations are scientific. How does science differ from everyday observations, like the belief that opposites attract? As you will learn in Chapter 13, opposites do not, in fact, find each other very attractive. First, science relies on objectivityobjectivityThe practice of basing conclusions on facts, without the influence of personal emotion and bias.            objectivity        The practice of basing conclusions on facts, without the influence of personal emotion and bias.            , rather than subjectivity. ObjectivityObjectivityThe practice of basing conclusions on facts, without the influence of personal emotion and bias.            Objectivity        The practice of basing conclusions on facts, without the influence of personal emotion and bias.             means that conclusions are based on facts, without influence from personal emotions or biases. In contrast, subjectivity means that conclusions reflect personal points of view. In a study by Allport and Postman (1945), research participants described what they had heard about photos from memory. Some participants switched the race of a man threatening another person from White (which was the objective fact in the photo) to Black, possibly to fit a biased, subjective worldview. Scientists strive to be objective, but any observation by a human is, by definition, subjective. Recognizing when we are being subjective can be difficult, so scientists cannot rely on their introspections to maintain objectivity. Most of us have had the experience of witnessing an accident in the presence of other people. It can be astonishing to hear the different accounts of what happened. Didn’t we all see the same thing? Individuals like to believe that their own view of the events is the accurate one. As discussed in Chapters 9 and 10, objective facts can be altered easily when processed subjectively by individuals. The scientific methods described in this chapter promote objectivity and help prevent biased, subjective observations from distorting a scientist’s work. The second important difference between science and everyday observations is the use of systematic as opposed to hit-or-miss observation. By hit or miss, we mean making conclusions based only on whatever is happening around us. If we want to make conclusions about the human mind, we cannot restrict our observations to our immediate circle of acquaintances, friends, and loved ones. Our observations of the people we see frequently are probably quite valid. It’s just that the people we know represent a small slice of the greater population. For example, we might be surprised to learn that our favorite candidate lost an election because everyone we know voted for that candidate. Science strives to be objective, making judgments that are free from personal emotion or bias. In contrast, people’s subjectivity emerged in a classic experiment by Allport and Postman (1945). Some participants remembered hearing that this picture illustrated a Black man threatening another person rather than the objective fact—a White person is doing the threatening. Based on observations of their surroundings, college students often believe that drinking alcohol, and even binge drinking (five drinks on one occasion for men, four drinks for women), is a nearly universal behavior for anyone over the age of 18 or so. We can see why a college student might believe this after making hit-or-miss observations. Among 18- to 22-year-olds enrolled in college, nearly two-thirds reported binge drinking in the previous 30 days (NIAAA, 2015). In contrast, the overall prevalence of binge drinking at least once in the past year among U. S. adults was 24.9%, and the rate among all young adults between 18 and 24 years was 33.4% (see Figure 2.1), well below the frequency observed among those in college (CDC, 2016). Possibly even more surprising to college students, 43% of the adult population did not drink alcohol at all in the last month, and 29% did not drink alcohol during the past year (NIAAA, 2016). Science provides ways to make systematic observations. Judgments that we make based on the people we know might not apply to larger groups of people. Based only on personal experience, college students living in Nebraska and California might disagree about the prevalence of binge drinking. Finally, science relies on observable, repeatable evidence, whereas everyday observation often ignores evidence, especially when it runs counter to strongly held beliefs and expectations. Many people are convinced that women talk more than men. If you hold this belief, you are likely to notice and remember instances that support your belief more than instances that contradict it. This difference in attention and memory is termed confirmation biasconfirmation biasThe tendency to notice and remember instances that support your beliefs more than instances that contradict them.            confirmation bias        The tendency to notice and remember instances that support your beliefs more than instances that contradict them.            , and it represents one reason why objective and systematic observation are so important in scientific inquiries. Careful scientific studies have called into question the belief that women talk more than men. One group of researchers recorded students’ talking throughout the day and concluded that the widespread and highly publicized stereotype about female talkativeness is unfounded (Mehl, Vazire, Ramírez-Esparza, Slatcher, & Pennebaker, 2007, p. 82). Other studies make the argument that in most circumstances, men may actually talk more than women (James & Drakich, 1993;  Leaper & Ayres, 2007). We have all had the experience of witnessing an event with other people, only to discover that everyone seems to have a different memory of what happened. Scientific knowledge is both stable and changing. It is a work in progress, not a finished product. The fact that we may learn something new tomorrow should not make you assume that today’s knowledge is flawed. Most change occurs slowly on the cutting edges of science, not quickly or at the main core of its knowledge base. Unlike with many other fields, we expect science to improve over time. An important feature of scientific literacy is to learn to be comfortable with the idea that scientific knowledge is always open to improvement and will never be considered certain (AAAS, 2009).Critical thinkingCritical thinkingThe ability to think clearly, rationally, and independently.            Critical thinking        The ability to think clearly, rationally, and independently.            , or the ability to think clearly, rationally, and independently, is one of the foundations of scientific reasoning. The skilled critical thinker can follow logical arguments, identify mistakes in reasoning, prioritize ideas according to their importance, and apply logic to personal attitudes, beliefs, and values. Critical thinking is not built in; rather, it is a skill that people need to learn. You can begin by using five critical thinking questions to evaluate new information you come across in your everyday life, starting with what you read in this textbook (Bernstein, 2011): What am I being asked to believe or accept?What evidence supports this position?Are there other ways that this evidence could be interpreted?What other evidence would I need to evaluate these alternatives?What are the most reasonable conclusions?It is also helpful to recognize the signs that you are not thinking critically (excerpt taken from Lau, 2016): I prefer being given the correct answers rather than figuring them out myself. I don’t like to think a lot about my decisions, as I rely only on gut feelings. I don’t usually review the mistakes I have made. I don’t like to be criticized. Popular press reports often state, Experts agree that some fact or another is true. It is very important to use your very best critical thinking in evaluating these statements. The humor website Cracked.com often publishes lists from their contributors. One such list was called 28 Underrated Ways Life Is Different for Men and Women (Cracked Readers, 2016). This list was the result of a contest in which the participants needed to submit at least one scientific article to back up their claim. This sounds like a good opportunity to apply the critical thinking skills that you’ve learned in this chapter. We’d like you to walk through this process, and then you’ll have a chance to try one of the other facts from this site or another of your choice. A popular press website, Cracked.com, suggested that eye-tracking data was proof that men stare at men’s crotches a lot more than women do. Evaluating claims like this requires our best critical thinking skills. Coming in at #4 on the Cracked.com list is the statement, accompanied by an image of eye-tracking results, that Men stare at men’s crotches a lot more than women do!What Am I Being Asked to Believe or Accept?Our answer: We are being asked to accept the fact that men stare at men’s crotches a lot more than women do. Note that a lot more isn’t very specific. This could mean that more men than women stare at men’s crotches, or that men spend more time staring, or something completely different. The best scientific statements are very specific. What Evidence Supports This Position?Our answer: Here, things become very complicated. Cracked.com does not provide the source that their contributor used, so finding the original piece required some fancy Googling. The result traces back to an in-house eye-tracking study conducted in 2005 by the Nielsen/Norman Group, experts in user experience research. So far as we can tell, the study was never published in a peer-reviewed scientific journal (a huge weakness), but it was written up by a number of blogs. The best report was a description in the USC Annenberg’s Online Journalism Review (Ruel, 2007). Ruel’s report noted that the 255 participants between the ages of 18 and 64, 58% female and 42% male, looked for different lengths of time at parts of a photo of baseball player George Brett while undergoing eye-tracking analysis. In addition, when asked to browse the American Kennel Club website, the Nielsen/Norman researchers found that men fixated longer on the dogs’ genitalia than the women did. Are There Other Ways That This Evidence Could Be Interpreted?Our answer: Eye-tracking data, handled well, can be an excellent research tool. However, we don’t know much about how fixation length was calculated by Nielsen/Norman. We don’t know if the heat map image in Cracked is from a single participant or represents an average of viewer responses. Although the image seems clear (women or a single female participant fixated only on the head and shoulders area), psychologists would usually want a quantitative measure of a supposed difference between two groups. Inferential statistics are needed before we can make conclusions about the behavior of populations of men and women. What Other Evidence Do We Need?Our answer: One of the key points discussed in this chapter is the concept of generalization. When we say Men do something more than women, we are making a very general statement about gender than crosses other variables like age, race or ethnicity, nationality, education, and so on. We would want to ensure that the Nielsen/Norman sample was truly representative before making such a claim. One of the biggest weaknesses in the interpretation of these data is the use of two types of stimuli—a photo of George Brett and photos of dogs. A much wider selection of stimuli would help us understand if a general principle were involved or whether people were just reacting to these particular stimuli. What Are the Most Reasonable Conclusions?Our answer: While the result presented by Cracked.com is entertaining, which is the purpose of the website, it does not represent good science. We would want to see quite a bit more detail about the methods and analyses, along with publication in peer-reviewed sources, before we accept the conclusions as valid. Using our model critical thinking questions, explore one of the other Cracked.com list items or a popular press headline about psychology of your choice, and evaluate the item. Do you think you might have evaluated the claim differently before reading this chapter? Why or why not?Critical thinking not only is essential to good science, but also provides the underpinning of a free society. Our ability to think clearly, rationally, and independently gives us the confidence to question the actions of people in authority instead of engaging in blind obedience. We hope you will continue to practice good critical thinking skills long after you finish reading this textbook. Scientific research results do not support the common stereotype that women talk more than men.Learning scientific facts is not the same as understanding how science works. Science, including psychological science, is more than a collection of facts—it is a process (see Figure 2.2). From top to bottom, an hourglass shape starts out wide, narrows, and then expands again. The steps of scientific reasoning follow the same pattern. Beginning broadly with an examination of a phenomenon or research question, scientists then narrow their thinking to generate specific hypotheses and methods. After obtaining the results, they finally consider the broader implications of a study. Science seeks to develop theoriestheoriesA set of facts and relationships between facts that can explain and predict related phenomena.            theories        A set of facts and relationships between facts that can explain and predict related phenomena.            , which are sets of facts and relationships between facts that can be used to explain and predict phenomena (Cacioppo, Semin, & Berntson, 2004). In other words, scientists construct the best possible models of reality based on the facts known to date. Unfortunately, the English language can be the source of considerable confusion regarding the nature of scientific theories. In addition to its use in science, the word theory can be used in nonscientific ways to describe a guess, such as I have a theory about why my professor seems unusually cheerful this morning, or a hypothetical situation, as in That’s the theory, but it may not work in practice. Confusion over the multiple meanings of the word theory have led people mistakenly to view truly scientific theories, like the theory of evolution, as nothing more than casual guesses or hunches, rather than the thoroughly investigated and massively supported principles that they are. The best scientific theories not only explain and organize known facts, but also generate new predictions (see Figure 2.3). The word prediction comes from the Latin words for saying before. A scientific prediction is more than a guess or hunch. It is usually stated in a rigorous, mathematical form that allows the scientist to say that under a certain set of circumstances, a certain set of outcomes are likely to occur (if A, then B). In some cases, a theory’s predictions can be surprising. For example, you might believe that it’s impossible to be happy and sad at the same time. However, one model of emotion, discussed in Chapter 7, predicted that it is quite possible to feel happy and sad at the same time (Cacioppo, Berntson, Norris, & Gollan, 2012). This prediction was confirmed by research showing that first-year college students reported feeling either happy or sad, but not both, on a normal day of school, but experiencing both emotions simultaneously on the day they moved out of campus housing to go home for the summer. Theory building begins with generating hypotheses that are then systematically tested. Hypotheses that are not rejected contribute to the theory and help generate new hypotheses. Before attempting to generate your own scientific questions, it pays to become familiar with relevant theories and previous discoveries. As Sir Isaac Newton noted, scholars stand on the shoulders of giants (Turnbull, 1959)—we build on the work of those who came before us. New lines of research can also originate in observation. Scientists are observers not just in the laboratory, but also in everyday life. Scientific progress often takes a giant leap forward when a gifted observer recognizes a deeper meaningfulness in an everyday occurrence, such as when Newton observed a falling apple and considered its implications for a law of gravity. As we discovered in Chapter 1, Ivan Petrovich Pavlov realized that when his dogs learned to salivate to signals predicting the arrival of food, something more significant than slobbering dogs was happening. The learning that he observed explains why we get butterflies in our stomach before a performance and avoid foods that we think made us ill. Once you understand the theoretical foundations of your area of interest, you are ready to generate a hypothesis. A hypothesishypothesisA proposed explanation for a situation, usually taking the form If A happens, then B will be the result.            hypothesis        A proposed explanation for a situation, usually taking the form If A happens, then B will be the result.             is a type of inference, or an educated guess, based on prior evidence and logical possibilities (see Figure 2.4). A good hypothesis links concrete variables based on your theory and makes specific predictions. For example, researchers predicted that participants who viewed a video featuring a Stress is good for you message would show improved psychological symptoms and work performance relative to a group exposed to Stress is bad for you messages (Crum, Salovey, & Achor, 2013). The concrete variables in this study are exposure to the two different videos (Stress is good for you versus Stress is bad for you) and the measures of psychological symptoms and work performance. The researchers also must consider the possibility that there would be no difference in the effects of exposure to the video messages. Crum, Salovey and Achor (2013) tested a hypothesis that viewing a Stress is good for you message or a Stress is bad for you message would produce different outcomes. Participants viewing the Stress is good for you messages experienced increased soft work outcomes (maintain focus and communicate), and hard work outcomes (quality, quantity, accuracy, and efficiency) relative to the Stress is bad for you and control groups. Scientists can never prove that a hypothesis is true because some future experiment, possibly using new technology not currently available, might show the hypothesis to be false. All they can do is show when a hypothesis is false. A false hypothesis must always be modified or discarded. Once you have a hypothesis, you are ready to collect the data necessary to evaluate it. The existing scientific literature in your area of interest provides considerable guidance regarding your choice of methods, materials, types of data to collect, and ways to interpret your data. Science is a vastly collaborative enterprise. Not only do we stand on the shoulders of giants because we benefit from the work that has been done previously, but we depend on many others in the scientific community to help us improve our work and avoid mistakes. Normally, this evaluation is done by submitting research to conferences or for publication. During this process, research undergoes peer reviewpeer reviewThe process of having other experts examine research prior to its publication            peer review        The process of having other experts examine research prior to its publication            , in which it is scrutinized by other scientists who are experts in the same area. Only if other experts conclude that new research is important, accurate, and explained thoroughly will it be added to the existing body of scientific knowledge. To demonstrate the importance of this peer review, contrast this process to what happens when a person simply decides to transmit a tweet or launch a personal website. The author is solely responsible for the content, and there are no checks on the accuracy of that content. A contemporary theory of emotion correctly predicted the circumstances for when we might experience mixed emotions of happiness and sadness. Graduation from college is an important accomplishment, but we might feel sad about leaving our friends. The methods of psychological science can help us evaluate questions like the effects, if any, that playing violent video games has on physical aggression. One of the checks on science is the practice of replicating, or attempting to reproduce, scientific data. A scientist producing the original data might want to double-check his or her results, or other scientists might want to see if they can produce the same results (Simons, 2014). What happens if a study fails to replicate?When a group of nearly 300 psychologists led by Brian Nosek, known as the Open Science Collaboration (OSC), set out to replicate 100 studies from three well-respected psychology journals, the results were not encouraging (Nosek, et al., 2015). Only 36% of the replications produced significant results, meaning that the researchers were unable to duplicate the original findings of the studies the majority of the time. The average effect size, or the strength of a phenomenon, in the replications was only about half of that reported in the original studies. In other words, if a study found that 50% of the difference in physical aggressiveness was due to hot weather, the replication might show that only about 25% of the difference in aggressiveness was accounted for by the weather. Does this mean that we have to throw out these results?While failure to replicate should give any scientist a reason to reflect on his or her results, other psychologists are not particularly alarmed by the findings of Nosek and his colleagues (Gilbert, King, Pettigrew, & Wilson, 2016). The methods used in the replication efforts were often different from the original study. For example, a sample of Italians substituted for Americans in a study of attitudes toward African Americans and a study of college students being called on by a professor was replicated with people who had not attended college. These differences in sampling, rather than the validity of the original study, might have been the key reason for finding different outcomes. Scientific debates usually move us in the right direction. Regardless of how big a replication problem psychology might or might not have, psychologists are looking for new methods to make their results even more reliable. For example, submitting a public registered report of the methods and statistical analyses that a researcher plans to run prior to conducting a study discourages any after-the-fact efforts to tweak data and results to find something interesting when the main purpose of the study fails. Sharing data, detailed methods, and results in public places allows others to evaluate a researcher’s findings. Addressing unrealistic pressure on academics to publish or perish and putting quality of research ahead of quantity should also contribute to a more robust scientific environment. One of the studies cited frequently in psychology’s replication crisis is a 2008 paper by Schnall, Benton, and Harvey that reported that handwashing reduced the severity of participants’ moral judgments. Two efforts conducted by Johnson, Cheung, and Donnellan (2014) to replicate these findings failed, leading to a heated debate about what replication means. During peer review, research that fits with existing knowledge is typically accepted more rapidly than work that is less consistent with previous reports. Results often undergo replicationreplicationRepeating an experiment and producing the same results.            replication        Repeating an experiment and producing the same results.            , which means that other scientists independently attempt to reproduce the results of the study in question (Klein, et al., 2014). If the data are replicated, they will be accepted quickly. If other scientists are unable to replicate the data, their extra effort will have prevented inaccurate results from cluttering the scientific literature. Although this process might slow the publication of some innovative research, the result—more accuracy—is worth the effort. Questions for Detecting Good Critical ThinkingQuestions for Detecting Poor Critical Thinking(All answers should be no)What am I being asked to believe or accept?Do I prefer being given the correct answers rather than figuring them out myself?What evidence supports this position?Do I rely on gut feelings instead of thinking a lot about my decisions?Are there other ways that this evidence could be interpreted?Am I forgetting to review my conclusions to check for mistakes?What other evidence would I need to evaluate these alternatives?Am I oversensitive to criticism about my conclusions?What are the most reasonable conclusions?Psychological scientists use a variety of research methods, including descriptive, correlational, and experimental methods, depending on the type of question being asked. Descriptive methods, including surveys, case studies, and observations, provide a good starting place for a new research question. Correlational methods help psychologists see how two variables of interest, like the number of hours spent playing video games and level of physical aggression, relate to each other. Psychologists use experiments to test their hypotheses and to determine the causes of behavior. In the next sections, we will describe the common research methods used in psychological science and then compare how they might be used to approach a particular question—whether exposure to video game violence increases aggression. Each method—descriptive, correlational, and experimental—provides a different view of the phenomenon in question, and each has a particular profile of strengths and weaknesses. Each requires different types of statistical analyses, which are described in more depth later in the chapter. Many psychological studies combine several of these methods. When similar outcomes are observed using multiple methods, we have even more confidence in our results.Descriptive methodsDescriptive methodsResearch methods designed for making careful, systematic observations.            Descriptive methods        Research methods designed for making careful, systematic observations.             include case studies, naturalistic observations, and surveys. As we have seen, personal observations and common-sense ideas are especially vulnerable to bias, but descriptive methods allow a researcher to make careful, systematic, real-world observations. Descriptive methods can illuminate associations between variables and establish prevalence rates. Armed with these scientific observations, the researcher will be in a strong position to generate hypotheses. A case studycase studyAn in-depth analysis of the behavior of one person or a small number of people.            case study        An in-depth analysis of the behavior of one person or a small number of people.             provides an in-depth analysis of the behavior of one person or a small number of people. Many fields, including medicine, law, and business, use the case study method. Psychologists often use case studies when large numbers of participants are not available or when a particular participant possesses unique characteristics, as in the case described in this section. Interviews, background records, observation, personality tests, cognitive tests, and brain imaging provide information necessary to evaluate the case. Case studies not only are a useful source of hypotheses, but also can be used to test hypotheses. If you did a case study on a planet outside our solar system and discovered life there, you would disprove a hypothesis that no life exists outside our solar system. One of the most productive case studies in psychology chronicled more than 50 years of examinations of Henry Molaison (1926–2008), known in the scientific literature as the amnesiac patient H. M. In 1953, Molaison underwent brain surgery to control his frequent, severe seizures. Although the surgery may have saved his life, he was left with profound memory deficits, which are described in Chapter 9. Through painstaking testing and evaluation of Molaison, psychologists learned a great deal about the brain structures and processes that support the formation of memories (Corkin, 2002). Even after his death, Molaison continues to contribute to our knowledge. Researchers from the Massachusetts Institute of Technology (MIT), the Massachusetts General Hospital, and the University of California, San Diego, are analyzing Molaison’s brain today. How could you use the case study method to learn about exposure to video game violence and aggression? You could conduct a case study of Michael Carneal, who was sentenced to life in prison after he began shooting students at his high school in West Paducah, Kentucky, killing three students and seriously wounding five others. He had never shot a real gun before, but he was fond of playing first-person-shooter games like Doom. To conduct your case study, you gather background facts about Carneal’s case, possibly by interviewing others associated with the case, viewing legal and medical documents, and observing media accounts. You interview Carneal and possibly administer established personality and clinical tests like those discussed in Chapters 12 and 15. One of the most famous case studies in psychology is that of Henry Molaison (left), who was known in the literature as the amnesic patient H. M. until his death in 2008. For more than 50 years, Molaison allowed psychologists to evaluate his memory deficits resulting from brain surgery. After his death, scientists like Jacopo Annese (below) of the University of California, San Diego, began a careful examination of Molaison’s brain that continues today. What are the advantages of using the case study method to learn about the effects of playing violent video games on aggression? Mass shooters are thankfully quite rare, and the case study method is well suited to learning about unusual situations. A case study can contribute to science by testing hypotheses. If a hypothesis made a strong prediction that all mass shooters play violent video games, finding and documenting a case of a mass shooter who did not play violent video games would require rejection of the hypothesis. Based on the detailed data that you obtain from Carneal’s case, you will be better prepared to generate and test additional hypotheses. If you are interested in learning about larger groups of people than are possible with the case study method, you might pursue naturalistic observationnaturalistic observationAn in-depth study of a phenomenon in its natural setting.            naturalistic observation        An in-depth study of a phenomenon in its natural setting.            , or in-depth study of a phenomenon in its natural setting. Compared to the case study method, we are looking at a larger group of people, which will strengthen our ability to apply our results to the general population. We also have the advantage of observing individuals in their natural, everyday circumstances. Michael Carneal is serving a life sentence in prison for shooting and killing fellow students at his high school in 1997, when he was 14. A classic example of the method of naturalistic observation is the careful, long-term study of chimpanzees conducted in their habitat by Jane Goodall. In the summer of 1960, Goodall, then 26 years old, began her painstaking observations of chimpanzees living in Gombe National Park in Tanzania. Among her discoveries was that chimpanzees were not vegetarians, as previously assumed (Goodall, 1971, p. 34):I saw that one of them was holding a pink-looking object from which he was from time to time pulling pieces with his teeth. There was a female and a youngster and they were both reaching out toward the male, their hands actually touching his mouth. Presently the female picked up a piece of the pink thing and put it to her mouth: it was at this moment that I realized the chimps were eating meat. As a result of Goodall’s years spent following the chimpanzees, scientists have a rich, accurate knowledge of the behavior of these animals in the wild. Impressed by Goodall’s results, you plan to pursue further knowledge about violent video games and aggression by attending local area network (LAN) parties, where attendees bring their computers to a gathering place to play multiplayer, networked video games. You hope that by observing people playing violent video games and watching their subsequent behavior for signs of aggression, you might reach some conclusions about the relationships between video game violence and aggression. As in Goodall’s case, this approach has the advantages of providing insight into natural, real-world behaviors with large numbers of participants. Some naturalistic observations are conducted when people know that they are being observed, while in other cases, people are unaware of being observed. Both situations raise challenges. If we know we are being observed, we might act differently. Your LAN party participants know that aggression is not viewed positively in our culture, so they might act less aggressively when they know that they’re being watched. Watching people who do not know that they’re being watched raises ethical issues, which will be explored later in this chapter. How would you feel if you discovered that you had been an unwitting participant in a study?Jane Goodall used naturalistic observation to illuminate the world of the chimpanzee. The use of naturalistic observation illustrates the importance of choosing a method that is well suited to the research goals. Like the case study method, naturalistic observation can be helpful for developing hypotheses, but other methods must be used to test them. Most hypotheses in psychology look at the relationships between two or more concepts, like the exposure to violent video games and aggression in this example. Testing a hypothesis would allow you to say whether a relationship between exposure to violent video games and aggression exists, how strong the relationship is, what direction it goes in, and so on. It might appear to you that the people you observe are more aggressive following their LAN parties, but you have no way to demonstrate your point. People engage in lots of behaviors during LAN parties. Perhaps eating pizza or staying up all night at LAN parties enhances aggressive tendencies. With only your naturalistic observations to go on, you can’t say for sure. SurveysSurveysA descriptive method in which participants are asked the same questions            Surveys        A descriptive method in which participants are asked the same questions            , or questionnaires, allow us to ask large numbers of people questions about attitudes and behavior. Surveys provide a great deal of useful information quickly, at relatively little expense. Commercial online survey services make conducting surveys easier than in the past. One of the primary requirements for a good survey is the use of an appropriate samplesampleA subset of a population being studied.            sample        A subset of a population being studied.            , or subset of a population being studied. The populationpopulationThe entire group from which a sample is taken.            population        The entire group from which a sample is taken.             consists of the entire group from which a sample is taken. Good results require large samples that are typical, or representative, of the population that we wish to describe. Major pollsters, like the Pew Research Center and Gallup, take great pains to recruit survey participants who mirror the characteristics of the public across factors such as gender, age, race or ethnicity, education, occupation, income, and geographical location. Surveys use self-reporting, so results can be influenced by people’s natural tendency to want to appear socially appropriate (Corbett, 1991). As we will discover in Chapter 13, people have strong tendencies to conform to the expectations of others. In some surveys, this factor is not a problem. If you ask people whether they prefer lattes or mochas, you will probably get a fairly honest answer. However, when people believe that their true attitudes and behaviors will not be viewed favorably by others, they are more likely to lie, even when their answers are confidential and anonymous. To pursue the question of the effects of violent video games on aggression, you could do a naturalistic observation at a LAN party. Let’s see how scientists have used the survey method to explore video game violence. One survey involved 1,254 middle school students attending public schools in South Carolina and Pennsylvania (Olson, 2010). Although the sample did not include children from other geographic locations or children who are homeschooled or attending private schools, it appeared to be relatively representative of this age group. Children were asked to respond to 17 motives for playing video games on a 4-point scale, from strongly agree to strongly disagree. Like all surveys, this one depended on self-reporting, which raises the possibility that children would give the right answer instead of responding honestly. What did this scientific survey discover about violent video games? As shown in Figure 2.5, a surprising outcome of the survey was the more than 20% of boys who reported a violent video game helps me relax and helps me get my anger out. Cheryl  Olson (2010) asked middle school students why they liked to play violent video games. More than 20% of the boys agreed that a game helps me relax and helps me get my anger out. If we want to generalize our conclusions to people, it is very important for us to sample from the population of people rather than depending on handy convenience samples of undergraduate students enrolled in psychology courses. Given limited resources (time and money), how do psychological scientists reach a diverse sample of participants?You might think that using online recruitment, such as MTurk and Survey Monkey, could solve this problem, but that does not appear to be the case (Maner, 2016). These participants are probably not typical of the adult population in terms of their education and understanding of technology, and they have been exposed to the research methods used by behavioral scientists. Jon  Maner (2016) makes a strong case for conducting studies in the field as a way of recruiting more diverse and representative samples. He describes a study of diet and exercise effects on diabetes conducted in 27 geographically diverse clinical centers. Not only were the researchers able to find a large sample (over 3,000 people participated), but 45% of the sample identified themselves as members of underrepresented minority groups. An additional advantage of these field studies, according to Maner, is their stronger likelihood of being replicable. In another section of this chapter, we explored the controversy over the replicability of psychological research. Psychological scientists continue their search for methods that will provide more accurate results. Ensuring that a research sample is diverse, mirroring the population, is an important step in this direction.CorrelationsCorrelationsA measure of the direction and strength of the relationship between two variables.            Correlations        A measure of the direction and strength of the relationship between two variables.             measure the direction and strength of the relationship between two variablesvariablesA factor that has a range of values.            variables        A factor that has a range of values.            , or factors that have values that can change, like a person’s height and weight. Correlations allow psychologists to explore whether hours of sleep are related to student grade point averages (Trockel, Barnes, & Egget, 2000) or whether the age of parents is related to the rate of autism spectrum disorder among their children (Reichenberg, Gross, Kolevzon, & Susser, 2011). If you’re curious about the results of these studies, the first showed that sleep and GPA were not related, and the second showed that the age of parents is related to the rate of autism spectrum disorder among their children. We begin our analysis of correlations by measuring our variables. A measuremeasureA method for describing a variable’s quantity.            measure        A method for describing a variable’s quantity.             answers the simple question of how much of a variable we have observed. After we obtain measures of each variable, we compare the values of one variable to those of the other and conduct a statistical analysis of the results. Three possible outcomes from the comparison between our two variables can occur: positive, negative, or zero correlation. In a positive correlation, high levels of one variable are associated with high levels of the other. Height and weight usually show this type of relationship. In most cases, people who are taller weigh more than people who are shorter. Two variables also can show a negative correlation, in which high values of one variable are associated with low values of another. For example, high levels of alcohol consumption among college students are usually associated with low GPAs. The third possible outcome is a zero correlation, in which the two variables have no systematic relationship with each other. When variables have a zero correlation, knowing the value of one variable does not tell us anything about the value of the other (see Figure 2.6). For example, emergency room and law enforcement personnel are often convinced that they are busier with emergencies and crime on nights with a full moon. In contrast, numerous scientific studies of lunar cycles show zero correlation with emergency room admissions, traffic accidents, or other types of trauma (Stomp, ten Duis, & Nijsten, 2011). People taking surveys might be more interested in pleasing others or appearing normal than in answering honestly.(a) In positive correlations, high levels of one variable are associated with high levels of the other variable. (b) In negative correlations, high values of one variable are associated with low levels of the other variable. (c) In zero correlations, the two variables do not have any relationship with each other. Correlational research results are frequently misunderstood. Correlations permit us to discuss the relationships between two variables but tell us nothing about whether one variable causes changes in the other. Let us say that we discover a positive correlation between violent video games and aggression: Youth who play the most hours of violent video games have the most reports of physical aggression at school. However, we still cannot say that playing violent video games causes physical aggression at school. This conclusion may seem reasonable and possibly true, so why must we abandon it? First, the two variables in a correlation can influence each other simultaneously. Although it may be true that playing violent video games leads to physical aggression at school, youth who experience physical aggression at school may be more attracted to violent video games as an outlet for their frustration. Second, we might be observing a situation in which a third variablethird variableA variable that is responsible for a correlation observed between two other variables of interest.            third variable        A variable that is responsible for a correlation observed between two other variables of interest.             is responsible for the correlation between our two variables of interest. Consider the observation that many school shootings have been perpetrated by people who had been bullied relentlessly by others. Perhaps the experience of having been bullied (the third variable in this case) predisposes both a choice of violent recreation and a tendency to engage in aggressive behavior at school (see Figure 2.7). Third variables can be responsible for the correlation that we observe in two other variables. In our example of video game violence and aggression, being bullied could be a third variable that predicts both a choice of violent games and a tendency to be aggressive at school. The possibility of third variables is one reason that we must be careful when we reach conclusions based on correlational data. When you’re reading or listening to the news, watch for the use of words like link, association, or relationship used to describe two variables, such as a headline that states Lack of sleep linked to depression or Drinking red wine associated with lower rates of heart disease. These key words usually mean that the data are correlational but are often mistaken for causal. Now you know how you should—and should not—interpret these reports. If we cannot make conclusions about causality using correlations, why would we use them? In a number of circumstances, correlations are more appropriate than other research methods. For example, it would be unethical to ask pregnant women to consume different amounts of alcohol to assess the effects of prenatal alcohol on their infants. Instead, we can ask pregnant women to identify their alcohol intake in diaries, which can then be correlated with various measures of infant functioning. Although this method will not allow us to conclude that drinking during pregnancy causes damage to the fetus, we can correctly identify the strength and direction (positive, negative, or zero) of any correlation between drinks consumed and infants’ outcome measures (see Figure 2.8). Using this approach, researchers have shown that heavy maternal alcohol consumption is correlated with abnormal amounts of thickening of the outer layer of the brain (the cortex, which is discussed in Chapter 4; also see  Gautam, Warner, Kan, & Sowell, 2015). Even though we cannot make conclusions about causes based on correlations, we can obtain useful information. It would be unethical to assign pregnant women to drinking and nondrinking groups, but the negative correlation that we find between drinks consumed by pregnant women and measures of infant mental development can tell us that drinking during pregnancy is not a good idea. The more alcohol the women consumed, the lower their children scored on tests of infant mental development.The scientist’s most powerful tool for drawing conclusions about research questions is the formal experimentexperimentA research method that tests hypotheses and allows researchers to make conclusions about causality.            experiment        A research method that tests hypotheses and allows researchers to make conclusions about causality.            . Unlike cases in which descriptive methods are used, the researcher conducting an experiment has a great deal of control over the situation. Unlike correlational methods, the use of the formal experiment allows us to talk about cause (see Figure 2.9). A good experimental design features random assignment of participants to groups, appropriate control groups, control of situational variables, and carefully selected independent and dependent variables. A researcher begins designing an experiment with a hypothesis, which can be viewed as a highly educated guess based on systematic observations, a review of previous research, or a scientific theory. An experimental hypothesis takes this form: If I do this, that will happen. To test the hypothesis, the researcher manipulates or modifies the value of one or more variables and observes changes in the values of others. The variable controlled and manipulated by an experimenter (If I do this) is known as the independent variableindependent variableAn experimental variable controlled and manipulated by the experimenter; the if A happens part of a hypothesis            independent variable        An experimental variable controlled and manipulated by the experimenter; the if A happens part of a hypothesis            . We need some way to evaluate the effects of this manipulation. We use a dependent variabledependent variableA measure that demonstrates the effects of an independent variable; the result part of a hypothesis            dependent variable        A measure that demonstrates the effects of an independent variable; the result part of a hypothesis            , defined as a measure used to assess the effects of the manipulation of the independent variable, to tell us what will happen as a result of the independent variable. Like the independent variable, our choice of dependent variable is based on our original hypothesis. After determining our independent and dependent variables, we still have quite a bit of work to do. In most experiments, we want to know how simply going through the procedures of being in an experiment influences our dependent variable. Perhaps the hassle of going to a laboratory and filling out paperwork changes our behavior. To evaluate these irrelevant effects and establish a baseline of behavior under the experimental conditions, we assign some of our participants to a control groupcontrol groupA group that experiences all experimental procedures, with the exception of exposure to the independent variable.            control group        A group that experiences all experimental procedures, with the exception of exposure to the independent variable.            . In many experiments, the control group will experience all experimental procedures except exposure to the independent variable. When a new treatment is being tested, the control group might experience the standard treatment for a condition. The experience of the control group should be as similar as possible to that of the experimental groupsexperimental groupsA group of participants who are exposed to the independent variable.            experimental groups        A group of participants who are exposed to the independent variable.            , who experience different values of the independent variable. We want to ensure that our dependent variables reflect the outcomes of our independent variables instead of individual differences among the participants’ personalities, abilities, motivations, and similar factors. To prevent these individual differences from masking or distorting the effects of our independent variable, we randomly assign participants to experimental or control groups. Random assignmentRandom assignmentThe procedure in which each participant has an equal chance of being placed in any group in an experiment            Random assignment        The procedure in which each participant has an equal chance of being placed in any group in an experiment             means that each participant has an equal chance of being assigned to any group in an experiment. With random assignment, differences that we see between the behavior of one group and that of another are unlikely to be the result of the individual differences among the participants, which tend to cancel each other out. Individual differences among participants are an example of confounding variablesconfounding variablesVariables that are irrelevant to the hypothesis being tested but can alter a researcher’s conclusions.            confounding variables        Variables that are irrelevant to the hypothesis being tested but can alter a researcher’s conclusions.            , or variables that are irrelevant to the hypothesis being tested and can alter or distort our conclusions. For example, a researcher might want to test the effects of aerobic exercise on blood pressure. If some participants competed in triathlons without the researcher’s knowledge, their athletic experience would confound the interpretation of the results. Random assignment to groups typically controls for confounds because of these types of individual differences, but other sources of confounds exist. Situational confounds, such as time of day or noise levels in a laboratory, also could affect the interpretation of an experiment. Scientists attempt to run their experiments under the most constant circumstances possible to rule out situational confounding variables. Let’s return to our question about violent video games and aggression and see how one real experiment addressed the issue. The researchers tested a hypothesis stating that playing video games varying in violent content (the independent variable) would influence subsequent levels of aggression (the dependent variable; see  Anderson & Dill, 2000). The experimental group played a violent game (Wolfenstein 3D), while the control group played a nonviolent game (Myst). Participants were randomly assigned to groups. You can imagine how the results might be distorted if the experimental group consisted of football players and the control group consisted of members of the chess club. Following their game-playing, participants had an opportunity to choose the intensity and duration for a blast of noise to punish a remotely located loser (who didn’t exist) in a competitive reaction time activity, and their choices served as the dependent variable. We’re assuming that laboratory conditions were held constant to avoid situational confounds. For example, we know that people feel more aggressive in hot temperatures (Carlsmith & Anderson, 1979). What would happen to our experiment if the air conditioner in the laboratory broke while some participants played violent video games, but was fixed right before the next participants played nonviolent games? Results of this carefully controlled experiment confirmed the researchers’ hypothesis: Playing the violent game led to the stated intent to administer longer noise blasts to another person (see Figure 2.10). Anderson and Dill (2000) randomly assigned participants to groups playing violent and nonviolent video games. After playing, each participant was asked to choose the strength and duration of a blast of noise to be administered to another person. Keeping conditions in the laboratory as constant as possible would help to reduce the impact of situational variables. As powerful as it is, the experimental method, like the other methods discussed previously, has some limitations. Experiments can be somewhat artificial. Participants know that they are in a research study, and they may vary their behavior as a result. However, making a laboratory experiment more realistic can raise ethical challenges. In a study conducted in 1962, before current ethical guidelines for research had been adopted, military personnel were led to believe that their lives were in danger so that experimenters could realistically assess the effects of panic on performance (Berkun, Bialek, Kern, & Yagi, 1962). Although the responses of these participants were probably quite representative of real life, few of us would want to be put in their position. This type of research could not be conducted under today’s ethical standards, as explained later in this chapter. Artificiality is also a problem with the Anderson and Dill (2000) video game study. Our real-world question relates to the likelihood that playing violent video games may elicit real physical violence in real situations. Is a person’s willingness to inflict a loud sound on another person in a professor’s laboratory truly representative of the type of violence involved in mass shootings like Carneal committed? It may or may not be; we need additional research to find out. Researchers attempt to reduce the impact of confounding variables on their results. In a test of the effects of aerobic exercise on blood pressure, situational confounding variables, such as (1) traffic outside the building, (2) a noisy treadmill, and (3) a neighbor breathing heavily, can be controlled by holding the environment as constant as possible for all participants. Individual differences, such as (4) an early morning after little sleep or (5) superior fitness, can be controlled by randomly assigning participants to groups. To conduct an experiment, we must carefully operationalize, or define our variables in practical terms. One way to operationalize physical aggression is to measure how often a preschooler has a physical fight with others. Another issue with the experimental method arises from differences in the choices of independent and dependent variables. Independent (controlled) and dependent (measured) variables have to be defined and implemented in some concrete fashion. The process of translating abstract independent and dependent variables into measurable forms is called operationalizationoperationalizationDefining variables in ways that allow them to be measured.            operationalization        Defining variables in ways that allow them to be measured.            . The first step in operationalization is to identify the concept to be measured clearly, such as the violence in a video game. Next, the quantitative measures of the concept must be determined. What measures do you need to tell a violent video game from a nonviolent one? You might develop a rating scale based on the number of times violent images, such as depictions of physical injury or blood, are presented in a particular game. Finally, a method for obtaining this measure must be developed. The Entertainment Software Rating Board (ESRB) uses a panel of trained raters to evaluate a DVD of representative game-play submitted by a manufacturer of a new game to establish an appropriate age group rating (ESRB, 2014). There are many ways to operationalize variables in practical terms. Anderson and Dill (2000) operationalized aggression in terms of how lengthy and loud a sound blast a person was willing to inflict on another person. One of the odder dependent variables used in video game aggression research is the hot sauce paradigm, in which the amounts of hot sauce that participants choose to be administered to another participant are used to measure aggression (Lieberman, Solomon, Greenberg, & McGregor, 1999). Other researchers might choose different ways to operationalize aggression, such as frequency of physical fights among preschoolers. As a result, even though there is a large body of work regarding the impact of violent video games on aggression, the variables used are so different that few direct comparisons can be made among the many studies. The point of this discussion is not to convince you that scientists don’t know what they’re talking about, but rather to impress upon you the importance of reviewing research results using your best critical thinking skills. In seeking to understand something as complicated as the science of mind, it is unlikely that any single study could provide complete information about a phenomenon. Instead, progress in our understanding results from the work of many scientists using diverse methods to answer the same question. Conducting a meta-analysismeta-analysisA statistical analysis of many previous experiments on a single topic.            meta-analysis        A statistical analysis of many previous experiments on a single topic.            , or a statistical analysis of many previous experiments on the same topic, often provides a clearer picture than do single experiments observed in isolation. Nearly all studies of video games and aggression use participants playing alone. If we add the social perspective by studying people playing together, we find that aggressive behavior actually decreases following play, regardless of whether the game was violent. Meta-analyses have their own share of challenges, however. A meta-analysis is only as good as the studies on which it is based. Published studies available to researchers conducting a meta-analysis might be subject to publication biaspublication biasThe possibility that published studies are not representative of all work done on a particular phenomenon.            publication bias        The possibility that published studies are not representative of all work done on a particular phenomenon.            , or the possibility that they are not representative of all the work done on a particular problem. A file drawer problem also exists, in which journals are more likely to publish studies that demonstrate significant effects of an independent variable, such as video game violence, on a dependent variable, such as aggression, than studies that show no significant effects. If publication bias is present, the results of any meta-analysis might be misleading. As we will see in so many examples in this textbook, even the best research designs might mislead us if they fail to take multiple perspectives into account. Most of the research discussed so far regarding video games and aggression has involved single participants playing alone. What happens when we zoom out, using the social perspective to look at the effects of video games on groups of people playing together? Playing video games cooperatively is associated with less subsequent aggressive behavior, regardless of whether the game played was violent or not (Jerabeck & Ferguson, 2013). The research discussed throughout the remainder of this textbook has been subjected to considerable skeptical review by other experts in the field. Most has stood the dual tests of peer review and replication by others. Converging evidence from descriptive, correlational, and experimental research provides us with confidence in our conclusions. Psychology, like any science, has followed its share of wrong turns and dead ends, but most knowledge presented here has been carefully crafted to present the most accurate view possible of behavior and mental processes. The methods described previously can be used by psychologists to provide guidance across many disciplines. For example, how many times have you heard a parent complain about a child’s out-of-control behavior and blame it on too much sugar? How would a scientist know whether sugar or any other food ingredient affected children’s behavior? Scientific discoveries in this area could benefit the fields of nutrition, health, medicine, and child development. The gold standard for demonstrating the objective effects of any substance, whether a food additive, medication, or recreational drug, is the double-blind proceduredouble-blind procedureA research design that controls for placebo effects in which neither the participant nor the experimenter observing the participant knows whether the participant was given an active substance or treatment or a placebo.            double-blind procedure        A research design that controls for placebo effects in which neither the participant nor the experimenter observing the participant knows whether the participant was given an active substance or treatment or a placebo.            . This procedure requires a placeboplaceboAn inactive substance or treatment that cannot be distinguished from a real, active substance or treatment.            placebo        An inactive substance or treatment that cannot be distinguished from a real, active substance or treatment.            , an inactive substance that cannot be distinguished from a real, active substance. The first blind aspect of this procedure is the inability of participants to know whether they have taken a real substance or a placebo. This feature controls for effects of the participants’ expectations. When we drink coffee, for example, we expect to feel more alert, or when we take an aspirin, we expect our headache to disappear; therefore, we may feel better long before the substance has had time to produce effects. Not letting subjects know whether they received the real substance or the placebo helps offset these misleading effects. The second blind is achieved when the researchers also do not know whether a participant has been given a real substance or a placebo until the experiment is over. This aspect ensures that the researchers’ expectations do not tilt or bias their observations. If scientists expect participants to act more alert after drinking coffee, for example, this bias could be reflected in their observations and conclusions. Double-blind, placebo-controlled studies have provided insight into the relationship between common food additives in fruit drinks and hyperactivity in children. Returning to the question of food additives, what do double-blind, placebo-controlled studies have to say about their effects on child behavior? In one careful study, young children were given drinks that either had no additives (placebo) or a combination of colorings and preservatives used frequently in packaged foods (McCann, et al., 2007). Because it was a double-blind study, the children did not know which drink they had received, and the researchers responsible for observing the children did not know which drink had been served to each child. The outcome of this study showed that general measures of hyperactivity, or unusually large amounts of movement, were higher in the groups that had consumed the additives than in the group that had consumed the placebo. This finding has implications for attention deficit hyperactivity disorder (ADHD), which is discussed in Chapter 14. The ability of these common food additives to make normal children more hyperactive is cause for concern and worthy of further study (see Figure 2.11). The results obtained by  McCann et al. (2007) showed that hyperactivity was higher in children who consumed one of the drinks containing common food additives (left and middle bars) than in children who consumed the placebo drink containing no additives (right bar).Modifications of the methods discussed previously might be necessary for answering specific questions. Psychological scientists frequently ask questions about normal behaviors related to age. As Chapter 11 will show, aggression in preschoolers means something different from aggression in a 16-year-old. Considering the impact of video game violence on aggression within the context of age-related change adds a new and useful dimension to our hypothesis, but it requires additional attention to the research methods to be used. Psychologists have three specific techniques for assessing the normal behaviors associated with age: cross-sectional, longitudinal, and mixed longitudinal designs. To do a cross-sectional studycross-sectional studyAn experimental design for assessing age-related changes in which data are obtained simultaneously from people of differing ages.            cross-sectional study        An experimental design for assessing age-related changes in which data are obtained simultaneously from people of differing ages.            , we might gather groups of people of varying ages and assess both their exposure to violent video games and their levels of physical aggression. We might be able to plot a developmental course for age-related differences in both video game exposure and aggressive behavior. However, the cross-sectional method introduces what we refer to as cohort effects, or the generational effects of having been born at a particular point in history. Being 20 years old in 1959 was different from being 20 years old in 1989 or 2019 because of a variety of cultural influences. Today’s 10-year-olds, who do not know of a time without the Internet, might respond differently to violent video games than today’s 50-year-olds, for reasons that have nothing to do with age. Any such cohort effects could mask or distort our cross-sectional results. Cross-sectional studies usually show that intelligence scores decrease with age. These results are most likely a cohort effect. Performance on IQ tests has risen approximately 3 points per decade over the past 100 years, for reasons that are not fully understood (Flynn, 1984). A method that lessens this dilemma is the longitudinal studylongitudinal studyAn experimental design for assessing age-related changes in which data are obtained from the same individuals at intervals over a long period of time.            longitudinal study        An experimental design for assessing age-related changes in which data are obtained from the same individuals at intervals over a long period of time.            , in which a group of individuals is observed for a long period (see Figure 2.12). For example, the Fels Longitudinal Study began in 1929 to observe the effects of the Great Depression on children, and it now has enrolled great-grandchildren of the original participants. To use the longitudinal method to answer our question, we could start with a group of infants and carefully plot their exposure to violent video games and their levels of physical aggression into adulthood. The longitudinal approach has few logical drawbacks but is expensive and time consuming. Participants drop out of the study because of moves or lack of incentive. Researchers then must worry about whether those who remain in the study still comprise a representative sample. Longitudinal designs control for the cohort effects that are often seen in cross-sectional designs. This longitudinal study shows that verbal ability and verbal memory are fairly stable over the lifetime, but that perceptual speed gradually worsens with age. The third approach, the mixed longitudinal designmixed longitudinal designA method for assessing age-related changes that combines the cross-sectional and longitudinal approaches by observing a cross-section of participants over a shorter period than is used typically in longitudinal studies.            mixed longitudinal design        A method for assessing age-related changes that combines the cross-sectional and longitudinal approaches by observing a cross-section of participants over a shorter period than is used typically in longitudinal studies.            , combines the cross-sectional and longitudinal methods. Participants from a range of ages are observed for a limited time (usually about 5 years). This approach is faster and less expensive than the longitudinal method and avoids some of the cohort effects of the pure cross-sectional method. Research MethodStrengthsWeaknessesDescriptive methodsCase studyCan explore new and unusual phenomena; can falsify a hypothesisHas limited generalizationNaturalistic observationProvides insight into natural, real-world behaviorsParticipants act differently when watched; the process has ethical issuesSurveyProvides large amounts of data quickly and inexpensivelyRequires a large representative sample; people wish to appear socially appropriate and may lieCorrelationsAllow us to predict behavior; stimulate development of hypotheses; address some difficult ethical situationsCannot be used to discuss causalityExperimentsAllow control of situations, strong hypothesis testing, and judgment of causalityRaises artificiality and ethical concerns; are time consumingAssessing the effects of time:• Cross-sectional studyIs quick and relatively inexpensiveIs subject to cohort effects• Longitudinal studyReduces the impact of cohort effectsIs expensive and time consuming; people drop out• Mixed longitudinal studyIs less expensive and time consuming than longitudinal, with some control of cohort effectsIs still relatively expensive and time consuming; dropout problem remainsAsking the right questions and collecting good information are only the beginning of good science. Once we have collected our results, or data, we need to figure out what those data mean for our hypotheses and theories. The interpretation of data is not an arbitrary act—scientists follow specific rules when drawing their conclusions. A valid measure actually measures what it is supposed to measure. In this case, your bathroom scale is supposed to tell you how much you weigh.Data are only as good as the measures used to obtain them. How would we know whether a measure is good or bad? Two standards that any measure must meet are reliability and validity. ReliabilityReliabilityThe consistency of a measure, including test–retest, interrater, intermethod, and internal consistency.            Reliability        The consistency of a measure, including test–retest, interrater, intermethod, and internal consistency.             refers to the consistency of a measure. There are several meanings of reliability in science, including test–retest, interrater, inter-method, and internal consistency. For example, test–retest reliability of the Scholastic Aptitude Test (SAT) is quite good, as you might have noticed if you took the test more than once. If you were ill for the first test, or you invested in a preparation course before taking it the second time, your score might change; otherwise, your scores are likely to be about the same. Good measures also show high interrater reliability, or consistency in the interpretation of a measure across different observers. You can imagine how distressing it would be if, for example, one lab identified you as having a fatal disease on the basis of a blood test and another did not. Inter-method reliability describes the positive correlation of several approaches to measure a feature in an individual. Returning to the SAT example, it is likely that your SAT and high school grades are positively correlated, which supports the reliability of these measures. Finally, internal consistency results from measures within a single test that positively correlate with one another. ValidityValidityA quality of a measure that leads to correct conclusions (i.e., the measure evaluates the concept that it was designed to do).            Validity        A quality of a measure that leads to correct conclusions (i.e., the measure evaluates the concept that it was designed to do).             means that a measure leads to correct conclusions or evaluates the concept that it is designed to do. For example, your bathroom scale is supposed to measure how much you weigh. The data obtained from your bathroom scale can lead you to a valid conclusion (this is how much I weigh) or an invalid conclusion (wow, I’m much lighter than the doctor’s scale said I am). How would we determine whether a measure leads to valid conclusions? One approach is to see whether a measure correlates with other existing, established measures of the same concept. Many universities use the SAT to help select the best candidates for admission. This test is supposed to measure a student’s aptitude for success in college, but some universities have abandoned it in favor of other measures. We could ask which of the following measures—the SAT, SAT Subject Tests, or high school GPA—shows the highest positive correlation with first-year college grades. The results of this comparison indicate that the SAT Subject Tests show the highest positive correlation with first-year college grades compared to both the SAT and the high school GPA (Geiser & Studley, 2001). Thus, we can conclude that SAT Subject Tests are the most valid of the three measures for predicting first-year college grades. Reliability and validity are not the same. You can obtain a consistent result (reliability) that lacks meaning (validity), but a measure cannot be valid without also being reliable. For example, if you weigh 200 pounds and your bathroom scale consistently reports that you weigh 150 pounds, whether you’re looking at the number or your roommate reads it for you, the scale has reliability but not validity. If you get a wildly different number each time you step on the scale, you have neither reliability nor validity. The measure is not consistent (no reliability) and also fails to measure the construct—weight in this case—that it is designed to measure (no validity).Just as we might explore a new research topic using the descriptive research methods outlined earlier, we can use descriptive statistics to explore the characteristics of the data obtained from our research. Descriptive statisticsDescriptive statisticsStatistical methods that organize data into meaningful patterns and summaries, such as finding the average value.            Descriptive statistics        Statistical methods that organize data into meaningful patterns and summaries, such as finding the average value.             help us organize individual bits of data into meaningful patterns and summaries. For example, research investigating the ability of the SAT, SAT Subject Tests, and high school grades to predict grades in colleges and universities across the country would be overwhelming without some way of summarizing the individual data points for tens of thousands of students in meaningful ways. Descriptive data tell us only about the sample we have studied. To determine whether the results from our sample apply to larger populations requires additional methods, described in a subsequent section of this chapter. We might approach this mass of data first by asking how the scores and grades are distributed. We could arrange SAT scores from high to low and note how many students obtained each score. The result of our work would be a frequency distribution. We often illustrate frequency distributions with a bar chart, or histogram, like the one shown in Figure 2.13. Descriptive statistics, such as these frequency distributions of SAT mathematics scores, allow us to see meaningful patterns and summaries in large sets of data. Frequency distributions are a useful starting place, but we might also be interested in identifying the average score on our measures, or the central tendency of our data set. There are three types of measures for central tendency: the mean, median, and mode for each group of scores. The meanmeanThe numerical average of a set of scores.            mean        The numerical average of a set of scores.             is the numerical average of a set of scores, computed by adding all the scores together and dividing by the number of scores. For all students taking the SAT in 2009, the mean on the critical reading section was 501, the mean on the mathematics section was 515, and the mean on the writing portion was 493. Many colleges and universities use the SAT or other standardized tests to predict success in college courses. Evaluating the resulting data is easier if we use descriptive statistics to summarize the performances of the thousands of individual students who take the tests. The medianmedianThe halfway mark in a set of data, with half of the scores above it and half below.            median        The halfway mark in a set of data, with half of the scores above it and half below.             represents a halfway mark in the data set, with half of the scores above it and half below. The median is far less affected by extreme scores, or outliers, than the mean. In our SAT data, the median scores—500 for critical reading, 510 for mathematics, and 490 for writing—are quite close to the mean scores. Why, then, would you ever need to consider a median? In some sets of results, you might find some extreme scores that could affect the mean. For example, if you asked employees at a small business to report their current annual salaries, you might get the following results: $40,000, $45,000, $47,000, $52,000, and $350,000 (we’re assuming this is the boss). The mean in this example would be $106,800, but that figure doesn’t provide a good summary of these numbers. The median, $47,000 in this case, is more representative of your overall results. Look at Figure 2.14 for another example. In many cases, like the SAT data, means and medians are similar. In other cases, such as annual income, these two measures of central tendency provide different pictures. This graph shows mean and median annual incomes in the United States. The ratio of the median to the mean has decreased from 72% in 1990 to 65% in 2012. This suggests that the poorer citizens’ income has not increased as fast as the wealthier citizens’ income. The modemodeThe most frequently occurring score in a set of data            mode        The most frequently occurring score in a set of data             refers to the score that occurs most frequently, and it is easy to determine by looking at a histogram. We do not have exact numbers for modal scores on the SAT example, but the most frequent range of scores is between 400 and 490 for all three tests (critical reading, mathematics, and writing). The usefulness of the mode depends on the research question that you are asking. In the case of the SAT, the mode provides little interesting information, so let’s return to our drinking data mentioned earlier in the chapter. It might surprise you to learn that although the modal pattern of drinking among U. S. adults overall is current regular, among Asian adults living in the United States, the modal pattern is lifetime abstainer (Center for Behavioral Health Statistics and Quality, 2015). The mode is also an advantage over a mean or median when there is more than one mode in a set of data. For example, the mean age of onset for anorexia nervosa, which is discussed in Chapter 7, is 17 years of age, but the distribution is bimodal, which means that it has two substantial modes. One peak occurs around the age of 14, when many teens struggle with their changing shapes, and another occurs around the age of 18, when many teens leave home and make food choices without the watchful eyes of their parents (Halmi, Casper, Eckert, Goldberg, & Davis, 1979). An intervention program designed to coincide with the most likely ages of onset would probably be more effective than one timed to coincide with the mean age of onset (see Figure 2.15). The average age of onset for the anorexia nervosa is 17 years, but this measure masks the important fact that age of onset shows two modes—one at 14 years and a second at 18 years. For public health officials wishing to target vulnerable groups for preventive education, the modes provide better information than the mean. In addition to being curious about central tendency, we might want to know how clustered our scores are. The traditional way to look at the variance of scores is to use a measure known as the standard deviationstandard deviationA measure of how tightly clustered around the mean a group of scores is            standard deviation        A measure of how tightly clustered around the mean a group of scores is            , which tells us how tightly clustered around the mean a group of scores is. A smaller standard deviation suggests that most scores might be found near the mean, whereas a larger standard deviation means that the scores are spread out from the mean. Returning to our salary example, we had five salaries with a mean of $106,800. To obtain the standard deviation, which is easy to do with a calculator, you subtract each salary from 106,800, square each difference (to eliminate minus signs), add the squares, find the mean of those differences by dividing the total by five (the number of salaries), and take the square root of the result. In this case, we end up with a standard deviation of 136,021, which means that the average difference between a salary and the mean of the salaries is 136,021. If we discard the extreme salary ($350,000) and find the standard deviation of the remaining four salaries, it turns out to be smaller: 4,966.56. These results suggest that the distribution of the first four salaries is tightly clustered, whereas the distribution of all five salaries is more spread out. Many measures of interest to psychologists, such as scores on intelligence tests, which are discussed in Chapter 10, appear to form a normal distributionnormal distributionA symmetrical probability function            normal distribution        A symmetrical probability function            , illustrated in Figure 2.16. The ideal normal curve in this illustration has several important features. One, it is symmetrical. Equal numbers of scores should occur above and below the mean. Second, its shape indicates that most scores occur near the mean, which is where our measure of variability plays a role. In the standard normal curve, shown in Figure 2.16(a), 68% of the population falls within one standard deviation of the mean, 95% falls within two standard deviations, and 99% of the population falls within three standard deviations. Instruments for assessing intelligence, discussed in Chapter 10, frequently convert raw scores earned by many participants to fit a normal distribution with a mean of 100 and a standard deviation of 15. As a result, we would expect 68% of test takers to receive an IQ score between 85 and 115. Another 95% would most likely score between 70 and 130, leaving only 2.5% to score above 130 and another 2.5% to score below 70. Many measures of interest to psychologists take the approximate form of a normal distribution. These graphs compare a standard normal curve, shown in (a), to the distribution of scores on a standardized test of intelligence, the Wechsler Adult Intelligence Scale, shown in (b). In our discussion so far, we have been describing single variables, such as number of drinks consumed, salaries, and SAT scores. In psychological research, we often want to describe the relationships among multiple variables. We can illustrate the relationship between two variables in a scatterplot like the one shown in Figure 2.17. Each dot represents the intersection between scores on two variables of interest. For example, we can compare the distributions of math SAT scores as a function of GPA. From our scatterplot, it looks like math SAT scores and GPAs are systematically related to each other. As one increases, the other does as well, which you should recognize from the earlier discussion as a positive correlation. A scatterplot allows us to visualize the relationship between two variables, such as the math SAT score and the high school GPA for one small sample of college students. This relationship should look familiar to you—it is an example of a positive correlation. ESP stands for extrasensory perception, with extra in this case meaning outside the boundaries of the normal information that we obtain from our various senses, such as vision, hearing, and touch. The study of ESP is part of the larger field of parapsychology, or the study of psychic phenomena lying outside the typical boundaries of the field of psychology described in Chapter 1. Among the abilities grouped into the category of ESP are telepathy (the ability to communicate with other minds without using the usual methods of speaking or writing), clairvoyance (the ability to perceive objects that do not affect the known senses), precognition (knowledge of future events), and premonition (emotional anticipation of future events). In 2005, Gallup pollsters found that 41% of Americans said that they believed in ESP, with 25% not sure and 32% not believing (Gallup Poll News Service, 2005). Of the scientists who are members of the National Academy of Sciences, 96% said that they did not believe in ESP (McConnell & Clark, 1991). A study of ESP (Bem, 2011) generated considerable discussion in the scientific community about everything from the statistics that we use to the effects of investigator bias. Evaluating this study provides a good opportunity to practice your critical thinking skills and to apply what you have learned about validity and reliability. A total of 100 undergraduates (50 men and 50 women) participated. Stimuli (both erotic and nonerotic photographs) were selected from a standard set known as the International Affective Picture System. During each trial, participants saw two curtains on a computer screen and were asked to predict which curtain hid a picture. The sequencing of the erotic and nonerotic pictures and the left–right positions was determined by a random number generator after the participant made a selection. This timing was designed to test the precognition of future events (the participants selected a side of the screen before the random number generator selected a location for the picture). Here is where Bem’s methods get a bit murky. The first 40 participants saw 12 erotic pictures, 12 negative pictures (unpleasant images), and 12 neutral pictures. Then, for reasons not well explained in the paper, the method was changed for the remaining 60 participants, who were shown 18 erotic and 18 nonerotic photos, 8 of which were described as romantic but not erotic (couple at a wedding, etc.). It is quite unusual for researchers to change their methods in the middle of an experiment, and more troubling when there doesn’t seem to be a good reason to do so. The popularity of television shows like Ghost Hunters might be related to the large number of Americans who report a belief in ESP (41%). In contrast, 96% of the members of the National Academy of Sciences say they do not believe in ESP. The Question: Nine experiments involving more than 1,000 participants tested different types of precognition and premonition. We will focus our attention on the first experiment, which tested the following hypothesis: Participants should be able to anticipate the position (right side or left side of a computer screen) of an erotic photograph (see Figure 2.18). Is the following study of this hypothesis valid and reliable?The only potential ethical challenge in this study is the presentation of erotic photos. Potential participants should be warned of this aspect before they agree to continue with the study. If we have two choices, we have a 50% chance of guessing correctly on each trial. Bem reported that the future position of the erotic images was chosen correctly on 53.1% of the trials, and the future position of the nonerotic images was chosen correctly on 49.8% of the trials. Bem reported that his results were statistically significant, or were unlikely to happen because of chance. Participants in Bem’s (2011) study were supposed to predict behind which of two curtains a picture would appear. Bem concluded that the choices made by his participants were better than chance, supporting his hypothesis that precognition could be demonstrated. What do others think of Bem’s results? James Alcock, writing for the Skeptical Inquirer, concluded that just about everything that could be doNe wrong in an experiment occurred here (2011, p. 31). Among Alcock’s concerns were Bem’s changing of his method midway through the experiment and his questionable use of statistical analyses. As discussed earlier, replication provides an important check on possible researcher bias, and failure to replicate indicates serious flaws in an experiment. So far, the three known replications of Bem’s experiments have failed to produce significant results. Despite the flaws, however, Bem’s experiments have contributed to science by stimulating a lively discussion of scientific and statistical methods. Although this scatterplot gives a sense that GPAs and math SAT scores have a systematic relationship, we can compute that relationship exactly using a correlation coefficient. Correlation coefficients can range from −1.00 to +1.00. A correlation of −1.00 and a correlation of +1.00 are equally strong but differ in the direction of the effect. A zero correlation indicates that the two variables have no systematic relationship. The closer a correlation coefficient is to −1.00 or to +1.00, the stronger is the relationship between the two variables. When the score is −1.00 or +1.00, the correlation is perfect—all data points follow the pattern. A value between 0 and 1.00 or 0 and −1.00 indicates a correlation direction, but the relationship is not perfect—not every data point conforms to the pattern.Although we can learn a great deal from descriptive statistics, most research described in this textbook features the use of inferential statisticsinferential statisticsStatistical methods that allow experimenters to extend conclusions from samples to larger populations            inferential statistics        Statistical methods that allow experimenters to extend conclusions from samples to larger populations            , so called because they permit us to draw inferences or conclusions from data. Descriptive statistics allow us to talk about our sample data but do not allow us to extend our results to larger groups. To reach conclusions about how our observations of a sample might fit the bigger picture of groups of people, we use inferential statistics. Although inferential statistics can be powerful, we must be cautious about making generalizationsgeneralizationsThe tendency to respond to stimuli that are similar to an original conditioned stimulus (CS)            generalizations        The tendency to respond to stimuli that are similar to an original conditioned stimulus (CS)             from our results to larger populations. To generalize means to extend your conclusions to people outside your research sample. Psychology over the years has been justifiably criticized as being the psychology of the college sophomore. This criticism arises because researchers are usually college professors, and the students in their courses are a convenient source of willing study participants, especially when extra credit is available. Today’s psychological scientists recognize that college students do not comprise a representative sample of people, and they go to great lengths to recruit samples of participants that are more diverse in age, race, ethnicity, socioeconomic background, and other demographic variables. Having a diverse sample supports more generalization than using a sample of college students. To illustrate the use of inferential statistics, let’s consider the 2009 SAT mathematics test. Men scored an average of 534 and women scored an average of 499 on this test. Does this mean that men perform better than women on this test? Or do men and women perform similarly, and this group is just an unusual sample of test takers? The default position, stating that there is no real difference between two measures (math scores produced by men and women in this example), is known as the null hypothesisnull hypothesisA hypothesis stating the default position that there is no real difference between two measures            null hypothesis        A hypothesis stating the default position that there is no real difference between two measures            . Recall that we cannot prove a hypothesis to be correct, but we can demonstrate that a hypothesis is false. Rejecting the null hypothesis suggests that alternative hypotheses (there might be a relationship between gender and math scores) should be explored and tested. How do we know when a hypothesis should be rejected? Like most sciences, psychology has accepted odds of 5 out of 100 that an observed result is due to chance as an acceptable standard for statistical significancestatistical significanceA standard for deciding whether an observed result is because of chance.            statistical significance        A standard for deciding whether an observed result is because of chance.            . We can assess the likelihood of observing a result due to chance by repeating a study, like throwing dice multiple times. We could give the mathematics portion of the SAT to 100 randomly selected samples of male and female college-bound students. If men and women score about the same or women score higher than men in 5 or more of these 100 samples, we would reject our Men score higher than women hypothesis as false. This type of careful analysis of the SAT data has confirmed that the differences on the mathematics portion of the test between male and female test takers are statistically significant (Halpern, et al., 2007; see Figure 2.19). Does this result mean that we should consider this difference when deciding on a major or career? Probably not. Keep in mind that the goal of the SAT is to predict college grades. When the math SAT scores of men and women receiving the same letter grade in a mathematics course at the same university were compared, the women scored 33 points less than the men (Wainer & Steinberg, 1992). This is about the same difference that we observed between the mean scores of men and women on the SAT math test. It appears that for some unknown reason, the SAT might be underestimating women’s ability to achieve in college mathematics courses. Inferential statistics allow us to decide whether the observed differences between the performance of males and that of females on the math SAT represents a real gender difference or whether they just occur by chance. Although psychology and most other sciences have relied heavily on testing the significance of the null hypothesis using the 5 times out of 100 (p < .05) odds as a criterion, this approach has its share of weaknesses. Testing and rejecting a null hypothesis tend to encourage us to view variables, like exposure to violent video games, as producing discrete either/or outcomes (aggression or no aggression) when that is unlikely to be the case in reality. An alternative approach is the use of estimation, which includes a report of the 95% confidence interval in addition to the mean (Cumming, 2012; see Figure 2.20). Applying estimation to the video game question might look like this. You could report that the average score on a measure of aggression was 37 ± 2 following exposure to a violent game and 32 ± 2 following exposure to a nonviolent game. This means that 95% of participants are likely to score between 35 and 39 following exposure to an aggressive game, while 95% of participants are likely to score between 30 and 34 following exposure to a nonviolent game. The additional information provided by the confidence interval helps us evaluate the magnitude of the difference better than a simple reporting of the statistical significance of the difference. Earlier in this chapter, we reviewed a study comparing hyperactivity scores among children drinking two juices containing common food additives (A and B) and plain juice (placebo). Computing 95% confidence intervals, noted by the red lines, gives us an idea of how big the differences are (these are actually quite small, but significant). We shouldn’t assume that overlapping confidence intervals indicate a lack of statistical significance. Mixture A was different from placebo at the p < .05 level, while Mixture B was different from placebo at the p < .01 level.We mentioned earlier that deceiving participants into thinking they were truly in danger raised ethical questions. On what basis do researchers decide what they can and cannot do to their research participants?Many populations that are interesting to psychologists are unable to sign informed consent forms legally and require additional ethical protection. In the case of research with infants, parents are required to sign informed consent forms on their child’s behalf. Although most studies published in psychological journals involve the use of human participants, psychology also has a rich heritage of animal research. Separate guidelines have been developed for each type of subject. Researchers working in universities and other agencies receiving federal funding seek the approval of institutional review boards (IRBs) for human participant research and institutional animal care and use committees (IACUCs) for animal research before conducting their studies. The IRBs and IACUCs are guided by federal regulations and research ethics endorsed by professional societies such as the American Psychological Association (APA), the Association for Psychological Science (APS), and the Society for Neuroscience (SfN). IRBs and IACUCs must include at least one member of the community outside the university or agency, avoiding the possibility that inappropriate research might be conducted in secret. These procedures do not apply to institutions that do not have federal funding, such as private genetics research corporations or consumer product corporations (cosmetics, etc.), although efforts are being made to bring these organizations into compliance with federal standards. As you review the ethical guidelines for both human and animal research participants, keep in mind that the guidelines look simpler when you read about their provisions than when you try to implement them in the context of real research. This is why the final approval decision lies with a committee, as opposed to an individual.At the core of ethical standards for human research is the idea that participation is voluntary. No participant should be coerced into participating. Although psychologists are well aware that people who volunteer to participate in research are probably quite different in important ways from those who don’t volunteer, we have chosen to give research ethics a higher priority than our ability to generalize research results. The Tuskegee syphilis experiment, conducted by the U. S. Public Health Service from 1932 to 1972, involved 400 African-American men who had contracted syphilis. The study’s failure to treat or inform these participants about their health led to new regulations to prevent such unethical research from being repeated. To ensure that a participant is a willing volunteer, researchers must make provisions for reasonable incentives. Incentives, such as pay or extra credit for participation, must not be so extreme that they become the primary motivation for prospective volunteers. To decide whether to volunteer for research, a person must have some knowledge of what the research will entail. Researchers must provide prospective participants with an informed consentinformed consentPermission obtained from a research participant after the risks and benefits of an experimental procedure have been thoroughly explained            informed consent        Permission obtained from a research participant after the risks and benefits of an experimental procedure have been thoroughly explained             form, which provides details about the purpose of the study and what types of procedures will occur. In psychological research, we have the added burden of occasionally dealing with participants who are limited in their abilities to provide informed consent because of the conditions that make them interesting to study. Developmental psychologists have an obvious interest in children, but a person cannot sign a legal informed consent form until age 18. Can you obtain informed consent from a patient with schizophrenia, who suffers from hallucinations and irrational, delusional beliefs, or from a person in the later stages of Alzheimer’s disease, whose memory and reasoning have deteriorated because of the condition? In these cases, legal permission must be obtained from a qualified guardian. The university IRBs play an essential role in evaluating these ethical dilemmas case by case. Research also should be conducted in a manner that does no irreversible harm to participants. In some cases, to avoid participants’ desire to appear normal and their tendency to try to outguess the research, researchers might say that they are investigating one factor when they are interested in another. Most cases of deception are quite mild, such as when participants are told that a study is about memory when it is actually a study of some social behavior. When researchers must deceive their participants, extra care must be taken to debrief participants and answer all their questions following the experiment. While examining the papers of Dr. John Cutler, who led the Tuskegee syphilis study, Wellesley College historian Susan Reverby discovered that during the 1940s, U. S. and Guatemalan health officials had deliberately exposed prisoners, soldiers, and mental patients to syphilis and gonorrhea to test the effectiveness of penicillin. Research using human participants should be rigorously private and confidential. Privacy refers to the participants’ control over the sharing of their personal information with others, and methods for ensuring privacy are usually stated in the informed consent paperwork. For example, some studies involve the use of medical records, which participants agree to share with the researchers for the purpose of the experiment. Confidentiality refers to the participants’ right to have their data revealed to others only with their permission. Confidentiality is usually maintained by such practices as substituting codes for names and storing data in locked cabinets. Collecting data anonymously, so that even the researchers do not know the identity of participants, is the surest way to protect privacy and confidentiality. Science learns from its past ethical lapses. One of the most egregious examples of unethical research was the Tuskegee syphilis experiment, which lasted from 1932 until 1972. Researchers from the U. S. Public Health Service recruited about 400 impoverished African-American men who had contracted syphilis to study the progression of the disease. None of the men were told they had syphilis, and none were treated, even after penicillin became the standard treatment for the disease in 1947. Many current federal regulations related to research ethics were developed in response to this experiment.The topic of using animals in research is guaranteed to stimulate lively, and possibly heated, discussion. Some people are adamantly opposed to any animal research, whereas others accept the concept of using animals, so long as certain conditions are met. About 7% to 8% of published research in psychology journals involves the use of animals as subjects (American Psychological Association [APA], 2012). A total of 90% of the animals used are rodents and birds, with 5% or fewer studies involving monkeys and other primates. According to the APA, the use of dogs and cats in psychological research is rare. Ethical guidelines for animal research require setting a clear purpose for the experiment, providing excellent care for the animals, and minimizing pain and suffering. Research using animals must demonstrate a clear purpose, such as benefiting the health of humans or other animals. In addition to serving a clear purpose, animal research requires excellent housing, food, and veterinary care. The most controversial ethical standards relate to minimizing the pain and suffering experienced by animal research subjects. Federal regulations provide guidelines for the use of pain, surgery, stress, and deprivation with animal subjects, as well as the termination of an animal’s life. The standards approximate the community standards that we would expect from local humane societies tasked with euthanizing animals that are not adopted. In Chapter 1, we suggested using the problem of cyberbullying as our real-world problem. In this chapter, we’ll try to examine the different ways that we could approach this question using our best research methods and ethics. Bullying is not new, but cyberbullying is. To study a relatively new phenomenon, we might begin with descriptive methods. In addition, correlational methods might help us identify factors that predispose individuals to bullying or being bullied, as well as likely outcomes of cyberbullying (see Figure 2.21). A quick search of Google Scholar from 2015 to 2016 shows that survey and correlational approaches dominate this area of research. Survey data on a sensitive topic like cyberbullying must be interpreted with caution. The frequency of behaviors known to be undesirable is likely to be under-reported by perpetrators, and victims might underplay the severity of their experience of abuse as a coping strategy. A search of Google Scholar for articles published between 2015 and 2016 shows that survey and correlational studies dominate this area of research. Typical of this approach is a study by Kowalski & Limber (2013). You can see that youth engaging in traditional or cyberbullying share many outcomes, but appear to differ in their levels of anxiety and suicidal ideation. The most likely related question to be approached using experiments is the efficacy of intervention programs. For example, one research team evaluated whether the Cyber Friendly Schools Program used in Australian schools was effective in reducing cyberbullying (Cross et al., 2016). Schools were randomly assigned to intervention or control conditions. The study did show that perpetrating and being victimized became less likely as a result of the program. Cyberbullying is a sensitive issue, and that raises significant ethical concerns. First, most cyberbullying research focuses on schoolchildren and young adults. Using minors as research participants requires additional steps, such as obtaining both parental consent and the assent of the participant. We do not want your parents to have the ability to sign you up for research against your will. Second, a participant who has been a cyberbullying victim might react negatively to reminders of the experience. We would need to make this risk very clear in the informed consent materials and take special care to remind participants they can quit the study at any time. The informed consent form should include referrals to counseling services available to participants in the event that they feel a need to discuss their experiences. The end of chapter 1 noted that each chapter would feature a discussion of how the chapter’s material could be used to talk about relationships. For this chapter on research methods, we ask whether psychological research methods can be used to differentiate between liking and loving another person. Using the methods described in this chapter, you can probably think of many ways to approach this question. You could observe friends and compare their behavior to that of romantic partners. You could conduct a survey, asking people to rate certain characteristics of people they love versus those of people they like. Perhaps you could correlate physiological arousal measures, like pupil diameter or heart rate, with ratings of how much a participant likes or loves another person. Conducting an actual experiment might be a bit tricky, though, in terms of both methods and ethics. How could people be randomly assigned to love and like groups?A fun example of research on relationships was contributed by Bartels and Zeki (2000). These researchers chose to use correlational methods in the form of brain imaging, which we discuss further in Chapter 4. Brain imaging studies are typically correlational because we assume that brain activity in particular areas has a relationship with some ongoing behavior. It is important to recall that we cannot make conclusions about causality based on correlational data. Bartels and Zeki’s participants were instructed to supply photographs of people they like and people they love. The photographs were then presented to the participants while their brain activity was observed using functional magnetic resonance imaging (fMRI; see Chapter 4). Several differences in brain activity occurred when the participants viewed a lover as opposed to a friend. Not too surprisingly, areas of the brain associated with reward became more active when viewing lovers than when viewing friends. Areas of the brain associated with social judgment, negative emotions, and assessing the intentions and emotions of other people were less active when viewing lovers than when viewing friends. Love, according to these results, appears to be a combination of reward and less social judgment (see Figure 2.22). When people view a photo of a person they say they love, brain imaging shows increased activation in reward areas of the brain and decreased activation in areas involved with social judgment. The results of this scientific research might help us understand some of our everyday experiences with relationships. We feel rewarded in the presence of a loved one, and we have to admit that we may miss a flaw or two in someone we love. Friends and family members, whose social judgment has not been silenced, do not miss these flaws and often try in vain to alert us to them. Human ParticipantsAnimal SubjectsNo coercionNecessityInformed consentExcellent food, housing, and veterinary careNo harmMinimal pain and sufferingConfidentiality and privacyBe sure that you can define these terms and use them correctly.`;
